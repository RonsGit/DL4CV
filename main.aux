\relax 
\providecommand\zref@newlabel[2]{}
\abx@aux@refcontext{nty/global//global/global/global}
\providecommand \babel@aux [2]{\global \let \babel@toc \@gobbletwo }
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\BKM@entry[2]{}
\babel@aux{english}{}
\pgfsyspdfmark {pgfid2}{0}{52099153}
\pgfsyspdfmark {pgfid1}{5966969}{45620378}
\BKM@entry{id=1,dest={636861707465722A2E32},srcline={108}}{5C3337365C3337375C303030505C303030725C303030655C303030665C303030615C303030635C30303065}
\BKM@entry{id=2,dest={73656374696F6E2E302E31},srcline={110}}{5C3337365C3337375C303030475C303030655C303030745C303030745C303030695C3030306E5C303030675C3030305C3034305C303030535C303030745C303030615C303030725C303030745C303030655C303030645C3030303A5C3030305C3034305C303030415C303030625C3030306F5C303030755C303030745C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030505C303030725C3030306F5C3030306A5C303030655C303030635C303030745C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030485C3030306F5C303030775C3030305C3034305C303030745C3030306F5C3030305C3034305C3030304E5C303030615C303030765C303030695C303030675C303030615C303030745C303030655C3030305C3034305C303030495C30303074}
\BKM@entry{id=3,dest={73756273656374696F6E2E302E312E31},srcline={112}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030545C303030685C303030695C303030735C3030305C3034305C303030445C3030306F5C303030635C303030755C3030306D5C303030655C3030306E5C303030745C3030303F}
\pgfsyspdfmark {pgfid4}{0}{52099153}
\pgfsyspdfmark {pgfid3}{5966969}{45620378}
\@writefile{toc}{\contentsline {chapter}{Preface}{28}{chapter*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {0.1}Getting Started: About the Project and How to Navigate It}{28}{section.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.1}Why This Document?}{28}{subsection.0.1.1}\protected@file@percent }
\BKM@entry{id=4,dest={73756273656374696F6E2E302E312E32},srcline={144}}{5C3337365C3337375C303030595C3030306F5C303030755C303030725C3030305C3034305C303030465C303030655C303030655C303030645C303030625C303030615C303030635C3030306B5C3030305C3034305C3030304D5C303030615C303030745C303030745C303030655C303030725C30303073}
\BKM@entry{id=5,dest={73756273656374696F6E2E302E312E33},srcline={150}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030745C3030306F5C3030305C3034305C303030555C303030735C303030655C3030305C3034305C303030545C303030685C303030695C303030735C3030305C3034305C303030445C3030306F5C303030635C303030755C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030455C303030665C303030665C303030655C303030635C303030745C303030695C303030765C303030655C3030306C5C30303079}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.2}Your Feedback Matters}{29}{subsection.0.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.3}How to Use This Document Effectively}{29}{subsection.0.1.3}\protected@file@percent }
\BKM@entry{id=6,dest={73756273656374696F6E2E302E312E34},srcline={192}}{5C3337365C3337375C303030535C303030745C303030615C303030795C303030695C3030306E5C303030675C3030305C3034305C303030555C303030705C303030645C303030615C303030745C303030655C303030645C3030305C3034305C303030695C3030306E5C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030465C303030695C303030655C3030306C5C30303064}
\BKM@entry{id=7,dest={73756273656374696F6E2E302E312E35},srcline={209}}{5C3337365C3337375C303030445C303030655C303030705C303030655C3030306E5C303030645C303030655C3030306E5C303030635C303030795C3030305C3034305C303030545C303030725C303030655C30303065}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.4}Staying Updated in the Field}{30}{subsection.0.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.5}Dependency Tree}{31}{subsection.0.1.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Book chapter dependencies} Arrows indicate prerequisite relationships between chapters. Earlier foundations support later topics such as self-supervised learning, segmentation, video understanding, and 3D vision.}}{31}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:book_dependencies}{{1}{31}{\textbf {Book chapter dependencies} Arrows indicate prerequisite relationships between chapters. Earlier foundations support later topics such as self-supervised learning, segmentation, video understanding, and 3D vision}{figure.caption.3}{}}
\BKM@entry{id=8,dest={73756273656374696F6E2E302E312E36},srcline={220}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030745C303030725C303030695C303030625C303030755C303030745C3030306F5C303030725C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.6}Contributors}{32}{subsection.0.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Named contributors}{32}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Community feedback}{32}{section*.5}\protected@file@percent }
\BKM@entry{id=9,dest={73756273656374696F6E2E302E312E37},srcline={240}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030495C3030306D5C303030705C3030306F5C303030725C303030745C303030615C3030306E5C303030635C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C303030505C303030725C303030615C303030635C303030745C303030695C303030635C30303065}
\BKM@entry{id=10,dest={73756273656374696F6E2E302E312E38},srcline={243}}{5C3337365C3337375C303030465C303030695C3030306E5C303030615C3030306C5C3030305C3034305C303030525C303030655C3030306D5C303030615C303030725C3030306B5C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.7}The Importance of Practice}{33}{subsection.0.1.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.8}Final Remarks}{33}{subsection.0.1.8}\protected@file@percent }
\BKM@entry{id=11,dest={636861707465722E31},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030315C3030303A5C3030305C3034305C303030435C3030306F5C303030755C303030725C303030735C303030655C3030305C3034305C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=12,dest={73656374696F6E2E312E31},srcline={13}}{5C3337365C3337375C303030435C3030306F5C303030725C303030655C3030305C3034305C303030545C303030655C303030725C3030306D5C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030465C303030695C303030655C3030306C5C30303064}
\BKM@entry{id=13,dest={73756273656374696F6E2E312E312E31},srcline={17}}{5C3337365C3337375C303030415C303030725C303030745C303030695C303030665C303030695C303030635C303030695C303030615C3030306C5C3030305C3034305C303030495C3030306E5C303030745C303030655C3030306C5C3030306C5C303030695C303030675C303030655C3030306E5C303030635C303030655C3030305C3034305C3030305C3035305C303030415C303030495C3030305C303531}
\BKM@entry{id=14,dest={73756273656374696F6E2E312E312E32},srcline={22}}{5C3337365C3337375C3030304D5C303030615C303030635C303030685C303030695C3030306E5C303030655C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030305C3035305C3030304D5C3030304C5C3030305C303531}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Lecture 1: Course Introduction}{34}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@starttoc{default@1}}
\pgfsyspdfmark {pgfid6}{0}{52099153}
\pgfsyspdfmark {pgfid5}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Core Terms in the Field}{34}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Artificial Intelligence (AI)}{34}{subsection.1.1.1}\protected@file@percent }
\zref@newlabel{mdf@pagelabel-1}{\default{1.1.1}\page{34}\abspage{34}\mdf@pagevalue{34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Machine Learning (ML)}{34}{subsection.1.1.2}\protected@file@percent }
\zref@newlabel{mdf@pagelabel-2}{\default{1.1.2}\page{34}\abspage{34}\mdf@pagevalue{34}}
\BKM@entry{id=15,dest={73756273656374696F6E2E312E312E33},srcline={37}}{5C3337365C3337375C303030445C303030655C303030655C303030705C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030305C3035305C303030445C3030304C5C3030305C303531}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Deep Learning (DL)}{35}{subsection.1.1.3}\protected@file@percent }
\zref@newlabel{mdf@pagelabel-3}{\default{1.1.3}\page{35}\abspage{35}\mdf@pagevalue{35}}
\BKM@entry{id=16,dest={73756273656374696F6E2E312E312E34},srcline={44}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030655C303030725C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030435C303030565C3030305C303531}
\BKM@entry{id=17,dest={73756273656374696F6E2E312E312E35},srcline={49}}{5C3337365C3337375C303030435C3030306F5C3030306E5C3030306E5C303030655C303030635C303030745C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030445C3030306F5C303030745C30303073}
\BKM@entry{id=18,dest={73656374696F6E2E312E32},srcline={65}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030535C303030745C303030755C303030645C303030795C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030655C303030725C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030303F}
\BKM@entry{id=19,dest={73756273656374696F6E2E312E322E31},srcline={68}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030695C3030306E5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030655C303030725C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.4}Computer Vision (CV)}{36}{subsection.1.1.4}\protected@file@percent }
\zref@newlabel{mdf@pagelabel-4}{\default{1.1.4}\page{36}\abspage{36}\mdf@pagevalue{36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.5}Connecting the Dots}{36}{subsection.1.1.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces In this course, we study 'Deep Learning' for Computer Vision.}}{36}{figure.caption.6}\protected@file@percent }
\newlabel{fig:chapter1_slide13}{{1.1}{36}{In this course, we study 'Deep Learning' for Computer Vision}{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Why Study Deep Learning for Computer Vision?}{36}{section.1.2}\protected@file@percent }
\abx@aux@cite{0}{appen_road_annotation}
\abx@aux@segm{0}{0}{appen_road_annotation}
\abx@aux@cite{0}{appen_road_annotation}
\abx@aux@segm{0}{0}{appen_road_annotation}
\BKM@entry{id=20,dest={73656374696F6E2E312E33},srcline={83}}{5C3337365C3337375C303030485C303030695C303030735C303030745C3030306F5C303030725C303030695C303030635C303030615C3030306C5C3030305C3034305C3030304D5C303030695C3030306C5C303030655C303030735C303030745C3030306F5C3030306E5C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Motivation for Deep Learning in Computer Vision}{37}{subsection.1.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Road annotation for autonomous vehicles. Image credit: Appen \blx@tocontentsinit {0}\cite {appen_road_annotation}.}}{37}{figure.caption.7}\protected@file@percent }
\abx@aux@backref{2}{appen_road_annotation}{0}{37}{37}
\newlabel{fig:road_annotation}{{1.2}{37}{Road annotation for autonomous vehicles. Image credit: Appen \cite {appen_road_annotation}}{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Historical Milestones}{37}{section.1.3}\protected@file@percent }
\BKM@entry{id=21,dest={73756273656374696F6E2E312E332E31},srcline={88}}{5C3337365C3337375C303030485C303030755C303030625C303030655C3030306C5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030575C303030695C303030655C303030735C303030655C3030306C5C3030305C3034305C3030305C3035305C303030315C303030395C303030355C303030395C3030305C3035315C3030303A5C3030305C3034305C303030485C3030306F5C303030775C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030575C3030306F5C303030725C3030306B5C303030735C3030303F}
\abx@aux@cite{0}{hubel1959_receptivefields}
\abx@aux@segm{0}{0}{hubel1959_receptivefields}
\abx@aux@cite{0}{hubel1959_receptivefields}
\abx@aux@segm{0}{0}{hubel1959_receptivefields}
\abx@aux@cite{0}{hubel1959_receptivefields}
\abx@aux@segm{0}{0}{hubel1959_receptivefields}
\BKM@entry{id=22,dest={73756273656374696F6E2E312E332E32},srcline={98}}{5C3337365C3337375C3030304C5C303030615C303030725C303030725C303030795C3030305C3034305C303030525C3030306F5C303030625C303030655C303030725C303030745C303030735C3030305C3034305C3030305C3035305C303030315C303030395C303030365C303030335C3030305C3035315C3030303A5C3030305C3034305C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030455C303030645C303030675C303030655C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C3030304B5C303030655C303030795C303030705C3030306F5C303030695C3030306E5C303030745C30303073}
\abx@aux@cite{0}{roberts1963_3dsolids}
\abx@aux@segm{0}{0}{roberts1963_3dsolids}
\abx@aux@cite{0}{roberts1963_3dsolids}
\abx@aux@segm{0}{0}{roberts1963_3dsolids}
\abx@aux@cite{0}{roberts1963_3dsolids}
\abx@aux@segm{0}{0}{roberts1963_3dsolids}
\BKM@entry{id=23,dest={73756273656374696F6E2E312E332E33},srcline={108}}{5C3337365C3337375C303030445C303030615C303030765C303030695C303030645C3030305C3034305C3030304D5C303030615C303030725C303030725C3030305C3034305C3030305C3035305C303030315C303030395C303030375C303030305C303030735C3030305C3035315C3030303A5C3030305C3034305C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C303030615C3030305C3034305C303030335C303030445C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Hubel and Wiesel (1959): How Vision Works?}{38}{subsection.1.3.1}\protected@file@percent }
\abx@aux@backref{3}{hubel1959_receptivefields}{0}{38}{38}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Hubel \& Wiesel's study, revolutionizing our understanding of visual processing \blx@tocontentsinit {0}\cite {hubel1959_receptivefields}.}}{38}{figure.caption.8}\protected@file@percent }
\abx@aux@backref{5}{hubel1959_receptivefields}{0}{38}{38}
\newlabel{fig:chapter1_slide16}{{1.3}{38}{Hubel \& Wiesel's study, revolutionizing our understanding of visual processing \cite {hubel1959_receptivefields}}{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Larry Roberts (1963): From Edges to Keypoints}{38}{subsection.1.3.2}\protected@file@percent }
\abx@aux@backref{6}{roberts1963_3dsolids}{0}{38}{38}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Larry Roberts' 1963 thesis introduced edge detection as a critical component of early computer vision systems \blx@tocontentsinit {0}\cite {roberts1963_3dsolids}.}}{38}{figure.caption.9}\protected@file@percent }
\abx@aux@backref{8}{roberts1963_3dsolids}{0}{38}{38}
\newlabel{fig:chapter1_roberts}{{1.4}{38}{Larry Roberts' 1963 thesis introduced edge detection as a critical component of early computer vision systems \cite {roberts1963_3dsolids}}{figure.caption.9}{}}
\abx@aux@cite{0}{marr1982_vision}
\abx@aux@segm{0}{0}{marr1982_vision}
\abx@aux@cite{0}{marr1982_vision}
\abx@aux@segm{0}{0}{marr1982_vision}
\abx@aux@cite{0}{marr1982_vision}
\abx@aux@segm{0}{0}{marr1982_vision}
\BKM@entry{id=24,dest={73756273656374696F6E2E312E332E34},srcline={126}}{5C3337365C3337375C303030525C303030655C303030635C3030306F5C303030675C3030306E5C303030695C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030765C303030695C303030615C3030305C3034305C303030505C303030615C303030725C303030745C303030735C3030305C3034305C3030305C3035305C303030315C303030395C303030375C303030305C303030735C3030305C303531}
\abx@aux@cite{0}{brooks1979_modelbased}
\abx@aux@segm{0}{0}{brooks1979_modelbased}
\abx@aux@cite{0}{fischler1973_pictorialstructures}
\abx@aux@segm{0}{0}{fischler1973_pictorialstructures}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}David Marr (1970s): From Features to a 3D Model}{39}{subsection.1.3.3}\protected@file@percent }
\abx@aux@backref{9}{marr1982_vision}{0}{39}{39}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces David Marr's theory of multi-stage visual processing \blx@tocontentsinit {0}\cite {marr1982_vision}.}}{39}{figure.caption.10}\protected@file@percent }
\abx@aux@backref{11}{marr1982_vision}{0}{39}{39}
\newlabel{fig:chapter1_marr}{{1.5}{39}{David Marr's theory of multi-stage visual processing \cite {marr1982_vision}}{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.4}Recognition via Parts (1970s)}{39}{subsection.1.3.4}\protected@file@percent }
\abx@aux@backref{12}{brooks1979_modelbased}{0}{39}{39}
\abx@aux@backref{13}{fischler1973_pictorialstructures}{0}{39}{39}
\abx@aux@cite{0}{brooks1979_modelbased}
\abx@aux@segm{0}{0}{brooks1979_modelbased}
\abx@aux@cite{0}{fischler1973_pictorialstructures}
\abx@aux@segm{0}{0}{fischler1973_pictorialstructures}
\abx@aux@cite{0}{brooks1979_modelbased}
\abx@aux@segm{0}{0}{brooks1979_modelbased}
\abx@aux@cite{0}{fischler1973_pictorialstructures}
\abx@aux@segm{0}{0}{fischler1973_pictorialstructures}
\BKM@entry{id=25,dest={73756273656374696F6E2E312E332E35},srcline={141}}{5C3337365C3337375C303030525C303030655C303030635C3030306F5C303030675C3030306E5C303030695C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030765C303030695C303030615C3030305C3034305C303030455C303030645C303030675C303030655C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030315C303030395C303030385C303030305C303030735C3030305C303531}
\abx@aux@cite{0}{canny1986_edgedetection}
\abx@aux@segm{0}{0}{canny1986_edgedetection}
\abx@aux@cite{0}{lowe1987_objectrecognition}
\abx@aux@segm{0}{0}{lowe1987_objectrecognition}
\abx@aux@cite{0}{canny1986_edgedetection}
\abx@aux@segm{0}{0}{canny1986_edgedetection}
\abx@aux@cite{0}{lowe1987_objectrecognition}
\abx@aux@segm{0}{0}{lowe1987_objectrecognition}
\abx@aux@cite{0}{canny1986_edgedetection}
\abx@aux@segm{0}{0}{canny1986_edgedetection}
\abx@aux@cite{0}{lowe1987_objectrecognition}
\abx@aux@segm{0}{0}{lowe1987_objectrecognition}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces Recognition via parts: Generalized Cylinders and Pictorial Structures, foundational to modern object recognition \blx@tocontentsinit {0}\cite {brooks1979_modelbased, fischler1973_pictorialstructures}.}}{40}{figure.caption.11}\protected@file@percent }
\abx@aux@backref{16}{brooks1979_modelbased}{0}{40}{40}
\abx@aux@backref{17}{fischler1973_pictorialstructures}{0}{40}{40}
\newlabel{fig:chapter1_parts_recognition}{{1.6}{40}{Recognition via parts: Generalized Cylinders and Pictorial Structures, foundational to modern object recognition \cite {brooks1979_modelbased, fischler1973_pictorialstructures}}{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.5}Recognition via Edge Detection (1980s)}{40}{subsection.1.3.5}\protected@file@percent }
\abx@aux@backref{18}{canny1986_edgedetection}{0}{40}{40}
\abx@aux@backref{19}{lowe1987_objectrecognition}{0}{40}{40}
\BKM@entry{id=26,dest={73756273656374696F6E2E312E332E36},srcline={160}}{5C3337365C3337375C303030525C303030655C303030635C3030306F5C303030675C3030306E5C303030695C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030765C303030695C303030615C3030305C3034305C303030475C303030725C3030306F5C303030755C303030705C303030695C3030306E5C303030675C3030305C3034305C3030305C3035305C303030315C303030395C303030395C303030305C303030735C3030305C303531}
\abx@aux@cite{0}{shi1997_normalizedcuts}
\abx@aux@segm{0}{0}{shi1997_normalizedcuts}
\abx@aux@cite{0}{shi1997_normalizedcuts}
\abx@aux@segm{0}{0}{shi1997_normalizedcuts}
\abx@aux@cite{0}{shi1997_normalizedcuts}
\abx@aux@segm{0}{0}{shi1997_normalizedcuts}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces Recognition via edge detection: Canny Edge Detector and template matching by Lowe, foundational to object detection \blx@tocontentsinit {0}\cite {canny1986_edgedetection, lowe1987_objectrecognition}.}}{41}{figure.caption.12}\protected@file@percent }
\abx@aux@backref{22}{canny1986_edgedetection}{0}{41}{41}
\abx@aux@backref{23}{lowe1987_objectrecognition}{0}{41}{41}
\newlabel{fig:chapter1_edge_detection}{{1.7}{41}{Recognition via edge detection: Canny Edge Detector and template matching by Lowe, foundational to object detection \cite {canny1986_edgedetection, lowe1987_objectrecognition}}{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.6}Recognition via Grouping (1990s)}{41}{subsection.1.3.6}\protected@file@percent }
\abx@aux@backref{24}{shi1997_normalizedcuts}{0}{41}{41}
\BKM@entry{id=27,dest={73756273656374696F6E2E312E332E37},srcline={181}}{5C3337365C3337375C303030525C303030655C303030635C3030306F5C303030675C3030306E5C303030695C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030765C303030695C303030615C3030305C3034305C3030304D5C303030615C303030745C303030635C303030685C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030425C303030655C3030306E5C303030635C303030685C3030306D5C303030615C303030725C3030306B5C303030695C3030306E5C303030675C3030305C3034305C3030305C3035305C303030325C303030305C303030305C303030305C303030735C3030305C303531}
\abx@aux@cite{0}{lowe1999_sift}
\abx@aux@segm{0}{0}{lowe1999_sift}
\abx@aux@cite{0}{lowe1999_sift}
\abx@aux@segm{0}{0}{lowe1999_sift}
\abx@aux@cite{0}{lowe1999_sift}
\abx@aux@segm{0}{0}{lowe1999_sift}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces Recognition via grouping: Normalized Cuts by Shi and Malik, a groundbreaking approach to image segmentation \blx@tocontentsinit {0}\cite {shi1997_normalizedcuts}.}}{42}{figure.caption.13}\protected@file@percent }
\abx@aux@backref{26}{shi1997_normalizedcuts}{0}{42}{42}
\newlabel{fig:chapter1_grouping}{{1.8}{42}{Recognition via grouping: Normalized Cuts by Shi and Malik, a groundbreaking approach to image segmentation \cite {shi1997_normalizedcuts}}{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.7}Recognition via Matching and Benchmarking (2000s)}{42}{subsection.1.3.7}\protected@file@percent }
\abx@aux@backref{27}{lowe1999_sift}{0}{42}{42}
\abx@aux@cite{0}{viola2001_boosteddetection}
\abx@aux@segm{0}{0}{viola2001_boosteddetection}
\abx@aux@cite{0}{viola2001_boosteddetection}
\abx@aux@segm{0}{0}{viola2001_boosteddetection}
\abx@aux@cite{0}{viola2001_boosteddetection}
\abx@aux@segm{0}{0}{viola2001_boosteddetection}
\abx@aux@cite{0}{pascal2010_visualchallenge}
\abx@aux@segm{0}{0}{pascal2010_visualchallenge}
\abx@aux@cite{0}{pascal2010_visualchallenge}
\abx@aux@segm{0}{0}{pascal2010_visualchallenge}
\abx@aux@cite{0}{pascal2010_visualchallenge}
\abx@aux@segm{0}{0}{pascal2010_visualchallenge}
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces SIFT: A groundbreaking feature matching algorithm introduced by Lowe in 1999 \blx@tocontentsinit {0}\cite {lowe1999_sift}.}}{43}{figure.caption.14}\protected@file@percent }
\abx@aux@backref{29}{lowe1999_sift}{0}{43}{43}
\newlabel{fig:chapter1_sift}{{1.9}{43}{SIFT: A groundbreaking feature matching algorithm introduced by Lowe in 1999 \cite {lowe1999_sift}}{figure.caption.14}{}}
\abx@aux@backref{30}{viola2001_boosteddetection}{0}{43}{43}
\@writefile{lof}{\contentsline {figure}{\numberline {1.10}{\ignorespaces Viola-Jones face detection algorithm, a milestone in real-time object detection \blx@tocontentsinit {0}\cite {viola2001_boosteddetection}.}}{43}{figure.caption.15}\protected@file@percent }
\abx@aux@backref{32}{viola2001_boosteddetection}{0}{43}{43}
\newlabel{fig:chapter1_viola_jones}{{1.10}{43}{Viola-Jones face detection algorithm, a milestone in real-time object detection \cite {viola2001_boosteddetection}}{figure.caption.15}{}}
\abx@aux@backref{33}{pascal2010_visualchallenge}{0}{43}{43}
\BKM@entry{id=28,dest={73756273656374696F6E2E312E332E38},srcline={217}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030304E5C303030655C303030745C3030305C3034305C303030445C303030615C303030745C303030615C303030735C303030655C303030745C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C30303065}
\abx@aux@cite{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@segm{0}{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\abx@aux@cite{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@segm{0}{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\abx@aux@cite{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@segm{0}{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\BKM@entry{id=29,dest={73756273656374696F6E2E312E332E39},srcline={234}}{5C3337365C3337375C303030415C3030306C5C303030655C303030785C3030304E5C303030655C303030745C3030303A5C3030305C3034305C303030415C3030305C3034305C303030525C303030655C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030695C3030306E5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030655C303030725C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030325C303030305C303030315C303030325C3030305C303531}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\@writefile{lof}{\contentsline {figure}{\numberline {1.11}{\ignorespaces PASCAL Visual Object Challenge: A benchmark for object detection \& recognition \blx@tocontentsinit {0}\cite {pascal2010_visualchallenge}.}}{44}{figure.caption.16}\protected@file@percent }
\abx@aux@backref{35}{pascal2010_visualchallenge}{0}{44}{44}
\newlabel{fig:chapter1_pascal}{{1.11}{44}{PASCAL Visual Object Challenge: A benchmark for object detection \& recognition \cite {pascal2010_visualchallenge}}{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.8}The ImageNet Dataset and Classification Challenge}{44}{subsection.1.3.8}\protected@file@percent }
\abx@aux@backref{36}{imagenet2009_hierarchicaldatabase}{0}{44}{44}
\abx@aux@backref{37}{krizhevsky2012_alexnet}{0}{44}{44}
\@writefile{lof}{\contentsline {figure}{\numberline {1.12}{\ignorespaces Advances in the ImageNet Classification Challenge \blx@tocontentsinit {0}\cite {imagenet2009_hierarchicaldatabase, krizhevsky2012_alexnet}.}}{44}{figure.caption.17}\protected@file@percent }
\abx@aux@backref{40}{imagenet2009_hierarchicaldatabase}{0}{44}{44}
\abx@aux@backref{41}{krizhevsky2012_alexnet}{0}{44}{44}
\newlabel{fig:chapter1_imagenet_challenge}{{1.12}{44}{Advances in the ImageNet Classification Challenge \cite {imagenet2009_hierarchicaldatabase, krizhevsky2012_alexnet}}{figure.caption.17}{}}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.9}AlexNet: A Revolution in Computer Vision (2012)}{45}{subsection.1.3.9}\protected@file@percent }
\abx@aux@backref{42}{krizhevsky2012_alexnet}{0}{45}{45}
\@writefile{lof}{\contentsline {figure}{\numberline {1.13}{\ignorespaces AlexNet’s performance in the 2012 ImageNet Challenge, showcasing its revolutionary impact on deep learning \blx@tocontentsinit {0}\cite {krizhevsky2012_alexnet}.}}{45}{figure.caption.18}\protected@file@percent }
\abx@aux@backref{44}{krizhevsky2012_alexnet}{0}{45}{45}
\newlabel{fig:chapter1_alexnet}{{1.13}{45}{AlexNet’s performance in the 2012 ImageNet Challenge, showcasing its revolutionary impact on deep learning \cite {krizhevsky2012_alexnet}}{figure.caption.18}{}}
\@writefile{toc}{\contentsline {subsubsection}{Building on AlexNet: Evolution of CNNs and Beyond}{45}{section*.19}\protected@file@percent }
\abx@aux@cite{0}{he2016_resnet}
\abx@aux@segm{0}{0}{he2016_resnet}
\abx@aux@cite{0}{rumelhart1986_backpropagation}
\abx@aux@segm{0}{0}{rumelhart1986_backpropagation}
\abx@aux@cite{0}{hochreiter1997_lstm}
\abx@aux@segm{0}{0}{hochreiter1997_lstm}
\abx@aux@cite{0}{donahue2015_ltrcnn}
\abx@aux@segm{0}{0}{donahue2015_ltrcnn}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\abx@aux@backref{45}{he2016_resnet}{0}{46}{46}
\abx@aux@backref{46}{rumelhart1986_backpropagation}{0}{46}{46}
\abx@aux@backref{47}{hochreiter1997_lstm}{0}{46}{46}
\abx@aux@backref{48}{donahue2015_ltrcnn}{0}{46}{46}
\abx@aux@backref{49}{vaswani2017_attention}{0}{46}{46}
\abx@aux@backref{50}{vit2020_transformers}{0}{46}{46}
\abx@aux@cite{0}{mamba2023_selective}
\abx@aux@segm{0}{0}{mamba2023_selective}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@cite{0}{clip2021_multimodal}
\abx@aux@segm{0}{0}{clip2021_multimodal}
\abx@aux@cite{0}{sam2023_segmentation}
\abx@aux@segm{0}{0}{sam2023_segmentation}
\abx@aux@cite{0}{flamingo2022_fewshot}
\abx@aux@segm{0}{0}{flamingo2022_fewshot}
\BKM@entry{id=30,dest={73656374696F6E2E312E34},srcline={322}}{5C3337365C3337375C3030304D5C303030695C3030306C5C303030655C303030735C303030745C3030306F5C3030306E5C303030655C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030455C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030695C3030306E5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030655C303030725C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E}
\BKM@entry{id=31,dest={73756273656374696F6E2E312E342E31},srcline={326}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030505C303030655C303030725C303030635C303030655C303030705C303030745C303030725C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030315C303030395C303030355C303030385C3030305C303531}
\abx@aux@backref{51}{mamba2023_selective}{0}{47}{47}
\abx@aux@backref{52}{dino2021_selfsupervised}{0}{47}{47}
\abx@aux@backref{53}{clip2021_multimodal}{0}{47}{47}
\abx@aux@backref{54}{sam2023_segmentation}{0}{47}{47}
\abx@aux@backref{55}{flamingo2022_fewshot}{0}{47}{47}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Milestones in the Evolution of Learning in Computer Vision}{47}{section.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}The Perceptron (1958)}{47}{subsection.1.4.1}\protected@file@percent }
\abx@aux@cite{0}{rosenblatt1958_perceptron}
\abx@aux@segm{0}{0}{rosenblatt1958_perceptron}
\abx@aux@cite{0}{minsky1969_perceptrons}
\abx@aux@segm{0}{0}{minsky1969_perceptrons}
\abx@aux@cite{0}{rosenblatt1958_perceptron}
\abx@aux@segm{0}{0}{rosenblatt1958_perceptron}
\abx@aux@cite{0}{rosenblatt1958_perceptron}
\abx@aux@segm{0}{0}{rosenblatt1958_perceptron}
\BKM@entry{id=32,dest={73756273656374696F6E2E312E342E32},srcline={339}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030415C303030495C3030305C3034305C303030575C303030695C3030306E5C303030745C303030655C303030725C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C3030306C5C303030615C303030795C303030655C303030725C3030305C3034305C303030505C303030655C303030725C303030635C303030655C303030705C303030745C303030725C3030306F5C3030306E5C303030735C3030305C3034305C3030305C3035305C303030315C303030395C303030365C303030395C3030305C303531}
\abx@aux@cite{0}{minsky1969_perceptrons}
\abx@aux@segm{0}{0}{minsky1969_perceptrons}
\abx@aux@cite{0}{minsky1969_perceptrons}
\abx@aux@segm{0}{0}{minsky1969_perceptrons}
\abx@aux@cite{0}{minsky1969_perceptrons}
\abx@aux@segm{0}{0}{minsky1969_perceptrons}
\BKM@entry{id=33,dest={73756273656374696F6E2E312E342E33},srcline={349}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C3030304E5C303030655C3030306F5C303030635C3030306F5C303030675C3030306E5C303030695C303030745C303030725C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030315C303030395C303030385C303030305C3030305C303531}
\abx@aux@cite{0}{fukushima1980_neocognitron}
\abx@aux@segm{0}{0}{fukushima1980_neocognitron}
\abx@aux@backref{56}{minsky1969_perceptrons}{0}{48}{48}
\abx@aux@backref{57}{rosenblatt1958_perceptron}{0}{48}{48}
\@writefile{lof}{\contentsline {figure}{\numberline {1.14}{\ignorespaces Frank Rosenblatt’s Perceptron, foundational to neural network research \blx@tocontentsinit {0}\cite {rosenblatt1958_perceptron}.}}{48}{figure.caption.20}\protected@file@percent }
\abx@aux@backref{59}{rosenblatt1958_perceptron}{0}{48}{48}
\newlabel{fig:chapter1_perceptron}{{1.14}{48}{Frank Rosenblatt’s Perceptron, foundational to neural network research \cite {rosenblatt1958_perceptron}}{figure.caption.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}The AI Winter and Multilayer Perceptrons (1969)}{48}{subsection.1.4.2}\protected@file@percent }
\abx@aux@backref{60}{minsky1969_perceptrons}{0}{48}{48}
\@writefile{lof}{\contentsline {figure}{\numberline {1.15}{\ignorespaces Minsky and Papert’s seminal book "Perceptrons," critiquing single-layer networks \blx@tocontentsinit {0}\cite {minsky1969_perceptrons}.}}{48}{figure.caption.21}\protected@file@percent }
\abx@aux@backref{62}{minsky1969_perceptrons}{0}{48}{48}
\newlabel{fig:chapter1_perceptrons_book}{{1.15}{48}{Minsky and Papert’s seminal book "Perceptrons," critiquing single-layer networks \cite {minsky1969_perceptrons}}{figure.caption.21}{}}
\abx@aux@cite{0}{fukushima1980_neocognitron}
\abx@aux@segm{0}{0}{fukushima1980_neocognitron}
\abx@aux@cite{0}{fukushima1980_neocognitron}
\abx@aux@segm{0}{0}{fukushima1980_neocognitron}
\BKM@entry{id=34,dest={73756273656374696F6E2E312E342E34},srcline={359}}{5C3337365C3337375C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030525C303030655C303030765C303030695C303030765C303030615C3030306C5C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030315C303030395C303030385C303030365C3030305C303531}
\abx@aux@cite{0}{rumelhart1986_backpropagation}
\abx@aux@segm{0}{0}{rumelhart1986_backpropagation}
\abx@aux@cite{0}{rumelhart1986_backpropagation}
\abx@aux@segm{0}{0}{rumelhart1986_backpropagation}
\abx@aux@cite{0}{rumelhart1986_backpropagation}
\abx@aux@segm{0}{0}{rumelhart1986_backpropagation}
\BKM@entry{id=35,dest={73756273656374696F6E2E312E342E35},srcline={369}}{5C3337365C3337375C3030304C5C303030655C3030304E5C303030655C303030745C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030455C3030306D5C303030655C303030725C303030675C303030655C3030306E5C303030635C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030315C303030395C303030395C303030385C3030305C303531}
\abx@aux@cite{0}{lecun1998_lenet}
\abx@aux@segm{0}{0}{lecun1998_lenet}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.3}The Neocognitron (1980)}{49}{subsection.1.4.3}\protected@file@percent }
\abx@aux@backref{63}{fukushima1980_neocognitron}{0}{49}{49}
\@writefile{lof}{\contentsline {figure}{\numberline {1.16}{\ignorespaces Kunihiko Fukushima’s Neocognitron: A precursor to modern CNNs \blx@tocontentsinit {0}\cite {fukushima1980_neocognitron}.}}{49}{figure.caption.22}\protected@file@percent }
\abx@aux@backref{65}{fukushima1980_neocognitron}{0}{49}{49}
\newlabel{fig:chapter1_neocognitron}{{1.16}{49}{Kunihiko Fukushima’s Neocognitron: A precursor to modern CNNs \cite {fukushima1980_neocognitron}}{figure.caption.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.4}Backpropagation and the Revival of Neural Networks (1986)}{49}{subsection.1.4.4}\protected@file@percent }
\abx@aux@backref{66}{rumelhart1986_backpropagation}{0}{49}{49}
\@writefile{lof}{\contentsline {figure}{\numberline {1.17}{\ignorespaces Backpropagation algorithm by Rumelhart et al., pivotal for training DNNs \blx@tocontentsinit {0}\cite {rumelhart1986_backpropagation}.}}{49}{figure.caption.23}\protected@file@percent }
\abx@aux@backref{68}{rumelhart1986_backpropagation}{0}{49}{49}
\newlabel{fig:chapter1_backprop}{{1.17}{49}{Backpropagation algorithm by Rumelhart et al., pivotal for training DNNs \cite {rumelhart1986_backpropagation}}{figure.caption.23}{}}
\abx@aux@cite{0}{lecun1998_lenet}
\abx@aux@segm{0}{0}{lecun1998_lenet}
\abx@aux@cite{0}{lecun1998_lenet}
\abx@aux@segm{0}{0}{lecun1998_lenet}
\BKM@entry{id=36,dest={73756273656374696F6E2E312E342E36},srcline={379}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030325C303030305C303030305C303030305C303030735C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030455C303030725C303030615C3030305C3034305C3030306F5C303030665C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\BKM@entry{id=37,dest={73756273656374696F6E2E312E342E37},srcline={389}}{5C3337365C3337375C303030445C303030655C303030655C303030705C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030455C303030785C303030705C3030306C5C3030306F5C303030735C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030325C303030305C303030305C303030375C3030302D5C303030325C303030305C303030325C303030305C3030305C303531}
\abx@aux@cite{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@segm{0}{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.5}LeNet and the Emergence of Convolutional Networks (1998)}{50}{subsection.1.4.5}\protected@file@percent }
\abx@aux@backref{69}{lecun1998_lenet}{0}{50}{50}
\@writefile{lof}{\contentsline {figure}{\numberline {1.18}{\ignorespaces Yann LeCun’s LeNet-5: The first practical convolutional network \blx@tocontentsinit {0}\cite {lecun1998_lenet}.}}{50}{figure.caption.24}\protected@file@percent }
\abx@aux@backref{71}{lecun1998_lenet}{0}{50}{50}
\newlabel{fig:chapter1_lenet}{{1.18}{50}{Yann LeCun’s LeNet-5: The first practical convolutional network \cite {lecun1998_lenet}}{figure.caption.24}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.6}The 2000s: The Era of Deep Learning}{50}{subsection.1.4.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.19}{\ignorespaces The 2000s: Advances in hardware and algorithms enabling deep learning.}}{50}{figure.caption.25}\protected@file@percent }
\newlabel{fig:chapter1_dl_2000s}{{1.19}{50}{The 2000s: Advances in hardware and algorithms enabling deep learning}{figure.caption.25}{}}
\BKM@entry{id=38,dest={73756273656374696F6E2E312E342E38},srcline={400}}{5C3337365C3337375C303030325C303030305C303030315C303030325C3030305C3034305C303030745C3030306F5C3030305C3034305C303030505C303030725C303030655C303030735C303030655C3030306E5C303030745C3030303A5C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030695C303030735C3030305C3034305C303030455C303030765C303030655C303030725C303030795C303030775C303030685C303030655C303030725C30303065}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\abx@aux@cite{0}{he2016_resnet}
\abx@aux@segm{0}{0}{he2016_resnet}
\abx@aux@cite{0}{ren2015_fasterrcnn}
\abx@aux@segm{0}{0}{ren2015_fasterrcnn}
\abx@aux@cite{0}{chen2017_deeplab}
\abx@aux@segm{0}{0}{chen2017_deeplab}
\abx@aux@cite{0}{he2017_maskrcnn}
\abx@aux@segm{0}{0}{he2017_maskrcnn}
\abx@aux@cite{0}{simonyan2014_twostream}
\abx@aux@segm{0}{0}{simonyan2014_twostream}
\abx@aux@cite{0}{toshev2014pose_estimation}
\abx@aux@segm{0}{0}{toshev2014pose_estimation}
\abx@aux@cite{0}{guo2014_atari}
\abx@aux@segm{0}{0}{guo2014_atari}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.7}Deep Learning Explosion (2007-2020)}{51}{subsection.1.4.7}\protected@file@percent }
\abx@aux@backref{72}{imagenet2009_hierarchicaldatabase}{0}{51}{51}
\abx@aux@backref{73}{krizhevsky2012_alexnet}{0}{51}{51}
\@writefile{lof}{\contentsline {figure}{\numberline {1.20}{\ignorespaces Exponential growth in deep learning research, from 2007 to 2020.}}{51}{figure.caption.26}\protected@file@percent }
\newlabel{fig:chapter1_dl_explosion}{{1.20}{51}{Exponential growth in deep learning research, from 2007 to 2020}{figure.caption.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.8}2012 to Present: Deep Learning is Everywhere}{51}{subsection.1.4.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Core Vision Tasks}{51}{section*.27}\protected@file@percent }
\abx@aux@backref{74}{krizhevsky2012_alexnet}{0}{51}{51}
\abx@aux@backref{75}{he2016_resnet}{0}{51}{51}
\abx@aux@backref{76}{ren2015_fasterrcnn}{0}{51}{51}
\abx@aux@backref{77}{chen2017_deeplab}{0}{51}{51}
\abx@aux@backref{78}{he2017_maskrcnn}{0}{51}{51}
\@writefile{toc}{\contentsline {subsubsection}{Video and Temporal Analysis}{51}{section*.28}\protected@file@percent }
\abx@aux@backref{79}{simonyan2014_twostream}{0}{51}{51}
\abx@aux@backref{80}{toshev2014pose_estimation}{0}{51}{51}
\abx@aux@cite{0}{vinyals2015_captioning}
\abx@aux@segm{0}{0}{vinyals2015_captioning}
\abx@aux@cite{0}{karpathy2015_visualsemantic}
\abx@aux@segm{0}{0}{karpathy2015_visualsemantic}
\abx@aux@cite{0}{dalle2021_texttoimage}
\abx@aux@segm{0}{0}{dalle2021_texttoimage}
\abx@aux@cite{0}{clip2021_multimodal}
\abx@aux@segm{0}{0}{clip2021_multimodal}
\abx@aux@cite{0}{flamingo2022_fewshot}
\abx@aux@segm{0}{0}{flamingo2022_fewshot}
\abx@aux@cite{0}{levy2016_medicalimaging}
\abx@aux@segm{0}{0}{levy2016_medicalimaging}
\abx@aux@cite{0}{dieleman2014_galaxycnn}
\abx@aux@segm{0}{0}{dieleman2014_galaxycnn}
\abx@aux@cite{0}{sam2023_segmentation}
\abx@aux@segm{0}{0}{sam2023_segmentation}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@cite{0}{mamba2023_selective}
\abx@aux@segm{0}{0}{mamba2023_selective}
\abx@aux@backref{81}{guo2014_atari}{0}{52}{52}
\@writefile{toc}{\contentsline {subsubsection}{Generative and Multimodal Models}{52}{section*.29}\protected@file@percent }
\abx@aux@backref{82}{vinyals2015_captioning}{0}{52}{52}
\abx@aux@backref{83}{karpathy2015_visualsemantic}{0}{52}{52}
\abx@aux@backref{84}{dalle2021_texttoimage}{0}{52}{52}
\abx@aux@backref{85}{clip2021_multimodal}{0}{52}{52}
\abx@aux@backref{86}{flamingo2022_fewshot}{0}{52}{52}
\@writefile{toc}{\contentsline {subsubsection}{Specialized Domains}{52}{section*.30}\protected@file@percent }
\abx@aux@backref{87}{levy2016_medicalimaging}{0}{52}{52}
\abx@aux@backref{88}{dieleman2014_galaxycnn}{0}{52}{52}
\@writefile{toc}{\contentsline {subsubsection}{State-of-the-Art Foundation Models}{52}{section*.31}\protected@file@percent }
\abx@aux@backref{89}{sam2023_segmentation}{0}{52}{52}
\abx@aux@backref{90}{dino2021_selfsupervised}{0}{52}{52}
\abx@aux@backref{91}{mamba2023_selective}{0}{52}{52}
\abx@aux@cite{0}{dalle2021_texttoimage}
\abx@aux@segm{0}{0}{dalle2021_texttoimage}
\abx@aux@cite{0}{dalle2021_texttoimage}
\abx@aux@segm{0}{0}{dalle2021_texttoimage}
\abx@aux@cite{0}{dalle2021_texttoimage}
\abx@aux@segm{0}{0}{dalle2021_texttoimage}
\abx@aux@cite{0}{dalle2021_texttoimage}
\abx@aux@segm{0}{0}{dalle2021_texttoimage}
\@writefile{lof}{\contentsline {figure}{\numberline {1.21}{\ignorespaces The iconic avocado-shaped armchair, generated by DALL-E, exemplifies the creative potential of generative models \blx@tocontentsinit {0}\cite {dalle2021_texttoimage}.}}{53}{figure.caption.32}\protected@file@percent }
\abx@aux@backref{93}{dalle2021_texttoimage}{0}{53}{53}
\newlabel{fig:dalle_avocado}{{1.21}{53}{The iconic avocado-shaped armchair, generated by DALL-E, exemplifies the creative potential of generative models \cite {dalle2021_texttoimage}}{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.22}{\ignorespaces Another example for a peach-shaped armchair, generated by DALL-E \blx@tocontentsinit {0}\cite {dalle2021_texttoimage}.}}{53}{figure.caption.33}\protected@file@percent }
\abx@aux@backref{95}{dalle2021_texttoimage}{0}{53}{53}
\newlabel{fig:dalle_peach}{{1.22}{53}{Another example for a peach-shaped armchair, generated by DALL-E \cite {dalle2021_texttoimage}}{figure.caption.33}{}}
\@writefile{toc}{\contentsline {subsubsection}{Computation is Cheaper: More GFLOPs per Dollar}{53}{section*.34}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.23}{\ignorespaces The dramatic drop in GFLOPs cost over time, enabling more accessible deep learning applications.}}{54}{figure.caption.35}\protected@file@percent }
\newlabel{fig:gflops_cost}{{1.23}{54}{The dramatic drop in GFLOPs cost over time, enabling more accessible deep learning applications}{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.24}{\ignorespaces Advances in GPUs, including tensor cores, greatly enhancing GFLOPs per dollar.}}{54}{figure.caption.36}\protected@file@percent }
\newlabel{fig:gpu_tensor_cores}{{1.24}{54}{Advances in GPUs, including tensor cores, greatly enhancing GFLOPs per dollar}{figure.caption.36}{}}
\BKM@entry{id=39,dest={73656374696F6E2E312E35},srcline={477}}{5C3337365C3337375C3030304B5C303030655C303030795C3030305C3034305C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030435C303030565C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030465C303030755C303030745C303030755C303030725C303030655C3030305C3034305C303030445C303030695C303030725C303030655C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\abx@aux@cite{0}{buolamwini2018_gendershades}
\abx@aux@segm{0}{0}{buolamwini2018_gendershades}
\abx@aux@cite{0}{buolamwini2018_gendershades}
\abx@aux@segm{0}{0}{buolamwini2018_gendershades}
\abx@aux@cite{0}{buolamwini2018_gendershades}
\abx@aux@segm{0}{0}{buolamwini2018_gendershades}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Key Challenges in CV and Future Directions}{55}{section.1.5}\protected@file@percent }
\abx@aux@backref{96}{buolamwini2018_gendershades}{0}{55}{55}
\@writefile{lof}{\contentsline {figure}{\numberline {1.25}{\ignorespaces Ethical concerns: CV systems can amplify biases or cause harm, such as misidentifications \blx@tocontentsinit {0}\cite {buolamwini2018_gendershades}.}}{55}{figure.caption.37}\protected@file@percent }
\abx@aux@backref{98}{buolamwini2018_gendershades}{0}{55}{55}
\newlabel{fig:chapter1_ethics}{{1.25}{55}{Ethical concerns: CV systems can amplify biases or cause harm, such as misidentifications \cite {buolamwini2018_gendershades}}{figure.caption.37}{}}
\abx@aux@backref{99}{goodfellow2014_adversarial}{0}{55}{55}
\@writefile{lof}{\contentsline {figure}{\numberline {1.26}{\ignorespaces Adversarial examples: Adding imperceptible noise to a panda image causes the model to misclassify it \blx@tocontentsinit {0}\cite {goodfellow2014_adversarial}.}}{55}{figure.caption.38}\protected@file@percent }
\abx@aux@backref{101}{goodfellow2014_adversarial}{0}{55}{55}
\newlabel{fig:chapter1_adversarial}{{1.26}{55}{Adversarial examples: Adding imperceptible noise to a panda image causes the model to misclassify it \cite {goodfellow2014_adversarial}}{figure.caption.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.27}{\ignorespaces Complex scene understanding: AI struggles with nuanced contexts like social interactions.}}{56}{figure.caption.39}\protected@file@percent }
\newlabel{fig:chapter1_context}{{1.27}{56}{Complex scene understanding: AI struggles with nuanced contexts like social interactions}{figure.caption.39}{}}
\BKM@entry{id=40,dest={636861707465722E32},srcline={3}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030325C3030303A5C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=41,dest={73656374696F6E2E322E31},srcline={9}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Lecture 2: Image Classification}{57}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@1}}
\ttl@writefile{ptc}{\ttl@starttoc{default@2}}
\pgfsyspdfmark {pgfid8}{0}{52099153}
\pgfsyspdfmark {pgfid7}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction to Image Classification}{57}{section.2.1}\protected@file@percent }
\BKM@entry{id=42,dest={73656374696F6E2E322E32},srcline={23}}{5C3337365C3337375C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C30303073}
\BKM@entry{id=43,dest={73756273656374696F6E2E322E322E31},srcline={27}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030535C303030655C3030306D5C303030615C3030306E5C303030745C303030695C303030635C3030305C3034305C303030475C303030615C30303070}
\BKM@entry{id=44,dest={73756273656374696F6E2E322E322E32},srcline={37}}{5C3337365C3337375C303030525C3030306F5C303030625C303030755C303030735C303030745C3030306E5C303030655C303030735C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C303030435C303030615C3030306D5C303030655C303030725C303030615C3030305C3034305C3030304D5C3030306F5C303030765C303030655C3030306D5C303030655C3030306E5C30303074}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Image Classification Challenges}{58}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}The Semantic Gap}{58}{subsection.2.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Images are represented as grids of pixel values, lacking inherent semantic meaning.}}{58}{figure.caption.40}\protected@file@percent }
\newlabel{fig:chapter2_semantic_gap}{{2.1}{58}{Images are represented as grids of pixel values, lacking inherent semantic meaning}{figure.caption.40}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Robustness to Camera Movement}{58}{subsection.2.2.2}\protected@file@percent }
\BKM@entry{id=45,dest={73756273656374696F6E2E322E322E33},srcline={47}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C303030615C3030302D5C303030435C3030306C5C303030615C303030735C303030735C3030305C3034305C303030565C303030615C303030725C303030695C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=46,dest={73756273656374696F6E2E322E322E34},srcline={57}}{5C3337365C3337375C303030465C303030695C3030306E5C303030655C3030302D5C303030475C303030725C303030615C303030695C3030306E5C303030655C303030645C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Changes in camera position or angle result in varying pixel grids, complicating classification.}}{59}{figure.caption.41}\protected@file@percent }
\newlabel{fig:chapter2_camera_movement}{{2.2}{59}{Changes in camera position or angle result in varying pixel grids, complicating classification}{figure.caption.41}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Intra-Class Variation}{59}{subsection.2.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Cats of different breeds show significant visual differences, a phenomenon known as intra-class variation.}}{59}{figure.caption.42}\protected@file@percent }
\newlabel{fig:chapter2_intra_class_variation}{{2.3}{59}{Cats of different breeds show significant visual differences, a phenomenon known as intra-class variation}{figure.caption.42}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Fine-Grained Classification}{59}{subsection.2.2.4}\protected@file@percent }
\BKM@entry{id=47,dest={73756273656374696F6E2E322E322E35},srcline={67}}{5C3337365C3337375C303030425C303030615C303030635C3030306B5C303030675C303030725C3030306F5C303030755C3030306E5C303030645C3030305C3034305C303030435C3030306C5C303030755C303030745C303030745C303030655C30303072}
\BKM@entry{id=48,dest={73756273656374696F6E2E322E322E36},srcline={77}}{5C3337365C3337375C303030495C3030306C5C3030306C5C303030755C3030306D5C303030695C3030306E5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030435C303030685C303030615C3030306E5C303030675C303030655C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Fine-grained classification requires distinguishing subtle differences within visually similar categories.}}{60}{figure.caption.43}\protected@file@percent }
\newlabel{fig:chapter2_fine_grained}{{2.4}{60}{Fine-grained classification requires distinguishing subtle differences within visually similar categories}{figure.caption.43}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Background Clutter}{60}{subsection.2.2.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Background clutter can obscure target objects, complicating image classification.}}{60}{figure.caption.44}\protected@file@percent }
\newlabel{fig:chapter2_background_clutter}{{2.5}{60}{Background clutter can obscure target objects, complicating image classification}{figure.caption.44}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.6}Illumination Changes}{60}{subsection.2.2.6}\protected@file@percent }
\BKM@entry{id=49,dest={73756273656374696F6E2E322E322E37},srcline={87}}{5C3337365C3337375C303030445C303030655C303030665C3030306F5C303030725C3030306D5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030535C303030635C303030615C3030306C5C30303065}
\BKM@entry{id=50,dest={73756273656374696F6E2E322E322E38},srcline={97}}{5C3337365C3337375C3030304F5C303030635C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Variations in illumination conditions affect object appearance, requiring robust algorithms.}}{61}{figure.caption.45}\protected@file@percent }
\newlabel{fig:chapter2_illumination}{{2.6}{61}{Variations in illumination conditions affect object appearance, requiring robust algorithms}{figure.caption.45}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.7}Deformation and Object Scale}{61}{subsection.2.2.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Objects can deform and appear at varying scales, posing challenges for classification.}}{61}{figure.caption.46}\protected@file@percent }
\newlabel{fig:chapter2_deformation_scale}{{2.7}{61}{Objects can deform and appear at varying scales, posing challenges for classification}{figure.caption.46}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.8}Occlusions}{61}{subsection.2.2.8}\protected@file@percent }
\BKM@entry{id=51,dest={73756273656374696F6E2E322E322E39},srcline={107}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030305C3034305C3030306F5C303030665C3030305C3034305C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C30303073}
\BKM@entry{id=52,dest={73656374696F6E2E322E33},srcline={117}}{5C3337365C3337375C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C303030735C3030305C3034305C303030615C3030305C3034305C303030425C303030755C303030695C3030306C5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030425C3030306C5C3030306F5C303030635C3030306B5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304F5C303030745C303030685C303030655C303030725C3030305C3034305C303030545C303030615C303030735C3030306B5C30303073}
\BKM@entry{id=53,dest={73756273656374696F6E2E322E332E31},srcline={121}}{5C3337365C3337375C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Occlusions, such as partial visibility of objects, obscure critical features and hinder classification.}}{62}{figure.caption.47}\protected@file@percent }
\newlabel{fig:chapter2_occlusions}{{2.8}{62}{Occlusions, such as partial visibility of objects, obscure critical features and hinder classification}{figure.caption.47}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.9}Summary of Challenges}{62}{subsection.2.2.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Image Classification as a Building Block for Other Tasks}{62}{section.2.3}\protected@file@percent }
\BKM@entry{id=54,dest={73756273656374696F6E2E322E332E32},srcline={141}}{5C3337365C3337375C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C303030615C303030705C303030745C303030695C3030306F5C3030306E5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Object Detection}{63}{subsection.2.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Using sliding windows for object detection: classifying regions as background or containing an object.}}{63}{figure.caption.48}\protected@file@percent }
\newlabel{fig:chapter2_sliding_window_bg}{{2.9}{63}{Using sliding windows for object detection: classifying regions as background or containing an object}{figure.caption.48}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Using sliding windows for object detection: classifying regions containing objects (e.g., person).}}{63}{figure.caption.49}\protected@file@percent }
\newlabel{fig:chapter2_sliding_window_person}{{2.10}{63}{Using sliding windows for object detection: classifying regions containing objects (e.g., person)}{figure.caption.49}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Image Captioning}{64}{subsection.2.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Image captioning as sequential classification: determining the first word (e.g., "man").}}{64}{figure.caption.50}\protected@file@percent }
\newlabel{fig:chapter2_caption_man}{{2.11}{64}{Image captioning as sequential classification: determining the first word (e.g., "man")}{figure.caption.50}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Image captioning as sequential classification: determining the next word (e.g., "riding").}}{64}{figure.caption.51}\protected@file@percent }
\newlabel{fig:chapter2_caption_riding}{{2.12}{64}{Image captioning as sequential classification: determining the next word (e.g., "riding")}{figure.caption.51}{}}
\BKM@entry{id=55,dest={73756273656374696F6E2E322E332E33},srcline={168}}{5C3337365C3337375C303030445C303030655C303030635C303030695C303030735C303030695C3030306F5C3030306E5C3030302D5C3030304D5C303030615C3030306B5C303030695C3030306E5C303030675C3030305C3034305C303030695C3030306E5C3030305C3034305C303030425C3030306F5C303030615C303030725C303030645C3030305C3034305C303030475C303030615C3030306D5C303030655C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Image captioning: determining the end of the sentence with a "STOP" token.}}{65}{figure.caption.52}\protected@file@percent }
\newlabel{fig:chapter2_caption_stop}{{2.13}{65}{Image captioning: determining the end of the sentence with a "STOP" token}{figure.caption.52}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Decision-Making in Board Games}{65}{subsection.2.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces Board games like Go framed as classification problems: determining the optimal next move.}}{65}{figure.caption.53}\protected@file@percent }
\newlabel{fig:chapter2_board_games}{{2.14}{65}{Board games like Go framed as classification problems: determining the optimal next move}{figure.caption.53}{}}
\BKM@entry{id=56,dest={73756273656374696F6E2E322E332E34},srcline={180}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030303A5C3030305C3034305C3030304C5C303030655C303030765C303030655C303030725C303030615C303030675C303030695C3030306E5C303030675C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=57,dest={73656374696F6E2E322E34},srcline={184}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030735C303030745C303030725C303030755C303030635C303030745C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C30303072}
\BKM@entry{id=58,dest={73756273656374696F6E2E322E342E31},srcline={188}}{5C3337365C3337375C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030302D5C303030425C303030615C303030735C303030655C303030645C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030635C303030615C3030306C5C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030615C303030635C30303068}
\abx@aux@cite{0}{canny1986_edgedetection}
\abx@aux@segm{0}{0}{canny1986_edgedetection}
\abx@aux@cite{0}{harris1988_combined}
\abx@aux@segm{0}{0}{harris1988_combined}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Summary: Leveraging Image Classification}{66}{subsection.2.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Constructing an Image Classifier}{66}{section.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Feature-Based Image Classification: The Classical Approach}{66}{subsection.2.4.1}\protected@file@percent }
\abx@aux@backref{102}{canny1986_edgedetection}{0}{66}{66}
\abx@aux@backref{103}{harris1988_combined}{0}{66}{66}
\BKM@entry{id=59,dest={73756273656374696F6E2E322E342E32},srcline={219}}{5C3337365C3337375C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030485C303030615C3030306E5C303030645C3030302D5C303030435C303030725C303030615C303030665C303030745C303030655C303030645C3030305C3034305C303030525C303030755C3030306C5C303030655C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C303030445C303030615C303030745C303030615C3030302D5C303030445C303030725C303030695C303030765C303030655C3030306E5C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces Attempting to classify images using hard-coded features is highly challenging.}}{67}{figure.caption.54}\protected@file@percent }
\newlabel{fig:chapter2_classification_attempt}{{2.15}{67}{Attempting to classify images using hard-coded features is highly challenging}{figure.caption.54}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces Edges and corners as features for classification: an incomplete solution.}}{67}{figure.caption.55}\protected@file@percent }
\newlabel{fig:chapter2_edge_corners}{{2.16}{67}{Edges and corners as features for classification: an incomplete solution}{figure.caption.55}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}From Hand-Crafted Rules to Data-Driven Learning}{67}{subsection.2.4.2}\protected@file@percent }
\BKM@entry{id=60,dest={73756273656374696F6E2E322E342E33},srcline={239}}{5C3337365C3337375C303030505C303030725C3030306F5C303030675C303030725C303030615C3030306D5C3030306D5C303030695C3030306E5C303030675C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030445C303030615C303030745C303030615C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C303030725C3030306E5C3030305C3034305C303030505C303030615C303030725C303030615C303030645C303030695C303030675C3030306D}
\BKM@entry{id=61,dest={73756273656374696F6E2E322E342E34},srcline={251}}{5C3337365C3337375C303030445C303030615C303030745C303030615C3030302D5C303030445C303030725C303030695C303030765C303030655C3030306E5C3030305C3034305C3030304D5C303030615C303030635C303030685C303030695C3030306E5C303030655C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C3030304E5C303030655C303030775C3030305C3034305C303030465C303030725C3030306F5C3030306E5C303030745C303030695C303030655C30303072}
\@writefile{lof}{\contentsline {figure}{\numberline {2.17}{\ignorespaces A data-driven pipeline for training and evaluating machine learning-based image classifiers.}}{68}{figure.caption.56}\protected@file@percent }
\newlabel{fig:chapter2_data_driven}{{2.17}{68}{A data-driven pipeline for training and evaluating machine learning-based image classifiers}{figure.caption.56}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Programming with Data: The Modern Paradigm}{68}{subsection.2.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.4}Data-Driven Machine Learning: The New Frontier}{68}{subsection.2.4.4}\protected@file@percent }
\BKM@entry{id=62,dest={73656374696F6E2E322E35},srcline={267}}{5C3337365C3337375C303030445C303030615C303030745C303030615C303030735C303030655C303030745C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=63,dest={73756273656374696F6E2E322E352E31},srcline={271}}{5C3337365C3337375C3030304D5C3030304E5C303030495C303030535C303030545C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030545C3030306F5C303030795C3030305C3034305C303030445C303030615C303030745C303030615C303030735C303030655C30303074}
\abx@aux@cite{0}{lecun1998_lenet}
\abx@aux@segm{0}{0}{lecun1998_lenet}
\BKM@entry{id=64,dest={73756273656374696F6E2E322E352E32},srcline={284}}{5C3337365C3337375C303030435C303030495C303030465C303030415C303030525C3030303A5C3030305C3034305C303030525C303030655C303030615C3030306C5C3030302D5C303030575C3030306F5C303030725C3030306C5C303030645C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030525C303030655C303030635C3030306F5C303030675C3030306E5C303030695C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{krizhevsky2009_learning}
\abx@aux@segm{0}{0}{krizhevsky2009_learning}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Datasets in Image Classification}{69}{section.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}MNIST: The Toy Dataset}{69}{subsection.2.5.1}\protected@file@percent }
\abx@aux@backref{104}{lecun1998_lenet}{0}{69}{69}
\@writefile{lof}{\contentsline {figure}{\numberline {2.18}{\ignorespaces MNIST: A dataset of handwritten digits, often used as a toy benchmark.}}{69}{figure.caption.57}\protected@file@percent }
\newlabel{fig:chapter2_mnist}{{2.18}{69}{MNIST: A dataset of handwritten digits, often used as a toy benchmark}{figure.caption.57}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}CIFAR: Real-World Object Recognition}{69}{subsection.2.5.2}\protected@file@percent }
\abx@aux@backref{105}{krizhevsky2009_learning}{0}{69}{69}
\@writefile{lof}{\contentsline {figure}{\numberline {2.19}{\ignorespaces CIFAR-10: A dataset for object classification with 10 categories.}}{70}{figure.caption.58}\protected@file@percent }
\newlabel{fig:chapter2_cifar10}{{2.19}{70}{CIFAR-10: A dataset for object classification with 10 categories}{figure.caption.58}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.20}{\ignorespaces CIFAR-100: An extension of CIFAR-10 with 100 categories.}}{70}{figure.caption.59}\protected@file@percent }
\newlabel{fig:chapter2_cifar100}{{2.20}{70}{CIFAR-100: An extension of CIFAR-10 with 100 categories}{figure.caption.59}{}}
\BKM@entry{id=65,dest={73756273656374696F6E2E322E352E33},srcline={308}}{5C3337365C3337375C303030495C3030306D5C303030615C303030675C303030655C3030304E5C303030655C303030745C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030475C3030306F5C3030306C5C303030645C3030305C3034305C303030535C303030745C303030615C3030306E5C303030645C303030615C303030725C30303064}
\abx@aux@cite{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@segm{0}{0}{imagenet2009_hierarchicaldatabase}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}ImageNet: The Gold Standard}{71}{subsection.2.5.3}\protected@file@percent }
\abx@aux@backref{106}{imagenet2009_hierarchicaldatabase}{0}{71}{71}
\@writefile{lof}{\contentsline {figure}{\numberline {2.21}{\ignorespaces ImageNet: A dataset of 1,000 categories pivotal to computer vision progress.}}{71}{figure.caption.60}\protected@file@percent }
\newlabel{fig:chapter2_imagenet}{{2.21}{71}{ImageNet: A dataset of 1,000 categories pivotal to computer vision progress}{figure.caption.60}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.22}{\ignorespaces ImageNet top-5 accuracy: A widely adopted evaluation metric.}}{71}{figure.caption.61}\protected@file@percent }
\newlabel{fig:chapter2_imagenet_top5}{{2.22}{71}{ImageNet top-5 accuracy: A widely adopted evaluation metric}{figure.caption.61}{}}
\BKM@entry{id=66,dest={73756273656374696F6E2E322E352E34},srcline={331}}{5C3337365C3337375C3030304D5C303030495C303030545C3030305C3034305C303030505C3030306C5C303030615C303030635C303030655C303030735C3030303A5C3030305C3034305C303030535C303030635C303030655C3030306E5C303030655C3030305C3034305C303030525C303030655C303030635C3030306F5C303030675C3030306E5C303030695C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{zhou2017_places}
\abx@aux@segm{0}{0}{zhou2017_places}
\BKM@entry{id=67,dest={73756273656374696F6E2E322E352E35},srcline={342}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C3030306E5C303030675C3030305C3034305C303030445C303030615C303030745C303030615C303030735C303030655C303030745C3030305C3034305C303030535C303030695C3030307A5C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.4}MIT Places: Scene Recognition}{72}{subsection.2.5.4}\protected@file@percent }
\abx@aux@backref{107}{zhou2017_places}{0}{72}{72}
\@writefile{lof}{\contentsline {figure}{\numberline {2.23}{\ignorespaces MIT Places: A dataset for scene classification, focusing on diverse environmental contexts.}}{72}{figure.caption.62}\protected@file@percent }
\newlabel{fig:chapter2_places}{{2.23}{72}{MIT Places: A dataset for scene classification, focusing on diverse environmental contexts}{figure.caption.62}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.5}Comparing Dataset Sizes}{72}{subsection.2.5.5}\protected@file@percent }
\BKM@entry{id=68,dest={73756273656374696F6E2E322E352E36},srcline={361}}{5C3337365C3337375C3030304F5C3030306D5C3030306E5C303030695C303030675C3030306C5C3030306F5C303030745C3030303A5C3030305C3034305C303030465C303030655C303030775C3030302D5C303030535C303030685C3030306F5C303030745C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\abx@aux@cite{0}{lake2015_human}
\abx@aux@segm{0}{0}{lake2015_human}
\BKM@entry{id=69,dest={73756273656374696F6E2E322E352E37},srcline={372}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030445C303030615C303030745C303030615C303030735C303030655C303030745C303030735C3030305C3034305C303030445C303030725C303030695C303030765C303030695C3030306E5C303030675C3030305C3034305C303030505C303030725C3030306F5C303030675C303030725C303030655C303030735C30303073}
\BKM@entry{id=70,dest={73656374696F6E2E322E36},srcline={376}}{5C3337365C3337375C3030304E5C303030655C303030615C303030725C303030655C303030735C303030745C3030305C3034305C3030304E5C303030655C303030695C303030675C303030685C303030625C3030306F5C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C3030303A5C3030305C3034305C303030415C3030305C3034305C303030475C303030615C303030745C303030655C303030775C303030615C303030795C3030305C3034305C303030745C3030306F5C3030305C3034305C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {2.24}{\ignorespaces Comparing dataset sizes: MNIST, CIFAR, ImageNet, and MIT Places.}}{73}{figure.caption.63}\protected@file@percent }
\newlabel{fig:chapter2_dataset_sizes}{{2.24}{73}{Comparing dataset sizes: MNIST, CIFAR, ImageNet, and MIT Places}{figure.caption.63}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.6}Omniglot: Few-Shot Learning}{73}{subsection.2.5.6}\protected@file@percent }
\abx@aux@backref{108}{lake2015_human}{0}{73}{73}
\@writefile{lof}{\contentsline {figure}{\numberline {2.25}{\ignorespaces Omniglot: A dataset for few-shot learning, with minimal examples per category.}}{73}{figure.caption.64}\protected@file@percent }
\newlabel{fig:chapter2_omniglot}{{2.25}{73}{Omniglot: A dataset for few-shot learning, with minimal examples per category}{figure.caption.64}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.7}Conclusion: Datasets Driving Progress}{73}{subsection.2.5.7}\protected@file@percent }
\BKM@entry{id=71,dest={73756273656374696F6E2E322E362E31},srcline={380}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030425C303030655C303030675C303030695C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304E5C303030655C303030615C303030725C303030655C303030735C303030745C3030305C3034305C3030304E5C303030655C303030695C303030675C303030685C303030625C3030306F5C303030725C3030303F}
\BKM@entry{id=72,dest={73756273656374696F6E2E322E362E32},srcline={392}}{5C3337365C3337375C303030535C303030655C303030745C303030745C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030535C303030745C303030615C303030675C303030655C3030303A5C3030305C3034305C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030505C303030695C303030785C303030655C3030306C5C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C303030505C303030725C303030655C303030645C303030695C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=73,dest={73756273656374696F6E2E322E362E33},srcline={401}}{5C3337365C3337375C303030415C3030306C5C303030675C3030306F5C303030725C303030695C303030745C303030685C3030306D5C3030305C3034305C303030445C303030655C303030735C303030635C303030725C303030695C303030705C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Nearest Neighbor Classifier: A Gateway to Understanding Classification}{74}{section.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Why Begin with Nearest Neighbor?}{74}{subsection.2.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}Setting the Stage: From Pixels to Predictions}{74}{subsection.2.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.3}Algorithm Description}{74}{subsection.2.6.3}\protected@file@percent }
\BKM@entry{id=74,dest={73756273656374696F6E2E322E362E34},srcline={416}}{5C3337365C3337375C303030445C303030695C303030735C303030745C303030615C3030306E5C303030635C303030655C3030305C3034305C3030304D5C303030655C303030745C303030725C303030695C303030635C303030735C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030435C3030306F5C303030725C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304E5C303030655C303030615C303030725C303030655C303030735C303030745C3030305C3034305C3030304E5C303030655C303030695C303030675C303030685C303030625C3030306F5C30303072}
\@writefile{lof}{\contentsline {figure}{\numberline {2.26}{\ignorespaces Nearest Neighbor classifier: memorize training data and predict based on the closest match.}}{75}{figure.caption.65}\protected@file@percent }
\newlabel{fig:chapter2_nn_description}{{2.26}{75}{Nearest Neighbor classifier: memorize training data and predict based on the closest match}{figure.caption.65}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.4}Distance Metrics: The Core of Nearest Neighbor}{75}{subsection.2.6.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.27}{\ignorespaces L1 distance example: a simple and interpretable metric.}}{75}{figure.caption.66}\protected@file@percent }
\newlabel{fig:chapter2_l1_distance}{{2.27}{75}{L1 distance example: a simple and interpretable metric}{figure.caption.66}{}}
\newlabel{fig:chapter2_l1_l2_comparison}{{2.28}{76}{}{figure.caption.67}{}}
\BKM@entry{id=75,dest={73756273656374696F6E2E322E362E35},srcline={476}}{5C3337365C3337375C303030455C303030785C303030745C303030655C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C3030304E5C303030655C303030615C303030725C303030655C303030735C303030745C3030305C3034305C3030304E5C303030655C303030695C303030675C303030685C303030625C3030306F5C303030725C3030303A5C3030305C3034305C303030415C303030705C303030705C3030306C5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030425C303030655C303030795C3030306F5C3030306E5C303030645C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C30303073}
\newlabel{fig:chapter2_l1_l2_comparison_boundaries}{{2.29}{77}{}{figure.caption.68}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.30}{\ignorespaces Limitations of L1 distance: visually dissimilar objects with similar colors may be incorrectly classified.}}{77}{figure.caption.69}\protected@file@percent }
\newlabel{fig:chapter2_l1_poor_performance}{{2.30}{77}{Limitations of L1 distance: visually dissimilar objects with similar colors may be incorrectly classified}{figure.caption.69}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.5}Extending Nearest Neighbor: Applications Beyond Images}{78}{subsection.2.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Using Nearest Neighbor for Non-Image Data}{78}{section*.70}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Academic Paper Recommendation Example}{78}{section*.71}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.31}{\ignorespaces Nearest Neighbor using TF-IDF similarity for academic paper recommendations.}}{78}{figure.caption.72}\protected@file@percent }
\newlabel{fig:chapter2_nn_tfidf}{{2.31}{78}{Nearest Neighbor using TF-IDF similarity for academic paper recommendations}{figure.caption.72}{}}
\BKM@entry{id=76,dest={73756273656374696F6E2E322E362E36},srcline={515}}{5C3337365C3337375C303030485C303030795C303030705C303030655C303030725C303030705C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C3030304E5C303030655C303030615C303030725C303030655C303030735C303030745C3030305C3034305C3030304E5C303030655C303030695C303030675C303030685C303030625C3030306F5C30303072}
\@writefile{toc}{\contentsline {subsubsection}{Key Insights}{79}{section*.73}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.6}Hyperparameters in Nearest Neighbor}{79}{subsection.2.6.6}\protected@file@percent }
\BKM@entry{id=77,dest={73756273656374696F6E2E322E362E37},srcline={545}}{5C3337365C3337375C303030435C303030725C3030306F5C303030735C303030735C3030302D5C303030565C303030615C3030306C5C303030695C303030645C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {2.32}{\ignorespaces Train-validation-test split for robust evaluation.}}{80}{figure.caption.74}\protected@file@percent }
\newlabel{fig:chapter2_train_val_test}{{2.32}{80}{Train-validation-test split for robust evaluation}{figure.caption.74}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.7}Cross-Validation}{80}{subsection.2.6.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.33}{\ignorespaces Cross-validation accuracy for different values of \(k\). Each dot represents an individual trial, and the mean accuracy across folds is shown by the line. In this example, \(k = 7\) yields the highest average validation performance, so it is selected.}}{81}{figure.caption.75}\protected@file@percent }
\newlabel{fig:chapter2_cross_validation}{{2.33}{81}{Cross-validation accuracy for different values of \(k\). Each dot represents an individual trial, and the mean accuracy across folds is shown by the line. In this example, \(k = 7\) yields the highest average validation performance, so it is selected}{figure.caption.75}{}}
\BKM@entry{id=78,dest={73756273656374696F6E2E322E362E38},srcline={575}}{5C3337365C3337375C303030495C3030306D5C303030705C3030306C5C303030655C3030306D5C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030435C3030306F5C3030306D5C303030705C3030306C5C303030655C303030785C303030695C303030745C30303079}
\BKM@entry{id=79,dest={73756273656374696F6E2E322E362E39},srcline={601}}{5C3337365C3337375C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C303030445C303030655C303030635C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030425C3030306F5C303030755C3030306E5C303030645C303030615C303030725C303030695C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.8}Implementation and Complexity}{82}{subsection.2.6.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.34}{\ignorespaces \texttt  {train} method: Memorizing training data.}}{82}{figure.caption.76}\protected@file@percent }
\newlabel{fig:chapter2_train}{{2.34}{82}{\texttt {train} method: Memorizing training data}{figure.caption.76}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.35}{\ignorespaces \texttt  {predict} method: Computing similarity and predicting the closest label.}}{82}{figure.caption.77}\protected@file@percent }
\newlabel{fig:chapter2_predict}{{2.35}{82}{\texttt {predict} method: Computing similarity and predicting the closest label}{figure.caption.77}{}}
\BKM@entry{id=80,dest={73756273656374696F6E2E322E362E3130},srcline={621}}{5C3337365C3337375C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030655C3030306D5C303030655C3030306E5C303030745C303030735C3030303A5C3030305C3034305C3030306B5C3030302D5C3030304E5C303030655C303030615C303030725C303030655C303030735C303030745C3030305C3034305C3030304E5C303030655C303030695C303030675C303030685C303030625C3030306F5C303030725C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.9}Visualization of Decision Boundaries}{83}{subsection.2.6.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.36}{\ignorespaces Decision boundaries for Nearest Neighbor on a 2D dataset.}}{83}{figure.caption.78}\protected@file@percent }
\newlabel{fig:chapter2_decision_boundaries_start}{{2.36}{83}{Decision boundaries for Nearest Neighbor on a 2D dataset}{figure.caption.78}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.37}{\ignorespaces Outliers disrupting decision boundaries in Nearest Neighbor classification.}}{83}{figure.caption.79}\protected@file@percent }
\newlabel{fig:chapter2_outlier_effect}{{2.37}{83}{Outliers disrupting decision boundaries in Nearest Neighbor classification}{figure.caption.79}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.10}Improvements: k-Nearest Neighbors}{83}{subsection.2.6.10}\protected@file@percent }
\BKM@entry{id=81,dest={73756273656374696F6E2E322E362E3131},srcline={634}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030555C3030306E5C303030695C303030765C303030655C303030725C303030735C303030615C3030306C5C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030785C303030695C3030306D5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {2.38}{\ignorespaces k-Nearest Neighbors ($k=3$): Smoother decision boundaries and reduced outlier influence.}}{84}{figure.caption.80}\protected@file@percent }
\newlabel{fig:chapter2_knn_smoothing}{{2.38}{84}{k-Nearest Neighbors ($k=3$): Smoother decision boundaries and reduced outlier influence}{figure.caption.80}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.11}Limitations and Universal Approximation}{84}{subsection.2.6.11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.39}{\ignorespaces A step towards a dense coverage with Nearest Neighbor.}}{84}{figure.caption.81}\protected@file@percent }
\newlabel{fig:chapter2_dense_coverage}{{2.39}{84}{A step towards a dense coverage with Nearest Neighbor}{figure.caption.81}{}}
\BKM@entry{id=82,dest={73756273656374696F6E2E322E362E3132},srcline={661}}{5C3337365C3337375C303030555C303030735C303030695C3030306E5C303030675C3030305C3034305C303030435C3030304E5C3030304E5C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304E5C303030655C303030615C303030725C303030655C303030735C303030745C3030305C3034305C3030304E5C303030655C303030695C303030675C303030685C303030625C3030306F5C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {2.40}{\ignorespaces The curse of dimensionality: limitations of KNN in high-dimensional spaces.}}{85}{figure.caption.82}\protected@file@percent }
\newlabel{fig:chapter2_curse_dimensionality}{{2.40}{85}{The curse of dimensionality: limitations of KNN in high-dimensional spaces}{figure.caption.82}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.12}Using CNN Features for Nearest Neighbor Classification}{85}{subsection.2.6.12}\protected@file@percent }
\abx@aux@cite{0}{devlin2015_imagetocaption}
\abx@aux@segm{0}{0}{devlin2015_imagetocaption}
\BKM@entry{id=83,dest={73756273656374696F6E2E322E362E3133},srcline={687}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030465C303030725C3030306F5C3030306D5C3030305C3034305C3030304E5C303030655C303030615C303030725C303030655C303030735C303030745C3030305C3034305C3030304E5C303030655C303030695C303030675C303030685C303030625C3030306F5C303030725C3030305C3034305C303030745C3030306F5C3030305C3034305C303030415C303030645C303030765C303030615C3030306E5C303030635C303030655C303030645C3030305C3034305C3030304D5C3030304C5C3030305C3034305C303030465C303030725C3030306F5C3030306E5C303030745C303030695C303030655C303030725C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {2.41}{\ignorespaces Nearest Neighbor with CNN features: improved semantic similarity.}}{86}{figure.caption.83}\protected@file@percent }
\newlabel{fig:chapter2_nn_cnn}{{2.41}{86}{Nearest Neighbor with CNN features: improved semantic similarity}{figure.caption.83}{}}
\abx@aux@backref{109}{devlin2015_imagetocaption}{0}{86}{86}
\@writefile{lof}{\contentsline {figure}{\numberline {2.42}{\ignorespaces Nearest Neighbor captioning: retrieving captions from the closest matching image.}}{86}{figure.caption.84}\protected@file@percent }
\newlabel{fig:chapter2_nn_captioning}{{2.42}{86}{Nearest Neighbor captioning: retrieving captions from the closest matching image}{figure.caption.84}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.13}Conclusion: From Nearest Neighbor to Advanced ML Frontiers}{87}{subsection.2.6.13}\protected@file@percent }
\BKM@entry{id=84,dest={636861707465722E33},srcline={3}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030335C3030303A5C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C30303073}
\BKM@entry{id=85,dest={73656374696F6E2E332E31},srcline={9}}{5C3337365C3337375C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C303030735C3030303A5C3030305C3034305C303030415C3030305C3034305C303030465C3030306F5C303030755C3030306E5C303030645C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Lecture 3: Linear Classifiers}{88}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@2}}
\ttl@writefile{ptc}{\ttl@starttoc{default@3}}
\pgfsyspdfmark {pgfid10}{0}{52099153}
\pgfsyspdfmark {pgfid9}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Linear Classifiers: A Foundation for Neural Networks}{88}{section.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Neural networks are constructed from stacked building blocks, much like Lego blocks. Linear classifiers are one of these foundational components.}}{88}{figure.caption.85}\protected@file@percent }
\newlabel{fig:chapter3_lego_blocks}{{3.1}{88}{Neural networks are constructed from stacked building blocks, much like Lego blocks. Linear classifiers are one of these foundational components}{figure.caption.85}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Parametric linear classifier pipeline: The input image is flattened into a vector, multiplied with weights, and added to a bias vector to produce class scores.}}{89}{figure.caption.86}\protected@file@percent }
\newlabel{fig:chapter3_parametric_classifier}{{3.2}{89}{Parametric linear classifier pipeline: The input image is flattened into a vector, multiplied with weights, and added to a bias vector to produce class scores}{figure.caption.86}{}}
\BKM@entry{id=86,dest={73656374696F6E2A2E3837},srcline={64}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030335C3030302E5C303030315C3030302E5C303030315C3030303A5C3030305C3034305C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030525C3030306F5C3030306C5C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C303030425C303030695C303030615C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {subsection}{Enrichment 3.1.1: Understanding the Role of Bias in Linear Classifiers}{91}{section*.87}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Without Bias (\(b=0\)):}{91}{section*.88}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{With Bias (\(b = 3\)):}{91}{section*.89}\protected@file@percent }
\BKM@entry{id=87,dest={73756273656374696F6E2E332E312E32},srcline={133}}{5C3337365C3337375C303030415C3030305C3034305C303030545C3030306F5C303030795C3030305C3034305C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030303A5C3030305C3034305C303030475C303030725C303030615C303030795C303030735C303030635C303030615C3030306C5C303030655C3030305C3034305C303030435C303030615C303030745C3030305C3034305C303030495C3030306D5C303030615C303030675C30303065}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Bias shifts the decision boundary (orange line), enabling correct classification of the two points. Without bias (e.g., green line for the chosen $W$), no line passing through the origin can separate the points.}}{92}{figure.caption.90}\protected@file@percent }
\newlabel{fig:chapter3_bias_example}{{3.3}{92}{Bias shifts the decision boundary (orange line), enabling correct classification of the two points. Without bias (e.g., green line for the chosen $W$), no line passing through the origin can separate the points}{figure.caption.90}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}A Toy Example: Grayscale Cat Image}{92}{subsection.3.1.2}\protected@file@percent }
\BKM@entry{id=88,dest={73756273656374696F6E2E332E312E33},srcline={162}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030425C303030695C303030615C303030735C3030305C3034305C303030545C303030725C303030695C303030635C3030306B}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces A toy example of a grayscale \(2 \times 2\) cat image (Slide 14), stretched into a vector and passed through a linear classifier.}}{93}{figure.caption.91}\protected@file@percent }
\newlabel{fig:chapter3_slide14_toy_example}{{3.4}{93}{A toy example of a grayscale \(2 \times 2\) cat image (Slide 14), stretched into a vector and passed through a linear classifier}{figure.caption.91}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}The Bias Trick}{93}{subsection.3.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces The bias trick applied to the toy cat example: augmenting the image vector with a constant 1 and extending the weight matrix to incorporate the bias.}}{94}{figure.caption.92}\protected@file@percent }
\newlabel{fig:chapter3_bias_trick}{{3.5}{94}{The bias trick applied to the toy cat example: augmenting the image vector with a constant 1 and extending the weight matrix to incorporate the bias}{figure.caption.92}{}}
\BKM@entry{id=89,dest={73656374696F6E2E332E32},srcline={220}}{5C3337365C3337375C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C303030735C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030415C3030306C5C303030675C303030655C303030625C303030725C303030615C303030695C303030635C3030305C3034305C303030565C303030695C303030655C303030775C303030705C3030306F5C303030695C3030306E5C30303074}
\BKM@entry{id=90,dest={73756273656374696F6E2E332E322E31},srcline={224}}{5C3337365C3337375C303030535C303030635C303030615C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030505C303030725C3030306F5C303030705C303030655C303030725C303030745C303030695C303030655C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030495C3030306E5C303030735C303030695C303030675C303030685C303030745C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Linear Classifiers: The Algebraic Viewpoint}{95}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Scaling Properties and Insights}{95}{subsection.3.2.1}\protected@file@percent }
\BKM@entry{id=91,dest={73756273656374696F6E2E332E322E32},srcline={249}}{5C3337365C3337375C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030415C3030306C5C303030675C303030655C303030625C303030725C303030615C3030305C3034305C303030745C3030306F5C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C3030305C3034305C303030495C3030306E5C303030745C303030655C303030725C303030705C303030725C303030655C303030745C303030615C303030625C303030695C3030306C5C303030695C303030745C30303079}
\BKM@entry{id=92,dest={73656374696F6E2E332E33},srcline={262}}{5C3337365C3337375C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C303030735C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C3030305C3034305C303030565C303030695C303030655C303030775C303030705C3030306F5C303030695C3030306E5C30303074}
\BKM@entry{id=93,dest={73756273656374696F6E2E332E332E31},srcline={266}}{5C3337365C3337375C303030545C303030655C3030306D5C303030705C3030306C5C303030615C303030745C303030655C3030305C3034305C3030304D5C303030615C303030745C303030635C303030685C303030695C3030306E5C303030675C3030305C3034305C303030505C303030655C303030725C303030735C303030705C303030655C303030635C303030745C303030695C303030765C30303065}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Scaling effect in linear classifiers: uniform scaling of inputs leads to proportional scaling of output scores, as shown in this cat image example.}}{96}{figure.caption.93}\protected@file@percent }
\newlabel{fig:chapter3_scaling_bias_trick}{{3.6}{96}{Scaling effect in linear classifiers: uniform scaling of inputs leads to proportional scaling of output scores, as shown in this cat image example}{figure.caption.93}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}From Algebra to Visual Interpretability}{96}{subsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Linear Classifiers: The Visual Viewpoint}{96}{section.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Template Matching Perspective}{97}{subsection.3.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Visualizing the rows of the weight matrix \(\mathbf  {W}\) as learned templates for each class.}}{97}{figure.caption.94}\protected@file@percent }
\newlabel{fig:chapter3_template_matching}{{3.7}{97}{Visualizing the rows of the weight matrix \(\mathbf {W}\) as learned templates for each class}{figure.caption.94}{}}
\BKM@entry{id=94,dest={73756273656374696F6E2E332E332E32},srcline={287}}{5C3337365C3337375C303030495C3030306E5C303030745C303030655C303030725C303030705C303030725C303030655C303030745C303030695C3030306E5C303030675C3030305C3034305C303030545C303030655C3030306D5C303030705C3030306C5C303030615C303030745C303030655C30303073}
\BKM@entry{id=95,dest={73756273656374696F6E2E332E332E33},srcline={298}}{5C3337365C3337375C303030505C303030795C303030745C303030685C3030306F5C3030306E5C3030305C3034305C303030435C3030306F5C303030645C303030655C3030305C3034305C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030303A5C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030655C303030645C3030305C3034305C303030545C303030655C3030306D5C303030705C3030306C5C303030615C303030745C303030655C30303073}
\BKM@entry{id=96,dest={73756273656374696F6E2E332E332E34},srcline={327}}{5C3337365C3337375C303030545C303030655C3030306D5C303030705C3030306C5C303030615C303030745C303030655C3030305C3034305C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030303A5C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Interpreting Templates}{98}{subsection.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Python Code Example: Visualizing Learned Templates}{98}{subsection.3.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces The output of the code (building upon NumPy and Matplotlib) visualizes the rows of the weight matrix reshaped into the input image format, enabling inspection of the learned templates.}}{98}{figure.caption.95}\protected@file@percent }
\newlabel{fig:chapter3_visualize_class_templates}{{3.8}{98}{The output of the code (building upon NumPy and Matplotlib) visualizes the rows of the weight matrix reshaped into the input image format, enabling inspection of the learned templates}{figure.caption.95}{}}
\BKM@entry{id=97,dest={73756273656374696F6E2E332E332E35},srcline={341}}{5C3337365C3337375C3030304C5C3030306F5C3030306F5C3030306B5C303030695C3030306E5C303030675C3030305C3034305C303030415C303030685C303030655C303030615C30303064}
\BKM@entry{id=98,dest={73656374696F6E2E332E34},srcline={345}}{5C3337365C3337375C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C303030735C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030475C303030655C3030306F5C3030306D5C303030655C303030745C303030725C303030695C303030635C3030305C3034305C303030565C303030695C303030655C303030775C303030705C3030306F5C303030695C3030306E5C30303074}
\BKM@entry{id=99,dest={73756273656374696F6E2E332E342E31},srcline={349}}{5C3337365C3337375C303030495C3030306D5C303030615C303030675C303030655C303030735C3030305C3034305C303030615C303030735C3030305C3034305C303030485C303030695C303030675C303030685C3030302D5C303030445C303030695C3030306D5C303030655C3030306E5C303030735C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030505C3030306F5C303030695C3030306E5C303030745C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Template Limitations: Multiple Modes}{99}{subsection.3.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces The horse class template demonstrates the limitation of learning a single template for a category with multiple modes.}}{99}{figure.caption.96}\protected@file@percent }
\newlabel{fig:chapter3_multiple_modes}{{3.9}{99}{The horse class template demonstrates the limitation of learning a single template for a category with multiple modes}{figure.caption.96}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.5}Looking Ahead}{99}{subsection.3.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Linear Classifiers: The Geometric Viewpoint}{99}{section.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Images as High-Dimensional Points}{99}{subsection.3.4.1}\protected@file@percent }
\BKM@entry{id=100,dest={73756273656374696F6E2E332E342E32},srcline={364}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Left: Dimensionality-reduced visualization of a dataset. Right: Hyperplanes partitioning a higher-dimensional space into regions for classification.}}{100}{figure.caption.97}\protected@file@percent }
\newlabel{fig:chapter3_geometric_hyperplanes}{{3.10}{100}{Left: Dimensionality-reduced visualization of a dataset. Right: Hyperplanes partitioning a higher-dimensional space into regions for classification}{figure.caption.97}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Limitations of Linear Classifiers}{100}{subsection.3.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Examples of classification problems that linear classifiers cannot solve.}}{100}{figure.caption.98}\protected@file@percent }
\newlabel{fig:chapter3_viewpoint_failures}{{3.11}{100}{Examples of classification problems that linear classifiers cannot solve}{figure.caption.98}{}}
\BKM@entry{id=101,dest={73756273656374696F6E2E332E342E33},srcline={384}}{5C3337365C3337375C303030485C303030695C303030735C303030745C3030306F5C303030725C303030695C303030635C303030615C3030306C5C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030655C303030785C303030745C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030505C303030655C303030725C303030635C303030655C303030705C303030745C303030725C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030585C3030304F5C303030525C3030305C3034305C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=102,dest={73756273656374696F6E2E332E342E34},srcline={397}}{5C3337365C3337375C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030485C303030695C303030675C303030685C3030302D5C303030445C303030695C3030306D5C303030655C3030306E5C303030735C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030655C3030306F5C3030306D5C303030655C303030745C303030725C30303079}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Historical Context: The Perceptron and XOR Limitation}{101}{subsection.3.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces XOR Function: The perceptron can't separate blue \& green regions with a single line.}}{101}{figure.caption.99}\protected@file@percent }
\newlabel{fig:chapter3_xor_limitations}{{3.12}{101}{XOR Function: The perceptron can't separate blue \& green regions with a single line}{figure.caption.99}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.4}Challenges of High-Dimensional Geometry}{101}{subsection.3.4.4}\protected@file@percent }
\BKM@entry{id=103,dest={73656374696F6E2E332E35},srcline={408}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030303A5C3030305C3034305C303030535C303030685C3030306F5C303030725C303030745C303030635C3030306F5C3030306D5C303030695C3030306E5C303030675C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C30303073}
\BKM@entry{id=104,dest={73756273656374696F6E2E332E352E31},srcline={412}}{5C3337365C3337375C303030415C3030306C5C303030675C303030655C303030625C303030725C303030615C303030695C303030635C3030305C3034305C303030565C303030695C303030655C303030775C303030705C3030306F5C303030695C3030306E5C30303074}
\BKM@entry{id=105,dest={73756273656374696F6E2E332E352E32},srcline={419}}{5C3337365C3337375C303030565C303030695C303030735C303030755C303030615C3030306C5C3030305C3034305C303030565C303030695C303030655C303030775C303030705C3030306F5C303030695C3030306E5C30303074}
\BKM@entry{id=106,dest={73756273656374696F6E2E332E352E33},srcline={426}}{5C3337365C3337375C303030475C303030655C3030306F5C3030306D5C303030655C303030745C303030725C303030695C303030635C3030305C3034305C303030565C303030695C303030655C303030775C303030705C3030306F5C303030695C3030306E5C30303074}
\BKM@entry{id=107,dest={73756273656374696F6E2E332E352E34},srcline={433}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C303030735C3030305C3034305C303030415C303030725C303030655C3030306E5C303030275C303030745C3030305C3034305C303030455C3030306E5C3030306F5C303030755C303030675C30303068}
\BKM@entry{id=108,dest={73756273656374696F6E2E332E352E35},srcline={436}}{5C3337365C3337375C303030435C303030685C3030306F5C3030306F5C303030735C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030575C303030655C303030695C303030675C303030685C303030745C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Summary: Shortcomings of Linear Classifiers}{102}{section.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Algebraic Viewpoint}{102}{subsection.3.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Visual Viewpoint}{102}{subsection.3.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Geometric Viewpoint}{102}{subsection.3.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.4}Conclusion: Linear Classifiers Aren't Enough}{102}{subsection.3.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.5}Choosing the Weights for Linear Classifiers}{102}{subsection.3.5.5}\protected@file@percent }
\BKM@entry{id=109,dest={73656374696F6E2E332E36},srcline={447}}{5C3337365C3337375C3030304C5C3030306F5C303030735C303030735C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=110,dest={73756273656374696F6E2E332E362E31},srcline={463}}{5C3337365C3337375C303030435C3030306F5C303030725C303030655C3030305C3034305C303030525C303030655C303030715C303030755C303030695C303030725C303030655C3030306D5C303030655C3030306E5C303030745C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304C5C3030306F5C303030735C303030735C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=111,dest={73756273656374696F6E2E332E362E32},srcline={477}}{5C3337365C3337375C303030445C303030655C303030735C303030695C303030725C303030615C303030625C3030306C5C303030655C3030305C3034305C303030505C303030725C3030306F5C303030705C303030655C303030725C303030745C303030695C303030655C303030735C3030305C3034305C3030305C3035305C303030445C303030655C303030705C303030655C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C3030306F5C3030306E5C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030545C303030615C303030735C3030306B5C3030305C303531}
\BKM@entry{id=112,dest={73756273656374696F6E2E332E362E33},srcline={495}}{5C3337365C3337375C303030435C303030725C3030306F5C303030735C303030735C3030302D5C303030455C3030306E5C303030745C303030725C3030306F5C303030705C303030795C3030305C3034305C3030304C5C3030306F5C303030735C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Loss Functions}{103}{section.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}Core Requirements for Loss Functions}{103}{subsection.3.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}Desirable Properties (Depending on the Task)}{103}{subsection.3.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.3}Cross-Entropy Loss}{104}{subsection.3.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Softmax Function}{104}{section*.100}\protected@file@percent }
\newlabel{subsec:softmax}{{3.6.3}{104}{Softmax Function}{section*.100}{}}
\@writefile{toc}{\contentsline {paragraph}{Advanced Note: Boltzmann Perspective.}{104}{section*.101}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Loss Computation}{104}{section*.102}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example: CIFAR-10 Image Classification}{104}{section*.103}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces Cross-entropy loss computation for a cat image. Softmax normalizes raw scores into probabilities, and the loss is computed by comparing with the ground truth.}}{105}{figure.caption.104}\protected@file@percent }
\newlabel{fig:chapter3_ce_loss_example}{{3.13}{105}{Cross-entropy loss computation for a cat image. Softmax normalizes raw scores into probabilities, and the loss is computed by comparing with the ground truth}{figure.caption.104}{}}
\@writefile{toc}{\contentsline {paragraph}{Properties of Cross-Entropy Loss}{105}{section*.105}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Why "Cross-Entropy"?}{105}{section*.106}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 3.6.3.1: Why Cross-Entropy Uses Logarithms, Not Squared Errors}{106}{section*.107}\protected@file@percent }
\BKM@entry{id=113,dest={73756273656374696F6E2E332E362E34},srcline={733}}{5C3337365C3337375C3030304D5C303030755C3030306C5C303030745C303030695C303030635C3030306C5C303030615C303030735C303030735C3030305C3034305C303030535C303030565C3030304D5C3030305C3034305C3030304C5C3030306F5C303030735C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.4}Multiclass SVM Loss}{109}{subsection.3.6.4}\protected@file@percent }
\newlabel{subsec:chpater3_hinge_loss}{{3.6.4}{109}{Multiclass SVM Loss}{subsection.3.6.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Loss Definition}{109}{section*.108}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Example Computation}{109}{section*.109}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Loss for the Cat Image}{110}{section*.110}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces SVM loss computation for the cat image. Each term corresponds to a margin violation for an incorrect class.}}{110}{figure.caption.111}\protected@file@percent }
\newlabel{fig:chapter3_svm_loss_cat}{{3.14}{110}{SVM loss computation for the cat image. Each term corresponds to a margin violation for an incorrect class}{figure.caption.111}{}}
\@writefile{toc}{\contentsline {paragraph}{Loss for the Car Image}{110}{section*.112}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces SVM loss computation for the car image. As the car score exceeds the rest by more than the margin, the loss is 0.}}{111}{figure.caption.113}\protected@file@percent }
\newlabel{fig:chapter3_svm_loss_car}{{3.15}{111}{SVM loss computation for the car image. As the car score exceeds the rest by more than the margin, the loss is 0}{figure.caption.113}{}}
\@writefile{toc}{\contentsline {paragraph}{Loss for the Frog Image}{111}{section*.114}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.16}{\ignorespaces SVM loss computation for the frog image. With the correct class score being the lowest, the loss is the largest.}}{111}{figure.caption.115}\protected@file@percent }
\newlabel{fig:chapter3_svm_loss_frog}{{3.16}{111}{SVM loss computation for the frog image. With the correct class score being the lowest, the loss is the largest}{figure.caption.115}{}}
\BKM@entry{id=114,dest={73756273656374696F6E2E332E362E35},srcline={844}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C303030735C3030306F5C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C303030435C303030725C3030306F5C303030735C303030735C3030302D5C303030455C3030306E5C303030745C303030725C3030306F5C303030705C303030795C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C303030635C3030306C5C303030615C303030735C303030735C3030305C3034305C303030535C303030565C3030304D5C3030305C3034305C3030304C5C3030306F5C303030735C303030735C303030655C30303073}
\@writefile{toc}{\contentsline {paragraph}{Total Loss}{112}{section*.116}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.17}{\ignorespaces Total loss computed as the average of losses over the three images.}}{112}{figure.caption.117}\protected@file@percent }
\newlabel{fig:chapter3_svm_total_loss}{{3.17}{112}{Total loss computed as the average of losses over the three images}{figure.caption.117}{}}
\@writefile{toc}{\contentsline {subsubsection}{Key Questions and Insights}{112}{section*.118}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.5}Comparison of Cross-Entropy and Multiclass SVM Losses}{112}{subsection.3.6.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.18}{\ignorespaces Impact of scaling on SVM and cross-entropy loss. The CE loss decreases, while the SVM loss remains unchanged.}}{113}{figure.caption.119}\protected@file@percent }
\newlabel{fig:chapter3_loss_comparison_scaling}{{3.18}{113}{Impact of scaling on SVM and cross-entropy loss. The CE loss decreases, while the SVM loss remains unchanged}{figure.caption.119}{}}
\@writefile{toc}{\contentsline {subsubsection}{Debugging with Initial Loss Values}{113}{section*.120}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Conclusion: SVM, Cross Entropy, and the Evolving Landscape of Loss Functions}{114}{section*.121}\protected@file@percent }
\BKM@entry{id=115,dest={73656374696F6E2A2E313232},srcline={928}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030335C3030302E5C303030365C3030302E5C303030365C3030303A5C3030305C3034305C303030415C303030645C303030645C303030695C303030745C303030695C303030765C303030655C3030305C3034305C3030304D5C303030615C303030725C303030675C303030695C3030306E5C3030305C3034305C303030535C3030306F5C303030665C303030745C3030306D5C303030615C303030785C3030305C3034305C3030305C3035305C303030415C3030304D5C3030302D5C303030535C3030306F5C303030665C303030745C3030306D5C303030615C303030785C3030305C303531}
\abx@aux@cite{0}{liu2017_sphereface}
\abx@aux@segm{0}{0}{liu2017_sphereface}
\@writefile{toc}{\contentsline {subsection}{Enrichment 3.6.6: Additive Margin Softmax (AM-Softmax)}{115}{section*.122}\protected@file@percent }
\newlabel{enr:chapter3_amsoftmax}{{3.6.6}{115}{\color {ocre}Enrichment \thesubsection : Additive Margin Softmax (AM-Softmax)}{section*.122}{}}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{115}{section*.123}\protected@file@percent }
\newlabel{subsec:chapter3_amsoftmax_motivation}{{3.6.6}{115}{Motivation}{section*.123}{}}
\@writefile{toc}{\contentsline {paragraph}{Why angles on a hypersphere}{115}{section*.124}\protected@file@percent }
\newlabel{eq:chapter3_amsoftmax_angles}{{3.1}{115}{Why angles on a hypersphere}{equation.3.1}{}}
\@writefile{toc}{\contentsline {paragraph}{A-Softmax (SphereFace): multiplicative angular margin and its pitfalls}{115}{section*.125}\protected@file@percent }
\abx@aux@backref{110}{liu2017_sphereface}{0}{115}{115}
\newlabel{eq:chapter3_amsoftmax_asf_piecewise_recap}{{3.2}{115}{A-Softmax (SphereFace): multiplicative angular margin and its pitfalls}{equation.3.2}{}}
\newlabel{eq:chapter3_amsoftmax_asf_lambda_recap}{{3.3}{115}{A-Softmax (SphereFace): multiplicative angular margin and its pitfalls}{equation.3.3}{}}
\abx@aux@cite{0}{wang2018_amsoftmax}
\abx@aux@segm{0}{0}{wang2018_amsoftmax}
\abx@aux@cite{0}{wang2018_amsoftmax}
\abx@aux@segm{0}{0}{wang2018_amsoftmax}
\@writefile{toc}{\contentsline {paragraph}{A-Softmax in detail: multiplicative transform $m$ and blending $\lambda $}{116}{section*.126}\protected@file@percent }
\newlabel{eq:chapter3_amsoftmax_asf_piecewise_recap_formula}{{3.4}{116}{A-Softmax in detail: multiplicative transform $m$ and blending $\lambda $}{equation.3.4}{}}
\newlabel{eq:chapter3_amsoftmax_asf_lambda_recap_formula}{{3.5}{116}{A-Softmax in detail: multiplicative transform $m$ and blending $\lambda $}{equation.3.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.19}{\ignorespaces Target-logit $\psi (\theta )$ for conventional softmax, A-Softmax with several $(m,\lambda )$, and AM-Softmax with $m=0.35$. AM-Softmax produces a smooth, strictly monotone target on $[0^\circ ,90^\circ ]$, which covers most practical feature–prototype angles. Figure courtesy of the AM-Softmax authors \blx@tocontentsinit {0}\cite {wang2018_amsoftmax}.}}{117}{figure.caption.127}\protected@file@percent }
\abx@aux@backref{112}{wang2018_amsoftmax}{0}{117}{117}
\newlabel{fig:chapter3_amsoftmax_psi}{{3.19}{117}{Target-logit $\psi (\theta )$ for conventional softmax, A-Softmax with several $(m,\lambda )$, and AM-Softmax with $m=0.35$. AM-Softmax produces a smooth, strictly monotone target on $[0^\circ ,90^\circ ]$, which covers most practical feature–prototype angles. Figure courtesy of the AM-Softmax authors \cite {wang2018_amsoftmax}}{figure.caption.127}{}}
\@writefile{toc}{\contentsline {subsubsection}{Method}{117}{section*.128}\protected@file@percent }
\newlabel{subsec:chapter3_amsoftmax_method_refined}{{3.6.6}{117}{Method}{section*.128}{}}
\@writefile{toc}{\contentsline {paragraph}{Normalization and rescaling}{117}{section*.129}\protected@file@percent }
\newlabel{eq:chapter3_amsoftmax_norm_scale_refined}{{3.6}{117}{Normalization and rescaling}{equation.3.6}{}}
\@writefile{toc}{\contentsline {paragraph}{AM-Softmax in detail: subtracting a fixed cosine margin $\boldsymbol  {m}$}{117}{section*.130}\protected@file@percent }
\newlabel{eq:chapter3_amsoftmax_additive}{{3.7}{117}{AM-Softmax in detail: subtracting a fixed cosine margin $\boldsymbol {m}$}{equation.3.7}{}}
\newlabel{eq:chapter3_amsoftmax_boundary}{{3.8}{117}{AM-Softmax in detail: subtracting a fixed cosine margin $\boldsymbol {m}$}{equation.3.8}{}}
\abx@aux@cite{0}{wang2018_amsoftmax}
\abx@aux@segm{0}{0}{wang2018_amsoftmax}
\abx@aux@cite{0}{wang2018_amsoftmax}
\abx@aux@segm{0}{0}{wang2018_amsoftmax}
\abx@aux@cite{0}{wang2018_amsoftmax}
\abx@aux@segm{0}{0}{wang2018_amsoftmax}
\abx@aux@cite{0}{wang2018_amsoftmax}
\abx@aux@segm{0}{0}{wang2018_amsoftmax}
\@writefile{toc}{\contentsline {paragraph}{A-Softmax versus AM-Softmax at a glance}{118}{section*.131}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.20}{\ignorespaces Conceptual comparison between the original softmax and AM-Softmax. A-Softmax imposes a multiplicative, unfixed angular margin controlled by $m$ (with optional blending $\lambda $), whereas AM-Softmax introduces a fixed hard margin in cosine space via a single $m$, directly promoting compact intra-class clusters and larger inter-class separation. Figure courtesy of the AM-Softmax authors \blx@tocontentsinit {0}\cite {wang2018_amsoftmax}.}}{118}{figure.caption.132}\protected@file@percent }
\abx@aux@backref{114}{wang2018_amsoftmax}{0}{118}{118}
\newlabel{fig:chapter3_amsoftmax_overview}{{3.20}{118}{Conceptual comparison between the original softmax and AM-Softmax. A-Softmax imposes a multiplicative, unfixed angular margin controlled by $m$ (with optional blending $\lambda $), whereas AM-Softmax introduces a fixed hard margin in cosine space via a single $m$, directly promoting compact intra-class clusters and larger inter-class separation. Figure courtesy of the AM-Softmax authors \cite {wang2018_amsoftmax}}{figure.caption.132}{}}
\@writefile{toc}{\contentsline {paragraph}{Why it helps and how it looks in feature space}{118}{section*.133}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.21}{\ignorespaces Feature distributions on the unit sphere across several loss functions in the authors’ visualization. AM-Softmax concentrates samples into compact class-aligned cones and enlarges inter-class gaps relative to conventional softmax and earlier margin losses. Figure courtesy of the AM-Softmax authors \blx@tocontentsinit {0}\cite {wang2018_amsoftmax}.}}{118}{figure.caption.134}\protected@file@percent }
\abx@aux@backref{116}{wang2018_amsoftmax}{0}{118}{118}
\newlabel{fig:chapter3_amsoftmax_featuredist}{{3.21}{118}{Feature distributions on the unit sphere across several loss functions in the authors’ visualization. AM-Softmax concentrates samples into compact class-aligned cones and enlarges inter-class gaps relative to conventional softmax and earlier margin losses. Figure courtesy of the AM-Softmax authors \cite {wang2018_amsoftmax}}{figure.caption.134}{}}
\abx@aux@cite{0}{wang2018_amsoftmax}
\abx@aux@segm{0}{0}{wang2018_amsoftmax}
\abx@aux@cite{0}{wang2018_amsoftmax}
\abx@aux@segm{0}{0}{wang2018_amsoftmax}
\@writefile{toc}{\contentsline {paragraph}{Gradient behavior and the role of normalization}{119}{section*.135}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.22}{\ignorespaces Feature-gradient norm versus feature norm for softmax with and without feature normalization (FN). Without FN, small-norm features can cause large, unstable gradients; with FN (e.g., $s=30$), gradients are well-controlled and primarily adjust directions. Figure courtesy of the AM-Softmax authors \blx@tocontentsinit {0}\cite {wang2018_amsoftmax}.}}{119}{figure.caption.136}\protected@file@percent }
\abx@aux@backref{118}{wang2018_amsoftmax}{0}{119}{119}
\newlabel{fig:chapter3_amsoftmax_gradnorm}{{3.22}{119}{Feature-gradient norm versus feature norm for softmax with and without feature normalization (FN). Without FN, small-norm features can cause large, unstable gradients; with FN (e.g., $s=30$), gradients are well-controlled and primarily adjust directions. Figure courtesy of the AM-Softmax authors \cite {wang2018_amsoftmax}}{figure.caption.136}{}}
\@writefile{toc}{\contentsline {paragraph}{Implementation pattern}{119}{section*.137}\protected@file@percent }
\abx@aux@cite{0}{wang2018_amsoftmax}
\abx@aux@segm{0}{0}{wang2018_amsoftmax}
\abx@aux@cite{0}{wang2018_amsoftmax}
\abx@aux@segm{0}{0}{wang2018_amsoftmax}
\abx@aux@cite{0}{wang2018_amsoftmax}
\abx@aux@segm{0}{0}{wang2018_amsoftmax}
\abx@aux@cite{0}{wang2018_amsoftmax}
\abx@aux@segm{0}{0}{wang2018_amsoftmax}
\abx@aux@cite{0}{wang2018_amsoftmax}
\abx@aux@segm{0}{0}{wang2018_amsoftmax}
\abx@aux@cite{0}{wang2018_amsoftmax}
\abx@aux@segm{0}{0}{wang2018_amsoftmax}
\abx@aux@cite{0}{wang2018_amsoftmax}
\abx@aux@segm{0}{0}{wang2018_amsoftmax}
\newlabel{subsec:chapter3_amsoftmax_experiments}{{3.6.6}{120}{Experiments and ablations}{section*.138}{}}
\@writefile{toc}{\contentsline {paragraph}{Benchmarks and observations}{120}{section*.139}\protected@file@percent }
\abx@aux@backref{119}{wang2018_amsoftmax}{0}{120}{120}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Verification and identification performance for AM-Softmax versus baselines (numbers reported by \blx@tocontentsinit {0}\cite {wang2018_amsoftmax}). Among AM-Softmax rows, the best entry per column is bolded.}}{120}{table.caption.140}\protected@file@percent }
\abx@aux@backref{121}{wang2018_amsoftmax}{0}{120}{120}
\newlabel{tab:chapter3_amsoftmax_main}{{3.1}{120}{Verification and identification performance for AM-Softmax versus baselines (numbers reported by \cite {wang2018_amsoftmax}). Among AM-Softmax rows, the best entry per column is bolded}{table.caption.140}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Effect of overlap removal on a modified ResNet-20, as reported by \blx@tocontentsinit {0}\cite {wang2018_amsoftmax}}}{120}{table.caption.141}\protected@file@percent }
\abx@aux@backref{123}{wang2018_amsoftmax}{0}{120}{120}
\newlabel{tab:chapter3_amsoftmax_overlap}{{3.2}{120}{Effect of overlap removal on a modified ResNet-20, as reported by \cite {wang2018_amsoftmax}}{table.caption.141}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.23}{\ignorespaces CMC (left) and ROC (right) on MegaFace with $10^6$ distractors comparing several losses. AM-Softmax achieves strong identification and verification performance. Note that Center Loss and NormFace curves use a deeper ResNet-28 backbone, whereas others use ResNet-20, as clarified by the AM-Softmax authors \blx@tocontentsinit {0}\cite {wang2018_amsoftmax}.}}{120}{figure.caption.142}\protected@file@percent }
\abx@aux@backref{125}{wang2018_amsoftmax}{0}{120}{120}
\newlabel{fig:chapter3_amsoftmax_cmc}{{3.23}{120}{CMC (left) and ROC (right) on MegaFace with $10^6$ distractors comparing several losses. AM-Softmax achieves strong identification and verification performance. Note that Center Loss and NormFace curves use a deeper ResNet-28 backbone, whereas others use ResNet-20, as clarified by the AM-Softmax authors \cite {wang2018_amsoftmax}}{figure.caption.142}{}}
\newlabel{subsec:chapter3_amsoftmax_limitations}{{3.6.6}{121}{Where AM-Softmax helps, and where it does not}{section*.143}{}}
\@writefile{toc}{\contentsline {paragraph}{Good fit}{121}{section*.144}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Limitations and cautions}{121}{section*.145}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Practical notes on tuning $s$ and $m$}{122}{section*.146}\protected@file@percent }
\BKM@entry{id=116,dest={636861707465722E34},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030345C3030303A5C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3034365C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=117,dest={73656374696F6E2E342E31},srcline={9}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{yenigun_overfitting}
\abx@aux@segm{0}{0}{yenigun_overfitting}
\abx@aux@cite{0}{yenigun_overfitting}
\abx@aux@segm{0}{0}{yenigun_overfitting}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Lecture 4: Regularization \& Optimization}{123}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@3}}
\ttl@writefile{ptc}{\ttl@starttoc{default@4}}
\pgfsyspdfmark {pgfid14}{0}{52099153}
\pgfsyspdfmark {pgfid13}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction to Regularization}{123}{section.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Illustration of underfitting, good fitting, and overfitting in classification and regression tasks \blx@tocontentsinit {0}\cite {yenigun_overfitting}. Good regularization aims to strike the balance between underfitting and overfitting.}}{123}{figure.caption.147}\protected@file@percent }
\abx@aux@backref{127}{yenigun_overfitting}{0}{123}{123}
\newlabel{fig:chapter4_overfitting_underfitting}{{4.1}{123}{Illustration of underfitting, good fitting, and overfitting in classification and regression tasks \cite {yenigun_overfitting}. Good regularization aims to strike the balance between underfitting and overfitting}{figure.caption.147}{}}
\BKM@entry{id=118,dest={73756273656374696F6E2E342E312E31},srcline={30}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030695C303030735C3030305C3034305C303030555C303030735C303030655C303030645C3030303F}
\BKM@entry{id=119,dest={73756273656374696F6E2E342E312E32},srcline={44}}{5C3337365C3337375C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030535C303030695C3030306D5C303030705C3030306C5C303030655C303030725C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030735C3030305C3034305C303030415C303030725C303030655C3030305C3034305C303030505C303030725C303030655C303030665C303030655C303030725C303030725C303030655C30303064}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}How Regularization is Used?}{124}{subsection.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Regularization: Simpler Models Are Preferred}{124}{subsection.4.1.2}\protected@file@percent }
\BKM@entry{id=120,dest={73656374696F6E2E342E32},srcline={50}}{5C3337365C3337375C303030545C303030795C303030705C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C3030304C5C303030315C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C30303032}
\BKM@entry{id=121,dest={73756273656374696F6E2E342E322E31},srcline={52}}{5C3337365C3337375C3030304C5C303030315C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C3030304C5C303030615C303030735C303030735C3030306F5C3030305C303531}
\BKM@entry{id=122,dest={73756273656374696F6E2E342E322E32},srcline={87}}{5C3337365C3337375C3030304C5C303030325C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030525C303030695C303030645C303030675C303030655C3030305C303531}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Types of Regularization: L1 and L2}{125}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}L1 Regularization (Lasso)}{125}{subsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}L2 Regularization (Ridge)}{125}{subsection.4.2.2}\protected@file@percent }
\newlabel{subsec:L2_Reg}{{4.2.2}{125}{L2 Regularization (Ridge)}{subsection.4.2.2}{}}
\BKM@entry{id=123,dest={73756273656374696F6E2E342E322E33},srcline={126}}{5C3337365C3337375C303030435C303030685C3030306F5C3030306F5C303030735C303030695C3030306E5C303030675C3030305C3034305C303030425C303030655C303030745C303030775C303030655C303030655C3030306E5C3030305C3034305C3030304C5C303030315C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C303030325C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=124,dest={73656374696F6E2A2E313438},srcline={142}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030345C3030302E5C303030325C3030302E5C303030345C3030303A5C3030305C3034305C303030435C303030615C3030306E5C3030305C3034305C303030575C303030655C3030305C3034305C303030435C3030306F5C3030306D5C303030625C303030695C3030306E5C303030655C3030305C3034305C3030304C5C303030315C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C303030325C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030303F}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Choosing Between L1 and L2 Regularization}{126}{subsection.4.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 4.2.4: Can We Combine L1 and L2 Regularization?}{126}{section*.148}\protected@file@percent }
\BKM@entry{id=125,dest={73756273656374696F6E2E342E322E35},srcline={185}}{5C3337365C3337375C303030455C303030785C303030705C303030725C303030655C303030735C303030735C303030695C3030306E5C303030675C3030305C3034305C303030505C303030725C303030655C303030665C303030655C303030725C303030655C3030306E5C303030635C303030655C303030735C3030305C3034305C303030545C303030685C303030725C3030306F5C303030755C303030675C303030685C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {paragraph}{When to Use Elastic Net?}{127}{section*.149}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{When Not to Use Elastic Net?}{127}{section*.150}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary:}{127}{section*.151}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.5}Expressing Preferences Through Regularization}{127}{subsection.4.2.5}\protected@file@percent }
\BKM@entry{id=126,dest={73656374696F6E2E342E33},srcline={198}}{5C3337365C3337375C303030495C3030306D5C303030705C303030615C303030635C303030745C3030305C3034305C3030306F5C303030665C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030535C303030635C303030615C3030306C5C303030695C3030306E5C303030675C3030305C3034305C3030306F5C3030306E5C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=127,dest={73756273656374696F6E2E342E332E31},srcline={209}}{5C3337365C3337375C303030505C303030725C303030615C303030635C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C303030495C3030306D5C303030705C3030306C5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=128,dest={73756273656374696F6E2E342E332E32},srcline={212}}{5C3337365C3337375C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030303A5C3030305C3034305C303030525C303030655C303030735C303030635C303030615C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C303030615C303030735C303030735C3030306F5C3030305C3034305C303030525C303030655C303030675C303030725C303030655C303030735C303030735C303030695C3030306F5C3030306E5C3030302E}
\BKM@entry{id=129,dest={73656374696F6E2E342E34},srcline={215}}{5C3337365C3337375C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C303030735C3030305C3034305C303030615C3030305C3034305C303030435C303030615C303030745C303030615C3030306C5C303030795C303030735C303030745C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030425C303030655C303030745C303030745C303030655C303030725C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=130,dest={73756273656374696F6E2E342E342E31},srcline={219}}{5C3337365C3337375C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C303030735C3030305C3034305C303030505C303030615C303030725C303030745C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=131,dest={73756273656374696F6E2E342E342E32},srcline={233}}{5C3337365C3337375C303030415C303030755C303030675C3030306D5C303030655C3030306E5C303030745C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C3030304C5C3030306F5C303030735C303030735C3030305C3034305C303030535C303030755C303030725C303030665C303030615C303030635C303030655C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030435C303030755C303030725C303030765C303030615C303030745C303030755C303030725C30303065}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Impact of Feature Scaling on Regularization}{128}{section.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Practical Implication}{128}{subsection.4.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Example: Rescaling and Lasso Regression.}{128}{subsection.4.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Regularization as a Catalyst for Better Optimization}{128}{section.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Regularization as Part of Optimization}{128}{subsection.4.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Augmenting the Loss Surface with Curvature}{128}{subsection.4.4.2}\protected@file@percent }
\BKM@entry{id=132,dest={73756273656374696F6E2E342E342E33},srcline={244}}{5C3337365C3337375C3030304D5C303030695C303030745C303030695C303030675C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030495C3030306E5C303030735C303030745C303030615C303030625C303030695C3030306C5C303030695C303030745C303030795C3030305C3034305C303030695C3030306E5C3030305C3034305C303030485C303030695C303030675C303030685C3030305C3034305C303030445C303030695C3030306D5C303030655C3030306E5C303030735C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=133,dest={73756273656374696F6E2E342E342E34},srcline={247}}{5C3337365C3337375C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306E5C303030645C303030695C303030745C303030695C3030306F5C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030465C303030615C303030735C303030745C303030655C303030725C3030305C3034305C303030435C3030306F5C3030306E5C303030765C303030655C303030725C303030675C303030655C3030306E5C303030635C30303065}
\BKM@entry{id=134,dest={73656374696F6E2E342E35},srcline={254}}{5C3337365C3337375C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030545C303030725C303030615C303030765C303030655C303030725C303030735C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C3030304C5C3030306F5C303030735C303030735C3030305C3034305C3030304C5C303030615C3030306E5C303030645C303030735C303030635C303030615C303030705C30303065}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Mitigating Instability in High Dimensions}{129}{subsection.4.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.4}Improving Conditioning for Faster Convergence}{129}{subsection.4.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Optimization: Traversing the Loss Landscape}{129}{section.4.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces The loss landscape. Each point corresponds to a weight matrix, and the height represents its corresponding loss value.}}{129}{figure.caption.152}\protected@file@percent }
\newlabel{fig:chapter4_landscape}{{4.2}{129}{The loss landscape. Each point corresponds to a weight matrix, and the height represents its corresponding loss value}{figure.caption.152}{}}
\BKM@entry{id=135,dest={73756273656374696F6E2E342E352E31},srcline={270}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C3030304C5C3030306F5C303030735C303030735C3030305C3034305C3030304C5C303030615C3030306E5C303030645C303030735C303030635C303030615C303030705C303030655C3030305C3034305C303030495C3030306E5C303030745C303030755C303030695C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=136,dest={73656374696F6E2A2E313534},srcline={286}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030345C3030302E5C303030355C3030302E5C303030325C3030303A5C3030305C3034305C303030575C303030685C303030795C3030305C3034305C303030455C303030785C303030705C3030306C5C303030695C303030635C303030695C303030745C3030305C3034305C303030415C3030306E5C303030615C3030306C5C303030795C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C303030535C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030415C303030725C303030655C3030305C3034305C3030304F5C303030665C303030745C303030655C3030306E5C3030305C3034305C303030495C3030306D5C303030705C303030725C303030615C303030635C303030745C303030695C303030635C303030615C3030306C}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}The Loss Landscape Intuition}{130}{subsection.4.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Traversing the loss landscape toward the minimum. The person starts at a random point and follows a path downwards.}}{130}{figure.caption.153}\protected@file@percent }
\newlabel{fig:chapter4_traversal}{{4.3}{130}{Traversing the loss landscape toward the minimum. The person starts at a random point and follows a path downwards}{figure.caption.153}{}}
\@writefile{toc}{\contentsline {subsection}{Enrichment 4.5.2: Why Explicit Analytical Solutions Are Often Impractical}{130}{section*.154}\protected@file@percent }
\newlabel{enrichment:why_analytical_impractical}{{4.5.2}{130}{\color {ocre}Enrichment \thesubsection : Why Explicit Analytical Solutions Are Often Impractical}{section*.154}{}}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 4.5.2.1: High Dimensionality}{130}{section*.155}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 4.5.2.2: Non-Convexity of the Loss Landscape}{130}{section*.156}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 4.5.2.3: Complexity of Regularization Terms}{131}{section*.157}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 4.5.2.4: Lack of Generalizability and Flexibility}{131}{section*.158}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 4.5.2.5: Memory and Computational Cost}{131}{section*.159}\protected@file@percent }
\BKM@entry{id=137,dest={73756273656374696F6E2E342E352E33},srcline={316}}{5C3337365C3337375C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030495C303030645C303030655C303030615C3030305C3034305C3030305C3034335C303030315C3030303A5C3030305C3034305C303030525C303030615C3030306E5C303030645C3030306F5C3030306D5C3030305C3034305C303030535C303030655C303030615C303030725C303030635C30303068}
\BKM@entry{id=138,dest={73756273656374696F6E2E342E352E34},srcline={328}}{5C3337365C3337375C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030495C303030645C303030655C303030615C3030305C3034305C3030305C3034335C303030325C3030303A5C3030305C3034305C303030465C3030306F5C3030306C5C3030306C5C3030306F5C303030775C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030535C3030306C5C3030306F5C303030705C30303065}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.3}Optimization Idea \#1: Random Search}{132}{subsection.4.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Random search: A naive optimization approach.}}{132}{figure.caption.160}\protected@file@percent }
\newlabel{fig:chapter4_random_search}{{4.4}{132}{Random search: A naive optimization approach}{figure.caption.160}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.4}Optimization Idea \#2: Following the Slope}{132}{subsection.4.5.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Following the slope to descend the landscape.}}{132}{figure.caption.161}\protected@file@percent }
\newlabel{fig:chapter4_following_slope}{{4.5}{132}{Following the slope to descend the landscape}{figure.caption.161}{}}
\BKM@entry{id=139,dest={73756273656374696F6E2E342E352E35},srcline={342}}{5C3337365C3337375C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C303030735C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C3030304D5C303030615C303030745C303030685C303030655C3030306D5C303030615C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C303030425C303030615C303030735C303030695C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.5}Gradients: The Mathematical Basis}{133}{subsection.4.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Why Does the Gradient Point to the Steepest Ascent?}{133}{section*.162}\protected@file@percent }
\BKM@entry{id=140,dest={73656374696F6E2E342E36},srcline={420}}{5C3337365C3337375C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030655C303030735C303030635C303030655C3030306E5C30303074}
\@writefile{toc}{\contentsline {subsubsection}{Why Does the Negative Gradient Indicate the Steepest Descent?}{134}{section*.163}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces The gradient \( \nabla L(w) \) points to the steepest ascent, while \( -\nabla L(w) \) leads to the steepest descent.}}{134}{figure.caption.164}\protected@file@percent }
\newlabel{fig:chapter4_gradient_steepest_directions}{{4.6}{134}{The gradient \( \nabla L(w) \) points to the steepest ascent, while \( -\nabla L(w) \) leads to the steepest descent}{figure.caption.164}{}}
\BKM@entry{id=141,dest={73756273656374696F6E2E342E362E31},srcline={424}}{5C3337365C3337375C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}From Gradient Computation to Gradient Descent}{135}{section.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}Gradient Computation Methods}{135}{subsection.4.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Numerical Gradient: Approximating Gradients via Finite Differences}{135}{section*.165}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Process:}{135}{section*.166}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Numerical Gradient: Computing the slope of \( L(W) \) with respect to a perturbed element of \( W \).}}{135}{figure.caption.167}\protected@file@percent }
\newlabel{fig:chapter4_numeric_gradient}{{4.7}{135}{Numerical Gradient: Computing the slope of \( L(W) \) with respect to a perturbed element of \( W \)}{figure.caption.167}{}}
\BKM@entry{id=142,dest={73756273656374696F6E2E342E362E32},srcline={484}}{5C3337365C3337375C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030655C303030735C303030635C303030655C3030306E5C303030745C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030495C303030745C303030655C303030725C303030615C303030745C303030695C303030765C303030655C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030415C3030306C5C303030675C3030306F5C303030725C303030695C303030745C303030685C3030306D}
\@writefile{toc}{\contentsline {paragraph}{Advantages:}{136}{section*.168}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Disadvantages:}{136}{section*.169}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Analytical Gradient: Exact Gradients via Calculus}{136}{section*.170}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Analytical Gradient: Exact computation of gradients via calculus.}}{136}{figure.caption.171}\protected@file@percent }
\newlabel{fig:chapter4_analytical_gradient}{{4.8}{136}{Analytical Gradient: Exact computation of gradients via calculus}{figure.caption.171}{}}
\@writefile{toc}{\contentsline {paragraph}{Advantages:}{136}{section*.172}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Relation to Gradient Descent:}{136}{section*.173}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.2}Gradient Descent: The Iterative Optimization Algorithm}{137}{subsection.4.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Motivation and Concept}{137}{section*.174}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Steps of Gradient Descent:}{137}{section*.175}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces Gradient Descent: Iterative optimization using gradient updates.}}{137}{figure.caption.176}\protected@file@percent }
\newlabel{fig:chapter4_gradient_descent}{{4.9}{137}{Gradient Descent: Iterative optimization using gradient updates}{figure.caption.176}{}}
\@writefile{toc}{\contentsline {subsubsection}{Hyperparameters of Gradient Descent}{137}{section*.177}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{1. Learning Rate (\( \eta \)):}{137}{section*.178}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. Weight Initialization:}{137}{section*.179}\protected@file@percent }
\BKM@entry{id=143,dest={73656374696F6E2E342E37},srcline={531}}{5C3337365C3337375C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030655C303030735C303030635C303030655C3030306E5C30303074}
\BKM@entry{id=144,dest={73756273656374696F6E2E342E372E31},srcline={533}}{5C3337365C3337375C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030655C303030735C303030635C303030655C3030306E5C303030745C3030305C3034305C303030545C303030685C303030725C3030306F5C303030755C303030675C303030685C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=145,dest={73756273656374696F6E2E342E372E32},srcline={548}}{5C3337365C3337375C303030505C303030725C3030306F5C303030705C303030655C303030725C303030745C303030695C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030655C303030735C303030635C303030655C3030306E5C30303074}
\@writefile{toc}{\contentsline {paragraph}{3. Stopping Criterion:}{138}{section*.180}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Visualizing Gradient Descent}{138}{section.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.1}Understanding Gradient Descent Through Visualization}{138}{subsection.4.7.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces Visualization of Gradient Descent Using a Contour Plot. The path starts at a high-loss region (blue) and iteratively moves toward a lower-loss region (red).}}{138}{figure.caption.181}\protected@file@percent }
\newlabel{fig:chapter4_gradient_descent_contour}{{4.10}{138}{Visualization of Gradient Descent Using a Contour Plot. The path starts at a high-loss region (blue) and iteratively moves toward a lower-loss region (red)}{figure.caption.181}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.2}Properties of Gradient Descent}{138}{subsection.4.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Curved Paths Toward the Minimum}{138}{section*.182}\protected@file@percent }
\BKM@entry{id=146,dest={73756273656374696F6E2E342E372E33},srcline={570}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030655C303030735C303030635C303030655C3030306E5C303030745C3030305C3034305C3030304D5C3030306F5C303030765C303030655C303030735C3030305C3034305C303030415C3030306C5C3030306C5C3030305C3034305C303030505C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C303030735C3030305C3034305C303030545C3030306F5C303030675C303030655C303030745C303030685C303030655C30303072}
\@writefile{toc}{\contentsline {subsubsection}{Slowing Down Near the Minimum}{139}{section*.183}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.3}Why Gradient Descent Moves \emph  {All} Parameters Together}{139}{subsection.4.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The gradient is one \( d \)-dimensional arrow}{139}{section*.184}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Axis-aligned moves may crawl or diverge}{139}{section*.185}\protected@file@percent }
\BKM@entry{id=147,dest={73756273656374696F6E2E342E372E34},srcline={666}}{5C3337365C3337375C303030425C303030615C303030745C303030635C303030685C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030655C303030735C303030635C303030655C3030306E5C30303074}
\BKM@entry{id=148,dest={73656374696F6E2E342E38},srcline={677}}{5C3337365C3337375C303030535C303030745C3030306F5C303030635C303030685C303030615C303030735C303030745C303030695C303030635C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030655C303030735C303030635C303030655C3030306E5C303030745C3030305C3034305C3030305C3035305C303030535C303030475C303030445C3030305C303531}
\BKM@entry{id=149,dest={73756273656374696F6E2E342E382E31},srcline={679}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030535C303030745C3030306F5C303030635C303030685C303030615C303030735C303030745C303030695C303030635C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030655C303030735C303030635C303030655C3030306E5C30303074}
\@writefile{toc}{\contentsline {paragraph}{When does coordinate descent shine?}{140}{section*.186}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Take-away}{140}{section*.187}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7.4}Batch Gradient Descent}{140}{subsection.4.7.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.8}Stochastic Gradient Descent (SGD)}{140}{section.4.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.1}Introduction to Stochastic Gradient Descent}{140}{subsection.4.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Minibatch Gradient Computation}{141}{section*.188}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces Stochastic Gradient Descent: Leveraging minibatches to approximate loss and gradients.}}{141}{figure.caption.189}\protected@file@percent }
\newlabel{fig:chapter4_sgd_intro}{{4.11}{141}{Stochastic Gradient Descent: Leveraging minibatches to approximate loss and gradients}{figure.caption.189}{}}
\@writefile{toc}{\contentsline {subsubsection}{Data Sampling and Epochs}{141}{section*.190}\protected@file@percent }
\BKM@entry{id=150,dest={73756273656374696F6E2E342E382E32},srcline={717}}{5C3337365C3337375C303030415C303030645C303030765C303030615C3030306E5C303030745C303030615C303030675C303030655C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030535C303030475C30303044}
\abx@aux@cite{0}{alger2019_data}
\abx@aux@segm{0}{0}{alger2019_data}
\@writefile{toc}{\contentsline {subsubsection}{Why "Stochastic"?}{142}{section*.191}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces SGD approximates the expectation over all possible samples via minibatch sampling.}}{142}{figure.caption.192}\protected@file@percent }
\newlabel{fig:chapter4_sgd_sampling}{{4.12}{142}{SGD approximates the expectation over all possible samples via minibatch sampling}{figure.caption.192}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.2}Advantages and Challenges of SGD}{142}{subsection.4.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Advantages}{142}{section*.193}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Challenges of SGD}{142}{section*.194}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{High Condition Numbers}{142}{section*.195}\protected@file@percent }
\abx@aux@backref{128}{alger2019_data}{0}{142}{142}
\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces Visualization of oscillations in SGD caused by high condition numbers.}}{143}{figure.caption.196}\protected@file@percent }
\newlabel{fig:chapter4_high_condition_number}{{4.13}{143}{Visualization of oscillations in SGD caused by high condition numbers}{figure.caption.196}{}}
\@writefile{toc}{\contentsline {paragraph}{Saddle Points and Local Minima}{143}{section*.197}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.14}{\ignorespaces Examples of saddle points and local minima in loss landscapes.}}{143}{figure.caption.198}\protected@file@percent }
\newlabel{fig:chapter4_saddle_point}{{4.14}{143}{Examples of saddle points and local minima in loss landscapes}{figure.caption.198}{}}
\@writefile{toc}{\contentsline {paragraph}{Noisy Gradients}{143}{section*.199}\protected@file@percent }
\BKM@entry{id=151,dest={73756273656374696F6E2E342E382E33},srcline={771}}{5C3337365C3337375C3030304C5C3030306F5C3030306F5C3030306B5C303030695C3030306E5C303030675C3030305C3034305C303030415C303030685C303030655C303030615C303030645C3030303A5C3030305C3034305C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030695C3030306E5C303030675C3030305C3034305C303030535C303030475C30303044}
\BKM@entry{id=152,dest={73656374696F6E2E342E39},srcline={774}}{5C3337365C3337375C303030535C303030475C303030445C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304D5C3030306F5C3030306D5C303030655C3030306E5C303030745C303030755C3030306D}
\BKM@entry{id=153,dest={73756273656374696F6E2E342E392E31},srcline={775}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=154,dest={73756273656374696F6E2E342E392E32},srcline={778}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030535C303030475C303030445C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304D5C3030306F5C3030306D5C303030655C3030306E5C303030745C303030755C3030306D5C3030305C3034305C303030575C3030306F5C303030725C3030306B5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {4.15}{\ignorespaces Noisy gradient updates in SGD resulting in slower convergence.}}{144}{figure.caption.200}\protected@file@percent }
\newlabel{fig:chapter4_noisy_gradients}{{4.15}{144}{Noisy gradient updates in SGD resulting in slower convergence}{figure.caption.200}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.3}Looking Ahead: Improving SGD}{144}{subsection.4.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.9}SGD with Momentum}{144}{section.4.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.1}Motivation}{144}{subsection.4.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.2}How SGD with Momentum Works}{144}{subsection.4.9.2}\protected@file@percent }
\BKM@entry{id=155,dest={73756273656374696F6E2E342E392E33},srcline={802}}{5C3337365C3337375C303030495C3030306E5C303030745C303030755C303030695C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030425C303030655C303030685C303030695C3030306E5C303030645C3030305C3034305C3030304D5C3030306F5C3030306D5C303030655C3030306E5C303030745C303030755C3030306D}
\@writefile{toc}{\contentsline {subsubsection}{Update Equations}{145}{section*.201}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.16}{\ignorespaces SGD with Momentum: Implementation in PyTorch.}}{145}{figure.caption.202}\protected@file@percent }
\newlabel{fig:chapter4_sgd_momentum}{{4.16}{145}{SGD with Momentum: Implementation in PyTorch}{figure.caption.202}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.3}Intuition Behind Momentum}{145}{subsection.4.9.3}\protected@file@percent }
\BKM@entry{id=156,dest={73756273656374696F6E2E342E392E34},srcline={818}}{5C3337365C3337375C303030425C303030655C3030306E5C303030655C303030665C303030695C303030745C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304D5C3030306F5C3030306D5C303030655C3030306E5C303030745C303030755C3030306D}
\@writefile{lof}{\contentsline {figure}{\numberline {4.17}{\ignorespaces Alternative formulation of SGD with Momentum.}}{146}{figure.caption.203}\protected@file@percent }
\newlabel{fig:chapter4_momentum_alternative}{{4.17}{146}{Alternative formulation of SGD with Momentum}{figure.caption.203}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.4}Benefits of Momentum}{146}{subsection.4.9.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.18}{\ignorespaces Momentum accelerates convergence by smoothing oscillations and reducing noise.}}{146}{figure.caption.204}\protected@file@percent }
\newlabel{fig:chapter4_momentum_benefits}{{4.18}{146}{Momentum accelerates convergence by smoothing oscillations and reducing noise}{figure.caption.204}{}}
\BKM@entry{id=157,dest={73756273656374696F6E2E342E392E35},srcline={834}}{5C3337365C3337375C303030445C3030306F5C303030775C3030306E5C303030735C303030695C303030645C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304D5C3030306F5C3030306D5C303030655C3030306E5C303030745C303030755C3030306D}
\BKM@entry{id=158,dest={73756273656374696F6E2E342E392E36},srcline={844}}{5C3337365C3337375C3030304E5C303030655C303030735C303030745C303030655C303030725C3030306F5C303030765C3030305C3034305C3030304D5C3030306F5C3030306D5C303030655C3030306E5C303030745C303030755C3030306D5C3030303A5C3030305C3034305C303030415C3030305C3034305C3030304C5C3030306F5C3030306F5C3030306B5C3030302D5C303030415C303030685C303030655C303030615C303030645C3030305C3034305C303030535C303030745C303030725C303030615C303030745C303030655C303030675C30303079}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.5}Downsides of Momentum}{147}{subsection.4.9.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.6}Nesterov Momentum: A Look-Ahead Strategy}{147}{subsection.4.9.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Overview}{147}{section*.205}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Mathematical Formulation}{147}{section*.206}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.19}{\ignorespaces Nesterov Momentum: Look-ahead Gradient Update.}}{147}{figure.caption.207}\protected@file@percent }
\newlabel{fig:chapter4_nesterov_momentum}{{4.19}{147}{Nesterov Momentum: Look-ahead Gradient Update}{figure.caption.207}{}}
\@writefile{toc}{\contentsline {subsubsection}{Motivation and Advantages}{148}{section*.208}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Reformulation for Practical Implementation}{148}{section*.209}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Comparison with SGD and SGD+Momentum}{148}{section*.210}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Limitations of Nesterov Momentum and the Need for Adaptivity}{149}{section*.211}\protected@file@percent }
\BKM@entry{id=159,dest={73656374696F6E2E342E3130},srcline={952}}{5C3337365C3337375C303030415C303030645C303030615C303030475C303030725C303030615C303030645C3030303A5C3030305C3034305C303030415C303030645C303030615C303030705C303030745C303030695C303030765C303030655C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030415C3030306C5C303030675C3030306F5C303030725C303030695C303030745C303030685C3030306D}
\BKM@entry{id=160,dest={73756273656374696F6E2E342E31302E31},srcline={963}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030415C303030645C303030615C303030475C303030725C303030615C303030645C3030305C3034305C303030575C3030306F5C303030725C3030306B5C30303073}
\@writefile{toc}{\contentsline {paragraph}{Motivation for a Better Optimizer: AdaGrad}{150}{section*.212}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.10}AdaGrad: Adaptive Gradient Algorithm}{150}{section.4.10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.20}{\ignorespaces AdaGrad Implementation in PyTorch. Each parameter is updated individually, with learning rates adjusted based on the historical squared gradients.}}{150}{figure.caption.213}\protected@file@percent }
\newlabel{fig:chapter4_adagrad_impl}{{4.20}{150}{AdaGrad Implementation in PyTorch. Each parameter is updated individually, with learning rates adjusted based on the historical squared gradients}{figure.caption.213}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.10.1}How AdaGrad Works}{150}{subsection.4.10.1}\protected@file@percent }
\BKM@entry{id=161,dest={73756273656374696F6E2E342E31302E32},srcline={991}}{5C3337365C3337375C303030415C303030645C303030765C303030615C3030306E5C303030745C303030615C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030415C303030645C303030615C303030475C303030725C303030615C30303064}
\BKM@entry{id=162,dest={73756273656374696F6E2E342E31302E33},srcline={1003}}{5C3337365C3337375C303030445C303030695C303030735C303030615C303030645C303030765C303030615C3030306E5C303030745C303030615C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030415C303030645C303030615C303030475C303030725C303030615C30303064}
\@writefile{toc}{\contentsline {paragraph}{Updating the Weight Matrix Components}{151}{section*.214}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Does This Work?}{151}{section*.215}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.10.2}Advantages of AdaGrad}{151}{subsection.4.10.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.10.3}Disadvantages of AdaGrad}{151}{subsection.4.10.3}\protected@file@percent }
\BKM@entry{id=163,dest={73656374696F6E2E342E3131},srcline={1021}}{5C3337365C3337375C303030525C3030304D5C303030535C303030505C303030725C3030306F5C303030705C3030303A5C3030305C3034305C303030525C3030306F5C3030306F5C303030745C3030305C3034305C3030304D5C303030655C303030615C3030306E5C3030305C3034305C303030535C303030715C303030755C303030615C303030725C303030655C3030305C3034305C303030505C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=164,dest={73756273656374696F6E2E342E31312E31},srcline={1023}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030525C3030304D5C303030535C303030505C303030725C3030306F5C30303070}
\BKM@entry{id=165,dest={73756273656374696F6E2E342E31312E32},srcline={1028}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030525C3030304D5C303030535C303030505C303030725C3030306F5C303030705C3030305C3034305C303030575C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=166,dest={73756273656374696F6E2E342E31312E33},srcline={1050}}{5C3337365C3337375C303030555C303030705C303030645C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030575C303030655C303030695C303030675C303030685C303030745C3030305C3034305C3030304D5C303030615C303030745C303030725C303030695C303030785C3030305C3034305C303030435C3030306F5C3030306D5C303030705C3030306F5C3030306E5C303030655C3030306E5C303030745C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {4.11}RMSProp: Root Mean Square Propagation}{152}{section.4.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11.1}Motivation for RMSProp}{152}{subsection.4.11.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11.2}How RMSProp Works}{152}{subsection.4.11.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11.3}Updating the Weight Matrix Components}{152}{subsection.4.11.3}\protected@file@percent }
\BKM@entry{id=167,dest={73756273656374696F6E2E342E31312E34},srcline={1074}}{5C3337365C3337375C303030415C303030645C303030765C303030615C3030306E5C303030745C303030615C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030525C3030304D5C303030535C303030505C303030725C3030306F5C30303070}
\BKM@entry{id=168,dest={73756273656374696F6E2E342E31312E35},srcline={1090}}{5C3337365C3337375C303030445C3030306F5C303030775C3030306E5C303030735C303030695C303030645C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030525C3030304D5C303030535C303030505C303030725C3030306F5C30303070}
\@writefile{lof}{\contentsline {figure}{\numberline {4.21}{\ignorespaces The transformation from AdaGrad to RMSProp using a decay rate (\( \rho \)). RMSProp ensures better progress over the course of training by forgetting older squared gradients.}}{153}{figure.caption.216}\protected@file@percent }
\newlabel{fig:chapter4_rmsprop_conversion}{{4.21}{153}{The transformation from AdaGrad to RMSProp using a decay rate (\( \rho \)). RMSProp ensures better progress over the course of training by forgetting older squared gradients}{figure.caption.216}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11.4}Advantages of RMSProp}{153}{subsection.4.11.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11.5}Downsides of RMSProp}{153}{subsection.4.11.5}\protected@file@percent }
\newlabel{sec:downsides-rmsprop}{{4.11.5}{153}{Downsides of RMSProp}{subsection.4.11.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{No Momentum Carry-Over}{153}{section*.217}\protected@file@percent }
\newlabel{subsubsec:no-momentum-carry-over}{{4.11.5}{153}{No Momentum Carry-Over}{section*.217}{}}
\BKM@entry{id=169,dest={73756273656374696F6E2E342E31312E36},srcline={1135}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030415C303030645C303030615C3030306D5C3030302C5C3030305C3034305C303030615C3030305C3034305C303030535C3030304F5C303030545C303030415C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030655C30303072}
\BKM@entry{id=170,dest={73656374696F6E2E342E3132},srcline={1149}}{5C3337365C3337375C303030415C303030645C303030615C3030306D5C3030303A5C3030305C3034305C303030415C303030645C303030615C303030705C303030745C303030695C303030765C303030655C3030305C3034305C3030304D5C3030306F5C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030455C303030735C303030745C303030695C3030306D5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=171,dest={73756273656374696F6E2E342E31322E31},srcline={1151}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030415C303030645C303030615C3030306D}
\@writefile{toc}{\contentsline {subsubsection}{Bias in Early Updates}{154}{section*.218}\protected@file@percent }
\newlabel{subsubsec:bias-early-updates}{{4.11.5}{154}{Bias in Early Updates}{section*.218}{}}
\@writefile{toc}{\contentsline {subsubsection}{Sensitivity to Hyperparameters}{154}{section*.219}\protected@file@percent }
\newlabel{subsubsec:sensitivity-hparams}{{4.11.5}{154}{Sensitivity to Hyperparameters}{section*.219}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11.6}Motivation for Adam, a SOTA Optimizer}{154}{subsection.4.11.6}\protected@file@percent }
\newlabel{subsec:motivation-adam}{{4.11.6}{154}{Motivation for Adam, a SOTA Optimizer}{subsection.4.11.6}{}}
\BKM@entry{id=172,dest={73756273656374696F6E2E342E31322E32},srcline={1167}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030415C303030645C303030615C3030306D5C3030305C3034305C303030575C3030306F5C303030725C3030306B5C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {4.12}Adam: Adaptive Moment Estimation}{155}{section.4.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.12.1}Motivation for Adam}{155}{subsection.4.12.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.12.2}How Adam Works}{155}{subsection.4.12.2}\protected@file@percent }
\BKM@entry{id=173,dest={73756273656374696F6E2E342E31322E33},srcline={1197}}{5C3337365C3337375C303030425C303030695C303030615C303030735C3030305C3034305C303030435C3030306F5C303030725C303030725C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {4.22}{\ignorespaces Adam implementation without bias correction, as shown in PyTorch.}}{156}{figure.caption.220}\protected@file@percent }
\newlabel{fig:chapter4_adam_basic}{{4.22}{156}{Adam implementation without bias correction, as shown in PyTorch}{figure.caption.220}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.12.3}Bias Correction}{156}{subsection.4.12.3}\protected@file@percent }
\BKM@entry{id=174,dest={73756273656374696F6E2E342E31322E34},srcline={1219}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030415C303030645C303030615C3030306D5C3030305C3034305C303030575C3030306F5C303030725C3030306B5C303030735C3030305C3034305C303030575C303030655C3030306C5C3030306C5C3030305C3034305C303030695C3030306E5C3030305C3034305C303030505C303030725C303030615C303030635C303030745C303030695C303030635C30303065}
\BKM@entry{id=175,dest={73756273656374696F6E2E342E31322E35},srcline={1231}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C303030735C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304F5C303030745C303030685C303030655C303030725C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030655C303030725C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {4.23}{\ignorespaces Complete Adam implementation with bias correction as shown in PyTorch.}}{157}{figure.caption.221}\protected@file@percent }
\newlabel{fig:chapter4_adam_bias_correction}{{4.23}{157}{Complete Adam implementation with bias correction as shown in PyTorch}{figure.caption.221}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.12.4}Why Adam Works Well in Practice}{157}{subsection.4.12.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.24}{\ignorespaces Examples of Adam's hyperparameter usage in various deep learning papers.}}{157}{figure.caption.222}\protected@file@percent }
\newlabel{fig:chapter4_adam_hyperparams}{{4.24}{157}{Examples of Adam's hyperparameter usage in various deep learning papers}{figure.caption.222}{}}
\BKM@entry{id=176,dest={73756273656374696F6E2E342E31322E36},srcline={1241}}{5C3337365C3337375C303030415C303030645C303030765C303030615C3030306E5C303030745C303030615C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030415C303030645C303030615C3030306D}
\BKM@entry{id=177,dest={73756273656374696F6E2E342E31322E37},srcline={1249}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030415C303030645C303030615C3030306D}
\BKM@entry{id=178,dest={73656374696F6E2E342E3133},srcline={1259}}{5C3337365C3337375C303030415C303030645C303030615C3030306D5C303030575C3030303A5C3030305C3034305C303030445C303030655C303030635C3030306F5C303030755C303030705C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030575C303030655C303030695C303030675C303030685C303030745C3030305C3034305C303030445C303030655C303030635C303030615C303030795C3030305C3034305C303030665C303030725C3030306F5C3030306D5C3030305C3034305C3030304C5C303030325C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=179,dest={73756273656374696F6E2E342E31332E31},srcline={1261}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030415C303030645C303030615C3030306D5C30303057}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.12.5}Comparison with Other Optimizers}{158}{subsection.4.12.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.25}{\ignorespaces Comparison of optimizers: SGD, SGD+Momentum, RMSProp, and Adam. Adam converges faster with fewer oscillations.}}{158}{figure.caption.223}\protected@file@percent }
\newlabel{fig:chapter4_adam_comparison}{{4.25}{158}{Comparison of optimizers: SGD, SGD+Momentum, RMSProp, and Adam. Adam converges faster with fewer oscillations}{figure.caption.223}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.12.6}Advantages of Adam}{158}{subsection.4.12.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.12.7}Limitations of Adam}{158}{subsection.4.12.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Looking Ahead}{158}{section*.224}\protected@file@percent }
\BKM@entry{id=180,dest={73756273656374696F6E2E342E31332E32},srcline={1277}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030415C303030645C303030615C3030306D5C303030575C3030305C3034305C303030575C3030306F5C303030725C3030306B5C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {4.13}AdamW: Decoupling Weight Decay from L2 Regularization}{159}{section.4.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.13.1}Motivation for AdamW}{159}{subsection.4.13.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.26}{\ignorespaces Integration of L2 regularization and weight decay in AdamW. Decoupling these ensures consistent penalization of parameter magnitudes.}}{159}{figure.caption.225}\protected@file@percent }
\newlabel{fig:chapter4_adamw_weight_decay}{{4.26}{159}{Integration of L2 regularization and weight decay in AdamW. Decoupling these ensures consistent penalization of parameter magnitudes}{figure.caption.225}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.13.2}How AdamW Works}{159}{subsection.4.13.2}\protected@file@percent }
\BKM@entry{id=181,dest={73756273656374696F6E2E342E31332E33},srcline={1301}}{5C3337365C3337375C3030304E5C3030306F5C303030745C303030655C3030305C3034305C3030306F5C3030306E5C3030305C3034305C303030575C303030655C303030695C303030675C303030685C303030745C3030305C3034305C303030445C303030655C303030635C303030615C303030795C3030305C3034305C303030695C3030306E5C3030305C3034305C303030415C303030645C303030615C3030306D5C30303057}
\BKM@entry{id=182,dest={73756273656374696F6E2E342E31332E34},srcline={1316}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030415C303030645C303030615C3030306D5C303030575C3030305C3034305C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030655C3030306D5C303030655C3030306E5C30303074}
\@writefile{lof}{\contentsline {figure}{\numberline {4.27}{\ignorespaces Pseudo-code for AdamW, illustrating the decoupling of weight decay from L2 regularization.}}{160}{figure.caption.226}\protected@file@percent }
\newlabel{fig:chapter4_adamw_pseudocode}{{4.27}{160}{Pseudo-code for AdamW, illustrating the decoupling of weight decay from L2 regularization}{figure.caption.226}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.13.3}Note on Weight Decay in AdamW}{160}{subsection.4.13.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.13.4}The AdamW Improvement}{160}{subsection.4.13.4}\protected@file@percent }
\BKM@entry{id=183,dest={73756273656374696F6E2E342E31332E35},srcline={1329}}{5C3337365C3337375C303030415C303030645C303030765C303030615C3030306E5C303030745C303030615C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030415C303030645C303030615C3030306D5C30303057}
\BKM@entry{id=184,dest={73756273656374696F6E2E342E31332E36},srcline={1340}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030415C303030645C303030615C3030306D5C303030575C3030305C3034305C303030695C303030735C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030445C303030655C303030665C303030615C303030755C3030306C5C303030745C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030655C30303072}
\BKM@entry{id=185,dest={73756273656374696F6E2E342E31332E37},srcline={1348}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030415C303030645C303030615C3030306D5C30303057}
\BKM@entry{id=186,dest={73656374696F6E2E342E3134},srcline={1355}}{5C3337365C3337375C303030535C303030655C303030635C3030306F5C3030306E5C303030645C3030302D5C3030304F5C303030725C303030645C303030655C303030725C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=187,dest={73756273656374696F6E2E342E31342E31},srcline={1357}}{5C3337365C3337375C3030304F5C303030765C303030655C303030725C303030765C303030695C303030655C303030775C3030305C3034305C3030306F5C303030665C3030305C3034305C303030535C303030655C303030635C3030306F5C3030306E5C303030645C3030302D5C3030304F5C303030725C303030645C303030655C303030725C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.13.5}Advantages of AdamW}{161}{subsection.4.13.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.13.6}Why AdamW is the Default Optimizer}{161}{subsection.4.13.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.13.7}Limitations of AdamW}{161}{subsection.4.13.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.14}Second-Order Optimization}{161}{section.4.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.14.1}Overview of Second-Order Optimization}{161}{subsection.4.14.1}\protected@file@percent }
\BKM@entry{id=188,dest={73756273656374696F6E2E342E31342E32},srcline={1377}}{5C3337365C3337375C303030515C303030755C303030615C303030645C303030725C303030615C303030745C303030695C303030635C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030785C303030695C3030306D5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030555C303030735C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030485C303030655C303030735C303030735C303030695C303030615C3030306E}
\BKM@entry{id=189,dest={73756273656374696F6E2E342E31342E33},srcline={1393}}{5C3337365C3337375C303030505C303030725C303030615C303030635C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030535C303030655C303030635C3030306F5C3030306E5C303030645C3030302D5C3030304F5C303030725C303030645C303030655C303030725C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {4.28}{\ignorespaces Second-order optimization forms a quadratic surface to approximate the objective function and adaptively determines step size.}}{162}{figure.caption.227}\protected@file@percent }
\newlabel{fig:chapter4_second_order_quadratic}{{4.28}{162}{Second-order optimization forms a quadratic surface to approximate the objective function and adaptively determines step size}{figure.caption.227}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.14.2}Quadratic Approximation Using the Hessian}{162}{subsection.4.14.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.14.3}Practical Challenges of Second-Order Methods}{162}{subsection.4.14.3}\protected@file@percent }
\BKM@entry{id=190,dest={73756273656374696F6E2E342E31342E34},srcline={1410}}{5C3337365C3337375C303030465C303030695C303030725C303030735C303030745C3030302D5C3030304F5C303030725C303030645C303030655C303030725C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C303030735C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030785C303030695C3030306D5C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030535C303030655C303030635C3030306F5C3030306E5C303030645C3030302D5C3030304F5C303030725C303030645C303030655C303030725C3030305C3034305C303030425C303030655C303030685C303030615C303030765C303030695C3030306F5C30303072}
\BKM@entry{id=191,dest={73756273656374696F6E2E342E31342E35},srcline={1418}}{5C3337365C3337375C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030695C3030306E5C303030675C3030305C3034305C303030535C303030655C303030635C3030306F5C3030306E5C303030645C3030302D5C3030304F5C303030725C303030645C303030655C303030725C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030425C303030465C303030475C303030535C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C3030302D5C303030425C303030465C303030475C30303053}
\@writefile{lof}{\contentsline {figure}{\numberline {4.29}{\ignorespaces Challenges of second-order optimization in high-dimensional spaces, including storage and computational costs.}}{163}{figure.caption.228}\protected@file@percent }
\newlabel{fig:chapter4_second_order_limitations}{{4.29}{163}{Challenges of second-order optimization in high-dimensional spaces, including storage and computational costs}{figure.caption.228}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.14.4}First-Order Methods Approximating Second-Order Behavior}{163}{subsection.4.14.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.14.5}Improving Second-Order Optimization: BFGS and L-BFGS}{163}{subsection.4.14.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.30}{\ignorespaces BFGS and L-BFGS use approximations to reduce the computational and memory demands of second-order optimization.}}{164}{figure.caption.229}\protected@file@percent }
\newlabel{fig:chapter4_bfgs_lbfgs}{{4.30}{164}{BFGS and L-BFGS use approximations to reduce the computational and memory demands of second-order optimization}{figure.caption.229}{}}
\@writefile{toc}{\contentsline {subsubsection}{BFGS: An Approximation of the Hessian Matrix}{164}{section*.230}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{L-BFGS: Reducing Memory Requirements}{164}{section*.231}\protected@file@percent }
\BKM@entry{id=192,dest={73756273656374696F6E2E342E31342E36},srcline={1471}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030305C3034305C3030306F5C303030665C3030305C3034305C303030535C303030655C303030635C3030306F5C3030306E5C303030645C3030302D5C3030304F5C303030725C303030645C303030655C303030725C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030615C303030635C303030685C303030655C30303073}
\@writefile{toc}{\contentsline {subsubsection}{Advantages and Limitations of BFGS and L-BFGS}{165}{section*.232}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Advantages:}{165}{section*.233}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Limitations:}{165}{section*.234}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Applications of L-BFGS}{165}{section*.235}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.14.6}Summary of Second-Order Optimization Approaches}{165}{subsection.4.14.6}\protected@file@percent }
\BKM@entry{id=193,dest={636861707465722E35},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030355C3030303A5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=194,dest={73656374696F6E2E352E31},srcline={10}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=195,dest={73756273656374696F6E2E352E312E31},srcline={13}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C30303073}
\BKM@entry{id=196,dest={73756273656374696F6E2E352E312E32},srcline={22}}{5C3337365C3337375C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030735C3030305C3034305C303030615C303030735C3030305C3034305C303030615C3030305C3034305C303030535C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Lecture 5: Neural Networks}{166}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@4}}
\ttl@writefile{ptc}{\ttl@starttoc{default@5}}
\pgfsyspdfmark {pgfid16}{0}{52099153}
\pgfsyspdfmark {pgfid15}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction to Neural Networks}{166}{section.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Limitations of Linear Classifiers}{166}{subsection.5.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Feature Transforms as a Solution}{166}{subsection.5.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Feature Transforms in Action}{166}{section*.236}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Cartesian to polar transformation enabling linear separability in the feature space.}}{167}{figure.caption.237}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Color histogram as a feature representation for images.}}{167}{figure.caption.238}\protected@file@percent }
\newlabel{fig:chapter5_color_histogram}{{5.2}{167}{Color histogram as a feature representation for images}{figure.caption.238}{}}
\BKM@entry{id=197,dest={73756273656374696F6E2E352E312E33},srcline={60}}{5C3337365C3337375C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C3030304D5C303030615C3030306E5C303030755C303030615C3030306C5C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=198,dest={73756273656374696F6E2E352E312E34},srcline={67}}{5C3337365C3337375C303030445C303030615C303030745C303030615C3030302D5C303030445C303030725C303030695C303030765C303030655C3030306E5C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Histogram of Oriented Gradients (HoG) as a feature representation for images.}}{168}{figure.caption.239}\protected@file@percent }
\newlabel{fig:chapter5_hog}{{5.3}{168}{Histogram of Oriented Gradients (HoG) as a feature representation for images}{figure.caption.239}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}Challenges in Manual Feature Transformations}{168}{subsection.5.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.4}Data-Driven Feature Transformations}{168}{subsection.5.1.4}\protected@file@percent }
\BKM@entry{id=199,dest={73756273656374696F6E2E352E312E35},srcline={85}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030625C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030655C3030305C3034305C303030525C303030655C303030705C303030725C303030655C303030735C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=200,dest={73756273656374696F6E2E352E312E36},srcline={95}}{5C3337365C3337375C303030525C303030655C303030615C3030306C5C3030302D5C303030575C3030306F5C303030725C3030306C5C303030645C3030305C3034305C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030303A5C3030305C3034305C303030325C303030305C303030315C303030315C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030304E5C303030655C303030745C3030305C3034305C303030575C303030695C3030306E5C3030306E5C303030655C30303072}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Bag of Words approach for feature transformation.}}{169}{figure.caption.240}\protected@file@percent }
\newlabel{fig:chapter5_bag_of_words}{{5.4}{169}{Bag of Words approach for feature transformation}{figure.caption.240}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.5}Combining Multiple Representations}{169}{subsection.5.1.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Combining multiple feature representations into a single feature vector.}}{169}{figure.caption.241}\protected@file@percent }
\newlabel{fig:chapter5_combined_features}{{5.5}{169}{Combining multiple feature representations into a single feature vector}{figure.caption.241}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.6}Real-World Example: 2011 ImageNet Winner}{169}{subsection.5.1.6}\protected@file@percent }
\BKM@entry{id=201,dest={73656374696F6E2E352E32},srcline={115}}{5C3337365C3337375C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030425C303030615C303030735C303030695C303030635C30303073}
\BKM@entry{id=202,dest={73756273656374696F6E2E352E322E31},srcline={117}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Feature extraction pipeline of the 2011 ImageNet winner.}}{170}{figure.caption.242}\protected@file@percent }
\newlabel{fig:chapter5_imagenet_pipeline}{{5.6}{170}{Feature extraction pipeline of the 2011 ImageNet winner}{figure.caption.242}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Neural Networks: The Basics}{170}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Motivation for Neural Networks}{170}{subsection.5.2.1}\protected@file@percent }
\BKM@entry{id=203,dest={73756273656374696F6E2E352E322E32},srcline={137}}{5C3337365C3337375C303030465C303030755C3030306C5C3030306C5C303030795C3030305C3034305C303030435C3030306F5C3030306E5C3030306E5C303030655C303030635C303030745C303030655C303030645C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Comparison between the classic feature extraction pipeline and the neural network-based pipeline. The neural network pipeline is fully tunable end-to-end based on training data.}}{171}{figure.caption.243}\protected@file@percent }
\newlabel{fig:chapter5_classic_vs_nn_pipeline}{{5.7}{171}{Comparison between the classic feature extraction pipeline and the neural network-based pipeline. The neural network pipeline is fully tunable end-to-end based on training data}{figure.caption.243}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Mathematical/functional notation of linear classifiers compared to small neural networks.}}{171}{figure.caption.244}\protected@file@percent }
\newlabel{fig:chapter5_nn_functional_notation}{{5.8}{171}{Mathematical/functional notation of linear classifiers compared to small neural networks}{figure.caption.244}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Fully Connected Networks}{171}{subsection.5.2.2}\protected@file@percent }
\BKM@entry{id=204,dest={73756273656374696F6E2E352E322E33},srcline={147}}{5C3337365C3337375C303030495C3030306E5C303030745C303030655C303030725C303030705C303030725C303030655C303030745C303030695C3030306E5C303030675C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces A visual representation of a fully connected neural network with a single hidden layer, an input layer, and an output layer.}}{172}{figure.caption.245}\protected@file@percent }
\newlabel{fig:chapter5_fc_network}{{5.9}{172}{A visual representation of a fully connected neural network with a single hidden layer, an input layer, and an output layer}{figure.caption.245}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Interpreting Neural Networks}{172}{subsection.5.2.3}\protected@file@percent }
\BKM@entry{id=205,dest={73656374696F6E2E352E33},srcline={164}}{5C3337365C3337375C303030425C303030755C303030695C3030306C5C303030645C303030695C3030306E5C303030675C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=206,dest={73756273656374696F6E2E352E332E31},srcline={174}}{5C3337365C3337375C303030415C303030635C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E5C303030735C3030303A5C3030305C3034305C3030304E5C3030306F5C3030306E5C3030302D5C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030425C303030725C303030695C303030645C303030675C303030655C303030735C3030305C3034305C303030425C303030655C303030745C303030775C303030655C303030655C3030306E5C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces Visualization of learned templates in the first layer of a neural network, including a left-facing and a right-facing horse.}}{173}{figure.caption.246}\protected@file@percent }
\newlabel{fig:chapter5_learned_templates}{{5.10}{173}{Visualization of learned templates in the first layer of a neural network, including a left-facing and a right-facing horse}{figure.caption.246}{}}
\@writefile{toc}{\contentsline {paragraph}{Network Pruning: Pruning Redundant Representations}{173}{section*.247}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Building Neural Networks}{173}{section.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces A visual representation of a deep neural network.}}{173}{figure.caption.248}\protected@file@percent }
\newlabel{fig:chapter5_deep_nn}{{5.11}{173}{A visual representation of a deep neural network}{figure.caption.248}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Activation Functions: Non-Linear Bridges Between Layers}{174}{subsection.5.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.12}{\ignorespaces Collapsing multiple linear layers reduces the network to a linear classifier.}}{174}{figure.caption.249}\protected@file@percent }
\newlabel{fig:chapter5_linear_collapse}{{5.12}{174}{Collapsing multiple linear layers reduces the network to a linear classifier}{figure.caption.249}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Non-Linearity Matters.}{174}{section*.250}\protected@file@percent }
\BKM@entry{id=207,dest={73756273656374696F6E2E352E332E32},srcline={207}}{5C3337365C3337375C303030415C3030305C3034305C303030535C303030695C3030306D5C303030705C3030306C5C303030655C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C3030305C3034305C303030695C3030306E5C3030305C3034305C303030325C303030305C3030305C3034305C3030304C5C303030695C3030306E5C303030655C30303073}
\BKM@entry{id=208,dest={73656374696F6E2E352E34},srcline={224}}{5C3337365C3337375C303030425C303030695C3030306F5C3030306C5C3030306F5C303030675C303030695C303030635C303030615C3030306C5C3030305C3034305C303030495C3030306E5C303030735C303030705C303030695C303030725C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {5.13}{\ignorespaces Examples of common activation functions.}}{175}{figure.caption.251}\protected@file@percent }
\newlabel{fig:chapter5_activation_functions}{{5.13}{175}{Examples of common activation functions}{figure.caption.251}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}A Simple Neural Network in 20 Lines}{175}{subsection.5.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.14}{\ignorespaces Minimal implementation of a neural network in under 20 lines of code.}}{175}{figure.caption.252}\protected@file@percent }
\newlabel{fig:chapter5_simple_nn}{{5.14}{175}{Minimal implementation of a neural network in under 20 lines of code}{figure.caption.252}{}}
\BKM@entry{id=209,dest={73756273656374696F6E2E352E342E31},srcline={249}}{5C3337365C3337375C303030425C303030695C3030306F5C3030306C5C3030306F5C303030675C303030695C303030635C303030615C3030306C5C3030305C3034305C303030415C3030306E5C303030615C3030306C5C3030306F5C303030675C303030795C3030303A5C3030305C3034305C303030615C3030305C3034305C3030304C5C3030306F5C3030306F5C303030735C303030655C3030305C3034305C303030415C3030306E5C303030615C3030306C5C3030306F5C303030675C30303079}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Biological Inspiration}{176}{section.5.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.15}{\ignorespaces Biological inspiration: flow of impulses in neurons.}}{176}{figure.caption.253}\protected@file@percent }
\newlabel{fig:chapter5_biological_inspiration}{{5.15}{176}{Biological inspiration: flow of impulses in neurons}{figure.caption.253}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.16}{\ignorespaces Comparison of biological neurons and artificial neurons.}}{176}{figure.caption.254}\protected@file@percent }
\newlabel{fig:chapter5_artificial_neurons}{{5.16}{176}{Comparison of biological neurons and artificial neurons}{figure.caption.254}{}}
\BKM@entry{id=210,dest={73656374696F6E2E352E35},srcline={257}}{5C3337365C3337375C303030535C303030705C303030615C303030635C303030655C3030305C3034305C303030575C303030615C303030725C303030705C303030695C3030306E5C303030675C3030303A5C3030305C3034305C303030415C3030306E5C3030306F5C303030745C303030685C303030655C303030725C3030305C3034305C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030415C303030725C303030745C303030695C303030665C303030695C303030635C303030695C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=211,dest={73756273656374696F6E2E352E352E31},srcline={271}}{5C3337365C3337375C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030545C303030685C303030655C303030695C303030725C3030305C3034305C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Biological Analogy: a Loose Analogy}{177}{subsection.5.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Space Warping: Another Motivation for Artificial Neural Networks}{177}{section.5.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.17}{\ignorespaces Transforming input features \( x_1, x_2 \) into a new feature space \( h \) via \( h = \mathbf  {W} \mathbf  {x} \).}}{177}{figure.caption.255}\protected@file@percent }
\newlabel{fig:chapter5_linear_transformation}{{5.17}{177}{Transforming input features \( x_1, x_2 \) into a new feature space \( h \) via \( h = \mathbf {W} \mathbf {x} \)}{figure.caption.255}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.1}Linear Transformations and Their Limitations}{177}{subsection.5.5.1}\protected@file@percent }
\BKM@entry{id=212,dest={73756273656374696F6E2E352E352E32},srcline={283}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030695C3030306E5C303030675C3030305C3034305C3030304E5C3030306F5C3030306E5C3030302D5C3030304C5C303030695C3030306E5C303030655C303030615C303030725C303030695C303030745C303030795C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030525C303030655C3030304C5C30303055}
\@writefile{lof}{\contentsline {figure}{\numberline {5.18}{\ignorespaces Regions in the input space divided by linear decision boundaries.}}{178}{figure.caption.256}\protected@file@percent }
\newlabel{fig:chapter5_input_regions}{{5.18}{178}{Regions in the input space divided by linear decision boundaries}{figure.caption.256}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.2}Introducing Non-Linearity with ReLU}{178}{subsection.5.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.19}{\ignorespaces Transformation of quadrants using ReLU, collapsing regions onto specific axes.}}{178}{figure.caption.257}\protected@file@percent }
\newlabel{fig:chapter5_relu_quadrants}{{5.19}{178}{Transformation of quadrants using ReLU, collapsing regions onto specific axes}{figure.caption.257}{}}
\BKM@entry{id=213,dest={73756273656374696F6E2E352E352E33},srcline={305}}{5C3337365C3337375C3030304D5C303030615C3030306B5C303030695C3030306E5C303030675C3030305C3034305C303030445C303030615C303030745C303030615C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030306C5C303030795C3030305C3034305C303030535C303030655C303030705C303030615C303030725C303030615C303030625C3030306C5C30303065}
\BKM@entry{id=214,dest={73756273656374696F6E2E352E352E34},srcline={313}}{5C3337365C3337375C303030535C303030635C303030615C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030555C303030705C3030303A5C3030305C3034305C303030495C3030306E5C303030635C303030725C303030655C303030615C303030735C303030695C3030306E5C303030675C3030305C3034305C303030525C303030655C303030705C303030725C303030655C303030735C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030505C3030306F5C303030775C303030655C30303072}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.3}Making Data Linearly Separable}{179}{subsection.5.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.20}{\ignorespaces Non-linearity (e.g., ReLU) enables linear separability in the transformed feature space.}}{179}{figure.caption.258}\protected@file@percent }
\newlabel{fig:chapter5_relu_separability}{{5.20}{179}{Non-linearity (e.g., ReLU) enables linear separability in the transformed feature space}{figure.caption.258}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.4}Scaling Up: Increasing Representation Power}{179}{subsection.5.5.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.21}{\ignorespaces Adding hidden units increases the complexity of decision boundaries in the input space.}}{179}{figure.caption.259}\protected@file@percent }
\newlabel{fig:chapter5_complex_boundaries}{{5.21}{179}{Adding hidden units increases the complexity of decision boundaries in the input space}{figure.caption.259}{}}
\BKM@entry{id=215,dest={73756273656374696F6E2E352E352E35},srcline={323}}{5C3337365C3337375C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=216,dest={73656374696F6E2E352E36},srcline={333}}{5C3337365C3337375C303030555C3030306E5C303030695C303030765C303030655C303030725C303030735C303030615C3030306C5C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030785C303030695C3030306D5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030685C303030655C3030306F5C303030725C303030655C3030306D}
\BKM@entry{id=217,dest={73756273656374696F6E2E352E362E31},srcline={344}}{5C3337365C3337375C303030505C303030725C303030615C303030635C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030655C303030785C303030745C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030425C303030755C3030306D5C303030705C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.5}Regularizing Neural Networks}{180}{subsection.5.5.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.22}{\ignorespaces Using stronger L2 regularization to simplify decision boundaries and reduce overfitting.}}{180}{figure.caption.260}\protected@file@percent }
\newlabel{fig:chapter5_regularization}{{5.22}{180}{Using stronger L2 regularization to simplify decision boundaries and reduce overfitting}{figure.caption.260}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Universal Approximation Theorem}{180}{section.5.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.1}Practical Context: The Bump Function}{180}{subsection.5.6.1}\protected@file@percent }
\BKM@entry{id=218,dest={73756273656374696F6E2E352E362E32},srcline={361}}{5C3337365C3337375C303030515C303030755C303030655C303030735C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030465C303030755C303030725C303030745C303030685C303030655C303030725C3030305C3034305C303030455C303030785C303030705C3030306C5C3030306F5C303030725C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=219,dest={73756273656374696F6E2E352E362E33},srcline={370}}{5C3337365C3337375C303030525C303030655C303030615C3030306C5C303030695C303030745C303030795C3030305C3034305C303030435C303030685C303030655C303030635C3030306B5C3030303A5C3030305C3034305C303030555C3030306E5C303030695C303030765C303030655C303030725C303030735C303030615C3030306C5C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030785C303030695C3030306D5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030695C303030735C3030305C3034305C3030304E5C3030306F5C303030745C3030305C3034305C303030455C3030306E5C3030306F5C303030755C303030675C30303068}
\@writefile{lof}{\contentsline {figure}{\numberline {5.23}{\ignorespaces A bump function constructed using four hidden units, demonstrating the capacity of neural networks to approximate complex functions.}}{181}{figure.caption.261}\protected@file@percent }
\newlabel{fig:chapter5_bump_function}{{5.23}{181}{A bump function constructed using four hidden units, demonstrating the capacity of neural networks to approximate complex functions}{figure.caption.261}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.2}Questions for Further Exploration}{181}{subsection.5.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.3}Reality Check: Universal Approximation is Not Enough}{181}{subsection.5.6.3}\protected@file@percent }
\BKM@entry{id=220,dest={73656374696F6E2A2E323633},srcline={390}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030355C3030302E5C303030365C3030302E5C303030345C3030303A5C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C303030765C303030735C3030305C3034305C303030535C303030685C303030615C3030306C5C3030306C5C3030306F5C303030775C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\abx@aux@cite{0}{eldan2016_power}
\abx@aux@segm{0}{0}{eldan2016_power}
\abx@aux@cite{0}{telgarsky2016_benefits}
\abx@aux@segm{0}{0}{telgarsky2016_benefits}
\abx@aux@cite{0}{bengio2013_representation}
\abx@aux@segm{0}{0}{bengio2013_representation}
\abx@aux@cite{0}{poggio2017_theory}
\abx@aux@segm{0}{0}{poggio2017_theory}
\@writefile{lof}{\contentsline {figure}{\numberline {5.24}{\ignorespaces Reality check: Neural networks can approximate complex functions, but universal approximation does not guarantee practical learnability.}}{182}{figure.caption.262}\protected@file@percent }
\newlabel{fig:chapter5_reality_check}{{5.24}{182}{Reality check: Neural networks can approximate complex functions, but universal approximation does not guarantee practical learnability}{figure.caption.262}{}}
\@writefile{toc}{\contentsline {subsection}{Enrichment 5.6.4: Deep Networks vs Shallow Networks}{182}{section*.263}\protected@file@percent }
\abx@aux@backref{129}{eldan2016_power}{0}{182}{182}
\abx@aux@backref{130}{telgarsky2016_benefits}{0}{182}{182}
\abx@aux@backref{131}{bengio2013_representation}{0}{182}{182}
\abx@aux@cite{0}{hochreiter1997_lstm}
\abx@aux@segm{0}{0}{hochreiter1997_lstm}
\abx@aux@cite{0}{pascanu2013_difficulty}
\abx@aux@segm{0}{0}{pascanu2013_difficulty}
\BKM@entry{id=221,dest={73656374696F6E2E352E37},srcline={417}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030765C303030655C303030785C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E5C303030735C3030303A5C3030305C3034305C303030415C3030305C3034305C303030535C303030705C303030655C303030635C303030695C303030615C3030306C5C3030305C3034305C303030435C303030615C303030735C30303065}
\BKM@entry{id=222,dest={73756273656374696F6E2E352E372E31},srcline={438}}{5C3337365C3337375C3030304E5C3030306F5C3030306E5C3030302D5C303030435C3030306F5C3030306E5C303030765C303030655C303030785C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\abx@aux@backref{132}{poggio2017_theory}{0}{183}{183}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 5.6.4.1: Why Not Just Use a Very Deep and Wide Network?}{183}{section*.264}\protected@file@percent }
\abx@aux@backref{133}{hochreiter1997_lstm}{0}{183}{183}
\abx@aux@backref{134}{pascanu2013_difficulty}{0}{183}{183}
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Convex Functions: A Special Case}{183}{section.5.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.25}{\ignorespaces The parabola \( f(x) = x^2 \) is an example of a convex function.}}{183}{figure.caption.265}\protected@file@percent }
\newlabel{fig:chapter5_convex_function}{{5.25}{183}{The parabola \( f(x) = x^2 \) is an example of a convex function}{figure.caption.265}{}}
\BKM@entry{id=223,dest={73756273656374696F6E2E352E372E32},srcline={448}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030765C303030655C303030785C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030695C3030306E5C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.1}Non-Convex Functions}{184}{subsection.5.7.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.26}{\ignorespaces \( f(x) = \cos (x) \) is an example of a non-convex function.}}{184}{figure.caption.266}\protected@file@percent }
\newlabel{fig:chapter5_nonconvex_function}{{5.26}{184}{\( f(x) = \cos (x) \) is an example of a non-convex function}{figure.caption.266}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.2}Convex Optimization in Linear Classifiers}{184}{subsection.5.7.2}\protected@file@percent }
\BKM@entry{id=224,dest={73756273656374696F6E2E352E372E33},srcline={470}}{5C3337365C3337375C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=225,dest={73756273656374696F6E2E352E372E34},srcline={479}}{5C3337365C3337375C303030425C303030725C303030695C303030645C303030675C303030695C3030306E5C303030675C3030305C3034305C303030745C3030306F5C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {5.27}{\ignorespaces Optimization problems for linear classifiers are convex.}}{185}{figure.caption.267}\protected@file@percent }
\newlabel{fig:chapter5_linear_convex_optimization}{{5.27}{185}{Optimization problems for linear classifiers are convex}{figure.caption.267}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.3}Challenges with Neural Networks}{185}{subsection.5.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.4}Bridging to Backpropagation: Efficient Gradient Computation}{185}{subsection.5.7.4}\protected@file@percent }
\BKM@entry{id=226,dest={636861707465722E36},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030365C3030303A5C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=227,dest={73656374696F6E2E362E31},srcline={10}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030695C3030306E5C303030675C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C30303073}
\BKM@entry{id=228,dest={73756273656374696F6E2E362E312E31},srcline={25}}{5C3337365C3337375C303030415C3030305C3034305C303030425C303030615C303030645C3030305C3034305C303030495C303030645C303030655C303030615C3030303A5C3030305C3034305C3030304D5C303030615C3030306E5C303030755C303030615C3030306C5C3030306C5C303030795C3030305C3034305C303030445C303030655C303030725C303030695C303030765C303030695C3030306E5C303030675C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C30303073}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Lecture 6: Backpropagation}{186}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@5}}
\ttl@writefile{ptc}{\ttl@starttoc{default@6}}
\pgfsyspdfmark {pgfid18}{0}{52099153}
\pgfsyspdfmark {pgfid17}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Introduction: The Challenge of Computing Gradients}{186}{section.6.1}\protected@file@percent }
\newlabel{sec:introduction}{{6.1}{186}{Introduction: The Challenge of Computing Gradients}{section.6.1}{}}
\BKM@entry{id=229,dest={73756273656374696F6E2E362E312E32},srcline={36}}{5C3337365C3337375C303030415C3030305C3034305C303030425C303030655C303030745C303030745C303030655C303030725C3030305C3034305C303030495C303030645C303030655C303030615C3030303A5C3030305C3034305C303030555C303030745C303030695C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030725C303030615C303030705C303030685C303030735C3030305C3034305C3030305C3035305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C303531}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}A Bad Idea: Manually Deriving Gradients}{187}{subsection.6.1.1}\protected@file@percent }
\newlabel{sec:manual-gradients}{{6.1.1}{187}{A Bad Idea: Manually Deriving Gradients}{subsection.6.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Manually deriving gradients for a simple linear classifier using the SVM loss. This process becomes infeasible for deep networks.}}{187}{figure.caption.268}\protected@file@percent }
\newlabel{fig:chapter6_manual_gradients}{{6.1}{187}{Manually deriving gradients for a simple linear classifier using the SVM loss. This process becomes infeasible for deep networks}{figure.caption.268}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}A Better Idea: Utilizing Computational Graphs (Backpropagation)}{187}{subsection.6.1.2}\protected@file@percent }
\newlabel{sec:comp-graphs}{{6.1.2}{187}{A Better Idea: Utilizing Computational Graphs (Backpropagation)}{subsection.6.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Computational graphs provide a structured, automatic approach to computing gradients.}}{187}{figure.caption.269}\protected@file@percent }
\newlabel{fig:chapter6_comp_graphs}{{6.2}{187}{Computational graphs provide a structured, automatic approach to computing gradients}{figure.caption.269}{}}
\BKM@entry{id=230,dest={73656374696F6E2E362E32},srcline={61}}{5C3337365C3337375C303030545C3030306F5C303030795C3030305C3034305C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030665C3030305C3035305C303030785C3030302C5C303030795C3030302C5C3030307A5C3030305C3035315C3030303D5C3030305C3035305C303030785C3030302B5C303030795C3030305C3035315C3030307A}
\@writefile{toc}{\contentsline {paragraph}{Why Use Computational Graphs?}{188}{section*.270}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Toy Example of Backpropagation: $f(x,y,z) = (x + y)\,z$}{188}{section.6.2}\protected@file@percent }
\newlabel{sec:toy-example}{{6.2}{188}{Toy Example of Backpropagation: \texorpdfstring {$f(x,y,z) = (x + y)\,z$}{f(x,y,z)=(x+y)z}}{section.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Computational graph representation of \(f(x,y,z) = (x + y)z\), showing forward and backward passes. Intermediate values of the forward pass are presented in green on-top of the graph edges, while the corresponding backpropagation values are presented in red below the edges.}}{188}{figure.caption.271}\protected@file@percent }
\newlabel{fig:chapter6_example_fxyz}{{6.3}{188}{Computational graph representation of \(f(x,y,z) = (x + y)z\), showing forward and backward passes. Intermediate values of the forward pass are presented in green on-top of the graph edges, while the corresponding backpropagation values are presented in red below the edges}{figure.caption.271}{}}
\BKM@entry{id=231,dest={73756273656374696F6E2E362E322E31},srcline={78}}{5C3337365C3337375C303030465C3030306F5C303030725C303030775C303030615C303030725C303030645C3030305C3034305C303030505C303030615C303030735C30303073}
\BKM@entry{id=232,dest={73756273656374696F6E2E362E322E32},srcline={85}}{5C3337365C3337375C303030425C303030615C303030635C3030306B5C303030775C303030615C303030725C303030645C3030305C3034305C303030505C303030615C303030735C303030735C3030303A5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030695C3030306E5C303030675C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C30303073}
\BKM@entry{id=233,dest={73656374696F6E2E362E33},srcline={94}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030303F}
\BKM@entry{id=234,dest={73756273656374696F6E2E362E332E31},srcline={95}}{5C3337365C3337375C3030304C5C3030306F5C303030635C303030615C3030306C5C3030305C3034305C3030305C3034365C3030305C3034305C303030535C303030635C303030615C3030306C5C303030615C303030625C3030306C5C303030655C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C303030735C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Forward Pass}{189}{subsection.6.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Backward Pass: Computing Gradients}{189}{subsection.6.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Why Backpropagation?}{189}{section.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Local \& Scalable Gradients Computation}{189}{subsection.6.3.1}\protected@file@percent }
\newlabel{subsec:local-upstream}{{6.3.1}{189}{Local \& Scalable Gradients Computation}{subsection.6.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces During backpropagation, each node in the computational graph computes its downstream gradients using the local gradient (computed based on the local operation over the input. For instance, if we denote the input to the node as x and the node computes $\frac  {1}{x}$, then the local gradient is $\frac  {\partial }{\partial x}{[\frac  {1}{x}]}=-\frac  {1}{x^2}$) and the upstream gradient that is simply given as input from subsequent nodes.}}{189}{figure.caption.272}\protected@file@percent }
\newlabel{fig:chapter6_local_upstream}{{6.4}{189}{During backpropagation, each node in the computational graph computes its downstream gradients using the local gradient (computed based on the local operation over the input. For instance, if we denote the input to the node as x and the node computes $\frac {1}{x}$, then the local gradient is $\frac {\partial }{\partial x}{[\frac {1}{x}]}=-\frac {1}{x^2}$) and the upstream gradient that is simply given as input from subsequent nodes}{figure.caption.272}{}}
\BKM@entry{id=235,dest={73756273656374696F6E2E362E332E32},srcline={128}}{5C3337365C3337375C303030505C303030615C303030695C303030725C303030695C3030306E5C303030675C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C303030735C3030305C3034305C3030305C3034365C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030655C303030725C303030735C3030305C3034305C303030695C303030735C3030305C3034305C303030455C303030615C303030735C30303079}
\BKM@entry{id=236,dest={73756273656374696F6E2E362E332E33},srcline={131}}{5C3337365C3337375C3030304D5C3030306F5C303030645C303030755C3030306C5C303030615C303030725C303030695C303030745C303030795C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030435C303030755C303030735C303030745C3030306F5C3030306D5C3030305C3034305C3030304E5C3030306F5C303030645C303030655C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Visualizing the backpropagation process for a node f that is given as inputs x, y and outputs z. As can be seen, the backpropagation gives us the upstream gradient from subsequent node in the graph, and we are only left with the local gradients computation: $\frac  {\partial z}{\partial x}, \frac  {\partial z}{\partial y}$ in order to compute the downstream gradients: $\frac  {\partial L}{\partial x}, \frac  {\partial L}{\partial y}$ allowing us to continue the graph traversal in the backpropagation process.}}{190}{figure.caption.273}\protected@file@percent }
\newlabel{fig:chapter6_indpendent_node}{{6.5}{190}{Visualizing the backpropagation process for a node f that is given as inputs x, y and outputs z. As can be seen, the backpropagation gives us the upstream gradient from subsequent node in the graph, and we are only left with the local gradients computation: $\frac {\partial z}{\partial x}, \frac {\partial z}{\partial y}$ in order to compute the downstream gradients: $\frac {\partial L}{\partial x}, \frac {\partial L}{\partial y}$ allowing us to continue the graph traversal in the backpropagation process}{figure.caption.273}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Pairing Backpropagation Gradients \& Optimizers is Easy}{190}{subsection.6.3.2}\protected@file@percent }
\BKM@entry{id=237,dest={73756273656374696F6E2E362E332E34},srcline={150}}{5C3337365C3337375C303030555C303030745C303030695C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030505C303030615C303030745C303030745C303030655C303030725C3030306E5C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030465C3030306C5C3030306F5C30303077}
\BKM@entry{id=238,dest={73756273656374696F6E2E362E332E35},srcline={155}}{5C3337365C3337375C303030415C303030645C303030645C303030695C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030475C303030615C303030745C303030655C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030695C303030735C303030745C303030725C303030695C303030625C303030755C303030745C3030306F5C30303072}
\BKM@entry{id=239,dest={73756273656374696F6E2E362E332E36},srcline={160}}{5C3337365C3337375C303030435C3030306F5C303030705C303030795C3030305C3034305C303030475C303030615C303030745C303030655C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030415C303030645C303030645C303030655C30303072}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.3}Modularity and Custom Nodes}{191}{subsection.6.3.3}\protected@file@percent }
\newlabel{sec:modularity-custom-nodes}{{6.3.3}{191}{Modularity and Custom Nodes}{subsection.6.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces A ``sigmoid node'' (in the blue rectangle) can replace multiple low-level operations (the intermediate nodes encapsulated within). Its known derivative simplifies backpropagation.}}{191}{figure.caption.274}\protected@file@percent }
\newlabel{fig:chapter6_sigmoid_node}{{6.6}{191}{A ``sigmoid node'' (in the blue rectangle) can replace multiple low-level operations (the intermediate nodes encapsulated within). Its known derivative simplifies backpropagation}{figure.caption.274}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.4}Utilizing Patterns in Gradient Flow}{191}{subsection.6.3.4}\protected@file@percent }
\newlabel{sec:gradient-flow-patterns}{{6.3.4}{191}{Utilizing Patterns in Gradient Flow}{subsection.6.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.5}Addition Gate: The Gradient Distributor}{191}{subsection.6.3.5}\protected@file@percent }
\BKM@entry{id=240,dest={73756273656374696F6E2E362E332E37},srcline={176}}{5C3337365C3337375C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030475C303030615C303030745C303030655C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030535C303030775C303030615C303030705C303030705C303030655C30303072}
\BKM@entry{id=241,dest={73756273656374696F6E2E362E332E38},srcline={197}}{5C3337365C3337375C3030304D5C303030615C303030785C3030305C3034305C303030475C303030615C303030745C303030655C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030525C3030306F5C303030755C303030745C303030655C30303072}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.6}Copy Gate: The Gradient Adder}{192}{subsection.6.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.7}Multiplication Gate: The Gradient Swapper}{192}{subsection.6.3.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.8}Max Gate: The Gradient Router}{192}{subsection.6.3.8}\protected@file@percent }
\BKM@entry{id=242,dest={73656374696F6E2E362E34},srcline={208}}{5C3337365C3337375C303030495C3030306D5C303030705C3030306C5C303030655C3030306D5C303030655C3030306E5C303030745C303030695C3030306E5C303030675C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030695C3030306E5C3030305C3034305C303030435C3030306F5C303030645C30303065}
\BKM@entry{id=243,dest={73756273656374696F6E2E362E342E31},srcline={220}}{5C3337365C3337375C303030465C3030306C5C303030615C303030745C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030415C3030305C3034305C303030445C303030695C303030725C303030655C303030635C303030745C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030615C303030635C30303068}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces Visualization of gradient flow patterns: (1) Add gate—gradient distributor, (2) Copy gate—gradient adder, (3) Multiplication gate—gradient swapper, and (4) Max gate—gradient router.}}{193}{figure.caption.275}\protected@file@percent }
\newlabel{fig:chapter6_gradient_patterns}{{6.7}{193}{Visualization of gradient flow patterns: (1) Add gate—gradient distributor, (2) Copy gate—gradient adder, (3) Multiplication gate—gradient swapper, and (4) Max gate—gradient router}{figure.caption.275}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Implementing Backpropagation in Code}{193}{section.6.4}\protected@file@percent }
\newlabel{sec:implementing-backprop}{{6.4}{193}{Implementing Backpropagation in Code}{section.6.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces A pseudo-implementation of forward and backward passes in a flat gradient backpropagation implementation.}}{193}{figure.caption.276}\protected@file@percent }
\newlabel{fig:chapter6_flat_backprop}{{6.8}{193}{A pseudo-implementation of forward and backward passes in a flat gradient backpropagation implementation}{figure.caption.276}{}}
\BKM@entry{id=244,dest={73656374696F6E2E362E35},srcline={250}}{5C3337365C3337375C303030415C3030305C3034305C3030304D5C3030306F5C303030725C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030755C3030306C5C303030615C303030725C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030615C303030635C303030685C3030303A5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030725C303030615C303030705C303030685C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030505C303030725C303030615C303030635C303030745C303030695C303030635C30303065}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}Flat Backpropagation: A Direct Approach}{194}{subsection.6.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Flat Backpropagation Works Well.}{194}{section*.277}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.5}A More Modular Approach: Computational Graphs in Practice}{194}{section.6.5}\protected@file@percent }
\newlabel{sec:modular-backprop}{{6.5}{194}{A More Modular Approach: Computational Graphs in Practice}{section.6.5}{}}
\BKM@entry{id=245,dest={73756273656374696F6E2E362E352E31},srcline={267}}{5C3337365C3337375C303030545C3030306F5C303030705C3030306F5C3030306C5C3030306F5C303030675C303030695C303030635C303030615C3030306C5C3030305C3034305C3030304F5C303030725C303030645C303030655C303030725C303030695C3030306E5C303030675C3030305C3034305C303030695C3030306E5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030725C303030615C303030705C303030685C30303073}
\BKM@entry{id=246,dest={73756273656374696F6E2E362E352E32},srcline={275}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030415C303030505C303030495C3030303A5C3030305C3034305C303030465C3030306F5C303030725C303030775C303030615C303030725C303030645C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030425C303030615C303030635C3030306B5C303030775C303030615C303030725C303030645C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C30303073}
\BKM@entry{id=247,dest={73756273656374696F6E2E362E352E33},srcline={284}}{5C3337365C3337375C303030415C303030645C303030765C303030615C3030306E5C303030745C303030615C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030615C3030305C3034305C3030304D5C3030306F5C303030645C303030755C3030306C5C303030615C303030725C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030725C303030615C303030705C30303068}
\BKM@entry{id=248,dest={73656374696F6E2E362E36},srcline={294}}{5C3337365C3337375C303030495C3030306D5C303030705C3030306C5C303030655C3030306D5C303030655C3030306E5C303030745C303030695C3030306E5C303030675C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030505C303030795C303030545C3030306F5C303030725C303030635C303030685C3030305C3034305C303030415C303030755C303030745C3030306F5C303030675C303030725C303030615C30303064}
\@writefile{lof}{\contentsline {figure}{\numberline {6.9}{\ignorespaces API for a computational graph, requiring an implementation of both the forward and backward methods.}}{195}{figure.caption.278}\protected@file@percent }
\newlabel{fig:chapter6_computational_graph_api}{{6.9}{195}{API for a computational graph, requiring an implementation of both the forward and backward methods}{figure.caption.278}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.1}Topological Ordering in Computational Graphs}{195}{subsection.6.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.2}The API: Forward and Backward Methods}{195}{subsection.6.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.3}Advantages of a Modular Computational Graph}{195}{subsection.6.5.3}\protected@file@percent }
\BKM@entry{id=249,dest={73756273656374696F6E2E362E362E31},srcline={310}}{5C3337365C3337375C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030303A5C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030475C303030615C303030745C303030655C3030305C3034305C303030695C3030306E5C3030305C3034305C303030415C303030755C303030745C3030306F5C303030675C303030725C303030615C30303064}
\BKM@entry{id=250,dest={73756273656374696F6E2E362E362E32},srcline={322}}{5C3337365C3337375C303030455C303030785C303030745C303030655C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030415C303030755C303030745C3030306F5C303030675C303030725C303030615C303030645C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030435C303030755C303030735C303030745C3030306F5C3030306D5C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Implementing Backpropagation with PyTorch Autograd}{196}{section.6.6}\protected@file@percent }
\newlabel{sec:pytorch-autograd}{{6.6}{196}{Implementing Backpropagation with PyTorch Autograd}{section.6.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.10}{\ignorespaces Example of PyTorch Autograd implementation for a multiplication gate (\( z = x \cdot y \)).}}{196}{figure.caption.279}\protected@file@percent }
\newlabel{fig:chapter6_autograd_multiplication}{{6.10}{196}{Example of PyTorch Autograd implementation for a multiplication gate (\( z = x \cdot y \))}{figure.caption.279}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.1}Example: Multiplication Gate in Autograd}{196}{subsection.6.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6.2}Extending Autograd for Custom Functions}{196}{subsection.6.6.2}\protected@file@percent }
\BKM@entry{id=251,dest={73656374696F6E2E362E37},srcline={338}}{5C3337365C3337375C303030425C303030655C303030795C3030306F5C3030306E5C303030645C3030305C3034305C303030535C303030635C303030615C3030306C5C303030615C303030725C303030735C3030303A5C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030565C303030655C303030635C303030745C3030306F5C303030725C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030545C303030655C3030306E5C303030735C3030306F5C303030725C30303073}
\BKM@entry{id=252,dest={73756273656374696F6E2E362E372E31},srcline={350}}{5C3337365C3337375C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C303030735C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C3030304A5C303030615C303030635C3030306F5C303030625C303030695C303030615C3030306E5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {6.11}{\ignorespaces Example of PyTorch's \texttt  {sigmoid} layer implementation with automatic differentiation.}}{197}{figure.caption.280}\protected@file@percent }
\newlabel{fig:chapter6_autograd_sigmoid}{{6.11}{197}{Example of PyTorch's \texttt {sigmoid} layer implementation with automatic differentiation}{figure.caption.280}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.7}Beyond Scalars: Backpropagation for Vectors and Tensors}{197}{section.6.7}\protected@file@percent }
\newlabel{sec:vector-backprop}{{6.7}{197}{Beyond Scalars: Backpropagation for Vectors and Tensors}{section.6.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.12}{\ignorespaces Recap of scalar derivatives, gradients, and Jacobians.}}{197}{figure.caption.281}\protected@file@percent }
\newlabel{fig:chapter6_jacobians}{{6.12}{197}{Recap of scalar derivatives, gradients, and Jacobians}{figure.caption.281}{}}
\BKM@entry{id=253,dest={73756273656374696F6E2E362E372E32},srcline={374}}{5C3337365C3337375C303030455C303030785C303030745C303030655C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030565C303030655C303030635C303030745C3030306F5C303030725C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.1}Gradients vs.\ Jacobians}{198}{subsection.6.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.2}Extending Backpropagation to Vectors}{198}{subsection.6.7.2}\protected@file@percent }
\newlabel{sec:vector_backprop}{{6.7.2}{198}{Extending Backpropagation to Vectors}{subsection.6.7.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.13}{\ignorespaces A node \( f \) receiving two vectors \(\mathbf  {x}\in \mathbb  {R}^{D_x}\) and \(\mathbf  {y}\in \mathbb  {R}^{D_y}\) and producing \(\mathbf  {z}\in \mathbb  {R}^{D_z}\). We extend backpropagation to handle vector inputs and outputs.}}{198}{figure.caption.282}\protected@file@percent }
\newlabel{fig:chapter6_vector_backprop}{{6.13}{198}{A node \( f \) receiving two vectors \(\mathbf {x}\in \mathbb {R}^{D_x}\) and \(\mathbf {y}\in \mathbb {R}^{D_y}\) and producing \(\mathbf {z}\in \mathbb {R}^{D_z}\). We extend backpropagation to handle vector inputs and outputs}{figure.caption.282}{}}
\BKM@entry{id=254,dest={73756273656374696F6E2E362E372E33},srcline={418}}{5C3337365C3337375C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030303A5C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030455C3030306C5C303030655C3030306D5C303030655C3030306E5C303030745C303030775C303030695C303030735C303030655C3030305C3034305C303030525C303030655C3030304C5C30303055}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.3}Example: Backpropagation for Elementwise ReLU}{199}{subsection.6.7.3}\protected@file@percent }
\newlabel{sec:relu_vector_backprop}{{6.7.3}{199}{Example: Backpropagation for Elementwise ReLU}{subsection.6.7.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.14}{\ignorespaces Backprop through an elementwise ReLU node. Negative inputs produce zeros in the output (and zero gradients), while positive inputs pass gradients through.}}{199}{figure.caption.283}\protected@file@percent }
\newlabel{fig:chapter6_relu_backprop}{{6.14}{199}{Backprop through an elementwise ReLU node. Negative inputs produce zeros in the output (and zero gradients), while positive inputs pass gradients through}{figure.caption.283}{}}
\BKM@entry{id=255,dest={73756273656374696F6E2E362E372E34},srcline={469}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030765C303030695C303030615C3030305C3034305C3030304C5C3030306F5C303030635C303030615C3030306C5C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030535C3030306C5C303030695C303030635C303030655C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {6.15}{\ignorespaces A more memory-efficient approach: do not form \(\tfrac  {\partial \mathbf  {y}}{\partial \mathbf  {x}}\) explicitly. Instead, reuse the input mask (i.e., which elements of \(\mathbf  {x}\) are positive).}}{200}{figure.caption.284}\protected@file@percent }
\newlabel{fig:chapter6_relu_implicit}{{6.15}{200}{A more memory-efficient approach: do not form \(\tfrac {\partial \mathbf {y}}{\partial \mathbf {x}}\) explicitly. Instead, reuse the input mask (i.e., which elements of \(\mathbf {x}\) are positive)}{figure.caption.284}{}}
\BKM@entry{id=256,dest={73756273656374696F6E2E362E372E35},srcline={474}}{5C3337365C3337375C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304D5C303030615C303030745C303030725C303030695C303030635C303030655C303030735C3030303A5C3030305C3034305C303030415C3030305C3034305C303030435C3030306F5C3030306E5C303030635C303030725C303030655C303030745C303030655C3030305C3034305C303030455C303030785C303030615C3030306D5C303030705C3030306C5C30303065}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.4}Efficient Computation via Local Gradient Slices}{201}{subsection.6.7.4}\protected@file@percent }
\newlabel{sec:implicit_jacobian}{{6.7.4}{201}{Efficient Computation via Local Gradient Slices}{subsection.6.7.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.5}Backpropagation with Matrices: A Concrete Example}{201}{subsection.6.7.5}\protected@file@percent }
\newlabel{sec:gradient_slices}{{6.7.5}{201}{Backpropagation with Matrices: A Concrete Example}{subsection.6.7.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Numerical Setup.}{201}{section*.285}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.16}{\ignorespaces Computing a ``gradient slice'' for a single element \(\mathbf  {X}_{i,j}\). Rather than storing the entire local Jacobian, we only determine how \(\mathbf  {X}_{i,j}\) influences each output element of \(\mathbf  {Y}\), then combine that slice with the relevant elements of \(\tfrac  {\partial L}{\partial \mathbf  {Y}}\). }}{201}{figure.caption.286}\protected@file@percent }
\newlabel{fig:chapter6_gradient_slice}{{6.16}{201}{Computing a ``gradient slice'' for a single element \(\mathbf {X}_{i,j}\). Rather than storing the entire local Jacobian, we only determine how \(\mathbf {X}_{i,j}\) influences each output element of \(\mathbf {Y}\), then combine that slice with the relevant elements of \(\tfrac {\partial L}{\partial \mathbf {Y}}\)}{figure.caption.286}{}}
\@writefile{toc}{\contentsline {paragraph}{Slice Logic for One Input Element.}{202}{section*.287}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.17}{\ignorespaces Another view of the slice approach for \(\mathbf  {X}_{1,1}\). Only the first row of \(\mathbf  {Y}\) receives a nonzero local gradient from this input element. }}{202}{figure.caption.288}\protected@file@percent }
\newlabel{fig:chapter6_gradient_first}{{6.17}{202}{Another view of the slice approach for \(\mathbf {X}_{1,1}\). Only the first row of \(\mathbf {Y}\) receives a nonzero local gradient from this input element}{figure.caption.288}{}}
\@writefile{toc}{\contentsline {paragraph}{Another Example: \(\mathbf  {X}_{2,3}\).}{202}{section*.289}\protected@file@percent }
\BKM@entry{id=257,dest={73756273656374696F6E2E362E372E36},srcline={601}}{5C3337365C3337375C303030495C3030306D5C303030705C3030306C5C303030695C303030635C303030695C303030745C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030455C3030306E5C303030745C303030695C303030725C303030655C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C30303074}
\@writefile{lof}{\contentsline {figure}{\numberline {6.18}{\ignorespaces Similarly, \(\mathbf  {X}_{2,3}\) affects only the second row of \(\mathbf  {Y}\). By repeating this logic for all elements, we derive the standard matrix-multiplication backprop formulas. }}{203}{figure.caption.290}\protected@file@percent }
\newlabel{fig:chapter6_gradient_last}{{6.18}{203}{Similarly, \(\mathbf {X}_{2,3}\) affects only the second row of \(\mathbf {Y}\). By repeating this logic for all elements, we derive the standard matrix-multiplication backprop formulas}{figure.caption.290}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.6}Implicit Multiplication for the Entire Gradient}{203}{subsection.6.7.6}\protected@file@percent }
\newlabel{sec:implicit_mult}{{6.7.6}{203}{Implicit Multiplication for the Entire Gradient}{subsection.6.7.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.19}{\ignorespaces By using vector/matrix multiplications and slicing logic, we avoid forming massive Jacobians in memory. }}{203}{figure.caption.291}\protected@file@percent }
\newlabel{fig:chapter6_matrix_implicit}{{6.19}{203}{By using vector/matrix multiplications and slicing logic, we avoid forming massive Jacobians in memory}{figure.caption.291}{}}
\BKM@entry{id=258,dest={73756273656374696F6E2E362E372E37},srcline={640}}{5C3337365C3337375C303030415C3030305C3034305C303030435C303030685C303030615C303030695C3030306E5C3030305C3034305C303030565C303030695C303030655C303030775C3030305C3034305C3030306F5C303030665C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {paragraph}{Why Slices Are the Solution.}{204}{section*.292}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.7}A Chain View of Backpropagation}{204}{subsection.6.7.7}\protected@file@percent }
\newlabel{sec:chain_view_backprop}{{6.7.7}{204}{A Chain View of Backpropagation}{subsection.6.7.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{Reverse-Mode Automatic Differentiation}{204}{section*.293}\protected@file@percent }
\newlabel{sec:reverse_mode_ad}{{6.7.7}{204}{Reverse-Mode Automatic Differentiation}{section*.293}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.20}{\ignorespaces Reverse-mode automatic differentiation can exploit the associativity of matrix multiplication to replace potentially expensive matrix-matrix products with matrix-vector products, moving from right to left. }}{204}{figure.caption.294}\protected@file@percent }
\newlabel{fig:chapter6_reverse_mode}{{6.20}{204}{Reverse-mode automatic differentiation can exploit the associativity of matrix multiplication to replace potentially expensive matrix-matrix products with matrix-vector products, moving from right to left}{figure.caption.294}{}}
\BKM@entry{id=259,dest={73756273656374696F6E2E362E372E38},srcline={697}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030695C3030306E5C303030675C3030305C3034305C303030485C303030695C303030675C303030685C303030655C303030725C3030302D5C3030304F5C303030725C303030645C303030655C303030725C3030305C3034305C303030445C303030655C303030725C303030695C303030765C303030615C303030745C303030695C303030765C303030655C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsubsection}{Forward-Mode Automatic Differentiation}{205}{section*.295}\protected@file@percent }
\newlabel{sec:forward_mode_ad}{{6.7.7}{205}{Forward-Mode Automatic Differentiation}{section*.295}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.21}{\ignorespaces Forward-mode automatic differentiation is useful for computing the derivatives of scalar inputs with respect to multiple outputs. While not commonly used in deep learning, it is widely applied in physics simulations and sensitivity analysis. }}{205}{figure.caption.296}\protected@file@percent }
\newlabel{fig:chapter6_forward_mode}{{6.21}{205}{Forward-mode automatic differentiation is useful for computing the derivatives of scalar inputs with respect to multiple outputs. While not commonly used in deep learning, it is widely applied in physics simulations and sensitivity analysis}{figure.caption.296}{}}
\@writefile{toc}{\contentsline {paragraph}{When Is Forward-Mode Automatic Differentiation is Useful?}{205}{section*.297}\protected@file@percent }
\BKM@entry{id=260,dest={73756273656374696F6E2E362E372E39},srcline={726}}{5C3337365C3337375C303030415C303030705C303030705C3030306C5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030302D5C3030304E5C3030306F5C303030725C3030306D5C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.8}Computing Higher-Order Derivatives with Backpropagation}{206}{subsection.6.7.8}\protected@file@percent }
\newlabel{sec:higher_order_backprop}{{6.7.8}{206}{Computing Higher-Order Derivatives with Backpropagation}{subsection.6.7.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.22}{\ignorespaces Using backpropagation to compute Hessian-vector products as an efficient way to obtain second-order derivatives. }}{206}{figure.caption.298}\protected@file@percent }
\newlabel{fig:chapter6_hessian_backprop}{{6.22}{206}{Using backpropagation to compute Hessian-vector products as an efficient way to obtain second-order derivatives}{figure.caption.298}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Compute Hessians?}{206}{section*.299}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Efficient Hessian Computation: Hessian-Vector Products}{206}{section*.300}\protected@file@percent }
\BKM@entry{id=261,dest={73756273656374696F6E2E362E372E3130},srcline={744}}{5C3337365C3337375C303030415C303030755C303030745C3030306F5C3030306D5C303030615C303030745C303030695C303030635C3030305C3034305C303030445C303030695C303030665C303030665C303030655C303030725C303030655C3030306E5C303030745C303030695C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304B5C303030655C303030795C3030305C3034305C303030495C3030306E5C303030735C303030695C303030675C303030685C303030745C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.9}Application: Gradient-Norm Regularization}{207}{subsection.6.7.9}\protected@file@percent }
\newlabel{sec:higher_order_regularization}{{6.7.9}{207}{Application: Gradient-Norm Regularization}{subsection.6.7.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.23}{\ignorespaces An example of using second-order derivatives: Regularizing the gradient norm to improve optimization stability. }}{207}{figure.caption.301}\protected@file@percent }
\newlabel{fig:chapter6_gradient_norm_reg}{{6.23}{207}{An example of using second-order derivatives: Regularizing the gradient norm to improve optimization stability}{figure.caption.301}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.10}Automatic Differentiation: Summary of Key Insights}{207}{subsection.6.7.10}\protected@file@percent }
\BKM@entry{id=262,dest={636861707465722E37},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030375C3030303A5C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=263,dest={73656374696F6E2E372E31},srcline={10}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030465C303030755C3030306C5C3030306C5C303030795C3030302D5C303030435C3030306F5C3030306E5C3030306E5C303030655C303030635C303030745C303030655C303030645C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=264,dest={73656374696F6E2E372E32},srcline={26}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C3030306F5C3030306E5C303030655C3030306E5C303030745C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Lecture 7: Convolutional Networks}{208}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@6}}
\ttl@writefile{ptc}{\ttl@starttoc{default@7}}
\pgfsyspdfmark {pgfid20}{0}{52099153}
\pgfsyspdfmark {pgfid19}{5966969}{45620378}
\newlabel{chap:cnn}{{7}{208}{Lecture 7: Convolutional Networks}{chapter.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Introduction: The Limitations of Fully-Connected Networks}{208}{section.7.1}\protected@file@percent }
\newlabel{sec:cnn_intro}{{7.1}{208}{Introduction: The Limitations of Fully-Connected Networks}{section.7.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Fully-connected networks and linear classifiers do not respect the 2D spatial structure of images, requiring us to flatten image pixels into a single vector.}}{208}{figure.caption.302}\protected@file@percent }
\newlabel{fig:chapter7_flattening_problem}{{7.1}{208}{Fully-connected networks and linear classifiers do not respect the 2D spatial structure of images, requiring us to flatten image pixels into a single vector}{figure.caption.302}{}}
\BKM@entry{id=265,dest={73656374696F6E2E372E33},srcline={45}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C303030735C3030303A5C3030305C3034305C303030505C303030725C303030655C303030735C303030655C303030725C303030765C303030695C3030306E5C303030675C3030305C3034305C303030535C303030705C303030615C303030745C303030695C303030615C3030306C5C3030305C3034305C303030535C303030745C303030725C303030755C303030635C303030745C303030755C303030725C30303065}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Components of Convolutional Neural Networks}{209}{section.7.2}\protected@file@percent }
\newlabel{sec:cnn_components}{{7.2}{209}{Components of Convolutional Neural Networks}{section.7.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Key components of Convolutional Neural Networks (CNNs). In addition to fully-connected layers and activation functions, CNNs introduce convolutional layers, pooling layers, and normalization layers.}}{209}{figure.caption.303}\protected@file@percent }
\newlabel{fig:chapter7_cnn_components}{{7.2}{209}{Key components of Convolutional Neural Networks (CNNs). In addition to fully-connected layers and activation functions, CNNs introduce convolutional layers, pooling layers, and normalization layers}{figure.caption.303}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Convolutional Layers: Preserving Spatial Structure}{209}{section.7.3}\protected@file@percent }
\newlabel{sec:conv_layers_intro}{{7.3}{209}{Convolutional Layers: Preserving Spatial Structure}{section.7.3}{}}
\BKM@entry{id=266,dest={73756273656374696F6E2E372E332E31},srcline={57}}{5C3337365C3337375C303030495C3030306E5C303030705C303030755C303030745C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304F5C303030755C303030745C303030705C303030755C303030745C3030305C3034305C303030445C303030695C3030306D5C303030655C3030306E5C303030735C303030695C3030306F5C3030306E5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces A filter is applied to a local region of the input tensor, producing a single number at each spatial position.}}{210}{figure.caption.304}\protected@file@percent }
\newlabel{fig:chapter7_filter_application}{{7.3}{210}{A filter is applied to a local region of the input tensor, producing a single number at each spatial position}{figure.caption.304}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.1}Input and Output Dimensions}{210}{subsection.7.3.1}\protected@file@percent }
\newlabel{subsec:conv_input_output}{{7.3.1}{210}{Input and Output Dimensions}{subsection.7.3.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Common Filter Sizes}{210}{section*.305}\protected@file@percent }
\BKM@entry{id=267,dest={73756273656374696F6E2E372E332E32},srcline={96}}{5C3337365C3337375C303030465C303030695C3030306C5C303030745C303030655C303030725C3030305C3034305C303030415C303030705C303030705C3030306C5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304F5C303030755C303030745C303030705C303030755C303030745C3030305C3034305C303030435C303030615C3030306C5C303030635C303030755C3030306C5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=268,dest={73656374696F6E2A2E333038},srcline={121}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030335C3030302E5C303030335C3030303A5C3030305C3034305C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030685C303030725C3030306F5C303030755C303030675C303030685C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030535C3030306F5C303030625C303030655C3030306C5C3030305C3034305C3030304F5C303030705C303030655C303030725C303030615C303030745C3030306F5C30303072}
\@writefile{toc}{\contentsline {paragraph}{Why Are Kernel Sizes Typically Odd?}{211}{section*.306}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.2}Filter Application and Output Calculation}{211}{subsection.7.3.2}\protected@file@percent }
\newlabel{subsec:conv_filter_output}{{7.3.2}{211}{Filter Application and Output Calculation}{subsection.7.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces Applying two convolutional filters to a \(3 \times 32 \times 32\) input image produces two activation maps of shape \(1 \times 28 \times 28\) (no padding applied).}}{211}{figure.caption.307}\protected@file@percent }
\newlabel{fig:chapter7_two_filters}{{7.4}{211}{Applying two convolutional filters to a \(3 \times 32 \times 32\) input image produces two activation maps of shape \(1 \times 28 \times 28\) (no padding applied)}{figure.caption.307}{}}
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.3.3: Understanding Convolution Through the Sobel Operator}{212}{section*.308}\protected@file@percent }
\newlabel{enr:conv_sobel}{{7.3.3}{212}{\color {ocre}Enrichment \thesubsection : Understanding Convolution Through the Sobel Operator}{section*.308}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces A zoomed-in section of a grayscale image, used for demonstrating convolution.}}{212}{figure.caption.309}\protected@file@percent }
\newlabel{fig:chapter7_grayscale_zoom}{{7.5}{212}{A zoomed-in section of a grayscale image, used for demonstrating convolution}{figure.caption.309}{}}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 7.3.3.1: Using the Sobel Kernel for Edge Detection}{212}{section*.310}\protected@file@percent }
\newlabel{subsubsec:sobel_kernel}{{7.3.3.1}{212}{\color {ocre}Enrichment \thesubsubsection : Using the Sobel Kernel for Edge Detection}{section*.310}{}}
\@writefile{toc}{\contentsline {paragraph}{Approximating Image Gradients with the Sobel Operator}{212}{section*.311}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Basic Difference Operators}{213}{section*.312}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The Sobel Filters: Adding Robustness}{213}{section*.313}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 7.3.3.2: Why Does the Sobel Filter Use These Weights?}{213}{section*.314}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 7.3.3.3: Computing the Gradient Magnitude}{213}{section*.315}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces Computation of the first two cells of the image patch convolved with \(\text  {Sobel}_x\).}}{214}{figure.caption.316}\protected@file@percent }
\newlabel{fig:chapter7_convolve_x_1}{{7.6}{214}{Computation of the first two cells of the image patch convolved with \(\text {Sobel}_x\)}{figure.caption.316}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.7}{\ignorespaces Computation of the third and fourth cells of the image patch convolved with \(\text  {Sobel}_x\). As we can see, after sliding the 2D kernel over the first row, we move to the beginning of the second row and continue from there.}}{214}{figure.caption.317}\protected@file@percent }
\newlabel{fig:chapter7_convolve_x_2}{{7.7}{214}{Computation of the third and fourth cells of the image patch convolved with \(\text {Sobel}_x\). As we can see, after sliding the 2D kernel over the first row, we move to the beginning of the second row and continue from there}{figure.caption.317}{}}
\BKM@entry{id=269,dest={73656374696F6E2A2E333231},srcline={275}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030345C3030303A5C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C3030302D5C303030435C303030685C303030615C3030306E5C3030306E5C303030655C3030306C5C3030305C3034305C303030465C303030695C3030306C5C303030745C303030655C303030725C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {7.8}{\ignorespaces The Sobel example resulting in $G_x, G_y$ after applying the 2D sobel kernels.}}{215}{figure.caption.318}\protected@file@percent }
\newlabel{fig:chapter7_gx_gy}{{7.8}{215}{The Sobel example resulting in $G_x, G_y$ after applying the 2D sobel kernels}{figure.caption.318}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.9}{\ignorespaces The Sobel edge image $G$ resultant from combining $G_x, G_y$.}}{215}{figure.caption.319}\protected@file@percent }
\newlabel{fig:chapter7_sobel_end}{{7.9}{215}{The Sobel edge image $G$ resultant from combining $G_x, G_y$}{figure.caption.319}{}}
\@writefile{toc}{\contentsline {paragraph}{Hands-On Exploration}{215}{section*.320}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Enrichment 7.4: Convolutional Layers with Multi-Channel Filters}{216}{section*.321}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.10}{\ignorespaces The separate color channels (R, G, B) of the Lotus image. Each filter in a convolutional layer operates across all these channels simultaneously.}}{216}{figure.caption.322}\protected@file@percent }
\newlabel{fig:chapter7_lotus_ch}{{7.10}{216}{The separate color channels (R, G, B) of the Lotus image. Each filter in a convolutional layer operates across all these channels simultaneously}{figure.caption.322}{}}
\BKM@entry{id=270,dest={73656374696F6E2A2E333233},srcline={289}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030345C3030302E5C303030315C3030303A5C3030305C3034305C303030455C303030785C303030745C303030655C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C3030302D5C303030435C303030685C303030615C3030306E5C3030306E5C303030655C3030306C5C3030305C3034305C303030495C3030306E5C303030705C303030755C303030745C30303073}
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.4.1: Extending Convolution to Multi-Channel Inputs}{217}{section*.323}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.11}{\ignorespaces A \(5 \times 5\) patch of the Lotus image (visualized with a bold yellow box on one of the left leafs), displayed across three color channels (R, G, B).}}{217}{figure.caption.324}\protected@file@percent }
\newlabel{fig:chapter7_lotus_patch}{{7.11}{217}{A \(5 \times 5\) patch of the Lotus image (visualized with a bold yellow box on one of the left leafs), displayed across three color channels (R, G, B)}{figure.caption.324}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.12}{\ignorespaces The sample image patch over the different channels (R, G, B), along with a corresponding filter. Each filter channel operates on exactly one input channel.}}{217}{figure.caption.325}\protected@file@percent }
\newlabel{fig:chapter7_filter_and_patch}{{7.12}{217}{The sample image patch over the different channels (R, G, B), along with a corresponding filter. Each filter channel operates on exactly one input channel}{figure.caption.325}{}}
\@writefile{toc}{\contentsline {paragraph}{Multi-Channel Convolution Process}{218}{section*.326}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.13}{\ignorespaces Each filter channel performs a separate 2D convolution on its corresponding input channel. The results are then summed along with the bias to produce a single output pixel value.}}{218}{figure.caption.327}\protected@file@percent }
\newlabel{fig:chapter7_multi_dim_filter1}{{7.13}{218}{Each filter channel performs a separate 2D convolution on its corresponding input channel. The results are then summed along with the bias to produce a single output pixel value}{figure.caption.327}{}}
\@writefile{toc}{\contentsline {paragraph}{Sliding the Filter Across the Image}{218}{section*.328}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.14}{\ignorespaces The filter shifts spatially by one step, computing the next output pixel. This process continues across the entire image.}}{218}{figure.caption.329}\protected@file@percent }
\newlabel{fig:chapter7_multi_dim_filter2}{{7.14}{218}{The filter shifts spatially by one step, computing the next output pixel. This process continues across the entire image}{figure.caption.329}{}}
\@writefile{toc}{\contentsline {paragraph}{From Single Filters to Complete Convolutional Layers}{219}{section*.330}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{What Our Example Missed: Padding and Stride}{219}{section*.331}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Are Kernel Values Restricted?}{219}{section*.332}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Negative and Large Output Values}{219}{section*.333}\protected@file@percent }
\BKM@entry{id=271,dest={73756273656374696F6E2E372E342E32},srcline={358}}{5C3337365C3337375C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030655C3030305C3034305C303030465C303030695C3030306C5C303030745C303030655C303030725C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304F5C303030755C303030745C303030705C303030755C303030745C3030305C3034305C303030435C303030685C303030615C3030306E5C3030306E5C303030655C3030306C5C30303073}
\BKM@entry{id=272,dest={73756273656374696F6E2E372E342E33},srcline={374}}{5C3337365C3337375C303030545C303030775C3030306F5C3030305C3034305C303030495C3030306E5C303030745C303030655C303030725C303030705C303030725C303030655C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304F5C303030755C303030745C303030705C303030755C303030745C30303073}
\BKM@entry{id=273,dest={73756273656374696F6E2E372E342E34},srcline={383}}{5C3337365C3337375C303030425C303030615C303030745C303030635C303030685C3030305C3034305C303030505C303030725C3030306F5C303030635C303030655C303030735C303030735C303030695C3030306E5C303030675C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.2}Multiple Filters and Output Channels}{220}{subsection.7.4.2}\protected@file@percent }
\newlabel{subsec:conv_multiple_filters}{{7.4.2}{220}{Multiple Filters and Output Channels}{subsection.7.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.15}{\ignorespaces A convolutional layer with 6 filters, each of size \(3 \times 5 \times 5\), applied to an input image of shape \(3 \times 32 \times 32\), producing 6 activation maps of shape \(1 \times 28 \times 28\). Each filter has an associated bias term.}}{220}{figure.caption.334}\protected@file@percent }
\newlabel{fig:chapter7_multiple_filters}{{7.15}{220}{A convolutional layer with 6 filters, each of size \(3 \times 5 \times 5\), applied to an input image of shape \(3 \times 32 \times 32\), producing 6 activation maps of shape \(1 \times 28 \times 28\). Each filter has an associated bias term}{figure.caption.334}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.3}Two Interpretations of Convolutional Outputs}{220}{subsection.7.4.3}\protected@file@percent }
\newlabel{subsec:conv_output_interpretation}{{7.4.3}{220}{Two Interpretations of Convolutional Outputs}{subsection.7.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.4}Batch Processing with Convolutional Layers}{220}{subsection.7.4.4}\protected@file@percent }
\newlabel{subsec:conv_batch_processing}{{7.4.4}{220}{Batch Processing with Convolutional Layers}{subsection.7.4.4}{}}
\BKM@entry{id=274,dest={73656374696F6E2E372E35},srcline={402}}{5C3337365C3337375C303030425C303030755C303030695C3030306C5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=275,dest={73756273656374696F6E2E372E352E31},srcline={405}}{5C3337365C3337375C303030535C303030745C303030615C303030635C3030306B5C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {7.16}{\ignorespaces The general form of a convolutional layer applied to a batch of images, producing a batch of feature maps.}}{221}{figure.caption.335}\protected@file@percent }
\newlabel{fig:chapter7_general_conv}{{7.16}{221}{The general form of a convolutional layer applied to a batch of images, producing a batch of feature maps}{figure.caption.335}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.5}Building Convolutional Neural Networks}{221}{section.7.5}\protected@file@percent }
\newlabel{sec:conv_nets}{{7.5}{221}{Building Convolutional Neural Networks}{section.7.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.1}Stacking Convolutional Layers}{221}{subsection.7.5.1}\protected@file@percent }
\newlabel{subsec:stacking_convs}{{7.5.1}{221}{Stacking Convolutional Layers}{subsection.7.5.1}{}}
\BKM@entry{id=276,dest={73756273656374696F6E2E372E352E32},srcline={430}}{5C3337365C3337375C303030415C303030645C303030645C303030695C3030306E5C303030675C3030305C3034305C303030465C303030755C3030306C5C3030306C5C303030795C3030305C3034305C303030435C3030306F5C3030306E5C3030306E5C303030655C303030635C303030745C303030655C303030645C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=277,dest={73756273656374696F6E2E372E352E33},srcline={442}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C3030304E5C303030655C303030655C303030645C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304E5C3030306F5C3030306E5C3030302D5C3030304C5C303030695C3030306E5C303030655C303030615C303030725C303030695C303030745C30303079}
\@writefile{lof}{\contentsline {figure}{\numberline {7.17}{\ignorespaces A simple convolutional neural network with three stacked convolutional layers. Each layer extracts progressively higher-level features.}}{222}{figure.caption.336}\protected@file@percent }
\newlabel{fig:convnet_stack}{{7.17}{222}{A simple convolutional neural network with three stacked convolutional layers. Each layer extracts progressively higher-level features}{figure.caption.336}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.2}Adding Fully Connected Layers for Classification}{222}{subsection.7.5.2}\protected@file@percent }
\newlabel{subsec:flatten_fc}{{7.5.2}{222}{Adding Fully Connected Layers for Classification}{subsection.7.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.3}The Need for Non-Linearity}{222}{subsection.7.5.3}\protected@file@percent }
\newlabel{subsec:conv_non_linearity}{{7.5.3}{222}{The Need for Non-Linearity}{subsection.7.5.3}{}}
\BKM@entry{id=278,dest={73756273656374696F6E2E372E352E34},srcline={466}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C30303079}
\BKM@entry{id=279,dest={73656374696F6E2E372E36},srcline={477}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030745C303030725C3030306F5C3030306C5C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030535C303030705C303030615C303030745C303030695C303030615C3030306C5C3030305C3034305C303030445C303030695C3030306D5C303030655C3030306E5C303030735C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C30303073}
\BKM@entry{id=280,dest={73756273656374696F6E2E372E362E31},srcline={480}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030415C303030665C303030665C303030655C303030635C303030745C303030735C3030305C3034305C303030535C303030705C303030615C303030745C303030695C303030615C3030306C5C3030305C3034305C303030535C303030695C3030307A5C30303065}
\BKM@entry{id=281,dest={73756273656374696F6E2E372E362E32},srcline={492}}{5C3337365C3337375C3030304D5C303030695C303030745C303030695C303030675C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030535C303030685C303030725C303030695C3030306E5C3030306B5C303030695C3030306E5C303030675C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C3030304D5C303030615C303030705C303030735C3030303A5C3030305C3034305C303030505C303030615C303030645C303030645C303030695C3030306E5C30303067}
\@writefile{lof}{\contentsline {figure}{\numberline {7.18}{\ignorespaces A convolutional network with three layers, now incorporating non-linear activations (ReLU) between them. This introduces non-linearity, enhancing the model’s expressive power.}}{223}{figure.caption.337}\protected@file@percent }
\newlabel{fig:convnet_relu_stack}{{7.18}{223}{A convolutional network with three layers, now incorporating non-linear activations (ReLU) between them. This introduces non-linearity, enhancing the model’s expressive power}{figure.caption.337}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.4}Summary}{223}{subsection.7.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.6}Controlling Spatial Dimensions in Convolutional Layers}{223}{section.7.6}\protected@file@percent }
\newlabel{sec:conv_dimensions}{{7.6}{223}{Controlling Spatial Dimensions in Convolutional Layers}{section.7.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.1}How Convolution Affects Spatial Size}{223}{subsection.7.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.2}Mitigating Shrinking Feature Maps: Padding}{224}{subsection.7.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.19}{\ignorespaces Zero-padding around an image to maintain spatial dimensions during convolution.}}{224}{figure.caption.338}\protected@file@percent }
\newlabel{fig:padding_visualization}{{7.19}{224}{Zero-padding around an image to maintain spatial dimensions during convolution}{figure.caption.338}{}}
\@writefile{toc}{\contentsline {paragraph}{Choosing the Padding Size}{224}{section*.339}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Preserving Border Information with Padding}{224}{section*.340}\protected@file@percent }
\BKM@entry{id=282,dest={73756273656374696F6E2E372E362E33},srcline={523}}{5C3337365C3337375C303030525C303030655C303030635C303030655C303030705C303030745C303030695C303030765C303030655C3030305C3034305C303030465C303030695C303030655C3030306C5C303030645C303030735C3030303A5C3030305C3034305C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030575C303030685C303030615C303030745C3030305C3034305C303030455C303030615C303030635C303030685C3030305C3034305C303030505C303030695C303030785C303030655C3030306C5C3030305C3034305C303030535C303030655C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.3}Receptive Fields: Understanding What Each Pixel Sees}{225}{subsection.7.6.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.20}{\ignorespaces Receptive field of an output pixel for a single convolution operation.}}{225}{figure.caption.341}\protected@file@percent }
\newlabel{fig:receptive_field_single_layer}{{7.20}{225}{Receptive field of an output pixel for a single convolution operation}{figure.caption.341}{}}
\@writefile{toc}{\contentsline {paragraph}{The Problem of Limited Receptive Field Growth}{226}{section*.342}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.21}{\ignorespaces Receptive field expansion across multiple layers. Deeper layers see a larger portion of the input image.}}{226}{figure.caption.343}\protected@file@percent }
\newlabel{fig:receptive_field_multi_layers}{{7.21}{226}{Receptive field expansion across multiple layers. Deeper layers see a larger portion of the input image}{figure.caption.343}{}}
\BKM@entry{id=283,dest={73756273656374696F6E2E372E362E34},srcline={553}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030745C303030725C3030306F5C3030306C5C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030535C303030705C303030615C303030745C303030695C303030615C3030306C5C3030305C3034305C303030525C303030655C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030535C303030745C303030725C303030695C303030645C303030655C30303073}
\BKM@entry{id=284,dest={73656374696F6E2E372E37},srcline={565}}{5C3337365C3337375C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030575C303030685C303030615C303030745C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030465C303030695C3030306C5C303030745C303030655C303030725C303030735C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E}
\BKM@entry{id=285,dest={73756273656374696F6E2E372E372E31},srcline={568}}{5C3337365C3337375C3030304D5C3030304C5C303030505C303030735C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C303030435C3030304E5C3030304E5C303030735C3030303A5C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030535C303030705C303030615C303030745C303030695C303030615C3030306C5C3030305C3034305C303030535C303030745C303030725C303030755C303030635C303030745C303030755C303030725C30303065}
\BKM@entry{id=286,dest={73756273656374696F6E2E372E372E32},srcline={573}}{5C3337365C3337375C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030304C5C3030306F5C303030635C303030615C3030306C5C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C303030735C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030465C303030695C303030725C303030735C303030745C3030305C3034305C3030304C5C303030615C303030795C303030655C30303072}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6.4}Controlling Spatial Reduction with Strides}{227}{subsection.7.6.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.22}{\ignorespaces Effect of stride on convolution. A stride of 2 moves the filter by 2 pixels, reducing spatial dimensions.}}{227}{figure.caption.344}\protected@file@percent }
\newlabel{fig:stride_visualization}{{7.22}{227}{Effect of stride on convolution. A stride of 2 moves the filter by 2 pixels, reducing spatial dimensions}{figure.caption.344}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.7}Understanding What Convolutional Filters Learn}{227}{section.7.7}\protected@file@percent }
\newlabel{sec:cnn_feature_learning}{{7.7}{227}{Understanding What Convolutional Filters Learn}{section.7.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.1}MLPs vs. CNNs: Learning Spatial Structure}{227}{subsection.7.7.1}\protected@file@percent }
\newlabel{subsec:mlp_vs_cnn}{{7.7.1}{227}{MLPs vs. CNNs: Learning Spatial Structure}{subsection.7.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.2}Learning Local Features: The First Layer}{227}{subsection.7.7.2}\protected@file@percent }
\newlabel{subsec:learning_local_features}{{7.7.2}{227}{Learning Local Features: The First Layer}{subsection.7.7.2}{}}
\BKM@entry{id=287,dest={73756273656374696F6E2E372E372E33},srcline={590}}{5C3337365C3337375C303030425C303030755C303030695C3030306C5C303030645C303030695C3030306E5C303030675C3030305C3034305C3030304D5C3030306F5C303030725C303030655C3030305C3034305C303030435C3030306F5C3030306D5C303030705C3030306C5C303030655C303030785C3030305C3034305C303030505C303030615C303030745C303030745C303030655C303030725C3030306E5C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030445C303030655C303030655C303030705C303030655C303030725C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C30303073}
\BKM@entry{id=288,dest={73656374696F6E2E372E38},srcline={604}}{5C3337365C3337375C303030505C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C3030306C5C303030655C303030785C303030695C303030745C303030795C3030305C3034305C303030695C3030306E5C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {7.23}{\ignorespaces Visualization of first-layer filters from AlexNet. Filters specialize in detecting edge orientations, color contrasts, and simple patterns.}}{228}{figure.caption.345}\protected@file@percent }
\newlabel{fig:alexnet_first_layer}{{7.23}{228}{Visualization of first-layer filters from AlexNet. Filters specialize in detecting edge orientations, color contrasts, and simple patterns}{figure.caption.345}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.7.3}Building More Complex Patterns in Deeper Layers}{228}{subsection.7.7.3}\protected@file@percent }
\newlabel{subsec:deeper_features}{{7.7.3}{228}{Building More Complex Patterns in Deeper Layers}{subsection.7.7.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Hierarchical Learning via Composition}{228}{section*.346}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.8}Parameters and Computational Complexity in Convolutional Networks}{228}{section.7.8}\protected@file@percent }
\newlabel{sec:conv_params}{{7.8}{228}{Parameters and Computational Complexity in Convolutional Networks}{section.7.8}{}}
\BKM@entry{id=289,dest={73756273656374696F6E2E372E382E31},srcline={610}}{5C3337365C3337375C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030303A5C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C3030305C3034305C303030535C303030655C303030745C303030755C30303070}
\BKM@entry{id=290,dest={73756273656374696F6E2E372E382E32},srcline={620}}{5C3337365C3337375C3030304F5C303030755C303030745C303030705C303030755C303030745C3030305C3034305C303030565C3030306F5C3030306C5C303030755C3030306D5C303030655C3030305C3034305C303030435C303030615C3030306C5C303030635C303030755C3030306C5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=291,dest={73756273656374696F6E2E372E382E33},srcline={630}}{5C3337365C3337375C3030304E5C303030755C3030306D5C303030625C303030655C303030725C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030615C303030625C3030306C5C303030655C3030305C3034305C303030505C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C30303073}
\BKM@entry{id=292,dest={73756273656374696F6E2E372E382E34},srcline={648}}{5C3337365C3337375C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030795C3030302D5C303030415C303030635C303030635C303030755C3030306D5C303030755C3030306C5C303030615C303030745C303030655C3030305C3034305C3030304F5C303030705C303030655C303030725C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030305C3035305C3030304D5C303030415C303030435C303030735C3030305C303531}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.1}Example: Convolutional Layer Setup}{229}{subsection.7.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.2}Output Volume Calculation}{229}{subsection.7.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.3}Number of Learnable Parameters}{229}{subsection.7.8.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.24}{\ignorespaces The number of learnable parameters in a convolutional layer, with 76 parameters per filter and 760 total.}}{229}{figure.caption.347}\protected@file@percent }
\newlabel{fig:chapter7_params}{{7.24}{229}{The number of learnable parameters in a convolutional layer, with 76 parameters per filter and 760 total}{figure.caption.347}{}}
\BKM@entry{id=293,dest={73756273656374696F6E2E372E382E35},srcline={665}}{5C3337365C3337375C3030304D5C303030415C303030435C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030465C3030304C5C3030304F5C303030505C30303073}
\BKM@entry{id=294,dest={73756273656374696F6E2E372E382E36},srcline={677}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030795C3030302D5C303030415C303030645C303030645C3030305C3034305C3030304F5C303030705C303030655C303030725C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030305C3035305C3030304D5C303030415C303030435C303030735C3030305C3035315C3030305C3034305C3030304D5C303030615C303030745C303030745C303030655C30303072}
\BKM@entry{id=295,dest={73656374696F6E2A2E333439},srcline={686}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030385C3030302E5C303030375C3030303A5C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\abx@aux@cite{0}{solai2023_backpropconv}
\abx@aux@segm{0}{0}{solai2023_backpropconv}
\abx@aux@cite{0}{solai2023_backpropconv}
\abx@aux@segm{0}{0}{solai2023_backpropconv}
\abx@aux@cite{0}{solai2023_backpropconv}
\abx@aux@segm{0}{0}{solai2023_backpropconv}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.4}Multiply-Accumulate Operations (MACs)}{230}{subsection.7.8.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{MACs Calculation:}{230}{section*.348}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.5}MACs and FLOPs}{230}{subsection.7.8.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.8.6}Why Multiply-Add Operations (MACs) Matter}{230}{subsection.7.8.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.8.7: Backpropagation for Convolutional Neural Networks}{230}{section*.349}\protected@file@percent }
\abx@aux@backref{135}{solai2023_backpropconv}{0}{230}{230}
\@writefile{toc}{\contentsline {paragraph}{Key Idea: Convolution as a Graph Node}{230}{section*.350}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.25}{\ignorespaces Backpropagation in a convolution: A computational graph demonstrates convolving input tensor \(X\) and filter \(F\), then propagating gradients \(\frac  {dL}{dO}\). Source: \blx@tocontentsinit {0}\cite {solai2023_backpropconv}.}}{231}{figure.caption.351}\protected@file@percent }
\abx@aux@backref{137}{solai2023_backpropconv}{0}{231}{231}
\newlabel{fig:chapter7_backprop_conv}{{7.25}{231}{Backpropagation in a convolution: A computational graph demonstrates convolving input tensor \(X\) and filter \(F\), then propagating gradients \(\frac {dL}{dO}\). Source: \cite {solai2023_backpropconv}}{figure.caption.351}{}}
\@writefile{toc}{\contentsline {paragraph}{Computing \(\tfrac  {dO}{dF}\)}{231}{section*.352}\protected@file@percent }
\abx@aux@cite{0}{solai2023_backpropconv}
\abx@aux@segm{0}{0}{solai2023_backpropconv}
\abx@aux@cite{0}{solai2023_backpropconv}
\abx@aux@segm{0}{0}{solai2023_backpropconv}
\abx@aux@cite{0}{solai2023_backpropconv}
\abx@aux@segm{0}{0}{solai2023_backpropconv}
\@writefile{toc}{\contentsline {paragraph}{Computing \(\tfrac  {dL}{dX}\)}{232}{section*.353}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.26}{\ignorespaces Backpropagation through convolutions: The gradient computation involves convolution between the input \(X\) (or a rotated version of \(F\)) and the upstream gradient \(\tfrac  {dL}{dO}\). Source: \blx@tocontentsinit {0}\cite {solai2023_backpropconv}.}}{232}{figure.caption.354}\protected@file@percent }
\abx@aux@backref{139}{solai2023_backpropconv}{0}{232}{232}
\newlabel{fig:chapter7_backprop_through_conv}{{7.26}{232}{Backpropagation through convolutions: The gradient computation involves convolution between the input \(X\) (or a rotated version of \(F\)) and the upstream gradient \(\tfrac {dL}{dO}\). Source: \cite {solai2023_backpropconv}}{figure.caption.354}{}}
\abx@aux@backref{140}{solai2023_backpropconv}{0}{232}{232}
\BKM@entry{id=296,dest={73656374696F6E2A2E333535},srcline={775}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030395C3030303A5C3030305C3034305C303030505C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C3030305C3034305C303030535C303030685C303030615C303030725C303030695C3030306E5C303030675C3030305C3034305C303030695C3030306E5C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=297,dest={73656374696F6E2A2E333536},srcline={779}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030395C3030302E5C303030315C3030303A5C3030305C3034305C303030505C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C3030305C3034305C303030535C303030685C303030615C303030725C303030695C3030306E5C303030675C3030305C3034305C303030695C3030306E5C3030305C3034305C303030435C3030304E5C3030304E5C303030735C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C3030304D5C3030304C5C303030505C30303073}
\BKM@entry{id=298,dest={73656374696F6E2A2E333537},srcline={783}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030395C3030302E5C303030325C3030303A5C3030305C3034305C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030505C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C3030305C3034305C303030535C303030685C303030615C303030725C303030695C3030306E5C30303067}
\BKM@entry{id=299,dest={73656374696F6E2A2E333538},srcline={792}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030395C3030302E5C303030335C3030303A5C3030305C3034305C303030485C3030306F5C303030775C3030305C3034305C303030505C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C3030305C3034305C303030535C303030685C303030615C303030725C303030695C3030306E5C303030675C3030305C3034305C303030575C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=300,dest={73656374696F6E2A2E333539},srcline={803}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030395C3030302E5C303030345C3030303A5C3030305C3034305C303030575C303030685C303030655C3030306E5C3030305C3034305C303030445C3030306F5C303030655C303030735C3030305C3034305C303030505C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C3030305C3034305C303030535C303030685C303030615C303030725C303030695C3030306E5C303030675C3030305C3034305C3030304E5C3030306F5C303030745C3030305C3034305C3030304D5C303030615C3030306B5C303030655C3030305C3034305C303030435C3030306F5C3030306D5C303030705C3030306C5C303030655C303030745C303030655C3030305C3034305C303030535C303030655C3030306E5C303030735C303030655C3030303F}
\abx@aux@cite{0}{taigman2014_deepface}
\abx@aux@segm{0}{0}{taigman2014_deepface}
\@writefile{toc}{\contentsline {section}{Enrichment 7.9: Parameter Sharing in Convolutional Neural Networks}{233}{section*.355}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.9.1: Parameter Sharing in CNNs vs. MLPs}{233}{section*.356}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.9.2: Motivation for Parameter Sharing}{233}{section*.357}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.9.3: How Parameter Sharing Works}{233}{section*.358}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.9.4: When Does Parameter Sharing Not Make Complete Sense?}{233}{section*.359}\protected@file@percent }
\abx@aux@cite{0}{litjens2017_medicalcnn}
\abx@aux@segm{0}{0}{litjens2017_medicalcnn}
\BKM@entry{id=301,dest={73656374696F6E2A2E333630},srcline={813}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030395C3030302E5C303030355C3030303A5C3030305C3034305C303030415C3030306C5C303030745C303030655C303030725C3030306E5C303030615C303030745C303030695C303030765C303030655C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030615C303030635C303030685C303030655C303030735C3030305C3034305C303030575C303030685C303030655C3030306E5C3030305C3034305C303030505C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C3030305C3034305C303030535C303030685C303030615C303030725C303030695C3030306E5C303030675C3030305C3034305C303030465C303030615C303030695C3030306C5C30303073}
\abx@aux@backref{141}{taigman2014_deepface}{0}{234}{234}
\abx@aux@backref{142}{litjens2017_medicalcnn}{0}{234}{234}
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.9.5: Alternative Approaches When Parameter Sharing Fails}{234}{section*.360}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 7.9.5.1: Locally-Connected Layers}{234}{section*.361}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 7.9.5.2: Understanding Locally-Connected Layers}{234}{section*.362}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 7.9.5.3: Limitations of Locally-Connected Layers}{234}{section*.363}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 7.9.5.4: Hybrid Approaches}{235}{section*.364}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 7.9.5.5: A Glimpse at Attention Mechanisms}{235}{section*.365}\protected@file@percent }
\BKM@entry{id=302,dest={73656374696F6E2E372E3130},srcline={853}}{5C3337365C3337375C303030535C303030705C303030655C303030635C303030695C303030615C3030306C5C3030305C3034305C303030545C303030795C303030705C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030735C3030303A5C3030305C3034305C303030315C303030785C303030315C3030302C5C3030305C3034305C303030315C303030445C3030302C5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030335C303030445C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=303,dest={73756273656374696F6E2E372E31302E31},srcline={858}}{5C3337365C3337375C303030315C303030785C303030315C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {7.10}Special Types of Convolutions: 1x1, 1D, and 3D Convolutions}{236}{section.7.10}\protected@file@percent }
\newlabel{sec:special_convs}{{7.10}{236}{Special Types of Convolutions: 1x1, 1D, and 3D Convolutions}{section.7.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.10.1}1x1 Convolutions}{236}{subsection.7.10.1}\protected@file@percent }
\newlabel{subsec:1x1_convs}{{7.10.1}{236}{1x1 Convolutions}{subsection.7.10.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Dimensionality Reduction and Feature Selection}{236}{section*.366}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.27}{\ignorespaces A visualization of a 1x1 convolution. The input tensor is of volume \(56 \times 56 \times 64\) and the convolutional layer has 32 \emph  {1x1} filters, resulting in an output volume of \(56 \times 56 \times 32\).}}{236}{figure.caption.367}\protected@file@percent }
\newlabel{fig:chapter7_1x1_conv}{{7.27}{236}{A visualization of a 1x1 convolution. The input tensor is of volume \(56 \times 56 \times 64\) and the convolutional layer has 32 \emph {1x1} filters, resulting in an output volume of \(56 \times 56 \times 32\)}{figure.caption.367}{}}
\@writefile{toc}{\contentsline {subsubsection}{Efficiency of 1x1 Convolutions as a Bottleneck}{236}{section*.368}\protected@file@percent }
\newlabel{subsubsec:conv_efficiency_1x1}{{7.10.1}{236}{Efficiency of 1x1 Convolutions as a Bottleneck}{section*.368}{}}
\BKM@entry{id=304,dest={73756273656374696F6E2E372E31302E32},srcline={933}}{5C3337365C3337375C303030315C303030445C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {paragraph}{Example: Transforming 256 Channels to 256 Channels with a 3x3 Kernel.}{237}{section*.369}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Parameter and FLOP Savings.}{237}{section*.370}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.10.2}1D Convolutions}{237}{subsection.7.10.2}\protected@file@percent }
\newlabel{subsec:1D_convs}{{7.10.2}{237}{1D Convolutions}{subsection.7.10.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Numerical Example: 1D Convolution on Multichannel Time Series Data}{237}{section*.371}\protected@file@percent }
\BKM@entry{id=305,dest={73756273656374696F6E2E372E31302E33},srcline={1006}}{5C3337365C3337375C303030335C303030445C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {paragraph}{Computing the Output}{238}{section*.372}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Applications of 1D Convolutions}{238}{section*.373}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.10.3}3D Convolutions}{239}{subsection.7.10.3}\protected@file@percent }
\newlabel{subsec:3D_convs}{{7.10.3}{239}{3D Convolutions}{subsection.7.10.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.28}{\ignorespaces Visualization of \emph  {3D convolution}, where a \emph  {3D kernel} moves through a volumetric input to capture spatial-temporal relationships.}}{239}{figure.caption.374}\protected@file@percent }
\newlabel{fig:chapter7_3D_conv}{{7.28}{239}{Visualization of \emph {3D convolution}, where a \emph {3D kernel} moves through a volumetric input to capture spatial-temporal relationships}{figure.caption.374}{}}
\@writefile{toc}{\contentsline {paragraph}{Numerical Example: 3D Convolution on Volumetric Data}{239}{section*.375}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3D Convolution Formula}{240}{section*.376}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Computing the Output}{240}{section*.377}\protected@file@percent }
\BKM@entry{id=306,dest={73756273656374696F6E2E372E31302E34},srcline={1171}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304D5C3030306F5C303030625C303030695C3030306C5C303030655C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030455C3030306D5C303030625C303030655C303030645C303030645C303030655C303030645C3030305C3034305C303030535C303030795C303030735C303030745C303030655C3030306D5C30303073}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\abx@aux@cite{0}{sandler2018_mobilenetv2}
\abx@aux@segm{0}{0}{sandler2018_mobilenetv2}
\abx@aux@cite{0}{tan2019_efficientnet}
\abx@aux@segm{0}{0}{tan2019_efficientnet}
\abx@aux@cite{0}{howard2017_mobilenets}
\abx@aux@segm{0}{0}{howard2017_mobilenets}
\abx@aux@cite{0}{zhang2018_shufflenet}
\abx@aux@segm{0}{0}{zhang2018_shufflenet}
\abx@aux@cite{0}{tan2019_efficientnet}
\abx@aux@segm{0}{0}{tan2019_efficientnet}
\BKM@entry{id=307,dest={73756273656374696F6E2E372E31302E35},srcline={1176}}{5C3337365C3337375C303030535C303030705C303030615C303030745C303030695C303030615C3030306C5C3030305C3034305C303030535C303030655C303030705C303030615C303030725C303030615C303030625C3030306C5C303030655C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {paragraph}{Final Output Tensor}{241}{section*.378}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Applications of 3D Convolutions}{241}{section*.379}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Advantages of 3D Convolutions}{241}{section*.380}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Challenges of 3D Convolutions}{241}{section*.381}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.10.4}Efficient Convolutions for Mobile and Embedded Systems}{241}{subsection.7.10.4}\protected@file@percent }
\newlabel{subsec:efficient_convs}{{7.10.4}{241}{Efficient Convolutions for Mobile and Embedded Systems}{subsection.7.10.4}{}}
\abx@aux@backref{143}{krizhevsky2012_alexnet}{0}{241}{241}
\abx@aux@backref{144}{sandler2018_mobilenetv2}{0}{241}{241}
\abx@aux@backref{145}{tan2019_efficientnet}{0}{241}{241}
\abx@aux@backref{146}{howard2017_mobilenets}{0}{241}{241}
\abx@aux@backref{147}{zhang2018_shufflenet}{0}{241}{241}
\abx@aux@backref{148}{tan2019_efficientnet}{0}{241}{241}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.10.5}Spatial Separable Convolutions}{241}{subsection.7.10.5}\protected@file@percent }
\newlabel{subsec:spatial_separable_convs}{{7.10.5}{241}{Spatial Separable Convolutions}{subsection.7.10.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Concept and Intuition}{241}{section*.382}\protected@file@percent }
\abx@aux@cite{0}{lecun1998_lenet}
\abx@aux@segm{0}{0}{lecun1998_lenet}
\BKM@entry{id=308,dest={73756273656374696F6E2E372E31302E36},srcline={1223}}{5C3337365C3337375C303030445C303030655C303030705C303030745C303030685C303030775C303030695C303030735C303030655C3030305C3034305C303030535C303030655C303030705C303030615C303030725C303030615C303030625C3030306C5C303030655C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\abx@aux@cite{0}{chollet2017_xception}
\abx@aux@segm{0}{0}{chollet2017_xception}
\abx@aux@cite{0}{howard2017_mobilenets}
\abx@aux@segm{0}{0}{howard2017_mobilenets}
\@writefile{toc}{\contentsline {paragraph}{Limitations and Transition to Depthwise Separable Convolutions}{242}{section*.383}\protected@file@percent }
\abx@aux@backref{149}{lecun1998_lenet}{0}{242}{242}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.10.6}Depthwise Separable Convolutions}{242}{subsection.7.10.6}\protected@file@percent }
\newlabel{subsec:depthwise_separable_convs}{{7.10.6}{242}{Depthwise Separable Convolutions}{subsection.7.10.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Concept and Motivation}{242}{section*.384}\protected@file@percent }
\abx@aux@backref{150}{chollet2017_xception}{0}{242}{242}
\abx@aux@backref{151}{howard2017_mobilenets}{0}{242}{242}
\@writefile{toc}{\contentsline {paragraph}{Computational Efficiency}{243}{section*.385}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Standard \((K \times K)\) Convolution}{243}{section*.386}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Depthwise Separable Convolution}{243}{section*.387}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Example: \((K=3,\tmspace  +\thickmuskip {.2777em}C_{\mathrm  {in}}=128,\tmspace  +\thickmuskip {.2777em}C_{\mathrm  {out}}=256,\tmspace  +\thickmuskip {.2777em}H=W=32)\)}{243}{section*.388}\protected@file@percent }
\newlabel{subsubsec:depthwise_separable_example}{{7.10.6}{243}{Example: \((K=3,\;C_{\mathrm {in}}=128,\;C_{\mathrm {out}}=256,\;H=W=32)\)}{section*.388}{}}
\abx@aux@cite{0}{blog2023_separable_convolutions}
\abx@aux@segm{0}{0}{blog2023_separable_convolutions}
\abx@aux@cite{0}{blog2023_separable_convolutions}
\abx@aux@segm{0}{0}{blog2023_separable_convolutions}
\@writefile{lof}{\contentsline {figure}{\numberline {7.29}{\ignorespaces Illustration of a \emph  {depthwise separable convolution}. \textbf  {Step 1 (Depthwise)}: Each of the \(C_{\text  {in}}\) input channels is convolved separately by a \(k \times k\) filter, preserving the spatial dimensions but not mixing channels. \textbf  {Step 2 (Pointwise)}: To produce the desired \(C_{\text  {out}}\) channels, a series of \(1 \times 1 \times C_{\text  {in}}\) filters (kernels) is applied—one for each output channel—resulting in a stack of 2D feature maps with the same spatial size. Source: \blx@tocontentsinit {0}\cite {blog2023_separable_convolutions}. }}{244}{figure.caption.389}\protected@file@percent }
\abx@aux@backref{153}{blog2023_separable_convolutions}{0}{244}{244}
\newlabel{fig:chapter7_depthwise_conv}{{7.29}{244}{Illustration of a \emph {depthwise separable convolution}. \textbf {Step 1 (Depthwise)}: Each of the \(C_{\text {in}}\) input channels is convolved separately by a \(k \times k\) filter, preserving the spatial dimensions but not mixing channels. \textbf {Step 2 (Pointwise)}: To produce the desired \(C_{\text {out}}\) channels, a series of \(1 \times 1 \times C_{\text {in}}\) filters (kernels) is applied—one for each output channel—resulting in a stack of 2D feature maps with the same spatial size. Source: \cite {blog2023_separable_convolutions}}{figure.caption.389}{}}
\abx@aux@cite{0}{howard2017_mobilenets}
\abx@aux@segm{0}{0}{howard2017_mobilenets}
\abx@aux@cite{0}{zhang2018_shufflenet}
\abx@aux@segm{0}{0}{zhang2018_shufflenet}
\abx@aux@cite{0}{chollet2017_xception}
\abx@aux@segm{0}{0}{chollet2017_xception}
\abx@aux@cite{0}{tan2019_efficientnet}
\abx@aux@segm{0}{0}{tan2019_efficientnet}
\abx@aux@cite{0}{chollet2017_xception}
\abx@aux@segm{0}{0}{chollet2017_xception}
\BKM@entry{id=309,dest={73756273656374696F6E2E372E31302E37},srcline={1384}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030305C3034305C3030306F5C303030665C3030305C3034305C303030535C303030705C303030655C303030635C303030695C303030615C3030306C5C303030695C3030307A5C303030655C303030645C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {paragraph}{Reduction Factor}{245}{section*.390}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Practical Usage and Examples}{245}{section*.391}\protected@file@percent }
\abx@aux@backref{154}{howard2017_mobilenets}{0}{245}{245}
\abx@aux@backref{155}{zhang2018_shufflenet}{0}{245}{245}
\abx@aux@backref{156}{chollet2017_xception}{0}{245}{245}
\abx@aux@backref{157}{tan2019_efficientnet}{0}{245}{245}
\@writefile{toc}{\contentsline {paragraph}{Trade-Offs}{245}{section*.392}\protected@file@percent }
\abx@aux@backref{158}{chollet2017_xception}{0}{245}{245}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.10.7}Summary of Specialized Convolutions}{245}{subsection.7.10.7}\protected@file@percent }
\newlabel{subsec:conv_summary}{{7.10.7}{245}{Summary of Specialized Convolutions}{subsection.7.10.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.30}{\ignorespaces Illustration of \texttt  {torch.nn.Conv2d} and its parameters. The attached paragraph shows how the convolution operation applies filters to input data, computing feature maps based on the hyper-parameters of stride, padding, and kernel size.}}{246}{figure.caption.393}\protected@file@percent }
\newlabel{fig:chapter7_pytorch_conv2d}{{7.30}{246}{Illustration of \texttt {torch.nn.Conv2d} and its parameters. The attached paragraph shows how the convolution operation applies filters to input data, computing feature maps based on the hyper-parameters of stride, padding, and kernel size}{figure.caption.393}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.31}{\ignorespaces Comparison of PyTorch convolution layers: \texttt  {Conv1d}, \texttt  {Conv2d}, and \texttt  {Conv3d}. The figure highlights their function signatures and input parameter definitions in the PyTorch library.}}{246}{figure.caption.394}\protected@file@percent }
\newlabel{fig:chapter7_pytorch_conv_layers}{{7.31}{246}{Comparison of PyTorch convolution layers: \texttt {Conv1d}, \texttt {Conv2d}, and \texttt {Conv3d}. The figure highlights their function signatures and input parameter definitions in the PyTorch library}{figure.caption.394}{}}
\BKM@entry{id=310,dest={73656374696F6E2E372E3131},srcline={1412}}{5C3337365C3337375C303030505C3030306F5C3030306F5C3030306C5C303030695C3030306E5C303030675C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C30303073}
\BKM@entry{id=311,dest={73756273656374696F6E2E372E31312E31},srcline={1417}}{5C3337365C3337375C303030545C303030795C303030705C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030505C3030306F5C3030306F5C3030306C5C303030695C3030306E5C30303067}
\BKM@entry{id=312,dest={73756273656374696F6E2E372E31312E32},srcline={1435}}{5C3337365C3337375C303030455C303030665C303030665C303030655C303030635C303030745C3030305C3034305C3030306F5C303030665C3030305C3034305C303030505C3030306F5C3030306F5C3030306C5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {section}{\numberline {7.11}Pooling Layers}{247}{section.7.11}\protected@file@percent }
\newlabel{subsec:pooling_layers}{{7.11}{247}{Pooling Layers}{section.7.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.11.1}Types of Pooling}{247}{subsection.7.11.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Pooling Methods}{247}{section*.395}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.32}{\ignorespaces Example of \emph  {max pooling} with a \( 2 \times 2 \) kernel and stride of 2. The operation introduces slight spatial invariance, helping retain dominant features.}}{247}{figure.caption.396}\protected@file@percent }
\newlabel{fig:chapter7_max_pooling}{{7.32}{247}{Example of \emph {max pooling} with a \( 2 \times 2 \) kernel and stride of 2. The operation introduces slight spatial invariance, helping retain dominant features}{figure.caption.396}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.11.2}Effect of Pooling}{247}{subsection.7.11.2}\protected@file@percent }
\BKM@entry{id=313,dest={73656374696F6E2A2E333938},srcline={1451}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030315C303030315C3030302E5C303030335C3030303A5C3030305C3034305C303030505C3030306F5C3030306F5C3030306C5C303030695C3030306E5C303030675C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {7.33}{\ignorespaces Summary of pooling layers: input size, hyperparameters (kernel size, stride, pooling function), output size, and common pooling configurations.}}{248}{figure.caption.397}\protected@file@percent }
\newlabel{fig:chapter7_pooling_summary}{{7.33}{248}{Summary of pooling layers: input size, hyperparameters (kernel size, stride, pooling function), output size, and common pooling configurations}{figure.caption.397}{}}
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.11.3: Pooling Layers in Backpropagation}{248}{section*.398}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Forward Pass of Pooling Layers}{248}{section*.399}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example of Forward Pass}{248}{section*.400}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Backpropagation Through Pooling Layers}{249}{section*.401}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Max Pooling Backpropagation}{249}{section*.402}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Impact on Gradient Flow}{249}{section*.403}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Mitigation Strategies}{249}{section*.404}\protected@file@percent }
\BKM@entry{id=314,dest={73756273656374696F6E2E372E31312E34},srcline={1568}}{5C3337365C3337375C303030475C3030306C5C3030306F5C303030625C303030615C3030306C5C3030305C3034305C303030505C3030306F5C3030306F5C3030306C5C303030695C3030306E5C303030675C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {paragraph}{Average Pooling Backpropagation}{250}{section*.405}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Generalization of Backpropagation for Pooling}{250}{section*.406}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.11.4}Global Pooling Layers}{250}{subsection.7.11.4}\protected@file@percent }
\newlabel{subsec:global_pooling}{{7.11.4}{250}{Global Pooling Layers}{subsection.7.11.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{General Advantages}{250}{section*.407}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Global Average Pooling (GAP)}{250}{section*.408}\protected@file@percent }
\newlabel{subsubsec:gap}{{7.11.4}{250}{Global Average Pooling (GAP)}{section*.408}{}}
\@writefile{toc}{\contentsline {paragraph}{Operation}{250}{section*.409}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Upsides}{251}{section*.410}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Downsides}{251}{section*.411}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Backpropagation}{251}{section*.412}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Global Max Pooling (GMP)}{251}{section*.413}\protected@file@percent }
\newlabel{subsubsec:gmp}{{7.11.4}{251}{Global Max Pooling (GMP)}{section*.413}{}}
\@writefile{toc}{\contentsline {paragraph}{Operation.}{251}{section*.414}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Upsides}{251}{section*.415}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Downsides}{251}{section*.416}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Backpropagation}{251}{section*.417}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Comparison of GAP and GMP}{251}{section*.418}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Contrasting with Regular Pooling}{252}{section*.419}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Window Size}{252}{section*.420}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{When to Use Global Pooling}{252}{section*.421}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{When to Use Regular Pooling}{252}{section*.422}\protected@file@percent }
\BKM@entry{id=315,dest={73656374696F6E2E372E3132},srcline={1675}}{5C3337365C3337375C303030435C3030306C5C303030615C303030735C303030735C303030695C303030635C303030615C3030306C5C3030305C3034305C303030435C3030304E5C3030304E5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C30303073}
\abx@aux@cite{0}{lecun1998_lenet}
\abx@aux@segm{0}{0}{lecun1998_lenet}
\BKM@entry{id=316,dest={73756273656374696F6E2E372E31322E31},srcline={1680}}{5C3337365C3337375C3030304C5C303030655C3030304E5C303030655C303030745C3030302D5C303030355C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C30303065}
\@writefile{toc}{\contentsline {section}{\numberline {7.12}Classical CNN Architectures}{253}{section.7.12}\protected@file@percent }
\newlabel{subsec:lenet5}{{7.12}{253}{Classical CNN Architectures}{section.7.12}{}}
\abx@aux@backref{159}{lecun1998_lenet}{0}{253}{253}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.12.1}LeNet-5 Architecture}{253}{subsection.7.12.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.34}{\ignorespaces LeNet-5 architecture following the classical \([ \text  {Conv}, \text  {ReLU}, \text  {Pool} ] \times N\), Flatten, \([ \text  {FC}, \text  {ReLU} ] \times M\), FC design. The network reduces spatial dimensions while increasing the number of feature channels, a pattern common in CNNs.}}{253}{figure.caption.423}\protected@file@percent }
\newlabel{fig:lenet5_architecture}{{7.34}{253}{LeNet-5 architecture following the classical \([ \text {Conv}, \text {ReLU}, \text {Pool} ] \times N\), Flatten, \([ \text {FC}, \text {ReLU} ] \times M\), FC design. The network reduces spatial dimensions while increasing the number of feature channels, a pattern common in CNNs}{figure.caption.423}{}}
\@writefile{toc}{\contentsline {subsubsection}{Detailed Layer Breakdown}{254}{section*.424}\protected@file@percent }
\BKM@entry{id=317,dest={73756273656374696F6E2E372E31322E32},srcline={1796}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030415C303030725C303030655C3030305C3034305C303030435C3030304E5C3030304E5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C303030735C3030305C3034305C303030445C303030655C303030735C303030695C303030675C3030306E5C303030655C303030645C3030303F}
\@writefile{toc}{\contentsline {subsubsection}{Summary of LeNet-5}{255}{section*.425}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Key Architectural Trends in CNNs, Illustrated by LeNet-5}{255}{section*.426}\protected@file@percent }
\newlabel{subsubsec:lenet_trends}{{7.12.1}{255}{Key Architectural Trends in CNNs, Illustrated by LeNet-5}{section*.426}{}}
\@writefile{toc}{\contentsline {paragraph}{Hierarchical Feature Learning}{255}{section*.427}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Alternating Convolution and Pooling}{255}{section*.428}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Transition to Fully Connected (FC) Layers}{255}{section*.429}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.12.2}How Are CNN Architectures Designed?}{255}{subsection.7.12.2}\protected@file@percent }
\newlabel{subsec:cnn_design}{{7.12.2}{255}{How Are CNN Architectures Designed?}{subsection.7.12.2}{}}
\BKM@entry{id=318,dest={73656374696F6E2A2E343330},srcline={1813}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030315C303030335C3030303A5C3030305C3034305C303030565C303030615C3030306E5C303030695C303030735C303030685C303030695C3030306E5C303030675C3030305C3034305C3030305C3034365C3030305C3034305C303030455C303030785C303030705C3030306C5C3030306F5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C303030735C3030303A5C3030305C3034305C303030415C3030305C3034305C303030425C303030615C303030725C303030725C303030695C303030655C303030725C3030305C3034305C303030745C3030306F5C3030305C3034305C303030445C3030304C}
\BKM@entry{id=319,dest={73656374696F6E2A2E343332},srcline={1821}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030315C303030335C3030302E5C303030315C3030303A5C3030305C3034305C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030505C303030725C3030306F5C303030625C3030306C5C303030655C3030306D}
\@writefile{toc}{\contentsline {section}{Enrichment 7.13: Vanishing \& Exploding Gradients: A Barrier to DL}{256}{section*.430}\protected@file@percent }
\newlabel{enrichment:vanishing_exploding_gradients}{{7.13}{256}{\color {ocre}Enrichment \thesection : Vanishing \& Exploding Gradients: A Barrier to DL}{section*.430}{}}
\@writefile{toc}{\contentsline {paragraph}{Context}{256}{section*.431}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.13.1: Understanding the Problem}{256}{section*.432}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The Role of Gradients in Deep Networks}{256}{section*.433}\protected@file@percent }
\newlabel{subsubsec:gradient_flow}{{7.13.1}{256}{The Role of Gradients in Deep Networks}{section*.433}{}}
\@writefile{toc}{\contentsline {subsubsection}{Gradient Computation in Deep Networks}{256}{section*.434}\protected@file@percent }
\newlabel{eq:gradient_general}{{7.3}{256}{Gradient Computation in Deep Networks}{equation.7.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Key Components of Gradient Propagation}{256}{section*.435}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Impact of Depth in Neural Networks}{257}{section*.436}\protected@file@percent }
\newlabel{subsubsec:impact_of_depth}{{7.13.1}{257}{Impact of Depth in Neural Networks}{section*.436}{}}
\@writefile{toc}{\contentsline {subsubsection}{Practical Example: Vanishing Gradients with Sigmoid Activation}{259}{section*.437}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.35}{\ignorespaces Shows the sigmoid function and its derivative. As we can see, the \emph  {area of significance} is quite small, spanning between $-4 to 4$, and even in it, the derivative value is at most $0.25$}}{259}{figure.caption.438}\protected@file@percent }
\newlabel{fig:chapter7_sigmoid_derivative}{{7.35}{259}{Shows the sigmoid function and its derivative. As we can see, the \emph {area of significance} is quite small, spanning between $-4 to 4$, and even in it, the derivative value is at most $0.25$}{figure.caption.438}{}}
\newlabel{eq:gradient_first_layer_example}{{7.4}{259}{Practical Example: Vanishing Gradients with Sigmoid Activation}{equation.7.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Effect of Activation Gradients}{260}{section*.439}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Effect of Weight Multiplications}{260}{section*.440}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Conclusion: Vanishing Gradients}{260}{section*.441}\protected@file@percent }
\BKM@entry{id=320,dest={73656374696F6E2E372E3134},srcline={2042}}{5C3337365C3337375C303030425C303030615C303030745C303030635C303030685C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{ioffe2015_batchnorm}
\abx@aux@segm{0}{0}{ioffe2015_batchnorm}
\BKM@entry{id=321,dest={73756273656374696F6E2E372E31342E31},srcline={2047}}{5C3337365C3337375C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C3030304D5C303030655C303030615C3030306E5C3030302C5C3030305C3034305C303030565C303030615C303030725C303030695C303030615C3030306E5C303030635C303030655C3030302C5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=322,dest={73756273656374696F6E2E372E31342E32},srcline={2084}}{5C3337365C3337375C303030495C3030306E5C303030745C303030655C303030725C3030306E5C303030615C3030306C5C3030305C3034305C303030435C3030306F5C303030765C303030615C303030725C303030695C303030615C303030745C303030655C3030305C3034305C303030535C303030685C303030695C303030665C303030745C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030425C303030615C303030745C303030635C303030685C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3034305C3033315C303030735C3030305C3034305C303030525C3030306F5C3030306C5C30303065}
\@writefile{toc}{\contentsline {section}{\numberline {7.14}Batch Normalization}{262}{section.7.14}\protected@file@percent }
\newlabel{sec:batchnorm}{{7.14}{262}{Batch Normalization}{section.7.14}{}}
\abx@aux@backref{160}{ioffe2015_batchnorm}{0}{262}{262}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.14.1}Understanding Mean, Variance, and Normalization}{262}{subsection.7.14.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Mean:}{262}{section*.442}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Variance:}{262}{section*.443}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Standard Deviation:}{262}{section*.444}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Effect of Normalization:}{262}{section*.445}\protected@file@percent }
\BKM@entry{id=323,dest={73756273656374696F6E2E372E31342E33},srcline={2098}}{5C3337365C3337375C303030425C303030615C303030745C303030635C303030685C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030505C303030725C3030306F5C303030635C303030655C303030735C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.14.2}Internal Covariate Shift and Batch Normalization’s Role}{263}{subsection.7.14.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{What is Covariate Shift?}{263}{section*.446}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{What is Internal Covariate Shift?}{263}{section*.447}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.14.3}Batch Normalization Process}{263}{subsection.7.14.3}\protected@file@percent }
\newlabel{subsec:chapter7_batchnorm}{{7.14.3}{263}{Batch Normalization Process}{subsection.7.14.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.36}{\ignorespaces Summary of shapes and formulas in the Batch Normalization process: normalization followed by per-feature scaling and shifting. Each layer learns its own set of \(\gamma \) and \(\beta \) parameters.}}{264}{figure.caption.448}\protected@file@percent }
\newlabel{fig:chapter7_batchnorm_process}{{7.36}{264}{Summary of shapes and formulas in the Batch Normalization process: normalization followed by per-feature scaling and shifting. Each layer learns its own set of \(\gamma \) and \(\beta \) parameters}{figure.caption.448}{}}
\@writefile{toc}{\contentsline {paragraph}{Why is this flexibility useful?}{264}{section*.449}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Batch Normalization for Convolutional Neural Networks (CNNs)}{265}{section*.450}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.37}{\ignorespaces Extending Batch Normalization to CNNs by computing batch statistics across spatial dimensions (H, W) in addition to batch samples (N). Each feature map is normalized independently.}}{265}{figure.caption.451}\protected@file@percent }
\newlabel{fig:chapter7_batchnorm_cnn}{{7.37}{265}{Extending Batch Normalization to CNNs by computing batch statistics across spatial dimensions (H, W) in addition to batch samples (N). Each feature map is normalized independently}{figure.caption.451}{}}
\BKM@entry{id=324,dest={73756273656374696F6E2E372E31342E34},srcline={2191}}{5C3337365C3337375C303030425C303030615C303030745C303030635C303030685C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{santurkar2018_howdoesbatchnormhelp}
\abx@aux@segm{0}{0}{santurkar2018_howdoesbatchnormhelp}
\abx@aux@cite{0}{ioffe2015_batchnorm}
\abx@aux@segm{0}{0}{ioffe2015_batchnorm}
\abx@aux@cite{0}{ioffe2015_batchnorm}
\abx@aux@segm{0}{0}{ioffe2015_batchnorm}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.14.4}Batch Normalization and Optimization}{266}{subsection.7.14.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Beyond Covariate Shift: Why Does BatchNorm Improve Training?}{266}{section*.452}\protected@file@percent }
\abx@aux@backref{161}{santurkar2018_howdoesbatchnormhelp}{0}{266}{266}
\@writefile{lof}{\contentsline {figure}{\numberline {7.38}{\ignorespaces Training accuracy across different conditions (no BN, with BN, BN with artificially induced covariate shift). Performance differences indicate that covariate shift is not the key issue affecting training efficiency.}}{266}{figure.caption.453}\protected@file@percent }
\newlabel{fig:chapter7_batchnorm_training_stability}{{7.38}{266}{Training accuracy across different conditions (no BN, with BN, BN with artificially induced covariate shift). Performance differences indicate that covariate shift is not the key issue affecting training efficiency}{figure.caption.453}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.39}{\ignorespaces Impact of Batch Normalization on optimization: smoother loss landscape (left), more stable gradients (middle), and overall training stability (right) \blx@tocontentsinit {0}\cite {ioffe2015_batchnorm}.}}{267}{figure.caption.454}\protected@file@percent }
\abx@aux@backref{163}{ioffe2015_batchnorm}{0}{267}{267}
\newlabel{fig:batchnorm_loss_smooth}{{7.39}{267}{Impact of Batch Normalization on optimization: smoother loss landscape (left), more stable gradients (middle), and overall training stability (right) \cite {ioffe2015_batchnorm}}{figure.caption.454}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why Does BatchNorm Smooth the Loss Surface?}{267}{section*.455}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{1. Hessian Eigenvalues and Loss Surface Curvature}{267}{section*.456}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Computing Eigenvalues}{267}{section*.457}\protected@file@percent }
\abx@aux@cite{0}{santurkar2018_howdoesbatchnormhelp}
\abx@aux@segm{0}{0}{santurkar2018_howdoesbatchnormhelp}
\@writefile{toc}{\contentsline {paragraph}{Interpretation of Eigenvalues}{268}{section*.458}\protected@file@percent }
\abx@aux@backref{164}{santurkar2018_howdoesbatchnormhelp}{0}{268}{268}
\@writefile{toc}{\contentsline {paragraph}{2. Reducing the Lipschitz Constant}{268}{section*.459}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3. Implicit Regularization via Mini-Batch Noise}{268}{section*.460}\protected@file@percent }
\abx@aux@cite{0}{kohler2019_exponentialbn}
\abx@aux@segm{0}{0}{kohler2019_exponentialbn}
\@writefile{toc}{\contentsline {paragraph}{4. Decoupling Weight Norm from Direction: A Geometric Reparameterization}{269}{section*.461}\protected@file@percent }
\abx@aux@backref{165}{kohler2019_exponentialbn}{0}{269}{269}
\@writefile{toc}{\contentsline {paragraph}{5. Stabilizing Deep Networks and Preventing Dead Activations}{269}{section*.462}\protected@file@percent }
\abx@aux@cite{0}{santurkar2019_howdoesbatchnormhelp}
\abx@aux@segm{0}{0}{santurkar2019_howdoesbatchnormhelp}
\@writefile{toc}{\contentsline {subsubsection}{Conclusion: Why BatchNorm Helps—With Caution}{270}{section*.463}\protected@file@percent }
\abx@aux@backref{166}{santurkar2019_howdoesbatchnormhelp}{0}{270}{270}
\@writefile{toc}{\contentsline {subsubsection}{Batch Normalization in Test Time}{271}{section*.464}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.40}{\ignorespaces Batch Normalization in test time: mean and variance are fixed, computed using a running average during training.}}{271}{figure.caption.465}\protected@file@percent }
\newlabel{fig:chapter7_batchnorm_test}{{7.40}{271}{Batch Normalization in test time: mean and variance are fixed, computed using a running average during training}{figure.caption.465}{}}
\@writefile{toc}{\contentsline {subsubsection}{Limitations of BatchNorm}{271}{section*.466}\protected@file@percent }
\BKM@entry{id=325,dest={73656374696F6E2A2E343637},srcline={2376}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030315C303030345C3030302E5C303030355C3030303A5C3030305C3034305C303030425C303030615C303030745C303030635C303030685C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030505C3030306C5C303030615C303030635C303030655C3030306D5C303030655C3030306E5C30303074}
\abx@aux@cite{0}{ioffe2015_batchnorm}
\abx@aux@segm{0}{0}{ioffe2015_batchnorm}
\abx@aux@cite{0}{ioffe2015_batchnorm}
\abx@aux@segm{0}{0}{ioffe2015_batchnorm}
\abx@aux@cite{0}{he2016_resnet}
\abx@aux@segm{0}{0}{he2016_resnet}
\abx@aux@cite{0}{ioffe2015_batchnorm}
\abx@aux@segm{0}{0}{ioffe2015_batchnorm}
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.14.5: Batch Normalization Placement}{272}{section*.467}\protected@file@percent }
\newlabel{enr:bn_placement}{{7.14.5}{272}{\color {ocre}Enrichment \thesubsection : Batch Normalization Placement}{section*.467}{}}
\@writefile{toc}{\contentsline {subsubsection}{Batch Normalization Placement: Typical Ordering}{272}{section*.468}\protected@file@percent }
\abx@aux@backref{167}{ioffe2015_batchnorm}{0}{272}{272}
\abx@aux@backref{168}{ioffe2015_batchnorm}{0}{272}{272}
\abx@aux@backref{169}{he2016_resnet}{0}{272}{272}
\abx@aux@backref{170}{ioffe2015_batchnorm}{0}{272}{272}
\@writefile{toc}{\contentsline {paragraph}{Mathematical Rationale}{272}{section*.469}\protected@file@percent }
\BKM@entry{id=326,dest={73756273656374696F6E2E372E31342E36},srcline={2411}}{5C3337365C3337375C303030415C3030306C5C303030745C303030655C303030725C3030306E5C303030615C303030745C303030695C303030765C303030655C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C303030735C3030305C3034305C3030305C3035305C3030304C5C3030304E5C3030302C5C3030305C3034305C303030495C3030304E5C3030302C5C3030305C3034305C303030475C3030304E5C3030302C5C3030305C3034305C3030302E5C3030302E5C3030302E5C3030305C303531}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.14.6}Alternative Normalization Methods (LN, IN, GN, ...)}{273}{subsection.7.14.6}\protected@file@percent }
\newlabel{subsubsec:alt_norms}{{7.14.6}{273}{Alternative Normalization Methods (LN, IN, GN, ...)}{subsection.7.14.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{Layer Normalization (LN)}{273}{section*.470}\protected@file@percent }
\newlabel{subsubsec:layer_norm}{{7.14.6}{273}{Layer Normalization (LN)}{section*.470}{}}
\@writefile{toc}{\contentsline {paragraph}{Core Idea}{273}{section*.471}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.41}{\ignorespaces Layer Normalization in a fully connected layer: each sample’s hidden activations are normalized independently.}}{273}{figure.caption.472}\protected@file@percent }
\newlabel{fig:chapter7_layernorm_fc}{{7.41}{273}{Layer Normalization in a fully connected layer: each sample’s hidden activations are normalized independently}{figure.caption.472}{}}
\@writefile{toc}{\contentsline {paragraph}{Definition (Fully Connected Layers)}{273}{section*.473}\protected@file@percent }
\abx@aux@cite{0}{becominghuman2018_allaboutnorm}
\abx@aux@segm{0}{0}{becominghuman2018_allaboutnorm}
\abx@aux@cite{0}{becominghuman2018_allaboutnorm}
\abx@aux@segm{0}{0}{becominghuman2018_allaboutnorm}
\@writefile{toc}{\contentsline {paragraph}{Extension to Convolutional Layers}{274}{section*.474}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.42}{\ignorespaces Visualization of Layer Normalization applied across all channels and spatial dimensions within each image independently. Figure adapted from \blx@tocontentsinit {0}\cite {becominghuman2018_allaboutnorm}.}}{274}{figure.caption.475}\protected@file@percent }
\abx@aux@backref{172}{becominghuman2018_allaboutnorm}{0}{274}{274}
\newlabel{fig:chapter7_layernorm_visual}{{7.42}{274}{Visualization of Layer Normalization applied across all channels and spatial dimensions within each image independently. Figure adapted from \cite {becominghuman2018_allaboutnorm}}{figure.caption.475}{}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation}{274}{section*.476}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Advantages of Layer Normalization}{274}{section*.477}\protected@file@percent }
\abx@aux@cite{0}{becominghuman2018_allaboutnorm}
\abx@aux@segm{0}{0}{becominghuman2018_allaboutnorm}
\abx@aux@cite{0}{becominghuman2018_allaboutnorm}
\abx@aux@segm{0}{0}{becominghuman2018_allaboutnorm}
\@writefile{toc}{\contentsline {subsubsection}{Instance Normalization (IN)}{275}{section*.478}\protected@file@percent }
\newlabel{chapter7:subsubec_instance_norm}{{7.14.6}{275}{Instance Normalization (IN)}{section*.478}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.43}{\ignorespaces Visualization of Instance Normalization operation \blx@tocontentsinit {0}\cite {becominghuman2018_allaboutnorm}.}}{275}{figure.caption.479}\protected@file@percent }
\abx@aux@backref{174}{becominghuman2018_allaboutnorm}{0}{275}{275}
\newlabel{fig:chapter7_instancenorm_visual}{{7.43}{275}{Visualization of Instance Normalization operation \cite {becominghuman2018_allaboutnorm}}{figure.caption.479}{}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation}{275}{section*.480}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Advantages of Instance Normalization}{275}{section*.481}\protected@file@percent }
\abx@aux@cite{0}{sh-tsang2018_groupnorm}
\abx@aux@segm{0}{0}{sh-tsang2018_groupnorm}
\abx@aux@cite{0}{sh-tsang2018_groupnorm}
\abx@aux@segm{0}{0}{sh-tsang2018_groupnorm}
\@writefile{toc}{\contentsline {subsubsection}{Group Normalization (GN)}{276}{section*.482}\protected@file@percent }
\newlabel{chapter7_group_normalization}{{7.14.6}{276}{Group Normalization (GN)}{section*.482}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.44}{\ignorespaces Visualization of Group Normalization operation compared to the rest of normalization methods (BN, LN, and IN) \blx@tocontentsinit {0}\cite {sh-tsang2018_groupnorm}.}}{276}{figure.caption.483}\protected@file@percent }
\abx@aux@backref{176}{sh-tsang2018_groupnorm}{0}{276}{276}
\newlabel{fig:chpater7_groupnorm_visual}{{7.44}{276}{Visualization of Group Normalization operation compared to the rest of normalization methods (BN, LN, and IN) \cite {sh-tsang2018_groupnorm}}{figure.caption.483}{}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation}{276}{section*.484}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Advantages of Group Normalization}{276}{section*.485}\protected@file@percent }
\BKM@entry{id=327,dest={73656374696F6E2A2E343839},srcline={2597}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030315C303030345C3030302E5C303030375C3030303A5C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030425C303030615C303030745C303030635C303030685C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsubsection}{Why Do IN, LN, and GN Improve Optimization?}{277}{section*.486}\protected@file@percent }
\newlabel{subsubsec:why_alt_norm}{{7.14.6}{277}{Why Do IN, LN, and GN Improve Optimization?}{section*.486}{}}
\@writefile{toc}{\contentsline {paragraph}{Common Benefits Across IN, LN, and GN}{277}{section*.487}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary: How These Methods Enhance Training}{277}{section*.488}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.14.7: Backpropagation for Batch Normalization}{277}{section*.489}\protected@file@percent }
\newlabel{enrichment:bn_backprop_node}{{7.14.7}{277}{\color {ocre}Enrichment \thesubsection : Backpropagation for Batch Normalization}{section*.489}{}}
\@writefile{toc}{\contentsline {paragraph}{Chain Rule in the Graph}{278}{section*.490}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Gradients w.r.t.\ \(\gamma \) and \(\beta \)}{278}{subparagraph*.491}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Gradient w.r.t.\ \(\hat  {x}_i\)}{278}{subparagraph*.492}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Gradients Involving \(\mu \) and \(\sigma ^2\)}{278}{section*.493}\protected@file@percent }
\abx@aux@cite{0}{zakka2016_batchnorm}
\abx@aux@segm{0}{0}{zakka2016_batchnorm}
\@writefile{toc}{\contentsline {paragraph}{Final: Gradients w.r.t.\ Each \(x_i\)}{279}{section*.494}\protected@file@percent }
\abx@aux@backref{177}{zakka2016_batchnorm}{0}{279}{279}
\@writefile{toc}{\contentsline {paragraph}{Computational Efficiency}{279}{section*.495}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Extension to LN, IN, GN}{279}{section*.496}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{279}{section*.497}\protected@file@percent }
\BKM@entry{id=328,dest={73656374696F6E2A2E343938},srcline={2767}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030375C3030302E5C303030315C303030345C3030302E5C303030385C3030303A5C3030305C3034305C303030425C303030615C303030745C303030635C303030685C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3034365C3030305C3034305C303030325C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{janestreet_l2_bn}
\abx@aux@segm{0}{0}{janestreet_l2_bn}
\@writefile{toc}{\contentsline {subsection}{Enrichment 7.14.8: Batch Normalization \& \(\ell _2\) Regularization}{280}{section*.498}\protected@file@percent }
\newlabel{enrichment:bn_l2_regularization}{{7.14.8}{280}{\color {ocre}Enrichment \thesubsection : Batch Normalization \& \(\ell _2\) Regularization}{section*.498}{}}
\@writefile{toc}{\contentsline {paragraph}{Context and References}{280}{section*.499}\protected@file@percent }
\abx@aux@backref{178}{janestreet_l2_bn}{0}{280}{280}
\@writefile{toc}{\contentsline {paragraph}{1. \(\ell _2\) Regularization Without BatchNorm}{280}{section*.500}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. BN Cancels Weight Norm in the Forward Pass}{280}{section*.501}\protected@file@percent }
\abx@aux@cite{0}{keskar2017_flatminima}
\abx@aux@segm{0}{0}{keskar2017_flatminima}
\abx@aux@cite{0}{li2018_visualizing}
\abx@aux@segm{0}{0}{li2018_visualizing}
\@writefile{toc}{\contentsline {paragraph}{3. Why \(\ell _2\) Still Matters: Learning Dynamics Perspective}{281}{section*.502}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{4. Coexisting With Learning Rate Schedules}{282}{section*.503}\protected@file@percent }
\abx@aux@backref{179}{keskar2017_flatminima}{0}{282}{282}
\abx@aux@backref{180}{li2018_visualizing}{0}{282}{282}
\@writefile{toc}{\contentsline {paragraph}{5. Behavior of BN’s \(\gamma , \beta \)}{282}{section*.504}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{6. Recommendations}{282}{section*.505}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{7. Conclusion: BN \& L2 Are Complementary, Not Contradictory}{283}{section*.506}\protected@file@percent }
\BKM@entry{id=329,dest={636861707465722E38},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030385C3030303A5C3030305C3034305C303030435C3030304E5C3030304E5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C303030735C3030305C3034305C30303049}
\BKM@entry{id=330,dest={73656374696F6E2E382E31},srcline={10}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030425C303030755C303030695C3030306C5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030425C3030306C5C3030306F5C303030635C3030306B5C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C303030535C3030304F5C303030545C303030415C3030305C3034305C303030435C3030304E5C3030304E5C30303073}
\BKM@entry{id=331,dest={73656374696F6E2E382E32},srcline={16}}{5C3337365C3337375C303030415C3030306C5C303030655C303030785C3030304E5C303030655C30303074}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Lecture 8: CNN Architectures I}{284}{chapter.8}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@7}}
\ttl@writefile{ptc}{\ttl@starttoc{default@8}}
\pgfsyspdfmark {pgfid22}{0}{52099153}
\pgfsyspdfmark {pgfid21}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Introduction: From Building Blocks to SOTA CNNs}{284}{section.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.2}AlexNet}{284}{section.8.2}\protected@file@percent }
\abx@aux@backref{181}{krizhevsky2012_alexnet}{0}{284}{284}
\BKM@entry{id=332,dest={73756273656374696F6E2E382E322E31},srcline={32}}{5C3337365C3337375C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030445C303030655C303030745C303030615C303030695C3030306C5C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.1}Architecture Details}{285}{subsection.8.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{First Convolutional Layer (Conv1)}{285}{section*.507}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Memory Requirements}{285}{section*.508}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Number of Learnable Parameters}{285}{section*.509}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Computational Cost}{285}{section*.510}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Max Pooling Layer}{285}{section*.511}\protected@file@percent }
\BKM@entry{id=333,dest={73756273656374696F6E2E382E322E32},srcline={93}}{5C3337365C3337375C303030465C303030695C3030306E5C303030615C3030306C5C3030305C3034305C303030465C303030755C3030306C5C3030306C5C303030795C3030305C3034305C303030435C3030306F5C3030306E5C3030306E5C303030655C303030635C303030745C303030655C303030645C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C30303073}
\BKM@entry{id=334,dest={73756273656374696F6E2E382E322E33},srcline={127}}{5C3337365C3337375C3030304B5C303030655C303030795C3030305C3034305C303030545C303030615C3030306B5C303030655C303030615C303030775C303030615C303030795C303030735C3030305C3034305C303030665C303030725C3030306F5C3030306D5C3030305C3034305C303030415C3030306C5C303030655C303030785C3030304E5C303030655C30303074}
\@writefile{toc}{\contentsline {paragraph}{Memory and Computational Cost}{286}{section*.512}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.2}Final Fully Connected Layers}{286}{subsection.8.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Computational Cost}{286}{section*.513}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces The AlexNet architecture, including a table summarizing memory, parameters, and FLOPs per layer.}}{286}{figure.caption.514}\protected@file@percent }
\newlabel{fig:chapter8_alexnet_architecture}{{8.1}{286}{The AlexNet architecture, including a table summarizing memory, parameters, and FLOPs per layer}{figure.caption.514}{}}
\BKM@entry{id=335,dest={73756273656374696F6E2E382E322E34},srcline={141}}{5C3337365C3337375C3030305A5C303030465C3030304E5C303030655C303030745C3030303A5C3030305C3034305C303030415C3030306E5C3030305C3034305C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030655C3030306D5C303030655C3030306E5C303030745C3030305C3034305C3030306F5C3030306E5C3030305C3034305C303030415C3030306C5C303030655C303030785C3030304E5C303030655C30303074}
\abx@aux@cite{0}{zeiler2014_visualizing}
\abx@aux@segm{0}{0}{zeiler2014_visualizing}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.3}Key Takeaways from AlexNet}{287}{subsection.8.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces Trends in AlexNet: memory usage in early conv layers, parameter-heavy FC layers, and computational cost concentrated in convolutions.}}{287}{figure.caption.515}\protected@file@percent }
\newlabel{fig:chapter8_alexnet_trends}{{8.2}{287}{Trends in AlexNet: memory usage in early conv layers, parameter-heavy FC layers, and computational cost concentrated in convolutions}{figure.caption.515}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.4}ZFNet: An Improvement on AlexNet}{287}{subsection.8.2.4}\protected@file@percent }
\abx@aux@backref{182}{zeiler2014_visualizing}{0}{287}{287}
\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces The ZFNet architecture and its improvements over AlexNet.}}{287}{figure.caption.516}\protected@file@percent }
\newlabel{fig:chapter8_zfnet_architecture}{{8.3}{287}{The ZFNet architecture and its improvements over AlexNet}{figure.caption.516}{}}
\BKM@entry{id=336,dest={73656374696F6E2E382E33},srcline={160}}{5C3337365C3337375C303030565C303030475C303030475C3030303A5C3030305C3034305C303030415C3030305C3034305C303030505C303030725C303030695C3030306E5C303030635C303030695C303030705C3030306C5C303030655C303030645C3030305C3034305C303030435C3030304E5C3030304E5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C30303065}
\abx@aux@cite{0}{simonyan2014_vgg}
\abx@aux@segm{0}{0}{simonyan2014_vgg}
\BKM@entry{id=337,dest={73756273656374696F6E2E382E332E31},srcline={182}}{5C3337365C3337375C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C3030305C3034305C303030535C303030745C303030725C303030755C303030635C303030745C303030755C303030725C30303065}
\@writefile{toc}{\contentsline {subsubsection}{Key Modifications in ZFNet}{288}{section*.517}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.3}VGG: A Principled CNN Architecture}{288}{section.8.3}\protected@file@percent }
\newlabel{sec:vgg_architecture}{{8.3}{288}{VGG: A Principled CNN Architecture}{section.8.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Historical Context.}{288}{section*.518}\protected@file@percent }
\abx@aux@backref{183}{simonyan2014_vgg}{0}{288}{288}
\@writefile{lof}{\contentsline {figure}{\numberline {8.4}{\ignorespaces Comparison of AlexNet vs.\ VGG: model size, parameter count, and FLOPs.}}{288}{figure.caption.519}\protected@file@percent }
\newlabel{fig:chapter7_vgg_alexnet}{{8.4}{288}{Comparison of AlexNet vs.\ VGG: model size, parameter count, and FLOPs}{figure.caption.519}{}}
\@writefile{toc}{\contentsline {paragraph}{Core Design Principles.}{288}{section*.520}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.1}Network Structure}{288}{subsection.8.3.1}\protected@file@percent }
\BKM@entry{id=338,dest={73756273656374696F6E2E382E332E32},srcline={201}}{5C3337365C3337375C3030304B5C303030655C303030795C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030615C3030306C5C3030305C3034305C303030495C3030306E5C303030735C303030695C303030675C303030685C303030745C30303073}
\BKM@entry{id=339,dest={73756273656374696F6E2E382E332E33},srcline={227}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030545C303030685C303030695C303030735C3030305C3034305C303030535C303030745C303030725C303030615C303030745C303030655C303030675C303030795C3030305C3034305C303030575C3030306F5C303030725C3030306B5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {8.5}{\ignorespaces AlexNet vs.\ VGG-16 and VGG-19, highlighting VGG’s deeper, more uniform design. (Slide~\ref {fig:chapter7_vgg_alexnet})}}{289}{figure.caption.521}\protected@file@percent }
\newlabel{fig:chapter7_vgg_alexnet_compare}{{8.5}{289}{AlexNet vs.\ VGG-16 and VGG-19, highlighting VGG’s deeper, more uniform design. (Slide~\ref {fig:chapter7_vgg_alexnet})}{figure.caption.521}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.2}Key Architectural Insights}{289}{subsection.8.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Small-Kernel Convolutions (\(3\times 3\))}{289}{section*.522}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Pooling \(\,2\times 2\), Stride=2, No Padding}{289}{section*.523}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Doubling Channels After Each Pool}{289}{section*.524}\protected@file@percent }
\BKM@entry{id=340,dest={73756273656374696F6E2E382E332E34},srcline={234}}{5C3337365C3337375C303030505C303030725C303030615C303030635C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C3030304F5C303030625C303030735C303030655C303030725C303030765C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=341,dest={73756273656374696F6E2E382E332E35},srcline={244}}{5C3337365C3337375C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030565C303030655C303030725C303030795C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030565C303030475C303030475C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030615C303030635C30303068}
\abx@aux@cite{0}{simonyan2014_vgg}
\abx@aux@segm{0}{0}{simonyan2014_vgg}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.3}Why This Strategy Works}{290}{subsection.8.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Balanced Computation.}{290}{section*.525}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Influence on Later Architectures.}{290}{section*.526}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.4}Practical Observations}{290}{subsection.8.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.5}Training Very Deep Networks: The VGG Approach}{290}{subsection.8.3.5}\protected@file@percent }
\newlabel{subsec:vgg_training}{{8.3.5}{290}{Training Very Deep Networks: The VGG Approach}{subsection.8.3.5}{}}
\abx@aux@backref{184}{simonyan2014_vgg}{0}{290}{290}
\@writefile{toc}{\contentsline {subsubsection}{Incremental Training Strategy}{290}{section*.527}\protected@file@percent }
\BKM@entry{id=342,dest={73656374696F6E2E382E34},srcline={279}}{5C3337365C3337375C303030475C3030306F5C3030306F5C303030675C3030304C5C303030655C3030304E5C303030655C303030745C3030303A5C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030635C303030795C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030505C303030615C303030725C303030615C3030306C5C3030306C5C303030655C3030306C5C303030695C303030735C3030306D}
\abx@aux@cite{0}{szegedy2015_googlenet}
\abx@aux@segm{0}{0}{szegedy2015_googlenet}
\@writefile{toc}{\contentsline {subsubsection}{Optimization and Training Details}{291}{section*.528}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Effectiveness of the Approach}{291}{section*.529}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.4}GoogLeNet: Efficiency and Parallelism}{291}{section.8.4}\protected@file@percent }
\newlabel{sec:googlenet}{{8.4}{291}{GoogLeNet: Efficiency and Parallelism}{section.8.4}{}}
\abx@aux@backref{185}{szegedy2015_googlenet}{0}{291}{291}
\@writefile{lof}{\contentsline {figure}{\numberline {8.6}{\ignorespaces Comparison of AlexNet, VGG, and GoogLeNet, highlighting the architectural evolution toward efficiency.}}{291}{figure.caption.530}\protected@file@percent }
\newlabel{fig:chpater8_googlenet_vgg_comparison}{{8.6}{291}{Comparison of AlexNet, VGG, and GoogLeNet, highlighting the architectural evolution toward efficiency}{figure.caption.530}{}}
\BKM@entry{id=343,dest={73756273656374696F6E2E382E342E31},srcline={294}}{5C3337365C3337375C303030535C303030745C303030655C3030306D5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C3030303A5C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C303030455C303030615C303030725C3030306C5C303030795C3030305C3034305C303030445C3030306F5C303030775C3030306E5C303030735C303030615C3030306D5C303030705C3030306C5C303030695C3030306E5C30303067}
\BKM@entry{id=344,dest={73756273656374696F6E2E382E342E32},srcline={316}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030495C3030306E5C303030635C303030655C303030705C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C3030306F5C303030645C303030755C3030306C5C303030655C3030303A5C3030305C3034305C303030505C303030615C303030725C303030615C3030306C5C3030306C5C303030655C3030306C5C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030455C303030785C303030745C303030725C303030615C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.1}Stem Network: Efficient Early Downsampling}{292}{subsection.8.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.7}{\ignorespaces The stem network in GoogLeNet, highlighting its efficient early downsampling.}}{292}{figure.caption.531}\protected@file@percent }
\newlabel{fig:chpater8_googlenet_stem}{{8.7}{292}{The stem network in GoogLeNet, highlighting its efficient early downsampling}{figure.caption.531}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.2}The Inception Module: Parallel Feature Extraction}{292}{subsection.8.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.8}{\ignorespaces The Inception module visualized, with the first occurrence in the network highlighted.}}{293}{figure.caption.532}\protected@file@percent }
\newlabel{fig:chapter8_inception_module}{{8.8}{293}{The Inception module visualized, with the first occurrence in the network highlighted}{figure.caption.532}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why Does the Inception Module Improve Gradient Flow?}{293}{section*.533}\protected@file@percent }
\BKM@entry{id=345,dest={73756273656374696F6E2E382E342E33},srcline={361}}{5C3337365C3337375C303030475C3030306C5C3030306F5C303030625C303030615C3030306C5C3030305C3034305C303030415C303030765C303030655C303030725C303030615C303030675C303030655C3030305C3034305C303030505C3030306F5C3030306F5C3030306C5C303030695C3030306E5C303030675C3030305C3034305C3030305C3035305C303030475C303030415C303030505C3030305C303531}
\@writefile{toc}{\contentsline {paragraph}{Structure of the Inception Module}{294}{section*.534}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.3}Global Average Pooling (GAP)}{294}{subsection.8.4.3}\protected@file@percent }
\BKM@entry{id=346,dest={73756273656374696F6E2E382E342E34},srcline={378}}{5C3337365C3337375C303030415C303030755C303030785C303030695C3030306C5C303030695C303030615C303030725C303030795C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C303030735C3030303A5C3030305C3034305C303030415C3030305C3034305C303030575C3030306F5C303030725C3030306B5C303030615C303030725C3030306F5C303030755C3030306E5C303030645C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030565C303030615C3030306E5C303030695C303030735C303030685C303030695C3030306E5C303030675C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {8.9}{\ignorespaces GoogLeNet replaces fully connected layers with Global Average Pooling (GAP), drastically reducing parameters and FLOPs.}}{295}{figure.caption.535}\protected@file@percent }
\newlabel{fig:chapter8_googlenet_gap}{{8.9}{295}{GoogLeNet replaces fully connected layers with Global Average Pooling (GAP), drastically reducing parameters and FLOPs}{figure.caption.535}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4.4}Auxiliary Classifiers: A Workaround for Vanishing Gradients}{295}{subsection.8.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Were Auxiliary Classifiers Needed?}{295}{section*.536}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How Do They Help?}{295}{section*.537}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Auxiliary Classifier Design}{295}{section*.538}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.10}{\ignorespaces Auxiliary classifiers in GoogLeNet, placed at intermediate layers to aid gradient flow.}}{296}{figure.caption.539}\protected@file@percent }
\newlabel{fig:chapter8_googlenet_auxiliary}{{8.10}{296}{Auxiliary classifiers in GoogLeNet, placed at intermediate layers to aid gradient flow}{figure.caption.539}{}}
\@writefile{toc}{\contentsline {paragraph}{Gradient Flow and Regularization}{296}{section*.540}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Relevance Today}{296}{section*.541}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{296}{section*.542}\protected@file@percent }
\BKM@entry{id=347,dest={73656374696F6E2E382E35},srcline={433}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030525C303030695C303030735C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C303030525C303030655C303030735C303030695C303030645C303030755C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030525C303030655C303030735C3030304E5C303030655C303030745C303030735C3030305C303531}
\BKM@entry{id=348,dest={73756273656374696F6E2E382E352E31},srcline={435}}{5C3337365C3337375C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\abx@aux@cite{0}{he2016_resnet}
\abx@aux@segm{0}{0}{he2016_resnet}
\BKM@entry{id=349,dest={73756273656374696F6E2E382E352E32},srcline={448}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C3030304E5C303030655C303030655C303030645C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030525C303030655C303030735C303030695C303030645C303030755C303030615C3030306C5C3030305C3034305C303030435C3030306F5C3030306E5C3030306E5C303030655C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {8.5}The Rise of Residual Networks (ResNets)}{297}{section.8.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.1}Challenges in Training Deep Neural Networks}{297}{subsection.8.5.1}\protected@file@percent }
\abx@aux@backref{186}{he2016_resnet}{0}{297}{297}
\@writefile{lof}{\contentsline {figure}{\numberline {8.11}{\ignorespaces ResNets in 2015 compared to previous top-performing models in the ImageNet classification challenge. The error rate dropped significantly ( $\approx 0.5$ error of previous year) while the number of layers increased (x $7$).}}{297}{figure.caption.543}\protected@file@percent }
\newlabel{fig:chapter8_resnet_performance}{{8.11}{297}{ResNets in 2015 compared to previous top-performing models in the ImageNet classification challenge. The error rate dropped significantly ( $\approx 0.5$ error of previous year) while the number of layers increased (x $7$)}{figure.caption.543}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.2}The Need for Residual Connections}{297}{subsection.8.5.2}\protected@file@percent }
\BKM@entry{id=350,dest={73756273656374696F6E2E382E352E33},srcline={463}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030695C3030306E5C303030675C3030305C3034305C303030525C303030655C303030735C303030695C303030645C303030755C303030615C3030306C5C3030305C3034305C303030425C3030306C5C3030306F5C303030635C3030306B5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {8.12}{\ignorespaces A comparison of a 56-layer network and a 20-layer network. The 20-layer model performs better on the test set, while the 56-layer model underfits on the training set, indicating optimization difficulties.}}{298}{figure.caption.544}\protected@file@percent }
\newlabel{fig:chapter8_deeper_networks_underfit}{{8.12}{298}{A comparison of a 56-layer network and a 20-layer network. The 20-layer model performs better on the test set, while the 56-layer model underfits on the training set, indicating optimization difficulties}{figure.caption.544}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.3}Introducing Residual Blocks}{298}{subsection.8.5.3}\protected@file@percent }
\newlabel{sec:residual_blocks}{{8.5.3}{298}{Introducing Residual Blocks}{subsection.8.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.13}{\ignorespaces A comparison between a plain block (left) and a residual block (right). The shortcut connection enables direct gradient flow and allows layers to learn an identity mapping if needed.}}{298}{figure.caption.545}\protected@file@percent }
\newlabel{fig:chapter8_residual_block}{{8.13}{298}{A comparison between a plain block (left) and a residual block (right). The shortcut connection enables direct gradient flow and allows layers to learn an identity mapping if needed}{figure.caption.545}{}}
\BKM@entry{id=351,dest={73756273656374696F6E2E382E352E34},srcline={489}}{5C3337365C3337375C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030615C3030306C5C3030305C3034305C303030445C303030655C303030735C303030695C303030675C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C303030745C30303073}
\BKM@entry{id=352,dest={73756273656374696F6E2E382E352E35},srcline={506}}{5C3337365C3337375C303030425C3030306F5C303030745C303030745C3030306C5C303030655C3030306E5C303030655C303030635C3030306B5C3030305C3034305C303030425C3030306C5C3030306F5C303030635C3030306B5C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030445C303030655C303030655C303030705C303030655C303030725C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\@writefile{toc}{\contentsline {paragraph}{Intuition Behind Residual Connections}{299}{section*.546}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.4}Architectural Design of ResNets}{299}{subsection.8.5.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.14}{\ignorespaces ResNet structure: A stack of residual blocks, where each block consists of two 3x3 convolutional layers with a shortcut connection.}}{299}{figure.caption.547}\protected@file@percent }
\newlabel{fig:chapter8_resnet_structure}{{8.14}{299}{ResNet structure: A stack of residual blocks, where each block consists of two 3x3 convolutional layers with a shortcut connection}{figure.caption.547}{}}
\BKM@entry{id=353,dest={73756273656374696F6E2E382E352E36},srcline={532}}{5C3337365C3337375C303030525C303030655C303030735C3030304E5C303030655C303030745C3030305C3034305C303030575C303030695C3030306E5C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030535C303030745C303030725C303030655C303030615C3030306B5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030695C3030306E5C303030755C303030655C303030645C3030305C3034305C303030495C3030306E5C303030665C3030306C5C303030755C303030655C3030306E5C303030635C30303065}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.5}Bottleneck Blocks for Deeper Networks}{300}{subsection.8.5.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.15}{\ignorespaces Bottleneck residual block: Using 1x1 convolutions before and after the main 3x3 convolution reduces computational costs while increasing depth.}}{300}{figure.caption.548}\protected@file@percent }
\newlabel{fig:chapter8_bottleneck_block}{{8.15}{300}{Bottleneck residual block: Using 1x1 convolutions before and after the main 3x3 convolution reduces computational costs while increasing depth}{figure.caption.548}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.16}{\ignorespaces Switching to bottleneck blocks allowed a smooth transition from ResNet-34 to deeper models like ResNet-50, ResNet-101, and ResNet-152, while improving efficiency.}}{300}{figure.caption.549}\protected@file@percent }
\newlabel{fig:chapter8_resnet_deeper_models}{{8.16}{300}{Switching to bottleneck blocks allowed a smooth transition from ResNet-34 to deeper models like ResNet-50, ResNet-101, and ResNet-152, while improving efficiency}{figure.caption.549}{}}
\abx@aux@cite{0}{lin2014microsoft}
\abx@aux@segm{0}{0}{lin2014microsoft}
\BKM@entry{id=354,dest={73756273656374696F6E2E382E352E37},srcline={542}}{5C3337365C3337375C303030465C303030755C303030725C303030745C303030685C303030655C303030725C3030305C3034305C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030655C3030306D5C303030655C3030306E5C303030745C303030735C3030303A5C3030305C3034305C303030505C303030725C303030655C3030302D5C303030415C303030635C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030425C3030306C5C3030306F5C303030635C3030306B5C30303073}
\abx@aux@cite{0}{he2016identity}
\abx@aux@segm{0}{0}{he2016identity}
\BKM@entry{id=355,dest={73756273656374696F6E2E382E352E38},srcline={553}}{5C3337365C3337375C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030615C3030306C5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C303030735C3030306F5C3030306E5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030455C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030425C303030655C303030795C3030306F5C3030306E5C303030645C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C30303074}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.6}ResNet Winning Streak and Continued Influence}{301}{subsection.8.5.6}\protected@file@percent }
\abx@aux@backref{187}{lin2014microsoft}{0}{301}{301}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.7}Further Improvements: Pre-Activation Blocks}{301}{subsection.8.5.7}\protected@file@percent }
\abx@aux@backref{188}{he2016identity}{0}{301}{301}
\@writefile{lof}{\contentsline {figure}{\numberline {8.17}{\ignorespaces Pre-activation residual block, which improves accuracy by reordering the batch normalization and activation functions.}}{301}{figure.caption.550}\protected@file@percent }
\newlabel{fig:chapter8_pre_activation_resnet}{{8.17}{301}{Pre-activation residual block, which improves accuracy by reordering the batch normalization and activation functions}{figure.caption.550}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.8}Architectural Comparisons and Evolution Beyond ResNet}{302}{subsection.8.5.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The 2016 ImageNet Challenge: Lack of Novelty}{302}{section*.551}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Comparing Model Complexity and Efficiency}{302}{section*.552}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.18}{\ignorespaces Comparison of ResNets with other architectures such as VGG, GoogLeNet, Inception, and others in terms of accuracy, model complexity, and computational cost.}}{302}{figure.caption.553}\protected@file@percent }
\newlabel{fig:chapter8_architecture_comparison}{{8.18}{302}{Comparison of ResNets with other architectures such as VGG, GoogLeNet, Inception, and others in terms of accuracy, model complexity, and computational cost}{figure.caption.553}{}}
\@writefile{toc}{\contentsline {subsubsection}{Beyond ResNets: Refinements and Lightweight Models}{303}{section*.554}\protected@file@percent }
\BKM@entry{id=356,dest={636861707465722E39},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030395C3030303A5C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C30303049}
\BKM@entry{id=357,dest={73656374696F6E2E392E31},srcline={10}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=358,dest={73756273656374696F6E2E392E312E31},srcline={15}}{5C3337365C3337375C303030435C303030615C303030745C303030655C303030675C3030306F5C303030725C303030695C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030505C303030725C303030615C303030635C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030535C303030755C303030625C3030306A5C303030655C303030635C303030745C30303073}
\BKM@entry{id=359,dest={73656374696F6E2E392E32},srcline={45}}{5C3337365C3337375C303030415C303030635C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Lecture 9: Training Neural Networks I}{304}{chapter.9}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@8}}
\ttl@writefile{ptc}{\ttl@starttoc{default@9}}
\pgfsyspdfmark {pgfid24}{0}{52099153}
\pgfsyspdfmark {pgfid23}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}Introduction to Training Neural Networks}{304}{section.9.1}\protected@file@percent }
\newlabel{sec:chapter9_intro}{{9.1}{304}{Introduction to Training Neural Networks}{section.9.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.1}Categories of Practical Training Subjects}{304}{subsection.9.1.1}\protected@file@percent }
\newlabel{subsec:chapter9_training_categories}{{9.1.1}{304}{Categories of Practical Training Subjects}{subsection.9.1.1}{}}
\BKM@entry{id=360,dest={73756273656374696F6E2E392E322E31},srcline={50}}{5C3337365C3337375C303030535C303030695C303030675C3030306D5C3030306F5C303030695C303030645C3030305C3034305C303030415C303030635C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {9.2}Activation Functions}{305}{section.9.2}\protected@file@percent }
\newlabel{sec:chapter9_activation_functions}{{9.2}{305}{Activation Functions}{section.9.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.1}Sigmoid Activation Function}{305}{subsection.9.2.1}\protected@file@percent }
\newlabel{subsec:chapter9_sigmoid}{{9.2.1}{305}{Sigmoid Activation Function}{subsection.9.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Issues with the Sigmoid Function}{305}{section*.555}\protected@file@percent }
\newlabel{subsubsec:chapter9_sigmoid_issues}{{9.2.1}{305}{Issues with the Sigmoid Function}{section*.555}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.1}{\ignorespaces Sigmoid function visualization, highlighting gradient behavior at \( x = -10, 0, 10 \). Gradients are near zero at extreme values, causing vanishing gradients.}}{305}{figure.caption.556}\protected@file@percent }
\newlabel{fig:chapter9_sigmoid_gradients}{{9.1}{305}{Sigmoid function visualization, highlighting gradient behavior at \( x = -10, 0, 10 \). Gradients are near zero at extreme values, causing vanishing gradients}{figure.caption.556}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.2}{\ignorespaces Gradient updates when using sigmoid activation: all gradients with respect to the weights have the same sign, leading to inefficient learning dynamics and potential oscillations.}}{306}{figure.caption.557}\protected@file@percent }
\newlabel{fig:chapter9_sigmoid_grad_dynamics}{{9.2}{306}{Gradient updates when using sigmoid activation: all gradients with respect to the weights have the same sign, leading to inefficient learning dynamics and potential oscillations}{figure.caption.557}{}}
\@writefile{toc}{\contentsline {subsubsection}{The Tanh Activation Function}{307}{section*.558}\protected@file@percent }
\newlabel{subsubsec:chapter9_tanh}{{9.2.1}{307}{The Tanh Activation Function}{section*.558}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.3}{\ignorespaces The \(\tanh \) activation function compared to sigmoid. While \(\tanh \) is zero-centered, it still suffers from vanishing gradients in saturation regions.}}{307}{figure.caption.559}\protected@file@percent }
\newlabel{fig:chapter9_tanh}{{9.3}{307}{The \(\tanh \) activation function compared to sigmoid. While \(\tanh \) is zero-centered, it still suffers from vanishing gradients in saturation regions}{figure.caption.559}{}}
\BKM@entry{id=361,dest={73756273656374696F6E2E392E322E32},srcline={163}}{5C3337365C3337375C303030525C303030655C303030635C303030745C303030695C303030665C303030695C303030655C303030645C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030555C3030306E5C303030695C303030745C303030735C3030305C3034305C3030305C3035305C303030525C303030655C3030304C5C303030555C3030305C3035315C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030495C303030745C303030735C3030305C3034305C303030565C303030615C303030725C303030695C303030615C3030306E5C303030745C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.2}Rectified Linear Units (ReLU) and Its Variants}{308}{subsection.9.2.2}\protected@file@percent }
\newlabel{sec:chapter9_relu}{{9.2.2}{308}{Rectified Linear Units (ReLU) and Its Variants}{subsection.9.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Issues with ReLU}{308}{section*.560}\protected@file@percent }
\newlabel{subsubsec:chapter9_relu_issues}{{9.2.2}{308}{Issues with ReLU}{section*.560}{}}
\abx@aux@cite{0}{he2015_delving}
\abx@aux@segm{0}{0}{he2015_delving}
\@writefile{lof}{\contentsline {figure}{\numberline {9.4}{\ignorespaces ReLU activation function and its failure cases. When inputs are negative, ReLU neurons become inactive, leading to dead ReLUs.}}{309}{figure.caption.561}\protected@file@percent }
\newlabel{fig:chapter9_dead_relu}{{9.4}{309}{ReLU activation function and its failure cases. When inputs are negative, ReLU neurons become inactive, leading to dead ReLUs}{figure.caption.561}{}}
\@writefile{toc}{\contentsline {subsubsection}{Mitigation Strategies for ReLU Issues}{309}{section*.562}\protected@file@percent }
\abx@aux@backref{189}{he2015_delving}{0}{309}{309}
\abx@aux@cite{0}{he2015_delving}
\abx@aux@segm{0}{0}{he2015_delving}
\abx@aux@cite{0}{clevert2015_fast}
\abx@aux@segm{0}{0}{clevert2015_fast}
\@writefile{toc}{\contentsline {subsubsection}{Leaky ReLU and Parametric ReLU (PReLU)}{310}{section*.563}\protected@file@percent }
\newlabel{subsubsec:chapter9_leaky_prelu}{{9.2.2}{310}{Leaky ReLU and Parametric ReLU (PReLU)}{section*.563}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.5}{\ignorespaces Parametric ReLU (PReLU) and Leaky ReLU. PReLU generalizes Leaky ReLU by making \(\alpha \) a learnable parameter.}}{310}{figure.caption.564}\protected@file@percent }
\newlabel{fig:chapter9_prelu}{{9.5}{310}{Parametric ReLU (PReLU) and Leaky ReLU. PReLU generalizes Leaky ReLU by making \(\alpha \) a learnable parameter}{figure.caption.564}{}}
\abx@aux@backref{190}{he2015_delving}{0}{310}{310}
\@writefile{toc}{\contentsline {subsubsection}{Exponential Linear Unit (ELU)}{310}{section*.565}\protected@file@percent }
\newlabel{subsubsec:chapter9_elu}{{9.2.2}{310}{Exponential Linear Unit (ELU)}{section*.565}{}}
\abx@aux@backref{191}{clevert2015_fast}{0}{310}{310}
\abx@aux@cite{0}{klambauer2017_selu}
\abx@aux@segm{0}{0}{klambauer2017_selu}
\@writefile{lof}{\contentsline {figure}{\numberline {9.6}{\ignorespaces ELU activation function. Unlike ReLU, ELU allows small negative values for negative inputs, which improves learning stability.}}{311}{figure.caption.566}\protected@file@percent }
\newlabel{fig:chapter9_elu}{{9.6}{311}{ELU activation function. Unlike ReLU, ELU allows small negative values for negative inputs, which improves learning stability}{figure.caption.566}{}}
\@writefile{toc}{\contentsline {subsubsection}{Scaled Exponential Linear Unit (SELU)}{311}{section*.567}\protected@file@percent }
\newlabel{subsubsec:chapter9_selu}{{9.2.2}{311}{Scaled Exponential Linear Unit (SELU)}{section*.567}{}}
\abx@aux@backref{192}{klambauer2017_selu}{0}{311}{311}
\abx@aux@cite{0}{klambauer2017_selu}
\abx@aux@segm{0}{0}{klambauer2017_selu}
\abx@aux@cite{0}{klambauer2017_selu}
\abx@aux@segm{0}{0}{klambauer2017_selu}
\abx@aux@cite{0}{klambauer2017_selu}
\abx@aux@segm{0}{0}{klambauer2017_selu}
\abx@aux@cite{0}{klambauer2017_selu}
\abx@aux@segm{0}{0}{klambauer2017_selu}
\abx@aux@cite{0}{klambauer2017_selu}
\abx@aux@segm{0}{0}{klambauer2017_selu}
\abx@aux@cite{0}{klambauer2017_selu}
\abx@aux@segm{0}{0}{klambauer2017_selu}
\@writefile{toc}{\contentsline {paragraph}{Definition and Self-Normalization Properties}{312}{section*.568}\protected@file@percent }
\abx@aux@backref{193}{klambauer2017_selu}{0}{312}{312}
\@writefile{toc}{\contentsline {paragraph}{Derivation of \(\alpha \) and \(\lambda \)}{312}{section*.569}\protected@file@percent }
\abx@aux@backref{194}{klambauer2017_selu}{0}{312}{312}
\@writefile{toc}{\contentsline {paragraph}{Weight Initialization and Network Architecture Considerations}{312}{section*.570}\protected@file@percent }
\abx@aux@backref{195}{klambauer2017_selu}{0}{312}{312}
\abx@aux@backref{196}{klambauer2017_selu}{0}{312}{312}
\abx@aux@backref{197}{klambauer2017_selu}{0}{312}{312}
\@writefile{toc}{\contentsline {paragraph}{Practical Considerations and Limitations}{312}{section*.571}\protected@file@percent }
\abx@aux@backref{198}{klambauer2017_selu}{0}{312}{312}
\abx@aux@cite{0}{hendrycks2016_gelu}
\abx@aux@segm{0}{0}{hendrycks2016_gelu}
\@writefile{lof}{\contentsline {figure}{\numberline {9.7}{\ignorespaces SELU activation function. Unlike ELU, SELU has predefined $\alpha , \lambda $ values that ensure self-normalizing properties under certain conditions.}}{313}{figure.caption.572}\protected@file@percent }
\newlabel{fig:chapter9_selu}{{9.7}{313}{SELU activation function. Unlike ELU, SELU has predefined $\alpha , \lambda $ values that ensure self-normalizing properties under certain conditions}{figure.caption.572}{}}
\@writefile{toc}{\contentsline {subsubsection}{Gaussian Error Linear Unit (GELU)}{313}{section*.573}\protected@file@percent }
\newlabel{subsubsec:chapter9_gelu}{{9.2.2}{313}{Gaussian Error Linear Unit (GELU)}{section*.573}{}}
\abx@aux@backref{199}{hendrycks2016_gelu}{0}{313}{313}
\@writefile{toc}{\contentsline {paragraph}{Definition}{313}{section*.574}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.8}{\ignorespaces Visualization of GELU activation, highlighting its smoother transition compared to ReLU and its probabilistic activation mechanism.}}{314}{figure.caption.575}\protected@file@percent }
\newlabel{fig:chapter9_gelu}{{9.8}{314}{Visualization of GELU activation, highlighting its smoother transition compared to ReLU and its probabilistic activation mechanism}{figure.caption.575}{}}
\@writefile{toc}{\contentsline {paragraph}{Advantages of GELU}{314}{section*.576}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Comparisons with ReLU and ELU}{314}{section*.577}\protected@file@percent }
\BKM@entry{id=362,dest={73656374696F6E2A2E353739},srcline={403}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030395C3030302E5C303030325C3030302E5C303030335C3030303A5C3030305C3034305C303030535C303030775C303030695C303030735C303030685C3030303A5C3030305C3034305C303030415C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030475C303030615C303030745C303030655C303030645C3030305C3034305C303030415C303030635C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{ramachandran2017_swish}
\abx@aux@segm{0}{0}{ramachandran2017_swish}
\@writefile{toc}{\contentsline {paragraph}{Computational Considerations}{315}{section*.578}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 9.2.3: Swish: A Self-Gated Activation Function}{315}{section*.579}\protected@file@percent }
\newlabel{enr:chapter9_swish}{{9.2.3}{315}{\color {ocre}Enrichment \thesubsection : Swish: A Self-Gated Activation Function}{section*.579}{}}
\abx@aux@backref{200}{ramachandran2017_swish}{0}{315}{315}
\@writefile{lof}{\contentsline {figure}{\numberline {9.9}{\ignorespaces Visualization of the Swish activation function for different values of \(\beta \). When \(\beta =0\), the sigmoid component is constant at 0.5, making \(f(x)=x\cdot 0.5\) a linear function. For high values of \(\beta \) (e.g., \(\beta =10\)), the sigmoid approximates a binary step function (yielding near 0 for \(x<0\) and near 1 for \(x>0\)), so \(f(x)\) behaves like ReLU. With the standard choice \(\beta =1\), Swish smoothly interpolates between these two extremes, balancing linearity and nonlinearity for improved gradient flow and model performance.}}{315}{figure.caption.580}\protected@file@percent }
\newlabel{fig:chapter9_swish}{{9.9}{315}{Visualization of the Swish activation function for different values of \(\beta \). When \(\beta =0\), the sigmoid component is constant at 0.5, making \(f(x)=x\cdot 0.5\) a linear function. For high values of \(\beta \) (e.g., \(\beta =10\)), the sigmoid approximates a binary step function (yielding near 0 for \(x<0\) and near 1 for \(x>0\)), so \(f(x)\) behaves like ReLU. With the standard choice \(\beta =1\), Swish smoothly interpolates between these two extremes, balancing linearity and nonlinearity for improved gradient flow and model performance}{figure.caption.580}{}}
\abx@aux@cite{0}{tan2019_efficientnet}
\abx@aux@segm{0}{0}{tan2019_efficientnet}
\@writefile{toc}{\contentsline {subsubsection}{Advantages of Swish}{316}{section*.581}\protected@file@percent }
\newlabel{subsec:chapter9_swish_advantages}{{9.2.3}{316}{Advantages of Swish}{section*.581}{}}
\abx@aux@backref{201}{tan2019_efficientnet}{0}{316}{316}
\@writefile{toc}{\contentsline {subsubsection}{Disadvantages of Swish}{316}{section*.582}\protected@file@percent }
\newlabel{subsec:chapter9_swish_disadvantages}{{9.2.3}{316}{Disadvantages of Swish}{section*.582}{}}
\@writefile{toc}{\contentsline {subsubsection}{Comparison to Other Top-Tier Activations}{316}{section*.583}\protected@file@percent }
\newlabel{subsec:chapter9_swish_comparison}{{9.2.3}{316}{Comparison to Other Top-Tier Activations}{section*.583}{}}
\BKM@entry{id=363,dest={73756273656374696F6E2E392E322E34},srcline={467}}{5C3337365C3337375C303030435C303030685C3030306F5C3030306F5C303030735C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030525C303030695C303030675C303030685C303030745C3030305C3034305C303030415C303030635C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{ramachandran2017_searching}
\abx@aux@segm{0}{0}{ramachandran2017_searching}
\abx@aux@cite{0}{ramachandran2017_searching}
\abx@aux@segm{0}{0}{ramachandran2017_searching}
\BKM@entry{id=364,dest={73656374696F6E2E392E33},srcline={491}}{5C3337365C3337375C303030445C303030615C303030745C303030615C3030305C3034305C303030505C303030725C303030655C3030302D5C303030505C303030725C3030306F5C303030635C303030655C303030735C303030735C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {subsubsection}{Conclusion}{317}{section*.584}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.4}Choosing the Right Activation Function}{317}{subsection.9.2.4}\protected@file@percent }
\newlabel{subsec:chapter9_activation_choice}{{9.2.4}{317}{Choosing the Right Activation Function}{subsection.9.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.10}{\ignorespaces Performance comparison of different activation functions. Most modern activations, such as Swish, GELU, ELU, and SELU, perform similarly to ReLU in practice \blx@tocontentsinit {0}\cite {ramachandran2017_searching}.}}{317}{figure.caption.585}\protected@file@percent }
\abx@aux@backref{203}{ramachandran2017_searching}{0}{317}{317}
\newlabel{fig:chapter9_activation_comparison}{{9.10}{317}{Performance comparison of different activation functions. Most modern activations, such as Swish, GELU, ELU, and SELU, perform similarly to ReLU in practice \cite {ramachandran2017_searching}}{figure.caption.585}{}}
\@writefile{toc}{\contentsline {subsubsection}{General Guidelines for Choosing an Activation Function}{317}{section*.586}\protected@file@percent }
\newlabel{subsubsec:chapter9_activation_guidelines}{{9.2.4}{317}{General Guidelines for Choosing an Activation Function}{section*.586}{}}
\BKM@entry{id=365,dest={73756273656374696F6E2E392E332E31},srcline={496}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030505C303030725C303030655C3030302D5C303030505C303030725C3030306F5C303030635C303030655C303030735C303030735C303030695C3030306E5C303030675C3030305C3034305C3030304D5C303030615C303030745C303030745C303030655C303030725C30303073}
\BKM@entry{id=366,dest={73756273656374696F6E2E392E332E32},srcline={515}}{5C3337365C3337375C303030415C303030765C3030306F5C303030695C303030645C303030695C3030306E5C303030675C3030305C3034305C303030505C3030306F5C3030306F5C303030725C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030445C303030795C3030306E5C303030615C3030306D5C303030695C303030635C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {9.3}Data Pre-Processing}{318}{section.9.3}\protected@file@percent }
\newlabel{sec:chapter9_data_preprocessing}{{9.3}{318}{Data Pre-Processing}{section.9.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.1}Why Pre-Processing Matters}{318}{subsection.9.3.1}\protected@file@percent }
\newlabel{subsec:chapter9_why_preprocessing}{{9.3.1}{318}{Why Pre-Processing Matters}{subsection.9.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.11}{\ignorespaces Visualization of data pre-processing. The red cloud represents raw input data, the green cloud shows the effect of mean subtraction, and the blue cloud demonstrates the effect of feature rescaling.}}{318}{figure.caption.587}\protected@file@percent }
\newlabel{fig:chapter9_data_preprocessing}{{9.11}{318}{Visualization of data pre-processing. The red cloud represents raw input data, the green cloud shows the effect of mean subtraction, and the blue cloud demonstrates the effect of feature rescaling}{figure.caption.587}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.2}Avoiding Poor Training Dynamics}{318}{subsection.9.3.2}\protected@file@percent }
\newlabel{subsec:chapter9_avoid_poor_dynamics}{{9.3.2}{318}{Avoiding Poor Training Dynamics}{subsection.9.3.2}{}}
\BKM@entry{id=367,dest={73756273656374696F6E2E392E332E33},srcline={527}}{5C3337365C3337375C303030435C3030306F5C3030306D5C3030306D5C3030306F5C3030306E5C3030305C3034305C303030505C303030725C303030655C3030302D5C303030505C303030725C3030306F5C303030635C303030655C303030735C303030735C303030695C3030306E5C303030675C3030305C3034305C303030545C303030655C303030635C303030685C3030306E5C303030695C303030715C303030755C303030655C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {9.12}{\ignorespaces Unnormalized data can lead to unstable training dynamics: inefficient gradient updates.}}{319}{figure.caption.588}\protected@file@percent }
\newlabel{fig:chapter9_inefficient_gradients}{{9.12}{319}{Unnormalized data can lead to unstable training dynamics: inefficient gradient updates}{figure.caption.588}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.3}Common Pre-Processing Techniques}{319}{subsection.9.3.3}\protected@file@percent }
\newlabel{subsec:chapter9_common_preprocessing}{{9.3.3}{319}{Common Pre-Processing Techniques}{subsection.9.3.3}{}}
\BKM@entry{id=368,dest={73756273656374696F6E2E392E332E34},srcline={547}}{5C3337365C3337375C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030525C3030306F5C303030625C303030755C303030735C303030745C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=369,dest={73756273656374696F6E2E392E332E35},srcline={570}}{5C3337365C3337375C3030304D5C303030615C303030695C3030306E5C303030745C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306E5C303030735C303030695C303030735C303030745C303030655C3030306E5C303030635C303030795C3030305C3034305C303030445C303030755C303030725C303030695C3030306E5C303030675C3030305C3034305C303030495C3030306E5C303030665C303030655C303030725C303030655C3030306E5C303030635C30303065}
\BKM@entry{id=370,dest={73756273656374696F6E2E392E332E36},srcline={575}}{5C3337365C3337375C303030505C303030725C303030655C3030302D5C303030505C303030725C3030306F5C303030635C303030655C303030735C303030735C303030695C3030306E5C303030675C3030305C3034305C303030695C3030306E5C3030305C3034305C303030575C303030655C3030306C5C3030306C5C3030302D5C3030304B5C3030306E5C3030306F5C303030775C3030306E5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C30303073}
\BKM@entry{id=371,dest={73656374696F6E2E392E34},srcline={586}}{5C3337365C3337375C303030575C303030655C303030695C303030675C303030685C303030745C3030305C3034305C303030495C3030306E5C303030695C303030745C303030695C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.4}Normalization for Robust Optimization}{320}{subsection.9.3.4}\protected@file@percent }
\newlabel{subsec:chapter9_normalization_impact}{{9.3.4}{320}{Normalization for Robust Optimization}{subsection.9.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.13}{\ignorespaces Visualizing the impact of normalization on optimization.}}{320}{figure.caption.589}\protected@file@percent }
\newlabel{fig:chapter9_optimization_stability}{{9.13}{320}{Visualizing the impact of normalization on optimization}{figure.caption.589}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.5}Maintaining Consistency During Inference}{320}{subsection.9.3.5}\protected@file@percent }
\newlabel{subsec:chapter9_inference_consistency}{{9.3.5}{320}{Maintaining Consistency During Inference}{subsection.9.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.6}Pre-Processing in Well-Known Architectures}{320}{subsection.9.3.6}\protected@file@percent }
\newlabel{subsec:chapter9_preprocessing_architectures}{{9.3.6}{320}{Pre-Processing in Well-Known Architectures}{subsection.9.3.6}{}}
\BKM@entry{id=372,dest={73756273656374696F6E2E392E342E31},srcline={591}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030735C303030745C303030615C3030306E5C303030745C3030305C3034305C303030495C3030306E5C303030695C303030745C303030695C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {9.4}Weight Initialization}{321}{section.9.4}\protected@file@percent }
\newlabel{sec:weight_initialization}{{9.4}{321}{Weight Initialization}{section.9.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.1}Constant Initialization}{321}{subsection.9.4.1}\protected@file@percent }
\newlabel{subsec:constant_init}{{9.4.1}{321}{Constant Initialization}{subsection.9.4.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Zero Initialization}{321}{section*.590}\protected@file@percent }
\newlabel{subsubsec:zero_init}{{9.4.1}{321}{Zero Initialization}{section*.590}{}}
\@writefile{toc}{\contentsline {subsubsection}{Nonzero Constant Initialization}{322}{section*.591}\protected@file@percent }
\newlabel{subsubsec:constant_nonzero_init}{{9.4.1}{322}{Nonzero Constant Initialization}{section*.591}{}}
\@writefile{toc}{\contentsline {paragraph}{Forward Pass Analysis}{322}{section*.592}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Backpropagation and Gradient Symmetry}{322}{section*.593}\protected@file@percent }
\BKM@entry{id=373,dest={73756273656374696F6E2E392E342E32},srcline={705}}{5C3337365C3337375C303030425C303030725C303030655C303030615C3030306B5C303030695C3030306E5C303030675C3030305C3034305C303030535C303030795C3030306D5C3030306D5C303030655C303030745C303030725C303030795C3030303A5C3030305C3034305C303030525C303030615C3030306E5C303030645C3030306F5C3030306D5C3030305C3034305C303030495C3030306E5C303030695C303030745C303030695C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=374,dest={73756273656374696F6E2E392E342E33},srcline={726}}{5C3337365C3337375C303030565C303030615C303030725C303030695C303030615C3030306E5C303030635C303030655C3030302D5C303030425C303030615C303030735C303030655C303030645C3030305C3034305C303030495C3030306E5C303030695C303030745C303030695C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030455C3030306E5C303030735C303030755C303030725C303030695C3030306E5C303030675C3030305C3034305C303030535C303030745C303030615C303030625C3030306C5C303030655C3030305C3034305C303030495C3030306E5C303030665C3030306F5C303030725C3030306D5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030465C3030306C5C3030306F5C30303077}
\@writefile{toc}{\contentsline {paragraph}{Implications and Conclusion}{323}{section*.594}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.2}Breaking Symmetry: Random Initialization}{323}{subsection.9.4.2}\protected@file@percent }
\newlabel{subsec:random_init}{{9.4.2}{323}{Breaking Symmetry: Random Initialization}{subsection.9.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.3}Variance-Based Initialization: Ensuring Stable Information Flow}{323}{subsection.9.4.3}\protected@file@percent }
\newlabel{subsec:variance_init}{{9.4.3}{323}{Variance-Based Initialization: Ensuring Stable Information Flow}{subsection.9.4.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Key Requirements for Stable Propagation}{324}{section*.595}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Forward Pass Analysis}{324}{section*.596}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Is This Important?}{324}{section*.597}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Does Forward Signal Variance Also Matter?}{324}{section*.598}\protected@file@percent }
\BKM@entry{id=375,dest={73756273656374696F6E2E392E342E34},srcline={796}}{5C3337365C3337375C303030585C303030615C303030765C303030695C303030655C303030725C3030305C3034305C303030495C3030306E5C303030695C303030745C303030695C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{glorot2010_understanding}
\abx@aux@segm{0}{0}{glorot2010_understanding}
\@writefile{toc}{\contentsline {paragraph}{Challenges in Achieving Stable Variance}{325}{section*.599}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.4}Xavier Initialization}{325}{subsection.9.4.4}\protected@file@percent }
\newlabel{sec:xavier_init}{{9.4.4}{325}{Xavier Initialization}{subsection.9.4.4}{}}
\abx@aux@backref{204}{glorot2010_understanding}{0}{325}{325}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{325}{section*.600}\protected@file@percent }
\newlabel{subsec:xavier_motivation}{{9.4.4}{325}{Motivation}{section*.600}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.14}{\ignorespaces Xavier initialization: activations are nicely scaled for all the layers.}}{326}{figure.caption.601}\protected@file@percent }
\newlabel{fig:chapter9_xavier_init}{{9.14}{326}{Xavier initialization: activations are nicely scaled for all the layers}{figure.caption.601}{}}
\@writefile{toc}{\contentsline {subsubsection}{Mathematical Formulation}{326}{section*.602}\protected@file@percent }
\newlabel{subsec:xavier_math}{{9.4.4}{326}{Mathematical Formulation}{section*.602}{}}
\@writefile{toc}{\contentsline {subsubsection}{Assumptions}{326}{section*.603}\protected@file@percent }
\newlabel{subsec:xavier_assumptions}{{9.4.4}{326}{Assumptions}{section*.603}{}}
\abx@aux@cite{0}{hlav2023_xavier}
\abx@aux@segm{0}{0}{hlav2023_xavier}
\@writefile{toc}{\contentsline {subsubsection}{Derivation of Xavier Initialization}{327}{section*.604}\protected@file@percent }
\newlabel{subsec:xavier_derivation}{{9.4.4}{327}{Derivation of Xavier Initialization}{section*.604}{}}
\abx@aux@backref{205}{hlav2023_xavier}{0}{327}{327}
\@writefile{toc}{\contentsline {paragraph}{Forward Pass: Maintaining Activation Variance}{327}{section*.605}\protected@file@percent }
\newlabel{subsubsec:xavier_forward}{{9.4.4}{327}{Forward Pass: Maintaining Activation Variance}{section*.605}{}}
\@writefile{toc}{\contentsline {paragraph}{Backward Pass: Maintaining Gradient Variance}{327}{section*.606}\protected@file@percent }
\newlabel{subsubsec:xavier_backward}{{9.4.4}{327}{Backward Pass: Maintaining Gradient Variance}{section*.606}{}}
\@writefile{toc}{\contentsline {paragraph}{Balancing Forward and Backward Variance}{328}{section*.607}\protected@file@percent }
\newlabel{subsubsec:xavier_balancing}{{9.4.4}{328}{Balancing Forward and Backward Variance}{section*.607}{}}
\abx@aux@cite{0}{he2015_delving}
\abx@aux@segm{0}{0}{he2015_delving}
\@writefile{toc}{\contentsline {subsubsection}{Final Xavier Initialization Formulation}{329}{section*.608}\protected@file@percent }
\newlabel{subsubsec:xavier_final}{{9.4.4}{329}{Final Xavier Initialization Formulation}{section*.608}{}}
\@writefile{toc}{\contentsline {subsubsection}{Limitations of Xavier Initialization}{329}{section*.609}\protected@file@percent }
\newlabel{subsec:xavier_limitations}{{9.4.4}{329}{Limitations of Xavier Initialization}{section*.609}{}}
\abx@aux@backref{206}{he2015_delving}{0}{329}{329}
\BKM@entry{id=376,dest={73756273656374696F6E2E392E342E35},srcline={988}}{5C3337365C3337375C3030304B5C303030615C303030695C3030306D5C303030695C3030306E5C303030675C3030305C3034305C303030485C303030655C3030305C3034305C303030495C3030306E5C303030695C303030745C303030695C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{he2015_delving}
\abx@aux@segm{0}{0}{he2015_delving}
\abx@aux@cite{0}{hlav2023_kaiming}
\abx@aux@segm{0}{0}{hlav2023_kaiming}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.5}Kaiming He Initialization}{330}{subsection.9.4.5}\protected@file@percent }
\newlabel{subsec:kaiming_init}{{9.4.5}{330}{Kaiming He Initialization}{subsection.9.4.5}{}}
\abx@aux@backref{207}{he2015_delving}{0}{330}{330}
\abx@aux@backref{208}{hlav2023_kaiming}{0}{330}{330}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{330}{section*.610}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.15}{\ignorespaces Xavier initialization applied with ReLU: activations collapse to zero due to the mismatch between the initialization assumptions and the activation function properties. This prevents effective learning.}}{330}{figure.caption.611}\protected@file@percent }
\newlabel{fig:chapter9_xavier_relu_fail}{{9.15}{330}{Xavier initialization applied with ReLU: activations collapse to zero due to the mismatch between the initialization assumptions and the activation function properties. This prevents effective learning}{figure.caption.611}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.16}{\ignorespaces Kaiming initialization: activations are well-scaled across layers, preserving variance and enabling stable learning with ReLU.}}{331}{figure.caption.612}\protected@file@percent }
\newlabel{fig:chapter9_kaiming_init}{{9.16}{331}{Kaiming initialization: activations are well-scaled across layers, preserving variance and enabling stable learning with ReLU}{figure.caption.612}{}}
\@writefile{toc}{\contentsline {paragraph}{Mathematical Notation}{331}{section*.613}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Assumptions}{331}{section*.614}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Forward and Backward Pass Derivation}{332}{section*.615}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Forward Pass Analysis}{332}{section*.616}\protected@file@percent }
\abx@aux@cite{0}{he2015_delving}
\abx@aux@segm{0}{0}{he2015_delving}
\@writefile{toc}{\contentsline {paragraph}{Backward Pass Analysis}{333}{section*.617}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Final Kaiming Initialization Formulation}{333}{section*.618}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Implementation in Deep Learning Frameworks}{333}{section*.619}\protected@file@percent }
\abx@aux@cite{0}{zhang2019_fixup}
\abx@aux@segm{0}{0}{zhang2019_fixup}
\@writefile{toc}{\contentsline {subsubsection}{Initialization in Residual Networks (ResNets)}{334}{section*.620}\protected@file@percent }
\newlabel{subsubsec:resnet_init}{{9.4.5}{334}{Initialization in Residual Networks (ResNets)}{section*.620}{}}
\abx@aux@backref{209}{he2015_delving}{0}{334}{334}
\@writefile{toc}{\contentsline {paragraph}{Why Doesn't Kaiming Initialization Work for ResNets?}{334}{section*.621}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Fixup Initialization}{334}{section*.622}\protected@file@percent }
\abx@aux@backref{210}{zhang2019_fixup}{0}{334}{334}
\@writefile{lof}{\contentsline {figure}{\numberline {9.17}{\ignorespaces Fixup Initialization: The first convolution is initialized using Kaiming, while the second convolution is initialized to zero, ensuring stable variance across layers in ResNets.}}{334}{figure.caption.623}\protected@file@percent }
\newlabel{fig:chapter9_fixup_init}{{9.17}{334}{Fixup Initialization: The first convolution is initialized using Kaiming, while the second convolution is initialized to zero, ensuring stable variance across layers in ResNets}{figure.caption.623}{}}
\BKM@entry{id=377,dest={73756273656374696F6E2E392E342E36},srcline={1215}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030435C303030685C3030306F5C3030306F5C303030735C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030525C303030695C303030675C303030685C303030745C3030305C3034305C303030495C3030306E5C303030695C303030745C303030695C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030535C303030745C303030725C303030615C303030745C303030655C303030675C30303079}
\abx@aux@cite{0}{glorot2010_understanding}
\abx@aux@segm{0}{0}{glorot2010_understanding}
\abx@aux@cite{0}{he2015_delving}
\abx@aux@segm{0}{0}{he2015_delving}
\abx@aux@cite{0}{zhang2019_fixup}
\abx@aux@segm{0}{0}{zhang2019_fixup}
\abx@aux@cite{0}{huang2020_tfixup}
\abx@aux@segm{0}{0}{huang2020_tfixup}
\abx@aux@cite{0}{brock2021_highperformance}
\abx@aux@segm{0}{0}{brock2021_highperformance}
\BKM@entry{id=378,dest={73656374696F6E2E392E35},srcline={1246}}{5C3337365C3337375C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030655C303030635C303030685C3030306E5C303030695C303030715C303030755C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.6}Conclusion: Choosing the Right Initialization Strategy}{335}{subsection.9.4.6}\protected@file@percent }
\newlabel{subsec:initialization_conclusion}{{9.4.6}{335}{Conclusion: Choosing the Right Initialization Strategy}{subsection.9.4.6}{}}
\abx@aux@backref{211}{glorot2010_understanding}{0}{335}{335}
\abx@aux@backref{212}{he2015_delving}{0}{335}{335}
\abx@aux@backref{213}{zhang2019_fixup}{0}{335}{335}
\abx@aux@backref{214}{huang2020_tfixup}{0}{335}{335}
\@writefile{toc}{\contentsline {subsubsection}{Ongoing Research and Open Questions}{335}{section*.624}\protected@file@percent }
\abx@aux@backref{215}{brock2021_highperformance}{0}{335}{335}
\BKM@entry{id=379,dest={73756273656374696F6E2E392E352E31},srcline={1251}}{5C3337365C3337375C303030445C303030725C3030306F5C303030705C3030306F5C303030755C30303074}
\abx@aux@cite{0}{srivastava2014_dropout}
\abx@aux@segm{0}{0}{srivastava2014_dropout}
\@writefile{toc}{\contentsline {section}{\numberline {9.5}Regularization Techniques}{336}{section.9.5}\protected@file@percent }
\newlabel{sec:regularization}{{9.5}{336}{Regularization Techniques}{section.9.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5.1}Dropout}{336}{subsection.9.5.1}\protected@file@percent }
\newlabel{subsec:dropout}{{9.5.1}{336}{Dropout}{subsection.9.5.1}{}}
\abx@aux@backref{216}{srivastava2014_dropout}{0}{336}{336}
\@writefile{lof}{\contentsline {figure}{\numberline {9.18}{\ignorespaces Visualization of dropout: neurons are randomly dropped during training.}}{336}{figure.caption.625}\protected@file@percent }
\newlabel{fig:chapter9_dropout}{{9.18}{336}{Visualization of dropout: neurons are randomly dropped during training}{figure.caption.625}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.19}{\ignorespaces Python implementation of dropout in a few lines of code.}}{337}{figure.caption.626}\protected@file@percent }
\newlabel{fig:chapter9_dropout_code}{{9.19}{337}{Python implementation of dropout in a few lines of code}{figure.caption.626}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why Does Dropout Work?}{337}{section*.627}\protected@file@percent }
\newlabel{subsubsec:dropout_interpretation}{{9.5.1}{337}{Why Does Dropout Work?}{section*.627}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.20}{\ignorespaces Dropout prevents co-adaptation by enforcing redundant feature representations.}}{337}{figure.caption.628}\protected@file@percent }
\newlabel{fig:chapter9_dropout_coadaptation}{{9.20}{337}{Dropout prevents co-adaptation by enforcing redundant feature representations}{figure.caption.628}{}}
\@writefile{toc}{\contentsline {subsubsection}{Dropout at Test Time}{338}{section*.629}\protected@file@percent }
\newlabel{subsubsec:dropout_test}{{9.5.1}{338}{Dropout at Test Time}{section*.629}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.21}{\ignorespaces Mathematical formulation of dropout and the difficulty of marginalizing out the random variable.}}{339}{figure.caption.630}\protected@file@percent }
\newlabel{fig:chapter9_dropout_expectation}{{9.21}{339}{Mathematical formulation of dropout and the difficulty of marginalizing out the random variable}{figure.caption.630}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.22}{\ignorespaces Approximation of the expected activation for a single neuron, motivating test-time scaling.}}{339}{figure.caption.631}\protected@file@percent }
\newlabel{fig:chapter9_dropout_scaling}{{9.22}{339}{Approximation of the expected activation for a single neuron, motivating test-time scaling}{figure.caption.631}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.23}{\ignorespaces Test-time dropout implementation: scaling activations by the dropout probability.}}{340}{figure.caption.632}\protected@file@percent }
\newlabel{fig:chapter9_dropout_testtime}{{9.23}{340}{Test-time dropout implementation: scaling activations by the dropout probability}{figure.caption.632}{}}
\@writefile{toc}{\contentsline {subsubsection}{Inverted Dropout}{340}{section*.633}\protected@file@percent }
\newlabel{subsubsec:inverted_dropout}{{9.5.1}{340}{Inverted Dropout}{section*.633}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.24}{\ignorespaces Python implementation of inverted dropout, where scaling occurs during training.}}{340}{figure.caption.634}\protected@file@percent }
\newlabel{fig:chapter9_inverted_dropout}{{9.24}{340}{Python implementation of inverted dropout, where scaling occurs during training}{figure.caption.634}{}}
\BKM@entry{id=380,dest={73656374696F6E2A2E363337},srcline={1383}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030395C3030302E5C303030355C3030302E5C303030325C3030303A5C3030305C3034305C3030304F5C303030725C303030645C303030655C303030725C303030695C3030306E5C303030675C3030305C3034305C3030306F5C303030665C3030305C3034305C303030445C303030725C3030306F5C303030705C3030306F5C303030755C303030745C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030425C303030615C303030745C303030635C303030685C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsubsection}{Where is Dropout Used in CNNs?}{341}{section*.635}\protected@file@percent }
\newlabel{subsubsec:dropout_cnn_usage}{{9.5.1}{341}{Where is Dropout Used in CNNs?}{section*.635}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.25}{\ignorespaces Dropout usage in AlexNet and VGG16: applied to fully connected layers at the end of the architecture.}}{341}{figure.caption.636}\protected@file@percent }
\newlabel{fig:chapter9_dropout_cnn}{{9.25}{341}{Dropout usage in AlexNet and VGG16: applied to fully connected layers at the end of the architecture}{figure.caption.636}{}}
\@writefile{toc}{\contentsline {subsection}{Enrichment 9.5.2: Ordering of Dropout and Batch Normalization}{341}{section*.637}\protected@file@percent }
\abx@aux@cite{0}{ioffe2015_batchnorm}
\abx@aux@segm{0}{0}{ioffe2015_batchnorm}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 9.5.2.1: Impact of Dropout Placement on BN}{342}{section*.638}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 9.5.2.2: Why BN Before Dropout is Preferred}{342}{section*.639}\protected@file@percent }
\abx@aux@backref{217}{ioffe2015_batchnorm}{0}{342}{342}
\BKM@entry{id=381,dest={73756273656374696F6E2E392E352E33},srcline={1441}}{5C3337365C3337375C3030304F5C303030745C303030685C303030655C303030725C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030655C303030635C303030685C3030306E5C303030695C303030715C303030755C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5.3}Other Regularization Techniques}{343}{subsection.9.5.3}\protected@file@percent }
\newlabel{subsec:other_regularization}{{9.5.3}{343}{Other Regularization Techniques}{subsection.9.5.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{Data Augmentation as Implicit Regularization}{343}{section*.640}\protected@file@percent }
\newlabel{subsubsec:data_augmentation}{{9.5.3}{343}{Data Augmentation as Implicit Regularization}{section*.640}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.26}{\ignorespaces Data augmentation: random transformations applied before training.}}{343}{figure.caption.641}\protected@file@percent }
\newlabel{fig:chapter9_data_augmentation}{{9.26}{343}{Data augmentation: random transformations applied before training}{figure.caption.641}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.27}{\ignorespaces Test-time augmentation in ResNet: multiple fixed crops and scales are used to marginalize out randomness.}}{344}{figure.caption.642}\protected@file@percent }
\newlabel{fig:chapter9_test_time_augmentation}{{9.27}{344}{Test-time augmentation in ResNet: multiple fixed crops and scales are used to marginalize out randomness}{figure.caption.642}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.28}{\ignorespaces Color jittering as an example of augmentation used in AlexNet and ResNet.}}{344}{figure.caption.643}\protected@file@percent }
\newlabel{fig:chapter9_color_jitter}{{9.28}{344}{Color jittering as an example of augmentation used in AlexNet and ResNet}{figure.caption.643}{}}
\abx@aux@cite{0}{wan2013_dropconnect}
\abx@aux@segm{0}{0}{wan2013_dropconnect}
\@writefile{toc}{\contentsline {subsubsection}{DropConnect}{345}{section*.644}\protected@file@percent }
\newlabel{subsubsec:dropconnect}{{9.5.3}{345}{DropConnect}{section*.644}{}}
\abx@aux@backref{218}{wan2013_dropconnect}{0}{345}{345}
\@writefile{lof}{\contentsline {figure}{\numberline {9.29}{\ignorespaces DropConnect: Instead of zeroing out neurons like in Dropout, DropConnect randomly removes weights.}}{345}{figure.caption.645}\protected@file@percent }
\newlabel{fig:chapter9_dropconnect}{{9.29}{345}{DropConnect: Instead of zeroing out neurons like in Dropout, DropConnect randomly removes weights}{figure.caption.645}{}}
\@writefile{toc}{\contentsline {paragraph}{Comparison Between Dropout and DropConnect}{345}{section*.646}\protected@file@percent }
\abx@aux@cite{0}{graham2015_fractionalmaxpool}
\abx@aux@segm{0}{0}{graham2015_fractionalmaxpool}
\@writefile{toc}{\contentsline {paragraph}{Effectiveness and Use Cases}{346}{section*.647}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{346}{section*.648}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Fractional Max Pooling}{346}{section*.649}\protected@file@percent }
\newlabel{subsubsec:fractional_max_pooling}{{9.5.3}{346}{Fractional Max Pooling}{section*.649}{}}
\abx@aux@backref{219}{graham2015_fractionalmaxpool}{0}{346}{346}
\@writefile{lof}{\contentsline {figure}{\numberline {9.30}{\ignorespaces Fractional Max Pooling: randomized pooling regions varying in size across forward passes.}}{346}{figure.caption.650}\protected@file@percent }
\newlabel{fig:chapter9_fractional_max_pooling}{{9.30}{346}{Fractional Max Pooling: randomized pooling regions varying in size across forward passes}{figure.caption.650}{}}
\abx@aux@cite{0}{huang2016_stochasticdepth}
\abx@aux@segm{0}{0}{huang2016_stochasticdepth}
\abx@aux@cite{0}{devries2017_cutout}
\abx@aux@segm{0}{0}{devries2017_cutout}
\@writefile{toc}{\contentsline {subsubsection}{Stochastic Depth}{347}{section*.651}\protected@file@percent }
\newlabel{subsubsec:stochastic_depth}{{9.5.3}{347}{Stochastic Depth}{section*.651}{}}
\abx@aux@backref{220}{huang2016_stochasticdepth}{0}{347}{347}
\@writefile{lof}{\contentsline {figure}{\numberline {9.31}{\ignorespaces Stochastic Depth: at each forward pass, only a subset of layers is used. At test time, all layers are utilized.}}{347}{figure.caption.652}\protected@file@percent }
\newlabel{fig:chapter9_stochastic_depth}{{9.31}{347}{Stochastic Depth: at each forward pass, only a subset of layers is used. At test time, all layers are utilized}{figure.caption.652}{}}
\@writefile{toc}{\contentsline {subsubsection}{CutOut}{347}{section*.653}\protected@file@percent }
\newlabel{subsubsec:cutout}{{9.5.3}{347}{CutOut}{section*.653}{}}
\abx@aux@backref{221}{devries2017_cutout}{0}{347}{347}
\abx@aux@cite{0}{zhang2018_mixup}
\abx@aux@segm{0}{0}{zhang2018_mixup}
\@writefile{lof}{\contentsline {figure}{\numberline {9.32}{\ignorespaces CutOut: parts of the image are occluded to prevent over-reliance on specific features.}}{348}{figure.caption.654}\protected@file@percent }
\newlabel{fig:chapter9_cutout}{{9.32}{348}{CutOut: parts of the image are occluded to prevent over-reliance on specific features}{figure.caption.654}{}}
\@writefile{toc}{\contentsline {subsubsection}{MixUp}{348}{section*.655}\protected@file@percent }
\newlabel{subsubsec:mixup}{{9.5.3}{348}{MixUp}{section*.655}{}}
\abx@aux@backref{222}{zhang2018_mixup}{0}{348}{348}
\@writefile{lof}{\contentsline {figure}{\numberline {9.33}{\ignorespaces MixUp: blending two images and their labels to create intermediate samples.}}{348}{figure.caption.656}\protected@file@percent }
\newlabel{fig:chapter9_mixup}{{9.33}{348}{MixUp: blending two images and their labels to create intermediate samples}{figure.caption.656}{}}
\@writefile{toc}{\contentsline {subsubsection}{Summary and Regularization Guidelines}{349}{section*.657}\protected@file@percent }
\newlabel{subsubsec:regularization_guidelines}{{9.5.3}{349}{Summary and Regularization Guidelines}{section*.657}{}}
\BKM@entry{id=382,dest={636861707465722E3130},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030315C303030305C3030303A5C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C303030495C30303049}
\BKM@entry{id=383,dest={73656374696F6E2E31302E31},srcline={10}}{5C3337365C3337375C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030525C303030615C303030745C303030655C3030305C3034305C303030535C303030635C303030685C303030655C303030645C303030755C3030306C5C303030655C30303073}
\BKM@entry{id=384,dest={73756273656374696F6E2E31302E312E31},srcline={15}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030495C3030306D5C303030705C3030306F5C303030725C303030745C303030615C3030306E5C303030635C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030525C303030615C303030745C303030655C3030305C3034305C303030535C303030655C3030306C5C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {chapter}{\numberline {10}Lecture 10: Training Neural Networks II}{350}{chapter.10}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@9}}
\ttl@writefile{ptc}{\ttl@starttoc{default@10}}
\pgfsyspdfmark {pgfid26}{0}{52099153}
\pgfsyspdfmark {pgfid25}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {10.1}Learning Rate Schedules}{350}{section.10.1}\protected@file@percent }
\newlabel{sec:learning_rate_schedules}{{10.1}{350}{Learning Rate Schedules}{section.10.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.1}The Importance of Learning Rate Selection}{350}{subsection.10.1.1}\protected@file@percent }
\newlabel{subsec:learning_rate_selection}{{10.1.1}{350}{The Importance of Learning Rate Selection}{subsection.10.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.1}{\ignorespaces Effect of different learning rates on training. Yellow: too high, leading to divergence; Blue: too low, resulting in slow progress; Green: somewhat high, converging suboptimally; Red: well-chosen learning rate, ensuring efficient training.}}{350}{figure.caption.658}\protected@file@percent }
\newlabel{fig:chapter10_lr_selection}{{10.1}{350}{Effect of different learning rates on training. Yellow: too high, leading to divergence; Blue: too low, resulting in slow progress; Green: somewhat high, converging suboptimally; Red: well-chosen learning rate, ensuring efficient training}{figure.caption.658}{}}
\BKM@entry{id=385,dest={73756273656374696F6E2E31302E312E32},srcline={38}}{5C3337365C3337375C303030535C303030745C303030655C303030705C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030525C303030615C303030745C303030655C3030305C3034305C303030535C303030635C303030685C303030655C303030645C303030755C3030306C5C30303065}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.2}Step Learning Rate Schedule}{351}{subsection.10.1.2}\protected@file@percent }
\newlabel{subsec:step_lr}{{10.1.2}{351}{Step Learning Rate Schedule}{subsection.10.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.2}{\ignorespaces Step Learning Rate Decay: The learning rate is reduced by a factor of $0.1$ at epochs 30, 60, and 90, indicated by the dashed vertical lines. The red curve represents the noisy per-iteration loss, highlighting the inherent variance in training. The light blue curve shows the Exponential Moving Average (EMA) of the loss, providing a smoother trajectory of the loss progression. The EMA helps visualize the overall trend despite the noise, demonstrating the impact of learning rate drops on loss reduction over time.}}{351}{figure.caption.659}\protected@file@percent }
\newlabel{fig:chapter10_step_lr}{{10.2}{351}{Step Learning Rate Decay: The learning rate is reduced by a factor of $0.1$ at epochs 30, 60, and 90, indicated by the dashed vertical lines. The red curve represents the noisy per-iteration loss, highlighting the inherent variance in training. The light blue curve shows the Exponential Moving Average (EMA) of the loss, providing a smoother trajectory of the loss progression. The EMA helps visualize the overall trend despite the noise, demonstrating the impact of learning rate drops on loss reduction over time}{figure.caption.659}{}}
\BKM@entry{id=386,dest={73756273656374696F6E2E31302E312E33},srcline={69}}{5C3337365C3337375C303030435C3030306F5C303030735C303030695C3030306E5C303030655C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030525C303030615C303030745C303030655C3030305C3034305C303030445C303030655C303030635C303030615C30303079}
\@writefile{toc}{\contentsline {subsubsection}{Practical Considerations}{352}{section*.660}\protected@file@percent }
\newlabel{subsec:step_lr_practical}{{10.1.2}{352}{Practical Considerations}{section*.660}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.3}Cosine Learning Rate Decay}{352}{subsection.10.1.3}\protected@file@percent }
\newlabel{subsubsec:cosine_lr}{{10.1.3}{352}{Cosine Learning Rate Decay}{subsection.10.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.3}{\ignorespaces Cosine learning rate decay: smoothly reducing the learning rate following a cosine wave shape.}}{352}{figure.caption.661}\protected@file@percent }
\newlabel{fig:chapter10_cosine_lr}{{10.3}{352}{Cosine learning rate decay: smoothly reducing the learning rate following a cosine wave shape}{figure.caption.661}{}}
\BKM@entry{id=387,dest={73756273656374696F6E2E31302E312E34},srcline={106}}{5C3337365C3337375C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030525C303030615C303030745C303030655C3030305C3034305C303030445C303030655C303030635C303030615C30303079}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.4}Linear Learning Rate Decay}{353}{subsection.10.1.4}\protected@file@percent }
\newlabel{subsubsec:linear_lr}{{10.1.4}{353}{Linear Learning Rate Decay}{subsection.10.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.4}{\ignorespaces Linear learning rate decay: a simple alternative to cosine decay, reducing the learning rate linearly over time.}}{353}{figure.caption.662}\protected@file@percent }
\newlabel{fig:chapter10_linear_lr}{{10.4}{353}{Linear learning rate decay: a simple alternative to cosine decay, reducing the learning rate linearly over time}{figure.caption.662}{}}
\abx@aux@cite{0}{devlin2019_bert}
\abx@aux@segm{0}{0}{devlin2019_bert}
\abx@aux@cite{0}{liu2019_roberta}
\abx@aux@segm{0}{0}{liu2019_roberta}
\BKM@entry{id=388,dest={73756273656374696F6E2E31302E312E35},srcline={133}}{5C3337365C3337375C303030495C3030306E5C303030765C303030655C303030725C303030735C303030655C3030305C3034305C303030535C303030715C303030755C303030615C303030725C303030655C3030305C3034305C303030525C3030306F5C3030306F5C303030745C3030305C3034305C303030445C303030655C303030635C303030615C30303079}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\BKM@entry{id=389,dest={73756273656374696F6E2E31302E312E36},srcline={151}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030735C303030745C303030615C3030306E5C303030745C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030525C303030615C303030745C30303065}
\abx@aux@backref{223}{devlin2019_bert}{0}{354}{354}
\abx@aux@backref{224}{liu2019_roberta}{0}{354}{354}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.5}Inverse Square Root Decay}{354}{subsection.10.1.5}\protected@file@percent }
\newlabel{subsec:inverse_sqrt_decay}{{10.1.5}{354}{Inverse Square Root Decay}{subsection.10.1.5}{}}
\abx@aux@backref{225}{vaswani2017_attention}{0}{354}{354}
\@writefile{lof}{\contentsline {figure}{\numberline {10.5}{\ignorespaces Inverse Square Root learning rate decay.}}{354}{figure.caption.663}\protected@file@percent }
\newlabel{fig:chapter10_inverse_sqrt}{{10.5}{354}{Inverse Square Root learning rate decay}{figure.caption.663}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.6}Constant Learning Rate}{355}{subsection.10.1.6}\protected@file@percent }
\newlabel{subsec:constant_lr}{{10.1.6}{355}{Constant Learning Rate}{subsection.10.1.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.6}{\ignorespaces Constant learning rate decay.}}{355}{figure.caption.664}\protected@file@percent }
\newlabel{fig:chapter10_constant_lr}{{10.6}{355}{Constant learning rate decay}{figure.caption.664}{}}
\BKM@entry{id=390,dest={73756273656374696F6E2E31302E312E37},srcline={179}}{5C3337365C3337375C303030415C303030645C303030615C303030705C303030745C303030695C303030765C303030655C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030525C303030615C303030745C303030655C3030305C3034305C3030304D5C303030655C303030635C303030685C303030615C3030306E5C303030695C303030735C3030306D5C30303073}
\BKM@entry{id=391,dest={73756273656374696F6E2E31302E312E38},srcline={192}}{5C3337365C3337375C303030455C303030615C303030725C3030306C5C303030795C3030305C3034305C303030535C303030745C3030306F5C303030705C303030705C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.7}Adaptive Learning Rate Mechanisms}{356}{subsection.10.1.7}\protected@file@percent }
\newlabel{subsec:adaptive_lr}{{10.1.7}{356}{Adaptive Learning Rate Mechanisms}{subsection.10.1.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.8}Early Stopping}{356}{subsection.10.1.8}\protected@file@percent }
\newlabel{subsec:early_stopping}{{10.1.8}{356}{Early Stopping}{subsection.10.1.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.7}{\ignorespaces Early stopping mechanism: monitoring loss and accuracy trends to determine the best checkpoint.}}{356}{figure.caption.665}\protected@file@percent }
\newlabel{fig:chapter10_early_stopping}{{10.7}{356}{Early stopping mechanism: monitoring loss and accuracy trends to determine the best checkpoint}{figure.caption.665}{}}
\BKM@entry{id=392,dest={73656374696F6E2A2E363636},srcline={213}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030305C3030302E5C303030315C3030302E5C303030395C3030303A5C3030305C3034305C303030535C303030755C303030705C303030655C303030725C3030302D5C303030435C3030306F5C3030306E5C303030765C303030655C303030725C303030675C303030655C3030306E5C303030635C303030655C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304F5C3030306E5C303030655C303030435C303030795C303030635C3030306C5C30303065}
\abx@aux@cite{0}{smith2017_clr}
\abx@aux@segm{0}{0}{smith2017_clr}
\abx@aux@cite{0}{smith2018_superconvergence}
\abx@aux@segm{0}{0}{smith2018_superconvergence}
\abx@aux@cite{0}{smith2017_clr}
\abx@aux@segm{0}{0}{smith2017_clr}
\abx@aux@cite{0}{smith2018_superconvergence}
\abx@aux@segm{0}{0}{smith2018_superconvergence}
\@writefile{toc}{\contentsline {subsection}{Enrichment 10.1.9: Super-Convergence and OneCycle}{357}{section*.666}\protected@file@percent }
\newlabel{enr:chapter10_superconvergence}{{10.1.9}{357}{\color {ocre}Enrichment \thesubsection : Super-Convergence and OneCycle}{section*.666}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation}{357}{section*.667}\protected@file@percent }
\abx@aux@backref{226}{smith2017_clr}{0}{357}{357}
\abx@aux@backref{227}{smith2018_superconvergence}{0}{357}{357}
\@writefile{toc}{\contentsline {subsubsection}{Method: schedule and inverse momentum coupling}{357}{section*.668}\protected@file@percent }
\newlabel{enr:subsubsec_chapter10_onecycle_method}{{10.1.9}{357}{Method: schedule and inverse momentum coupling}{section*.668}{}}
\@writefile{toc}{\contentsline {paragraph}{What we are scheduling (symbols at a glance)}{357}{section*.669}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Notation and helper ramps (defined before use)}{357}{section*.670}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why this shape? (intuition)}{357}{section*.671}\protected@file@percent }
\abx@aux@backref{228}{smith2017_clr}{0}{357}{357}
\abx@aux@backref{229}{smith2018_superconvergence}{0}{357}{357}
\abx@aux@cite{0}{loshchilov2019_adamw}
\abx@aux@segm{0}{0}{loshchilov2019_adamw}
\abx@aux@cite{0}{smith2017_clr}
\abx@aux@segm{0}{0}{smith2017_clr}
\@writefile{toc}{\contentsline {paragraph}{Step-by-step construction of the schedules}{358}{section*.672}\protected@file@percent }
\newlabel{eq:chapter10_onecycle_lr}{{10.5}{358}{Step-by-step construction of the schedules}{equation.10.5}{}}
\newlabel{eq:chapter10_onecycle_mom}{{10.6}{358}{Step-by-step construction of the schedules}{equation.10.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Parameterization and defaults}{358}{section*.673}\protected@file@percent }
\abx@aux@backref{230}{loshchilov2019_adamw}{0}{358}{358}
\@writefile{toc}{\contentsline {paragraph}{What each hyper-parameter controls}{358}{section*.674}\protected@file@percent }
\abx@aux@cite{0}{smith2018_superconvergence}
\abx@aux@segm{0}{0}{smith2018_superconvergence}
\abx@aux@cite{0}{smith2018_superconvergence}
\abx@aux@segm{0}{0}{smith2018_superconvergence}
\abx@aux@cite{0}{smith2018_superconvergence}
\abx@aux@segm{0}{0}{smith2018_superconvergence}
\abx@aux@cite{0}{smith2018_superconvergence}
\abx@aux@segm{0}{0}{smith2018_superconvergence}
\@writefile{toc}{\contentsline {subsubsection}{Diagnostics: the LR range test}{359}{section*.675}\protected@file@percent }
\newlabel{enr:subsubsec_chapter10_onecycle_diagnostics}{{10.1.9}{359}{Diagnostics: the LR range test}{section*.675}{}}
\abx@aux@backref{231}{smith2017_clr}{0}{359}{359}
\@writefile{lof}{\contentsline {figure}{\numberline {10.8}{\ignorespaces LR range test diagnostics: (a) typical curve with a clear peak indicating $\eta _{\text  {max}}$ and a shoulder for $\eta _{\text  {min}}$ (b) ResNet-56 on CIFAR-10 shows a noisier curve that still reveals a practical LR band. Source: ~\blx@tocontentsinit {0}\cite {smith2018_superconvergence}.}}{359}{figure.caption.676}\protected@file@percent }
\abx@aux@backref{233}{smith2018_superconvergence}{0}{359}{359}
\newlabel{fig:chapter10_superconv_lr_range}{{10.8}{359}{LR range test diagnostics: (a) typical curve with a clear peak indicating $\eta _{\text {max}}$ and a shoulder for $\eta _{\text {min}}$ (b) ResNet-56 on CIFAR-10 shows a noisier curve that still reveals a practical LR band. Source: ~\cite {smith2018_superconvergence}}{figure.caption.676}{}}
\@writefile{toc}{\contentsline {subsubsection}{Empirical picture: CIFAR-10 and ImageNet}{359}{section*.677}\protected@file@percent }
\newlabel{enr:subsubsec_chapter10_onecycle_empirics}{{10.1.9}{359}{Empirical picture: CIFAR-10 and ImageNet}{section*.677}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.9}{\ignorespaces Super-convergence on CIFAR-10 with ResNet-56: (a) test accuracy of OneCycle versus a piecewise-constant schedule (b) test accuracy across different cycle lengths (stepsizes). Source: ~\blx@tocontentsinit {0}\cite {smith2018_superconvergence}.}}{359}{figure.caption.678}\protected@file@percent }
\abx@aux@backref{235}{smith2018_superconvergence}{0}{359}{359}
\newlabel{fig:chapter10_superconv_examples}{{10.9}{359}{Super-convergence on CIFAR-10 with ResNet-56: (a) test accuracy of OneCycle versus a piecewise-constant schedule (b) test accuracy across different cycle lengths (stepsizes). Source: ~\cite {smith2018_superconvergence}}{figure.caption.678}{}}
\abx@aux@cite{0}{smith2018_superconvergence}
\abx@aux@segm{0}{0}{smith2018_superconvergence}
\abx@aux@cite{0}{smith2018_superconvergence}
\abx@aux@segm{0}{0}{smith2018_superconvergence}
\@writefile{lof}{\contentsline {figure}{\numberline {10.10}{\ignorespaces Scaling to ImageNet: (a) ResNet-50 (b) Inception-ResNet-v2 trained with a standard LR policy (blue) versus a 1cycle policy that exhibits super-convergence, reducing epochs substantially. Source: ~\blx@tocontentsinit {0}\cite {smith2018_superconvergence}.}}{360}{figure.caption.679}\protected@file@percent }
\abx@aux@backref{237}{smith2018_superconvergence}{0}{360}{360}
\newlabel{fig:chapter10_superconv_imagenet}{{10.10}{360}{Scaling to ImageNet: (a) ResNet-50 (b) Inception-ResNet-v2 trained with a standard LR policy (blue) versus a 1cycle policy that exhibits super-convergence, reducing epochs substantially. Source: ~\cite {smith2018_superconvergence}}{figure.caption.679}{}}
\@writefile{toc}{\contentsline {subsubsection}{Intuition and comparisons}{360}{section*.680}\protected@file@percent }
\newlabel{enr:subsubsec_chapter10_onecycle_intuition}{{10.1.9}{360}{Intuition and comparisons}{section*.680}{}}
\@writefile{toc}{\contentsline {subsubsection}{When to use—and caveats}{361}{section*.681}\protected@file@percent }
\newlabel{enr:subsubsec_chapter10_onecycle_when}{{10.1.9}{361}{When to use—and caveats}{section*.681}{}}
\@writefile{toc}{\contentsline {paragraph}{Practical tuning guide}{362}{section*.682}\protected@file@percent }
\BKM@entry{id=393,dest={73656374696F6E2E31302E32},srcline={404}}{5C3337365C3337375C303030485C303030795C303030705C303030655C303030725C303030705C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C3030305C3034305C303030535C303030655C3030306C5C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=394,dest={73756273656374696F6E2E31302E322E31},srcline={409}}{5C3337365C3337375C303030475C303030725C303030695C303030645C3030305C3034305C303030535C303030655C303030615C303030725C303030635C30303068}
\BKM@entry{id=395,dest={73756273656374696F6E2E31302E322E32},srcline={428}}{5C3337365C3337375C303030525C303030615C3030306E5C303030645C3030306F5C3030306D5C3030305C3034305C303030535C303030655C303030615C303030725C303030635C30303068}
\abx@aux@cite{0}{bergstra2012_randomsearch}
\abx@aux@segm{0}{0}{bergstra2012_randomsearch}
\@writefile{toc}{\contentsline {section}{\numberline {10.2}Hyperparameter Selection}{363}{section.10.2}\protected@file@percent }
\newlabel{sec:hyperparameter_selection}{{10.2}{363}{Hyperparameter Selection}{section.10.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.1}Grid Search}{363}{subsection.10.2.1}\protected@file@percent }
\newlabel{subsec:grid_search}{{10.2.1}{363}{Grid Search}{subsection.10.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.11}{\ignorespaces Grid search mechanism for hyperparameter tuning.}}{363}{figure.caption.683}\protected@file@percent }
\newlabel{fig:chapter10_grid_search}{{10.11}{363}{Grid search mechanism for hyperparameter tuning}{figure.caption.683}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.2}Random Search}{363}{subsection.10.2.2}\protected@file@percent }
\newlabel{subsec:random_search}{{10.2.2}{363}{Random Search}{subsection.10.2.2}{}}
\abx@aux@backref{238}{bergstra2012_randomsearch}{0}{363}{363}
\BKM@entry{id=396,dest={73756273656374696F6E2E31302E322E33},srcline={458}}{5C3337365C3337375C303030535C303030745C303030655C303030705C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030485C303030795C303030705C303030655C303030725C303030705C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C3030305C3034305C303030545C303030755C3030306E5C303030695C3030306E5C30303067}
\@writefile{lof}{\contentsline {figure}{\numberline {10.12}{\ignorespaces Comparison of grid search and random search strategies. The green distribution over the horizontal axis represents the model performance based on the values of the important hyperparameter, while the orange distribution over the vertical axis represents the performance of the model based on the values of the unimportant one. As we can see, the yellow distribution is rather flat, while the green one has a clear pick, corresponding to a parameter value that maximizes the model's performance. Random search provides better coverage of the important hyperparameters (as it allows us to sample more values for each parameter in a fixed number of tries), and with it we manage to sample the distribution near the pick, as we would like.}}{364}{figure.caption.684}\protected@file@percent }
\newlabel{fig:chapter10_random_vs_grid}{{10.12}{364}{Comparison of grid search and random search strategies. The green distribution over the horizontal axis represents the model performance based on the values of the important hyperparameter, while the orange distribution over the vertical axis represents the performance of the model based on the values of the unimportant one. As we can see, the yellow distribution is rather flat, while the green one has a clear pick, corresponding to a parameter value that maximizes the model's performance. Random search provides better coverage of the important hyperparameters (as it allows us to sample more values for each parameter in a fixed number of tries), and with it we manage to sample the distribution near the pick, as we would like}{figure.caption.684}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.3}Steps for Hyperparameter Tuning}{364}{subsection.10.2.3}\protected@file@percent }
\newlabel{subsec:steps_hyperparam_tuning}{{10.2.3}{364}{Steps for Hyperparameter Tuning}{subsection.10.2.3}{}}
\BKM@entry{id=397,dest={73756273656374696F6E2E31302E322E34},srcline={527}}{5C3337365C3337375C303030495C3030306E5C303030745C303030655C303030725C303030705C303030725C303030655C303030745C303030695C3030306E5C303030675C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030435C303030755C303030725C303030765C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.4}Interpreting Learning Curves}{366}{subsection.10.2.4}\protected@file@percent }
\newlabel{subsec:learning_curves}{{10.2.4}{366}{Interpreting Learning Curves}{subsection.10.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.13}{\ignorespaces A learning curve that is very flat at the beginning and then drops sharply, indicating poor initialization.}}{366}{figure.caption.685}\protected@file@percent }
\newlabel{fig:chapter10_bad_init}{{10.13}{366}{A learning curve that is very flat at the beginning and then drops sharply, indicating poor initialization}{figure.caption.685}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.14}{\ignorespaces Learning curve plateauing, indicating the need for learning rate decay or weight decay tuning.}}{367}{figure.caption.686}\protected@file@percent }
\newlabel{fig:chapter10_plateau}{{10.14}{367}{Learning curve plateauing, indicating the need for learning rate decay or weight decay tuning}{figure.caption.686}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.15}{\ignorespaces Step decay applied too early, leading to stagnation. Adjusting the decay timing may help.}}{367}{figure.caption.687}\protected@file@percent }
\newlabel{fig:chapter10_step_decay}{{10.15}{367}{Step decay applied too early, leading to stagnation. Adjusting the decay timing may help}{figure.caption.687}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.16}{\ignorespaces Accuracy still increasing, suggesting longer training is needed.}}{368}{figure.caption.688}\protected@file@percent }
\newlabel{fig:chapter10_longer_training}{{10.16}{368}{Accuracy still increasing, suggesting longer training is needed}{figure.caption.688}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.17}{\ignorespaces Train-validation accuracy gap, indicating overfitting. Regularization techniques may help.}}{368}{figure.caption.689}\protected@file@percent }
\newlabel{fig:chapter10_overfitting}{{10.17}{368}{Train-validation accuracy gap, indicating overfitting. Regularization techniques may help}{figure.caption.689}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.18}{\ignorespaces Underfitting: train and validation accuracy increasing together but at a low level. Model capacity should be increased.}}{369}{figure.caption.690}\protected@file@percent }
\newlabel{fig:chapter10_underfitting}{{10.18}{369}{Underfitting: train and validation accuracy increasing together but at a low level. Model capacity should be increased}{figure.caption.690}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.19}{\ignorespaces Monitoring weight update to weight magnitude ratio, an important stability metric during training.}}{369}{figure.caption.691}\protected@file@percent }
\newlabel{fig:chapter10_weight_update_ratio}{{10.19}{369}{Monitoring weight update to weight magnitude ratio, an important stability metric during training}{figure.caption.691}{}}
\BKM@entry{id=398,dest={73756273656374696F6E2E31302E322E35},srcline={618}}{5C3337365C3337375C3030304D5C3030306F5C303030645C303030655C3030306C5C3030305C3034305C303030455C3030306E5C303030735C303030655C3030306D5C303030625C3030306C5C303030655C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030415C303030765C303030655C303030725C303030615C303030675C303030695C3030306E5C303030675C3030305C3034305C303030545C303030655C303030635C303030685C3030306E5C303030695C303030715C303030755C303030655C30303073}
\BKM@entry{id=399,dest={73756273656374696F6E2E31302E322E36},srcline={638}}{5C3337365C3337375C303030455C303030785C303030705C3030306F5C3030306E5C303030655C3030306E5C303030745C303030695C303030615C3030306C5C3030305C3034305C3030304D5C3030306F5C303030765C303030695C3030306E5C303030675C3030305C3034305C303030415C303030765C303030655C303030725C303030615C303030675C303030655C3030305C3034305C3030305C3035305C303030455C3030304D5C303030415C3030305C3035315C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030505C3030306F5C3030306C5C303030795C303030615C3030306B5C3030305C3034305C303030415C303030765C303030655C303030725C303030615C303030675C303030695C3030306E5C30303067}
\abx@aux@cite{0}{polyak1992_averagegradient}
\abx@aux@segm{0}{0}{polyak1992_averagegradient}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.5}Model Ensembles and Averaging Techniques}{370}{subsection.10.2.5}\protected@file@percent }
\newlabel{subsec:model_ensembles}{{10.2.5}{370}{Model Ensembles and Averaging Techniques}{subsection.10.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.20}{\ignorespaces Visualization of model ensemble using different checkpoints from a single model trained with a cyclic learning rate schedule.}}{370}{figure.caption.692}\protected@file@percent }
\newlabel{fig:chapter10_ensemble_checkpoints}{{10.20}{370}{Visualization of model ensemble using different checkpoints from a single model trained with a cyclic learning rate schedule}{figure.caption.692}{}}
\abx@aux@cite{0}{ioffe2015_batchnorm}
\abx@aux@segm{0}{0}{ioffe2015_batchnorm}
\BKM@entry{id=400,dest={73656374696F6E2E31302E33},srcline={656}}{5C3337365C3337375C303030545C303030725C303030615C3030306E5C303030735C303030665C303030655C303030725C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.6}Exponential Moving Average (EMA) and Polyak Averaging}{371}{subsection.10.2.6}\protected@file@percent }
\newlabel{subsec:polyak_averaging}{{10.2.6}{371}{Exponential Moving Average (EMA) and Polyak Averaging}{subsection.10.2.6}{}}
\abx@aux@backref{239}{polyak1992_averagegradient}{0}{371}{371}
\abx@aux@backref{240}{ioffe2015_batchnorm}{0}{371}{371}
\@writefile{lof}{\contentsline {figure}{\numberline {10.21}{\ignorespaces Visualization of Polyak averaging, showing how model weights are updated via an exponential moving average to reduce noise and stabilize training.}}{371}{figure.caption.693}\protected@file@percent }
\newlabel{fig:chapter10_polyak_averaging}{{10.21}{371}{Visualization of Polyak averaging, showing how model weights are updated via an exponential moving average to reduce noise and stabilize training}{figure.caption.693}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.3}Transfer Learning}{371}{section.10.3}\protected@file@percent }
\newlabel{subsec:transfer_learning}{{10.3}{371}{Transfer Learning}{section.10.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.22}{\ignorespaces Visualization of the transfer learning process and its impact on the Caltech-101 dataset, which is relatively small compared to ImageNet.}}{372}{figure.caption.694}\protected@file@percent }
\newlabel{fig:chapter10_transfer_learning_caltech}{{10.22}{372}{Visualization of the transfer learning process and its impact on the Caltech-101 dataset, which is relatively small compared to ImageNet}{figure.caption.694}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.23}{\ignorespaces Transfer learning outperforms dataset-specific solutions across multiple classification tasks (objects, scenes, birds, flowers, human attributes, etc.).}}{372}{figure.caption.695}\protected@file@percent }
\newlabel{fig:chapter10_transfer_learning_tasks}{{10.23}{372}{Transfer learning outperforms dataset-specific solutions across multiple classification tasks (objects, scenes, birds, flowers, human attributes, etc.)}{figure.caption.695}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.24}{\ignorespaces Transfer learning applied to image retrieval tasks, surpassing tailored solutions for tasks like Paris Buildings, Oxford Buildings, and Sculpture retrieval.}}{373}{figure.caption.696}\protected@file@percent }
\newlabel{fig:chapter10_transfer_learning_retrieval}{{10.24}{373}{Transfer learning applied to image retrieval tasks, surpassing tailored solutions for tasks like Paris Buildings, Oxford Buildings, and Sculpture retrieval}{figure.caption.696}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.25}{\ignorespaces Fine-tuning a pretrained model: freezing early layers, training a classifier, then gradually unfreezing later layers while lowering the learning rate.}}{373}{figure.caption.697}\protected@file@percent }
\newlabel{fig:chapter10_transfer_learning_finetuning}{{10.25}{373}{Fine-tuning a pretrained model: freezing early layers, training a classifier, then gradually unfreezing later layers while lowering the learning rate}{figure.caption.697}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.26}{\ignorespaces Fine-tuning provides significant performance improvements on multiple object detection tasks (VOC 2007 and ILSVRC 2013).}}{374}{figure.caption.698}\protected@file@percent }
\newlabel{fig:chapter10_transfer_learning_boost}{{10.26}{374}{Fine-tuning provides significant performance improvements on multiple object detection tasks (VOC 2007 and ILSVRC 2013)}{figure.caption.698}{}}
\BKM@entry{id=401,dest={73756273656374696F6E2E31302E332E31},srcline={740}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030745C3030306F5C3030305C3034305C303030505C303030655C303030725C303030665C3030306F5C303030725C3030306D5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C303030655C303030725C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030435C3030304E5C3030304E5C303030735C3030303F}
\@writefile{lof}{\contentsline {figure}{\numberline {10.27}{\ignorespaces Advancements in ImageNet models over the years have led to significant improvements in object detection performance on COCO.}}{375}{figure.caption.699}\protected@file@percent }
\newlabel{fig:chapter10_transfer_learning_coco}{{10.27}{375}{Advancements in ImageNet models over the years have led to significant improvements in object detection performance on COCO}{figure.caption.699}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.1}How to Perform Transfer Learning with CNNs?}{375}{subsection.10.3.1}\protected@file@percent }
\newlabel{subsec:how_to_transfer_learning}{{10.3.1}{375}{How to Perform Transfer Learning with CNNs?}{subsection.10.3.1}{}}
\BKM@entry{id=402,dest={73756273656374696F6E2E31302E332E32},srcline={764}}{5C3337365C3337375C303030545C303030725C303030615C3030306E5C303030735C303030665C303030655C303030725C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030425C303030655C303030795C3030306F5C3030306E5C303030645C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{zhou2019_unifiedvqa}
\abx@aux@segm{0}{0}{zhou2019_unifiedvqa}
\abx@aux@cite{0}{zhou2019_unifiedvqa}
\abx@aux@segm{0}{0}{zhou2019_unifiedvqa}
\abx@aux@cite{0}{zhou2019_unifiedvqa}
\abx@aux@segm{0}{0}{zhou2019_unifiedvqa}
\@writefile{lof}{\contentsline {figure}{\numberline {10.28}{\ignorespaces Guidelines for performing transfer learning based on dataset size and similarity to ImageNet.}}{376}{figure.caption.700}\protected@file@percent }
\newlabel{fig:chapter10_transfer_learning_table}{{10.28}{376}{Guidelines for performing transfer learning based on dataset size and similarity to ImageNet}{figure.caption.700}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.2}Transfer Learning Beyond Classification}{376}{subsection.10.3.2}\protected@file@percent }
\newlabel{subsec:transfer_learning_beyond}{{10.3.2}{376}{Transfer Learning Beyond Classification}{subsection.10.3.2}{}}
\abx@aux@backref{241}{zhou2019_unifiedvqa}{0}{376}{376}
\@writefile{lof}{\contentsline {figure}{\numberline {10.29}{\ignorespaces Multi-stage transfer learning applied to vision-language tasks \blx@tocontentsinit {0}\cite {zhou2019_unifiedvqa}.}}{376}{figure.caption.701}\protected@file@percent }
\abx@aux@backref{243}{zhou2019_unifiedvqa}{0}{376}{376}
\newlabel{fig:chapter10_transfer_learning_vqa}{{10.29}{376}{Multi-stage transfer learning applied to vision-language tasks \cite {zhou2019_unifiedvqa}}{figure.caption.701}{}}
\BKM@entry{id=403,dest={73756273656374696F6E2E31302E332E33},srcline={786}}{5C3337365C3337375C303030445C3030306F5C303030655C303030735C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C303030655C303030725C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030415C3030306C5C303030775C303030615C303030795C303030735C3030305C3034305C303030575C303030695C3030306E5C3030303F}
\abx@aux@cite{0}{he2018_rethinkingimagenet}
\abx@aux@segm{0}{0}{he2018_rethinkingimagenet}
\abx@aux@cite{0}{he2018_rethinkingimagenet}
\abx@aux@segm{0}{0}{he2018_rethinkingimagenet}
\abx@aux@cite{0}{he2018_rethinkingimagenet}
\abx@aux@segm{0}{0}{he2018_rethinkingimagenet}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.3}Does Transfer Learning Always Win?}{377}{subsection.10.3.3}\protected@file@percent }
\newlabel{subsec:transfer_learning_vs_scratch}{{10.3.3}{377}{Does Transfer Learning Always Win?}{subsection.10.3.3}{}}
\abx@aux@backref{244}{he2018_rethinkingimagenet}{0}{377}{377}
\@writefile{lof}{\contentsline {figure}{\numberline {10.30}{\ignorespaces Comparison of training from scratch (orange) vs. pretraining + fine-tuning (blue) on COCO object detection \blx@tocontentsinit {0}\cite {he2018_rethinkingimagenet}.}}{377}{figure.caption.702}\protected@file@percent }
\abx@aux@backref{246}{he2018_rethinkingimagenet}{0}{377}{377}
\newlabel{fig:chapter10_transfer_learning_vs_scratch}{{10.30}{377}{Comparison of training from scratch (orange) vs. pretraining + fine-tuning (blue) on COCO object detection \cite {he2018_rethinkingimagenet}}{figure.caption.702}{}}
\BKM@entry{id=404,dest={73656374696F6E2A2E373033},srcline={803}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030305C3030302E5C303030335C3030302E5C303030345C3030303A5C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030695C3030306E5C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030455C303030725C303030615C3030305C3034305C3030306F5C303030665C3030305C3034305C303030465C303030695C3030306E5C303030655C303030745C303030755C3030306E5C303030695C3030306E5C30303067}
\abx@aux@cite{0}{howard2018_universal}
\abx@aux@segm{0}{0}{howard2018_universal}
\abx@aux@cite{0}{li2022_understanding}
\abx@aux@segm{0}{0}{li2022_understanding}
\abx@aux@cite{0}{zhang2019_fixup}
\abx@aux@segm{0}{0}{zhang2019_fixup}
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\abx@aux@cite{0}{yun2019_cutmix}
\abx@aux@segm{0}{0}{yun2019_cutmix}
\abx@aux@cite{0}{cubuk2020_randaugment}
\abx@aux@segm{0}{0}{cubuk2020_randaugment}
\@writefile{toc}{\contentsline {subsection}{Enrichment 10.3.4: Regularization in the Era of Finetuning}{378}{section*.703}\protected@file@percent }
\newlabel{enrichment:fine_tuning_regularization}{{10.3.4}{378}{\color {ocre}Enrichment \thesubsection : Regularization in the Era of Finetuning}{section*.703}{}}
\@writefile{toc}{\contentsline {paragraph}{1. Freezing Most of the Backbone}{378}{section*.704}\protected@file@percent }
\abx@aux@backref{247}{howard2018_universal}{0}{378}{378}
\@writefile{toc}{\contentsline {paragraph}{2. Regularizing Small Trainable Heads: Caution With Dropout}{378}{section*.705}\protected@file@percent }
\abx@aux@backref{248}{li2022_understanding}{0}{378}{378}
\@writefile{toc}{\contentsline {paragraph}{3. Training From Scratch on Large Datasets}{378}{section*.706}\protected@file@percent }
\abx@aux@backref{249}{vit2020_transformers}{0}{378}{378}
\abx@aux@backref{250}{zhang2019_fixup}{0}{378}{378}
\@writefile{toc}{\contentsline {paragraph}{4. Implicit and Soft Regularization Prevail}{378}{section*.707}\protected@file@percent }
\abx@aux@backref{251}{yun2019_cutmix}{0}{378}{378}
\abx@aux@backref{252}{cubuk2020_randaugment}{0}{378}{378}
\@writefile{toc}{\contentsline {paragraph}{5. Summary}{378}{section*.708}\protected@file@percent }
\BKM@entry{id=405,dest={636861707465722E3131},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030315C303030315C3030303A5C3030305C3034305C303030435C3030304E5C3030304E5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C303030735C3030305C3034305C303030495C30303049}
\BKM@entry{id=406,dest={73656374696F6E2E31312E31},srcline={10}}{5C3337365C3337375C303030505C3030306F5C303030735C303030745C3030302D5C303030525C303030655C303030735C3030304E5C303030655C303030745C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C30303073}
\@writefile{toc}{\contentsline {chapter}{\numberline {11}Lecture 11: CNN Architectures II}{379}{chapter.11}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@10}}
\ttl@writefile{ptc}{\ttl@starttoc{default@11}}
\pgfsyspdfmark {pgfid28}{0}{52099153}
\pgfsyspdfmark {pgfid27}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {11.1}Post-ResNet Architectures}{379}{section.11.1}\protected@file@percent }
\newlabel{sec:post_resnet}{{11.1}{379}{Post-ResNet Architectures}{section.11.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.1}{\ignorespaces Comparison of ResNet variants and their top-1 accuracy on ImageNet. Improvements beyond ResNet-101 yield diminishing returns relative to the increased computational cost.}}{379}{figure.caption.709}\protected@file@percent }
\newlabel{fig:chapter11_resnet_variants}{{11.1}{379}{Comparison of ResNet variants and their top-1 accuracy on ImageNet. Improvements beyond ResNet-101 yield diminishing returns relative to the increased computational cost}{figure.caption.709}{}}
\BKM@entry{id=407,dest={73656374696F6E2E31312E32},srcline={35}}{5C3337365C3337375C303030475C303030725C3030306F5C303030755C303030705C303030655C303030645C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {11.2}Grouped Convolutions}{380}{section.11.2}\protected@file@percent }
\newlabel{sec:grouped_convs}{{11.2}{380}{Grouped Convolutions}{section.11.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.2}{\ignorespaces Regular convolution: each filter operates on all input channels and produces a single feature map.}}{380}{figure.caption.710}\protected@file@percent }
\newlabel{fig:chapter11_regular_convs}{{11.2}{380}{Regular convolution: each filter operates on all input channels and produces a single feature map}{figure.caption.710}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.3}{\ignorespaces Grouped convolution: input channels are split into groups, where each filter processes only its assigned subset. Example shown for $G=2$.}}{381}{figure.caption.711}\protected@file@percent }
\newlabel{fig:chapter11_grouped_convs}{{11.3}{381}{Grouped convolution: input channels are split into groups, where each filter processes only its assigned subset. Example shown for $G=2$}{figure.caption.711}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.4}{\ignorespaces Each group of filters processes only a subset of the input channels, producing its corresponding output channels.}}{381}{figure.caption.712}\protected@file@percent }
\newlabel{fig:chapter11_grouped_convs_process}{{11.4}{381}{Each group of filters processes only a subset of the input channels, producing its corresponding output channels}{figure.caption.712}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.5}{\ignorespaces The first group creates one output plane (darker blue), using its assigned input channels.}}{382}{figure.caption.713}\protected@file@percent }
\newlabel{fig:chapter11_grouped_convs_step1}{{11.5}{382}{The first group creates one output plane (darker blue), using its assigned input channels}{figure.caption.713}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.6}{\ignorespaces The first group produces another output plane using a different filter.}}{382}{figure.caption.714}\protected@file@percent }
\newlabel{fig:chapter11_grouped_convs_step2}{{11.6}{382}{The first group produces another output plane using a different filter}{figure.caption.714}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.7}{\ignorespaces The second group processes its assigned channels, producing an output plane (darker green).}}{383}{figure.caption.715}\protected@file@percent }
\newlabel{fig:chapter11_grouped_convs_step3}{{11.7}{383}{The second group processes its assigned channels, producing an output plane (darker green)}{figure.caption.715}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.8}{\ignorespaces The second group produces another output channel using a different filter, producing another output plane (darker green).}}{383}{figure.caption.716}\protected@file@percent }
\newlabel{fig:chapter11_grouped_convs_step4}{{11.8}{383}{The second group produces another output channel using a different filter, producing another output plane (darker green)}{figure.caption.716}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.9}{\ignorespaces Grouped convolution example with $G=4$, where each group is assigned a different color.}}{384}{figure.caption.717}\protected@file@percent }
\newlabel{fig:chapter11_grouped_convs_g4}{{11.9}{384}{Grouped convolution example with $G=4$, where each group is assigned a different color}{figure.caption.717}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.10}{\ignorespaces Depthwise convolution as a special case of grouped convolution, where each group corresponds to a single input channel. In this example, we have several filters per Group, as $C_\text  {out}>C_\text  {in}$. More specifically, we have 2 filters in each group, as the output channels are twice the input channels ($C_\text  {out}=2C_\text  {in}$)}}{384}{figure.caption.718}\protected@file@percent }
\newlabel{fig:chapter11_depthwise_convs}{{11.10}{384}{Depthwise convolution as a special case of grouped convolution, where each group corresponds to a single input channel. In this example, we have several filters per Group, as $C_\text {out}>C_\text {in}$. More specifically, we have 2 filters in each group, as the output channels are twice the input channels ($C_\text {out}=2C_\text {in}$)}{figure.caption.718}{}}
\BKM@entry{id=408,dest={73756273656374696F6E2E31312E322E31},srcline={139}}{5C3337365C3337375C303030475C303030725C3030306F5C303030755C303030705C303030655C303030645C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030505C303030795C303030545C3030306F5C303030725C303030635C30303068}
\@writefile{lof}{\contentsline {figure}{\numberline {11.11}{\ignorespaces Summary of grouped convolutions: input splitting, processing by groups, and computational efficiency.}}{385}{figure.caption.719}\protected@file@percent }
\newlabel{fig:chapter11_grouped_convs_summary}{{11.11}{385}{Summary of grouped convolutions: input splitting, processing by groups, and computational efficiency}{figure.caption.719}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2.1}Grouped Convolutions in PyTorch}{385}{subsection.11.2.1}\protected@file@percent }
\newlabel{subsec:grouped_convs_pytorch}{{11.2.1}{385}{Grouped Convolutions in PyTorch}{subsection.11.2.1}{}}
\BKM@entry{id=409,dest={73656374696F6E2E31312E33},srcline={211}}{5C3337365C3337375C303030525C303030655C303030735C3030304E5C303030655C303030585C303030745C3030303A5C3030305C3034305C3030304E5C303030655C303030785C303030745C3030302D5C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030525C303030655C303030735C303030695C303030645C303030755C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\abx@aux@cite{0}{xie2017_aggregated}
\abx@aux@segm{0}{0}{xie2017_aggregated}
\BKM@entry{id=410,dest={73756273656374696F6E2E31312E332E31},srcline={216}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030575C303030685C303030795C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C303030585C303030745C3030303F}
\@writefile{toc}{\contentsline {subsubsection}{Key Observations}{386}{section*.720}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{When to Use Grouped Convolutions?}{386}{section*.721}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11.3}ResNeXt: Next-Generation Residual Networks}{386}{section.11.3}\protected@file@percent }
\newlabel{sec:resnext}{{11.3}{386}{ResNeXt: Next-Generation Residual Networks}{section.11.3}{}}
\abx@aux@backref{253}{xie2017_aggregated}{0}{386}{386}
\BKM@entry{id=411,dest={73756273656374696F6E2E31312E332E32},srcline={221}}{5C3337365C3337375C3030304B5C303030655C303030795C3030305C3034305C303030495C3030306E5C3030306E5C3030306F5C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030415C303030675C303030675C303030725C303030655C303030675C303030615C303030745C303030655C303030645C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.1}Motivation: Why ResNeXt?}{387}{subsection.11.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.2}Key Innovation: Aggregated Transformations}{387}{subsection.11.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.12}{\ignorespaces Comparison of the bottleneck residual block (left) with the ResNeXt block using G parallel pathways (right).}}{387}{figure.caption.722}\protected@file@percent }
\newlabel{fig:chapter11_resnext_block}{{11.12}{387}{Comparison of the bottleneck residual block (left) with the ResNeXt block using G parallel pathways (right)}{figure.caption.722}{}}
\BKM@entry{id=412,dest={73756273656374696F6E2E31312E332E33},srcline={261}}{5C3337365C3337375C303030525C303030655C303030735C3030304E5C303030655C303030585C303030745C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030475C303030725C3030306F5C303030755C303030705C303030655C303030645C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=413,dest={73756273656374696F6E2E31312E332E34},srcline={278}}{5C3337365C3337375C303030415C303030645C303030765C303030615C3030306E5C303030745C303030615C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C303030585C303030745C3030305C3034305C3030304F5C303030765C303030655C303030725C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C30303074}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.3}ResNeXt and Grouped Convolutions}{388}{subsection.11.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.13}{\ignorespaces ResNeXt bottleneck block using grouped convolutions: Conv($1\times 1$, $4C \to Gc$), Conv($3\times 3$, $Gc \to Gc$, groups=$G$), Conv($1\times 1$, $Gc \to 4C$).}}{388}{figure.caption.723}\protected@file@percent }
\newlabel{fig:chapter11_resnext_grouped}{{11.13}{388}{ResNeXt bottleneck block using grouped convolutions: Conv($1\times 1$, $4C \to Gc$), Conv($3\times 3$, $Gc \to Gc$, groups=$G$), Conv($1\times 1$, $Gc \to 4C$)}{figure.caption.723}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.4}Advantages of ResNeXt Over ResNet}{388}{subsection.11.3.4}\protected@file@percent }
\BKM@entry{id=414,dest={73756273656374696F6E2E31312E332E35},srcline={293}}{5C3337365C3337375C303030525C303030655C303030735C3030304E5C303030655C303030585C303030745C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C3030305C3034305C3030304E5C303030615C3030306D5C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306E5C303030765C303030655C3030306E5C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=415,dest={73656374696F6E2E31312E34},srcline={303}}{5C3337365C3337375C303030535C303030715C303030755C303030655C303030655C3030307A5C303030655C3030302D5C303030615C3030306E5C303030645C3030302D5C303030455C303030785C303030635C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030535C303030455C3030304E5C303030655C303030745C3030305C303531}
\abx@aux@cite{0}{hu2018_senet}
\abx@aux@segm{0}{0}{hu2018_senet}
\BKM@entry{id=416,dest={73756273656374696F6E2E31312E342E31},srcline={310}}{5C3337365C3337375C303030535C303030715C303030755C303030655C303030655C3030307A5C303030655C3030302D5C303030615C3030306E5C303030645C3030302D5C303030455C303030785C303030635C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030535C303030455C3030305C3035315C3030305C3034305C303030425C3030306C5C3030306F5C303030635C3030306B}
\@writefile{lof}{\contentsline {figure}{\numberline {11.14}{\ignorespaces Increasing the number of groups while reducing the width of each group enhances performance without increasing computational cost. This improvement is achieved by expanding the number of parallel transformation pathways (without increasing network depth! meaning, without changing the number of ResNeXt blocks). For instance, ResNeXt-101-32x4d outperforms ResNeXt-101-4x24d despite having an equivalent number of FLOPs.}}{389}{figure.caption.724}\protected@file@percent }
\newlabel{fig:chapter11_resnext_performance}{{11.14}{389}{Increasing the number of groups while reducing the width of each group enhances performance without increasing computational cost. This improvement is achieved by expanding the number of parallel transformation pathways (without increasing network depth! meaning, without changing the number of ResNeXt blocks). For instance, ResNeXt-101-32x4d outperforms ResNeXt-101-4x24d despite having an equivalent number of FLOPs}{figure.caption.724}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3.5}ResNeXt Model Naming Convention}{389}{subsection.11.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11.4}Squeeze-and-Excitation Networks (SENet)}{389}{section.11.4}\protected@file@percent }
\newlabel{sec:senet}{{11.4}{389}{Squeeze-and-Excitation Networks (SENet)}{section.11.4}{}}
\abx@aux@backref{254}{hu2018_senet}{0}{389}{389}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.4.1}Squeeze-and-Excitation (SE) Block}{390}{subsection.11.4.1}\protected@file@percent }
\newlabel{subsec:se_block}{{11.4.1}{390}{Squeeze-and-Excitation (SE) Block}{subsection.11.4.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Squeeze: Global Information Embedding}{390}{section*.725}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Excitation: Adaptive Recalibration}{390}{section*.726}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Channel Recalibration}{391}{section*.727}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.15}{\ignorespaces The SE block incorporated into a ResNet bottleneck block. The SE mechanism applies global pooling (squeeze), learns channel-wise scaling factors (excitation), and rescales feature maps accordingly.}}{391}{figure.caption.728}\protected@file@percent }
\newlabel{fig:chapter11_se_block}{{11.15}{391}{The SE block incorporated into a ResNet bottleneck block. The SE mechanism applies global pooling (squeeze), learns channel-wise scaling factors (excitation), and rescales feature maps accordingly}{figure.caption.728}{}}
\@writefile{toc}{\contentsline {subsubsection}{How SE Blocks Enhance ResNet Bottleneck Blocks}{391}{section*.729}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Why Does SE Improve Performance?}{392}{section*.730}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Performance Gains, Scalability, and Integration of SE Blocks}{392}{section*.731}\protected@file@percent }
\newlabel{subsubsec:se_performance_scalability}{{11.4.1}{392}{Performance Gains, Scalability, and Integration of SE Blocks}{section*.731}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.16}{\ignorespaces Performance improvements from integrating SE blocks into ResNet, ResNeXt, Inception, and VGG architectures. Each gains roughly a 1–2\% boost in accuracy without requiring additional changes.}}{392}{figure.caption.732}\protected@file@percent }
\newlabel{fig:chapter11_se_performance}{{11.16}{392}{Performance improvements from integrating SE blocks into ResNet, ResNeXt, Inception, and VGG architectures. Each gains roughly a 1–2\% boost in accuracy without requiring additional changes}{figure.caption.732}{}}
\@writefile{toc}{\contentsline {subsubsection}{Impact on Various Tasks}{392}{section*.733}\protected@file@percent }
\BKM@entry{id=417,dest={73756273656374696F6E2E31312E342E32},srcline={421}}{5C3337365C3337375C303030535C303030455C3030305C3034305C303030425C3030306C5C3030306F5C303030635C3030306B5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030455C3030306E5C303030645C3030305C3034305C3030306F5C303030665C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030304E5C303030655C303030745C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C30303065}
\@writefile{toc}{\contentsline {subsubsection}{Practical Applications and Widespread Adoption}{393}{section*.734}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.4.2}SE Blocks and the End of the ImageNet Classification Challenge}{393}{subsection.11.4.2}\protected@file@percent }
\newlabel{subsec:senet_end_imagenet}{{11.4.2}{393}{SE Blocks and the End of the ImageNet Classification Challenge}{subsection.11.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.17}{\ignorespaces The evolution of top-performing models in the ImageNet classification challenge over the years. SENet achieved the lowest top-5 error rate in 2017, marking the effective end of the challenge.}}{393}{figure.caption.735}\protected@file@percent }
\newlabel{fig:chapter11_imagenet_completion}{{11.17}{393}{The evolution of top-performing models in the ImageNet classification challenge over the years. SENet achieved the lowest top-5 error rate in 2017, marking the effective end of the challenge}{figure.caption.735}{}}
\BKM@entry{id=418,dest={73756273656374696F6E2E31312E342E33},srcline={438}}{5C3337365C3337375C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030535C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030535C303030455C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\abx@aux@cite{0}{woo2018_cbam}
\abx@aux@segm{0}{0}{woo2018_cbam}
\abx@aux@cite{0}{howard2019_mobilenetv3}
\abx@aux@segm{0}{0}{howard2019_mobilenetv3}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.4.3}Challenges and Solutions for SE Networks}{394}{subsection.11.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Challenges of SE Networks}{394}{section*.736}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Solutions to SE Network Challenges}{394}{section*.737}\protected@file@percent }
\abx@aux@backref{255}{woo2018_cbam}{0}{394}{394}
\abx@aux@backref{256}{howard2019_mobilenetv3}{0}{394}{394}
\BKM@entry{id=419,dest={73656374696F6E2E31312E35},srcline={480}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030455C303030645C303030675C303030655C3030305C3034305C303030445C303030655C303030765C303030695C303030635C303030655C30303073}
\@writefile{toc}{\contentsline {subsubsection}{Shifting Research Directions: Efficiency and Mobile Deployability}{395}{section*.738}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{What Comes Next?}{395}{section*.739}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11.5}Efficient Architectures for Edge Devices}{395}{section.11.5}\protected@file@percent }
\newlabel{sec:efficient_edge_devices}{{11.5}{395}{Efficient Architectures for Edge Devices}{section.11.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.18}{\ignorespaces Rather than building the largest, most accurate networks, research in the years following SENet focuses on small, efficient models that optimize the accuracy/complexity trade-off. A superior model family shifts the entire accuracy-versus-complexity curve upward and to the left.}}{395}{figure.caption.740}\protected@file@percent }
\newlabel{fig:chapter11_accuracy_vs_complexity}{{11.18}{395}{Rather than building the largest, most accurate networks, research in the years following SENet focuses on small, efficient models that optimize the accuracy/complexity trade-off. A superior model family shifts the entire accuracy-versus-complexity curve upward and to the left}{figure.caption.740}{}}
\BKM@entry{id=420,dest={73756273656374696F6E2E31312E352E31},srcline={496}}{5C3337365C3337375C3030304D5C3030306F5C303030625C303030695C3030306C5C303030655C3030304E5C303030655C303030745C3030303A5C3030305C3034305C303030445C303030655C303030705C303030745C303030685C303030775C303030695C303030735C303030655C3030305C3034305C303030535C303030655C303030705C303030615C303030725C303030615C303030625C3030306C5C303030655C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.5.1}MobileNet: Depthwise Separable Convolutions}{396}{subsection.11.5.1}\protected@file@percent }
\newlabel{subsec:mobilenet_v1}{{11.5.1}{396}{MobileNet: Depthwise Separable Convolutions}{subsection.11.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.19}{\ignorespaces Standard convolution block (left) vs. Depthwise Separable Convolution (right). The latter significantly reduces computation while maintaining competitive accuracy.}}{396}{figure.caption.741}\protected@file@percent }
\newlabel{fig:chapter11_depthwise_vs_standard}{{11.19}{396}{Standard convolution block (left) vs. Depthwise Separable Convolution (right). The latter significantly reduces computation while maintaining competitive accuracy}{figure.caption.741}{}}
\@writefile{toc}{\contentsline {subsubsection}{Width Multiplier: Thinner Models}{397}{section*.742}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Resolution Multiplier: Reduced Representations}{397}{section*.743}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Computational Cost of Depthwise Separable Convolutions}{397}{section*.744}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary of Multipliers}{398}{section*.745}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{MobileNetV1 vs. Traditional Architectures}{398}{section*.746}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {11.1}{\ignorespaces Comparison of MobileNet-224, GoogLeNet, and VGG-16 on ImageNet. MobileNet significantly reduces computational cost while maintaining competitive accuracy.}}{398}{table.caption.747}\protected@file@percent }
\newlabel{tab:mobilenet_vs_classical}{{11.1}{398}{Comparison of MobileNet-224, GoogLeNet, and VGG-16 on ImageNet. MobileNet significantly reduces computational cost while maintaining competitive accuracy}{table.caption.747}{}}
\@writefile{toc}{\contentsline {subsubsection}{Depthwise Separable vs. Standard Convolutions in MobileNet}{398}{section*.748}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {11.2}{\ignorespaces Comparison of MobileNet with standard convolutions vs. depthwise separable convolutions on ImageNet.}}{398}{table.caption.749}\protected@file@percent }
\newlabel{tab:mobile_depthwise_vs_standard}{{11.2}{398}{Comparison of MobileNet with standard convolutions vs. depthwise separable convolutions on ImageNet}{table.caption.749}{}}
\@writefile{toc}{\contentsline {subsubsection}{Summary and Next Steps}{398}{section*.750}\protected@file@percent }
\newlabel{subsubsec:mobile_to_shufflenet}{{11.5.1}{398}{Summary and Next Steps}{section*.750}{}}
\BKM@entry{id=421,dest={73756273656374696F6E2E31312E352E32},srcline={638}}{5C3337365C3337375C303030535C303030685C303030755C303030665C303030665C3030306C5C303030655C3030304E5C303030655C303030745C3030303A5C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C303030435C303030685C303030615C3030306E5C3030306E5C303030655C3030306C5C3030305C3034305C3030304D5C303030695C303030785C303030695C3030306E5C303030675C3030305C3034305C303030765C303030695C303030615C3030305C3034305C303030475C303030725C3030306F5C303030755C303030705C303030655C303030645C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\abx@aux@cite{0}{zhang2018_shufflenet}
\abx@aux@segm{0}{0}{zhang2018_shufflenet}
\@writefile{lof}{\contentsline {figure}{\numberline {11.20}{\ignorespaces Problem: Grouped convolutions do not mix information across groups. Each output channel depends only on its corresponding input group, limiting feature learning.}}{399}{figure.caption.751}\protected@file@percent }
\newlabel{fig:chapter11_grouped_conv_problem}{{11.20}{399}{Problem: Grouped convolutions do not mix information across groups. Each output channel depends only on its corresponding input group, limiting feature learning}{figure.caption.751}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.5.2}ShuffleNet: Efficient Channel Mixing via Grouped Convolutions}{399}{subsection.11.5.2}\protected@file@percent }
\newlabel{subsec:shufflenet}{{11.5.2}{399}{ShuffleNet: Efficient Channel Mixing via Grouped Convolutions}{subsection.11.5.2}{}}
\abx@aux@backref{257}{zhang2018_shufflenet}{0}{399}{399}
\@writefile{lof}{\contentsline {figure}{\numberline {11.21}{\ignorespaces Solution: By introducing channel shuffling after a grouped convolution, ShuffleNet ensures that every output channel incorporates information from different groups.}}{400}{figure.caption.752}\protected@file@percent }
\newlabel{fig:chapter11_channel_shuffle}{{11.21}{400}{Solution: By introducing channel shuffling after a grouped convolution, ShuffleNet ensures that every output channel incorporates information from different groups}{figure.caption.752}{}}
\@writefile{toc}{\contentsline {subsubsection}{The ShuffleNet Unit}{400}{section*.753}\protected@file@percent }
\newlabel{subsubsec:shufflenet_unit}{{11.5.2}{400}{The ShuffleNet Unit}{section*.753}{}}
\@writefile{toc}{\contentsline {paragraph}{Core Design Features}{400}{section*.754}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Structure of a ShuffleNet Unit}{400}{section*.755}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.22}{\ignorespaces ShuffleNet Units: (a) Standard bottleneck unit with depthwise convolution (DWConv), (b) ShuffleNet unit incorporating pointwise group convolution (GConv) and channel shuffle, (c) ShuffleNet unit with stride = 2, where element-wise addition is replaced with channel concatenation.}}{401}{figure.caption.756}\protected@file@percent }
\newlabel{fig:chapter11_shufflenet_block}{{11.22}{401}{ShuffleNet Units: (a) Standard bottleneck unit with depthwise convolution (DWConv), (b) ShuffleNet unit incorporating pointwise group convolution (GConv) and channel shuffle, (c) ShuffleNet unit with stride = 2, where element-wise addition is replaced with channel concatenation}{figure.caption.756}{}}
\@writefile{toc}{\contentsline {paragraph}{Stride-2 Modification}{401}{section*.757}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{ShuffleNet Architecture}{401}{section*.758}\protected@file@percent }
\newlabel{subsubsec:shufflenet_architecture}{{11.5.2}{401}{ShuffleNet Architecture}{section*.758}{}}
\@writefile{toc}{\contentsline {paragraph}{Stage-wise Construction:}{401}{section*.759}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Scaling Factor}{402}{section*.760}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Design Rationale}{402}{section*.761}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Computational Efficiency of ShuffleNet}{402}{section*.762}\protected@file@percent }
\newlabel{subsubsec:shufflenet_efficiency}{{11.5.2}{402}{Computational Efficiency of ShuffleNet}{section*.762}{}}
\@writefile{toc}{\contentsline {subsubsection}{Inference Speed and Practical Performance}{402}{section*.763}\protected@file@percent }
\newlabel{subsubsec:shufflenet_inference}{{11.5.2}{402}{Inference Speed and Practical Performance}{section*.763}{}}
\BKM@entry{id=422,dest={73756273656374696F6E2E31312E352E33},srcline={767}}{5C3337365C3337375C3030304D5C3030306F5C303030625C303030695C3030306C5C303030655C3030304E5C303030655C303030745C303030565C303030325C3030303A5C3030305C3034305C303030495C3030306E5C303030765C303030655C303030725C303030745C303030655C303030645C3030305C3034305C303030425C3030306F5C303030745C303030745C3030306C5C303030655C3030306E5C303030655C303030635C3030306B5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030525C303030655C303030735C303030695C303030645C303030755C303030615C3030306C}
\abx@aux@cite{0}{sandler2018_mobilenetv2}
\abx@aux@segm{0}{0}{sandler2018_mobilenetv2}
\@writefile{toc}{\contentsline {subsubsection}{Performance Comparison: ShuffleNet vs. MobileNet}{403}{section*.764}\protected@file@percent }
\newlabel{subsubsec:shufflenet_vs_mobilenet}{{11.5.2}{403}{Performance Comparison: ShuffleNet vs. MobileNet}{section*.764}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11.3}{\ignorespaces ShuffleNet 1× outperforms MobileNetV1 on ImageNet by achieving higher accuracy with fewer Multi-Adds, albeit with a slightly higher parameter count.}}{403}{table.caption.765}\protected@file@percent }
\newlabel{tab:shufflenet_vs_mobilenet}{{11.3}{403}{ShuffleNet 1× outperforms MobileNetV1 on ImageNet by achieving higher accuracy with fewer Multi-Adds, albeit with a slightly higher parameter count}{table.caption.765}{}}
\@writefile{toc}{\contentsline {subsubsection}{Beyond ShuffleNet: Evolution of Efficient CNN Architectures}{403}{section*.766}\protected@file@percent }
\newlabel{subsubsec:efficient_cnn_trends}{{11.5.2}{403}{Beyond ShuffleNet: Evolution of Efficient CNN Architectures}{section*.766}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.5.3}MobileNetV2: Inverted Bottleneck and Linear Residual}{403}{subsection.11.5.3}\protected@file@percent }
\newlabel{subsec:mobilenetv2}{{11.5.3}{403}{MobileNetV2: Inverted Bottleneck and Linear Residual}{subsection.11.5.3}{}}
\abx@aux@backref{258}{sandler2018_mobilenetv2}{0}{403}{403}
\@writefile{toc}{\contentsline {paragraph}{Understanding Feature Representations and Manifolds}{403}{section*.767}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ReLU and Information Collapse}{403}{section*.768}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.23}{\ignorespaces Visualization of ReLU transformations on low-dimensional manifolds embedded in higher-dimensional spaces. For small $n$ (e.g., $n=2$ or $n=3$), ReLU collapses structural information, while for $n \geq 15$, the transformation largely preserves it.}}{404}{figure.caption.769}\protected@file@percent }
\newlabel{fig:chapter11_relu_transformations}{{11.23}{404}{Visualization of ReLU transformations on low-dimensional manifolds embedded in higher-dimensional spaces. For small $n$ (e.g., $n=2$ or $n=3$), ReLU collapses structural information, while for $n \geq 15$, the transformation largely preserves it}{figure.caption.769}{}}
\@writefile{toc}{\contentsline {subsubsection}{The MobileNetV2 Block: Inverted Residuals and Linear Bottleneck}{404}{section*.770}\protected@file@percent }
\newlabel{subsubsec:mobilenetv2_block}{{11.5.3}{404}{The MobileNetV2 Block: Inverted Residuals and Linear Bottleneck}{section*.770}{}}
\@writefile{toc}{\contentsline {paragraph}{Detailed Block Architecture}{404}{section*.771}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.24}{\ignorespaces Comparison of a standard ResNet bottleneck block and the MobileNetV2 inverted block. MobileNetV2 expands the feature space before applying non-linearity and projects back to a narrow representation in the end.}}{405}{figure.caption.772}\protected@file@percent }
\newlabel{fig:chapter11_mobilenetv2_vs_resnet}{{11.24}{405}{Comparison of a standard ResNet bottleneck block and the MobileNetV2 inverted block. MobileNetV2 expands the feature space before applying non-linearity and projects back to a narrow representation in the end}{figure.caption.772}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why is the Inverted Block Fitting to Efficient Networks?}{405}{section*.773}\protected@file@percent }
\newlabel{subsubsec:inverted_block_efficiency}{{11.5.3}{405}{Why is the Inverted Block Fitting to Efficient Networks?}{section*.773}{}}
\@writefile{toc}{\contentsline {paragraph}{1. Depthwise Convolutions Maintain Low Computational Cost}{405}{section*.774}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. Moderate Expansion Factor \((t)\) Balances Efficiency}{405}{section*.775}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3. Comparison to MobileNetV1}{405}{section*.776}\protected@file@percent }
\abx@aux@cite{0}{krishnamoorthi2018_quantizing}
\abx@aux@segm{0}{0}{krishnamoorthi2018_quantizing}
\@writefile{toc}{\contentsline {paragraph}{4. Comparison to ResNet Bottleneck Blocks}{406}{section*.777}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{5. Linear Bottleneck Preserves Subtle Features}{406}{section*.778}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{406}{section*.779}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{ReLU6 and Its Role in Low-Precision Inference}{406}{section*.780}\protected@file@percent }
\newlabel{subsubsec:relu6_mobilenetv2}{{11.5.3}{406}{ReLU6 and Its Role in Low-Precision Inference}{section*.780}{}}
\@writefile{toc}{\contentsline {paragraph}{Practical Observations and Alternatives}{407}{section*.781}\protected@file@percent }
\abx@aux@backref{259}{krishnamoorthi2018_quantizing}{0}{407}{407}
\@writefile{lof}{\contentsline {figure}{\numberline {11.25}{\ignorespaces Visualization of ReLU6, which bounds activations within a maximum value of 6. This was initially intended for quantization but later found to be suboptimal in certain cases.}}{407}{figure.caption.782}\protected@file@percent }
\newlabel{fig:chapter11_relu6_visualization}{{11.25}{407}{Visualization of ReLU6, which bounds activations within a maximum value of 6. This was initially intended for quantization but later found to be suboptimal in certain cases}{figure.caption.782}{}}
\@writefile{toc}{\contentsline {subsubsection}{MobileNetV2 Architecture and Performance}{407}{section*.783}\protected@file@percent }
\newlabel{subsubsec:mobilenetv2_architecture}{{11.5.3}{407}{MobileNetV2 Architecture and Performance}{section*.783}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11.4}{\ignorespaces MobileNetV2 Architecture: Expansion ratios and output channels per block.}}{407}{table.caption.784}\protected@file@percent }
\newlabel{tab:chapter11_mobilenetv2_architecture}{{11.4}{407}{MobileNetV2 Architecture: Expansion ratios and output channels per block}{table.caption.784}{}}
\@writefile{toc}{\contentsline {subsubsection}{Comparison to MobileNetV1, ShuffleNet, and NASNet}{408}{section*.785}\protected@file@percent }
\newlabel{subsubsec:mobilenetv2_comparison}{{11.5.3}{408}{Comparison to MobileNetV1, ShuffleNet, and NASNet}{section*.785}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11.5}{\ignorespaces Performance comparison of MobileNetV2 with other efficient architectures on ImageNet. The last column reports inference time on a Google Pixel 1 CPU using TF-Lite.}}{408}{table.caption.786}\protected@file@percent }
\newlabel{tab:chapter11_mobilenetv2_comparison}{{11.5}{408}{Performance comparison of MobileNetV2 with other efficient architectures on ImageNet. The last column reports inference time on a Google Pixel 1 CPU using TF-Lite}{table.caption.786}{}}
\BKM@entry{id=423,dest={73756273656374696F6E2E31312E352E34},srcline={985}}{5C3337365C3337375C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030535C303030655C303030615C303030725C303030635C303030685C3030305C3034305C3030305C3035305C3030304E5C303030415C303030535C3030305C3035315C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304D5C3030306F5C303030625C303030695C3030306C5C303030655C3030304E5C303030655C303030745C303030565C30303033}
\abx@aux@cite{0}{zoph2017_nas}
\abx@aux@segm{0}{0}{zoph2017_nas}
\abx@aux@cite{0}{zoph2018_learning}
\abx@aux@segm{0}{0}{zoph2018_learning}
\abx@aux@cite{0}{williams1992_simple}
\abx@aux@segm{0}{0}{williams1992_simple}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.5.4}Neural Architecture Search (NAS) and MobileNetV3}{409}{subsection.11.5.4}\protected@file@percent }
\newlabel{subsec:nas_mobilenetv3}{{11.5.4}{409}{Neural Architecture Search (NAS) and MobileNetV3}{subsection.11.5.4}{}}
\abx@aux@backref{260}{zoph2017_nas}{0}{409}{409}
\abx@aux@backref{261}{zoph2018_learning}{0}{409}{409}
\@writefile{toc}{\contentsline {subsubsection}{How NAS Works? Policy Gradient Optimization}{409}{section*.787}\protected@file@percent }
\newlabel{subsubsec:nas_policy_gradient}{{11.5.4}{409}{How NAS Works? Policy Gradient Optimization}{section*.787}{}}
\@writefile{toc}{\contentsline {paragraph}{What is a Policy Gradient?}{409}{section*.788}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Updating the Controller Using Policy Gradients}{409}{section*.789}\protected@file@percent }
\abx@aux@backref{262}{williams1992_simple}{0}{409}{409}
\@writefile{lof}{\contentsline {figure}{\numberline {11.26}{\ignorespaces Neural Architecture Search (NAS) The controller network samples architectures, trains child networks, evaluates them, and updates itself using policy gradient optimization}}{410}{figure.caption.790}\protected@file@percent }
\newlabel{fig:chapter11_nas_process}{{11.26}{410}{Neural Architecture Search (NAS) The controller network samples architectures, trains child networks, evaluates them, and updates itself using policy gradient optimization}{figure.caption.790}{}}
\@writefile{toc}{\contentsline {paragraph}{Searching for Reusable Block Designs}{410}{section*.791}\protected@file@percent }
\abx@aux@cite{0}{howard2019_mobilenetv3}
\abx@aux@segm{0}{0}{howard2019_mobilenetv3}
\abx@aux@cite{0}{howard2019_mobilenetv3}
\abx@aux@segm{0}{0}{howard2019_mobilenetv3}
\@writefile{lof}{\contentsline {figure}{\numberline {11.27}{\ignorespaces Examples of NAS-discovered \textbf  {Normal} and \textbf  {Reduction} cells, which are then stacked to form an overall architecture.}}{411}{figure.caption.792}\protected@file@percent }
\newlabel{fig:chapter11_nas_cells}{{11.27}{411}{Examples of NAS-discovered \textbf {Normal} and \textbf {Reduction} cells, which are then stacked to form an overall architecture}{figure.caption.792}{}}
\@writefile{toc}{\contentsline {subsubsection}{MobileNetV3: NAS-Optimized Mobile Network}{411}{section*.793}\protected@file@percent }
\newlabel{subsubsec:mobilenetv3}{{11.5.4}{411}{MobileNetV3: NAS-Optimized Mobile Network}{section*.793}{}}
\abx@aux@backref{263}{howard2019_mobilenetv3}{0}{411}{411}
\@writefile{toc}{\contentsline {subsubsection}{The MobileNetV3 Block Architecture and Refinements}{411}{section*.794}\protected@file@percent }
\newlabel{subsubsec:mobilenetv3_block}{{11.5.4}{411}{The MobileNetV3 Block Architecture and Refinements}{section*.794}{}}
\abx@aux@backref{264}{howard2019_mobilenetv3}{0}{411}{411}
\@writefile{toc}{\contentsline {paragraph}{Structure of the MobileNetV3 Block}{411}{section*.795}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Differences from Previous MobileNet Blocks}{411}{section*.796}\protected@file@percent }
\abx@aux@cite{0}{howard2019_mobilenetv3}
\abx@aux@segm{0}{0}{howard2019_mobilenetv3}
\abx@aux@cite{0}{howard2019_mobilenetv3}
\abx@aux@segm{0}{0}{howard2019_mobilenetv3}
\abx@aux@cite{0}{howard2017_mobilenets}
\abx@aux@segm{0}{0}{howard2017_mobilenets}
\abx@aux@cite{0}{sandler2018_mobilenetv2}
\abx@aux@segm{0}{0}{sandler2018_mobilenetv2}
\abx@aux@cite{0}{howard2019_mobilenetv3}
\abx@aux@segm{0}{0}{howard2019_mobilenetv3}
\abx@aux@cite{0}{howard2019_mobilenetv3}
\abx@aux@segm{0}{0}{howard2019_mobilenetv3}
\abx@aux@cite{0}{tan2019_mnasnet}
\abx@aux@segm{0}{0}{tan2019_mnasnet}
\abx@aux@cite{0}{cai2019_proxylessnas}
\abx@aux@segm{0}{0}{cai2019_proxylessnas}
\@writefile{toc}{\contentsline {subsubsection}{Why is MobileNetV3 More Efficient?}{412}{section*.797}\protected@file@percent }
\newlabel{subsubsec:mobilenetv3_efficiency}{{11.5.4}{412}{Why is MobileNetV3 More Efficient?}{section*.797}{}}
\@writefile{toc}{\contentsline {paragraph}{Key Optimizations That Improve Efficiency}{412}{section*.798}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Empirical Comparison of MobileNetV3}{412}{section*.799}\protected@file@percent }
\newlabel{par:chapter11_mobilenetv3_empirical}{{11.5.4}{412}{Empirical Comparison of MobileNetV3}{section*.799}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11.6}{\ignorespaces Performance comparison of efficient models on ImageNet. All results are reported at $224 \times 224$ resolution, with latency measured on a single big core of a Google Pixel phone CPU. Adapted from~\blx@tocontentsinit {0}\cite {howard2019_mobilenetv3}.}}{412}{table.caption.800}\protected@file@percent }
\abx@aux@backref{266}{howard2019_mobilenetv3}{0}{412}{412}
\newlabel{tab:chapter11_mobilenetv3_comparison}{{11.6}{412}{Performance comparison of efficient models on ImageNet. All results are reported at $224 \times 224$ resolution, with latency measured on a single big core of a Google Pixel phone CPU. Adapted from~\cite {howard2019_mobilenetv3}}{table.caption.800}{}}
\abx@aux@backref{267}{howard2017_mobilenets}{0}{412}{412}
\abx@aux@backref{268}{sandler2018_mobilenetv2}{0}{412}{412}
\abx@aux@backref{269}{howard2019_mobilenetv3}{0}{412}{412}
\abx@aux@backref{270}{howard2019_mobilenetv3}{0}{412}{412}
\abx@aux@backref{271}{tan2019_mnasnet}{0}{412}{412}
\abx@aux@backref{272}{cai2019_proxylessnas}{0}{412}{412}
\@writefile{lof}{\contentsline {figure}{\numberline {11.28}{\ignorespaces Comparison between MobileNetV2 and MobileNetV3. MobileNetV3 achieves superior performance while maintaining or reducing computational cost.}}{413}{figure.caption.801}\protected@file@percent }
\newlabel{fig:chapter11_mobilenetv3_vs_mobilenetv2}{{11.28}{413}{Comparison between MobileNetV2 and MobileNetV3. MobileNetV3 achieves superior performance while maintaining or reducing computational cost}{figure.caption.801}{}}
\@writefile{toc}{\contentsline {subsubsection}{The Computational Cost of NAS and Its Limitations}{413}{section*.802}\protected@file@percent }
\newlabel{subsubsec:nas_limitations}{{11.5.4}{413}{The Computational Cost of NAS and Its Limitations}{section*.802}{}}
\@writefile{toc}{\contentsline {paragraph}{Why is NAS Expensive?}{413}{section*.803}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.29}{\ignorespaces NAS requires training thousands of models, making it prohibitively expensive.}}{413}{figure.caption.804}\protected@file@percent }
\newlabel{fig:chapter11_nas_cost}{{11.29}{413}{NAS requires training thousands of models, making it prohibitively expensive}{figure.caption.804}{}}
\abx@aux@cite{0}{ma2018_shufflenetv2}
\abx@aux@segm{0}{0}{ma2018_shufflenetv2}
\@writefile{toc}{\contentsline {subsubsection}{ShuffleNetV2 and Practical Design Rules}{414}{section*.805}\protected@file@percent }
\newlabel{subsubsec:shufflenetv2}{{11.5.4}{414}{ShuffleNetV2 and Practical Design Rules}{section*.805}{}}
\@writefile{toc}{\contentsline {paragraph}{Why ShuffleNetV2?}{414}{section*.806}\protected@file@percent }
\abx@aux@backref{273}{ma2018_shufflenetv2}{0}{414}{414}
\@writefile{toc}{\contentsline {paragraph}{Four Key Guidelines for Practical Efficiency}{414}{section*.807}\protected@file@percent }
\abx@aux@cite{0}{tan2019_efficientnet}
\abx@aux@segm{0}{0}{tan2019_efficientnet}
\@writefile{toc}{\contentsline {paragraph}{From ShuffleNetV1 to ShuffleNetV2}{415}{section*.808}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Performance vs.\ MobileNetV3}{415}{section*.809}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The Need for Model Scaling and EfficientNets}{415}{section*.810}\protected@file@percent }
\newlabel{subsubsec:efficientnet_motivation}{{11.5.4}{415}{The Need for Model Scaling and EfficientNets}{section*.810}{}}
\@writefile{toc}{\contentsline {paragraph}{Beyond Hand-Designed and NAS-Optimized Models}{415}{section*.811}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Introducing EfficientNet}{415}{section*.812}\protected@file@percent }
\abx@aux@backref{274}{tan2019_efficientnet}{0}{415}{415}
\BKM@entry{id=424,dest={73656374696F6E2E31312E36},srcline={1240}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C3030305C3034305C303030435C3030306F5C3030306D5C303030705C3030306F5C303030755C3030306E5C303030645C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C3030305C3034305C303030535C303030635C303030615C3030306C5C303030695C3030306E5C30303067}
\BKM@entry{id=425,dest={73756273656374696F6E2E31312E362E31},srcline={1243}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030535C303030685C3030306F5C303030755C3030306C5C303030645C3030305C3034305C303030575C303030655C3030305C3034305C303030535C303030635C303030615C3030306C5C303030655C3030305C3034305C303030615C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C}
\@writefile{toc}{\contentsline {section}{\numberline {11.6}EfficientNet Compound Model Scaling}{416}{section.11.6}\protected@file@percent }
\newlabel{subsec:efficientnet}{{11.6}{416}{EfficientNet Compound Model Scaling}{section.11.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.6.1}How Should We Scale a Model}{416}{subsection.11.6.1}\protected@file@percent }
\newlabel{subsubsec:efficientnet_scaling}{{11.6.1}{416}{How Should We Scale a Model}{subsection.11.6.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.30}{\ignorespaces Different ways to scale a model: width, depth, resolution, or all jointly (compound scaling).}}{416}{figure.caption.813}\protected@file@percent }
\newlabel{fig:chapter11_model_scaling}{{11.30}{416}{Different ways to scale a model: width, depth, resolution, or all jointly (compound scaling)}{figure.caption.813}{}}
\@writefile{toc}{\contentsline {paragraph}{The Problem with Independent Scaling}{416}{section*.814}\protected@file@percent }
\BKM@entry{id=426,dest={73756273656374696F6E2E31312E362E32},srcline={1283}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C3030305C3034305C303030575C3030306F5C303030725C3030306B5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {11.31}{\ignorespaces Scaling only one dimension leads to diminishing returns; scaling all dimensions jointly yields better results.}}{417}{figure.caption.815}\protected@file@percent }
\newlabel{fig:chapter11_scaling_diminishing_returns}{{11.31}{417}{Scaling only one dimension leads to diminishing returns; scaling all dimensions jointly yields better results}{figure.caption.815}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.6.2}How EfficientNet Works}{417}{subsection.11.6.2}\protected@file@percent }
\newlabel{subsubsec:efficientnet_method}{{11.6.2}{417}{How EfficientNet Works}{subsection.11.6.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Step 1: Designing a Baseline Architecture}{417}{section*.816}\protected@file@percent }
\BKM@entry{id=427,dest={73756273656374696F6E2E31312E362E33},srcline={1349}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030695C303030735C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C3030305C3034305C3030304D5C3030306F5C303030725C303030655C3030305C3034305C303030455C303030665C303030665C303030655C303030635C303030745C303030695C303030765C30303065}
\@writefile{toc}{\contentsline {paragraph}{EfficientNet-B0 Architecture}{418}{section*.817}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {11.7}{\ignorespaces EfficientNet-B0 baseline architecture. MBConv blocks are used throughout. These are the MobileNetV2 inverted bottleneck blocks.}}{418}{table.caption.818}\protected@file@percent }
\newlabel{tab:chapter11_efficientnet_b0_arch}{{11.7}{418}{EfficientNet-B0 baseline architecture. MBConv blocks are used throughout. These are the MobileNetV2 inverted bottleneck blocks}{table.caption.818}{}}
\@writefile{toc}{\contentsline {paragraph}{Step 2: Finding Optimal Scaling Factors}{418}{section*.819}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 3: Scaling to Different Model Sizes}{418}{section*.820}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.6.3}Why is EfficientNet More Effective}{418}{subsection.11.6.3}\protected@file@percent }
\newlabel{subsubsec:efficientnet_advantages}{{11.6.3}{418}{Why is EfficientNet More Effective}{subsection.11.6.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Balanced Scaling Improves Efficiency}{418}{section*.821}\protected@file@percent }
\BKM@entry{id=428,dest={73756273656374696F6E2E31312E362E34},srcline={1377}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C30303074}
\@writefile{toc}{\contentsline {paragraph}{Comparison with MobileNetV3}{419}{section*.822}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Comparison with Other Networks}{419}{section*.823}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.32}{\ignorespaces EfficientNet achieves superior accuracy with fewer FLOPs and parameters compared to previous models.}}{419}{figure.caption.824}\protected@file@percent }
\newlabel{fig:chapter11_efficientnet_efficiency}{{11.32}{419}{EfficientNet achieves superior accuracy with fewer FLOPs and parameters compared to previous models}{figure.caption.824}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.6.4}Limitations of EfficientNet}{419}{subsection.11.6.4}\protected@file@percent }
\newlabel{subsubsec:efficientnet_limitations}{{11.6.4}{419}{Limitations of EfficientNet}{subsection.11.6.4}{}}
\BKM@entry{id=429,dest={73656374696F6E2E31312E37},srcline={1403}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C3030302D5C3030304C5C303030695C303030745C303030655C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030455C303030645C303030675C303030655C3030305C3034305C303030445C303030655C303030765C303030695C303030635C303030655C30303073}
\BKM@entry{id=430,dest={73756273656374696F6E2E31312E372E31},srcline={1406}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C3030302D5C3030304C5C303030695C303030745C30303065}
\abx@aux@cite{0}{tensorflow2020_efficientnetlite}
\abx@aux@segm{0}{0}{tensorflow2020_efficientnetlite}
\BKM@entry{id=431,dest={73756273656374696F6E2E31312E372E32},srcline={1413}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C3030302D5C3030304C5C303030695C303030745C303030655C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C30303065}
\BKM@entry{id=432,dest={73756273656374696F6E2E31312E372E33},srcline={1426}}{5C3337365C3337375C303030505C303030655C303030725C303030665C3030306F5C303030725C3030306D5C303030615C3030306E5C303030635C303030655C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C303030735C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304F5C303030745C303030685C303030655C303030725C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\abx@aux@cite{0}{tensorflow2020_efficientnetlite}
\abx@aux@segm{0}{0}{tensorflow2020_efficientnetlite}
\abx@aux@cite{0}{tensorflow2020_efficientnetlite}
\abx@aux@segm{0}{0}{tensorflow2020_efficientnetlite}
\@writefile{toc}{\contentsline {paragraph}{What’s Next? EfficientNetV2 and Beyond}{420}{section*.825}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{420}{section*.826}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11.7}EfficientNet-Lite Optimizing EfficientNet for Edge Devices}{420}{section.11.7}\protected@file@percent }
\newlabel{subsec:efficientnet_lite}{{11.7}{420}{EfficientNet-Lite Optimizing EfficientNet for Edge Devices}{section.11.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.7.1}Motivation for EfficientNet-Lite}{420}{subsection.11.7.1}\protected@file@percent }
\newlabel{subsubsec:efficientnet_lite_motivation}{{11.7.1}{420}{Motivation for EfficientNet-Lite}{subsection.11.7.1}{}}
\abx@aux@backref{275}{tensorflow2020_efficientnetlite}{0}{420}{420}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.7.2}EfficientNet-Lite Architecture}{420}{subsection.11.7.2}\protected@file@percent }
\newlabel{subsubsec:efficientnet_lite_arch}{{11.7.2}{420}{EfficientNet-Lite Architecture}{subsection.11.7.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.7.3}Performance and Comparison with Other Models}{420}{subsection.11.7.3}\protected@file@percent }
\newlabel{subsubsec:efficientnet_lite_comparison}{{11.7.3}{420}{Performance and Comparison with Other Models}{subsection.11.7.3}{}}
\abx@aux@cite{0}{tensorflow2020_efficientnetlite}
\abx@aux@segm{0}{0}{tensorflow2020_efficientnetlite}
\abx@aux@cite{0}{tensorflow2020_efficientnetlite}
\abx@aux@segm{0}{0}{tensorflow2020_efficientnetlite}
\@writefile{lof}{\contentsline {figure}{\numberline {11.33}{\ignorespaces EfficientNet-Lite significantly outperforms MobileNetV2 in accuracy while maintaining competitive inference speed. It also runs much faster than ResNet on edge devices. Image credit TensorFlow Blog \blx@tocontentsinit {0}\cite {tensorflow2020_efficientnetlite}.}}{421}{figure.caption.827}\protected@file@percent }
\abx@aux@backref{277}{tensorflow2020_efficientnetlite}{0}{421}{421}
\newlabel{fig:chapter11_efficientnet_lite_latency_accuracy}{{11.33}{421}{EfficientNet-Lite significantly outperforms MobileNetV2 in accuracy while maintaining competitive inference speed. It also runs much faster than ResNet on edge devices. Image credit TensorFlow Blog \cite {tensorflow2020_efficientnetlite}}{figure.caption.827}{}}
\@writefile{toc}{\contentsline {paragraph}{Model Size vs. Accuracy Trade-off}{421}{section*.828}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.34}{\ignorespaces EfficientNet-Lite achieves better accuracy than MobileNetV2 at similar model sizes and outperforms ResNet in edge inference efficiency. Image credit TensorFlow Blog \blx@tocontentsinit {0}\cite {tensorflow2020_efficientnetlite}.}}{421}{figure.caption.829}\protected@file@percent }
\abx@aux@backref{279}{tensorflow2020_efficientnetlite}{0}{421}{421}
\newlabel{fig:chapter11_efficientnet_lite_model_size_accuracy}{{11.34}{421}{EfficientNet-Lite achieves better accuracy than MobileNetV2 at similar model sizes and outperforms ResNet in edge inference efficiency. Image credit TensorFlow Blog \cite {tensorflow2020_efficientnetlite}}{figure.caption.829}{}}
\BKM@entry{id=433,dest={73656374696F6E2E31312E38},srcline={1449}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C303030565C303030325C3030303A5C3030305C3034305C303030465C303030615C303030735C303030745C303030655C303030725C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030655C303030645C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030635C30303079}
\BKM@entry{id=434,dest={73756273656374696F6E2E31312E382E31},srcline={1452}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C303030565C30303032}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\BKM@entry{id=435,dest={73756273656374696F6E2E31312E382E32},srcline={1469}}{5C3337365C3337375C303030465C303030755C303030735C303030655C303030645C3030302D5C3030304D5C303030425C303030435C3030306F5C3030306E5C303030765C3030303A5C3030305C3034305C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030695C3030306E5C303030675C3030305C3034305C303030455C303030615C303030725C3030306C5C303030795C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {11.8}EfficientNetV2: Faster Training and Improved Efficiency}{422}{section.11.8}\protected@file@percent }
\newlabel{subsec:efficientnetv2}{{11.8}{422}{EfficientNetV2: Faster Training and Improved Efficiency}{section.11.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.8.1}Motivation for EfficientNetV2}{422}{subsection.11.8.1}\protected@file@percent }
\newlabel{subsubsec:efficientnetv2_motivation}{{11.8.1}{422}{Motivation for EfficientNetV2}{subsection.11.8.1}{}}
\abx@aux@backref{280}{tan2021_efficientnetv2}{0}{422}{422}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.8.2}Fused-MBConv: Improving Early Layers}{422}{subsection.11.8.2}\protected@file@percent }
\newlabel{subsubsec:fused_mbconv}{{11.8.2}{422}{Fused-MBConv: Improving Early Layers}{subsection.11.8.2}{}}
\BKM@entry{id=436,dest={73756273656374696F6E2E31312E382E33},srcline={1487}}{5C3337365C3337375C303030505C303030725C3030306F5C303030675C303030725C303030655C303030735C303030735C303030695C303030765C303030655C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030303A5C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030535C3030306D5C303030615C3030306C5C3030306C5C303030655C303030725C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C30303073}
\BKM@entry{id=437,dest={73756273656374696F6E2E31312E382E34},srcline={1502}}{5C3337365C3337375C303030465C303030695C303030785C303030525C303030655C303030735C3030303A5C3030305C3034305C303030415C303030645C303030645C303030725C303030655C303030735C303030735C303030695C3030306E5C303030675C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C3030302D5C303030545C303030655C303030735C303030745C3030305C3034305C303030525C303030655C303030735C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030445C303030695C303030735C303030635C303030725C303030655C303030705C303030615C3030306E5C303030635C30303079}
\abx@aux@cite{0}{touvron2019_fixres}
\abx@aux@segm{0}{0}{touvron2019_fixres}
\@writefile{lof}{\contentsline {figure}{\numberline {11.35}{\ignorespaces Comparison of MBConv (left) and Fused-MBConv (right). Fused-MBConv replaces the separate expansion and depthwise convolution layers with a single $3\times 3$ convolution, improving efficiency in early layers.}}{423}{figure.caption.830}\protected@file@percent }
\newlabel{fig:chapter11_fused_mbconv}{{11.35}{423}{Comparison of MBConv (left) and Fused-MBConv (right). Fused-MBConv replaces the separate expansion and depthwise convolution layers with a single $3\times 3$ convolution, improving efficiency in early layers}{figure.caption.830}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.8.3}Progressive Learning: Efficient Training with Smaller Images}{423}{subsection.11.8.3}\protected@file@percent }
\newlabel{subsubsec:progressive_learning}{{11.8.3}{423}{Progressive Learning: Efficient Training with Smaller Images}{subsection.11.8.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.8.4}FixRes: Addressing Train-Test Resolution Discrepancy}{423}{subsection.11.8.4}\protected@file@percent }
\newlabel{subsubsec:fixres}{{11.8.4}{423}{FixRes: Addressing Train-Test Resolution Discrepancy}{subsection.11.8.4}{}}
\abx@aux@backref{281}{touvron2019_fixres}{0}{423}{423}
\@writefile{toc}{\contentsline {paragraph}{The Problem: Region of Classification (RoC) Mismatch}{423}{section*.831}\protected@file@percent }
\abx@aux@cite{0}{touvron2019_fixres}
\abx@aux@segm{0}{0}{touvron2019_fixres}
\abx@aux@cite{0}{touvron2019_fixres}
\abx@aux@segm{0}{0}{touvron2019_fixres}
\BKM@entry{id=438,dest={73756273656374696F6E2E31312E382E35},srcline={1534}}{5C3337365C3337375C3030304E5C3030306F5C3030306E5C3030302D5C303030555C3030306E5C303030695C303030665C3030306F5C303030725C3030306D5C3030305C3034305C303030535C303030635C303030615C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030655C303030645C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030635C30303079}
\BKM@entry{id=439,dest={73756273656374696F6E2E31312E382E36},srcline={1546}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C303030565C303030325C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C30303065}
\@writefile{toc}{\contentsline {paragraph}{FixRes Solution}{424}{section*.832}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.36}{\ignorespaces FixRes visualization \blx@tocontentsinit {0}\cite {touvron2019_fixres}. The red classification region is resampled as a crop fed to the neural network. Standard augmentations can make objects larger at training time than at test time (second column). FixRes mitigates this by either reducing the train-time resolution or increasing the test-time resolution (third and fourth columns), ensuring objects appear at similar sizes in both phases.}}{424}{figure.caption.833}\protected@file@percent }
\abx@aux@backref{283}{touvron2019_fixres}{0}{424}{424}
\newlabel{fig:chapter11_fixres_visualized}{{11.36}{424}{FixRes visualization \cite {touvron2019_fixres}. The red classification region is resampled as a crop fed to the neural network. Standard augmentations can make objects larger at training time than at test time (second column). FixRes mitigates this by either reducing the train-time resolution or increasing the test-time resolution (third and fourth columns), ensuring objects appear at similar sizes in both phases}{figure.caption.833}{}}
\@writefile{toc}{\contentsline {paragraph}{Implementation in EfficientNetV2}{424}{section*.834}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.8.5}Non-Uniform Scaling for Improved Efficiency}{424}{subsection.11.8.5}\protected@file@percent }
\newlabel{subsubsec:nonuniform_scaling}{{11.8.5}{424}{Non-Uniform Scaling for Improved Efficiency}{subsection.11.8.5}{}}
\BKM@entry{id=440,dest={73756273656374696F6E2E31312E382E37},srcline={1576}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C303030565C303030325C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C303030565C30303031}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.8.6}EfficientNetV2 Architecture}{425}{subsection.11.8.6}\protected@file@percent }
\newlabel{subsubsec:efficientnetv2_architecture}{{11.8.6}{425}{EfficientNetV2 Architecture}{subsection.11.8.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11.8}{\ignorespaces EfficientNetV2-S architecture. Early layers use Fused-MBConv for faster training, while later layers retain MBConv for efficiency.}}{425}{table.caption.835}\protected@file@percent }
\newlabel{tab:chapter11_efficientnetv2_architecture}{{11.8}{425}{EfficientNetV2-S architecture. Early layers use Fused-MBConv for faster training, while later layers retain MBConv for efficiency}{table.caption.835}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.8.7}EfficientNetV2 vs. EfficientNetV1}{425}{subsection.11.8.7}\protected@file@percent }
\newlabel{subsubsec:efficientnetv2_vs_v1}{{11.8.7}{425}{EfficientNetV2 vs. EfficientNetV1}{subsection.11.8.7}{}}
\BKM@entry{id=441,dest={73756273656374696F6E2E31312E382E38},srcline={1587}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C303030565C303030325C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C3030304F5C303030745C303030685C303030655C303030725C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\abx@aux@cite{0}{tan2019_efficientnet}
\abx@aux@segm{0}{0}{tan2019_efficientnet}
\abx@aux@cite{0}{tan2019_efficientnet}
\abx@aux@segm{0}{0}{tan2019_efficientnet}
\abx@aux@cite{0}{tan2019_efficientnet}
\abx@aux@segm{0}{0}{tan2019_efficientnet}
\abx@aux@cite{0}{radosavovic2020_regnet}
\abx@aux@segm{0}{0}{radosavovic2020_regnet}
\abx@aux@cite{0}{radosavovic2020_regnet}
\abx@aux@segm{0}{0}{radosavovic2020_regnet}
\abx@aux@cite{0}{zhang2020_resnest}
\abx@aux@segm{0}{0}{zhang2020_resnest}
\abx@aux@cite{0}{zhang2020_resnest}
\abx@aux@segm{0}{0}{zhang2020_resnest}
\abx@aux@cite{0}{brock2021_nfnet}
\abx@aux@segm{0}{0}{brock2021_nfnet}
\abx@aux@cite{0}{brock2021_nfnet}
\abx@aux@segm{0}{0}{brock2021_nfnet}
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.8.8}EfficientNetV2 vs.\ Other Models}{426}{subsection.11.8.8}\protected@file@percent }
\newlabel{subsec:efficientnetv2_comparison}{{11.8.8}{426}{EfficientNetV2 vs.\ Other Models}{subsection.11.8.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11.9}{\ignorespaces ImageNet performance. Inference measured on V100 GPU (FP16, batch size 16)~\blx@tocontentsinit {0}\cite {tan2021_efficientnetv2}.}}{426}{table.caption.836}\protected@file@percent }
\abx@aux@backref{285}{tan2021_efficientnetv2}{0}{426}{426}
\newlabel{tab:efficientnetv2_comparison}{{11.9}{426}{ImageNet performance. Inference measured on V100 GPU (FP16, batch size 16)~\cite {tan2021_efficientnetv2}}{table.caption.836}{}}
\abx@aux@backref{286}{tan2019_efficientnet}{0}{426}{426}
\abx@aux@backref{287}{tan2019_efficientnet}{0}{426}{426}
\abx@aux@backref{288}{tan2019_efficientnet}{0}{426}{426}
\abx@aux@backref{289}{radosavovic2020_regnet}{0}{426}{426}
\abx@aux@backref{290}{radosavovic2020_regnet}{0}{426}{426}
\abx@aux@backref{291}{zhang2020_resnest}{0}{426}{426}
\abx@aux@backref{292}{zhang2020_resnest}{0}{426}{426}
\abx@aux@backref{293}{brock2021_nfnet}{0}{426}{426}
\abx@aux@backref{294}{brock2021_nfnet}{0}{426}{426}
\abx@aux@backref{295}{touvron2021_deit}{0}{426}{426}
\abx@aux@backref{296}{vit2020_transformers}{0}{426}{426}
\@writefile{toc}{\contentsline {paragraph}{Training Speed and Efficiency}{427}{section*.837}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {11.10}{\ignorespaces Training efficiency comparison of EfficientNetV2 vs. EfficientNetV1. EfficientNetV2-L converges \textbf  {~6× faster} while requiring fewer epochs.}}{427}{table.caption.838}\protected@file@percent }
\newlabel{tab:chapter11_efficientnetv2_training}{{11.10}{427}{Training efficiency comparison of EfficientNetV2 vs. EfficientNetV1. EfficientNetV2-L converges \textbf {~6× faster} while requiring fewer epochs}{table.caption.838}{}}
\@writefile{toc}{\contentsline {paragraph}{Key Takeaways}{427}{section*.839}\protected@file@percent }
\BKM@entry{id=442,dest={73656374696F6E2E31312E39},srcline={1667}}{5C3337365C3337375C3030304E5C303030465C3030304E5C303030655C303030745C303030735C3030303A5C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030655C303030725C3030302D5C303030465C303030725C303030655C303030655C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C303030745C30303073}
\BKM@entry{id=443,dest={73756273656374696F6E2E31312E392E31},srcline={1670}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030575C303030685C303030795C3030305C3034305C303030445C3030306F5C3030305C3034305C303030575C303030655C3030305C3034305C3030304E5C303030655C303030655C303030645C3030305C3034305C3030304E5C303030465C3030304E5C303030655C303030745C303030735C3030303F}
\abx@aux@cite{0}{brock2021_nfnet}
\abx@aux@segm{0}{0}{brock2021_nfnet}
\BKM@entry{id=444,dest={73756273656374696F6E2E31312E392E32},srcline={1690}}{5C3337365C3337375C303030565C303030615C303030725C303030695C303030615C3030306E5C303030635C303030655C3030305C3034305C303030455C303030785C303030705C3030306C5C3030306F5C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030575C303030695C303030745C303030685C3030306F5C303030755C303030745C3030305C3034305C303030425C303030615C303030745C303030635C303030685C3030304E5C3030306F5C303030725C3030306D}
\abx@aux@cite{0}{zhang2019_fixup}
\abx@aux@segm{0}{0}{zhang2019_fixup}
\BKM@entry{id=445,dest={73756273656374696F6E2E31312E392E33},srcline={1707}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C3030304E5C3030306F5C303030745C3030305C3034305C303030525C303030655C303030735C303030635C303030615C3030306C5C303030655C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030525C303030655C303030735C303030695C303030645C303030755C303030615C3030306C5C3030305C3034305C303030425C303030725C303030615C3030306E5C303030635C303030685C3030303F}
\@writefile{toc}{\contentsline {section}{\numberline {11.9}NFNets: Normalizer-Free ResNets}{428}{section.11.9}\protected@file@percent }
\newlabel{subsec:nfnets}{{11.9}{428}{NFNets: Normalizer-Free ResNets}{section.11.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.9.1}Motivation: Why Do We Need NFNets?}{428}{subsection.11.9.1}\protected@file@percent }
\newlabel{subsubsec:nfnets_motivation}{{11.9.1}{428}{Motivation: Why Do We Need NFNets?}{subsection.11.9.1}{}}
\abx@aux@backref{297}{brock2021_nfnet}{0}{428}{428}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.9.2}Variance Explosion Without BatchNorm}{428}{subsection.11.9.2}\protected@file@percent }
\newlabel{subsubsec:nfnets_no_bn_variance}{{11.9.2}{428}{Variance Explosion Without BatchNorm}{subsection.11.9.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Variance Scaling in Residual Networks}{428}{section*.840}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Role of Weight Initialization}{428}{section*.841}\protected@file@percent }
\abx@aux@backref{298}{zhang2019_fixup}{0}{428}{428}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.9.3}Why Not Rescale the Residual Branch?}{428}{subsection.11.9.3}\protected@file@percent }
\newlabel{subsubsec:residual_reparameterization}{{11.9.3}{428}{Why Not Rescale the Residual Branch?}{subsection.11.9.3}{}}
\BKM@entry{id=446,dest={73756273656374696F6E2E31312E392E34},srcline={1732}}{5C3337365C3337375C3030304E5C303030465C3030304E5C303030655C303030745C303030735C3030303A5C3030305C3034305C303030575C303030655C303030695C303030675C303030685C303030745C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030495C3030306E5C303030735C303030745C303030655C303030615C303030645C3030305C3034305C3030306F5C303030665C3030305C3034305C303030425C3030304E}
\BKM@entry{id=447,dest={73756273656374696F6E2E31312E392E35},srcline={1763}}{5C3337365C3337375C3030304E5C303030465C3030304E5C303030655C303030745C303030735C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C303030745C3030302D5C30303044}
\abx@aux@cite{0}{he2018_resnetd}
\abx@aux@segm{0}{0}{he2018_resnetd}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.9.4}NFNets: Weight Normalization Instead of BN}{429}{subsection.11.9.4}\protected@file@percent }
\newlabel{subsubsec:nfnets_weight_normalization}{{11.9.4}{429}{NFNets: Weight Normalization Instead of BN}{subsection.11.9.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Why This Works}{429}{section*.842}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Relation to Earlier Weight Standardization}{429}{section*.843}\protected@file@percent }
\BKM@entry{id=448,dest={73756273656374696F6E2E31312E392E36},srcline={1779}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C303030735C3030306F5C3030306E5C3030305C3034305C303030415C303030635C303030725C3030306F5C303030735C303030735C3030305C3034305C303030445C303030695C303030765C303030655C303030725C303030735C303030655C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C30303073}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\abx@aux@cite{0}{brock2021_nfnet}
\abx@aux@segm{0}{0}{brock2021_nfnet}
\abx@aux@cite{0}{radosavovic2020_regnet}
\abx@aux@segm{0}{0}{radosavovic2020_regnet}
\abx@aux@cite{0}{zhang2020_resnest}
\abx@aux@segm{0}{0}{zhang2020_resnest}
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\abx@aux@cite{0}{he2016_resnet}
\abx@aux@segm{0}{0}{he2016_resnet}
\abx@aux@cite{0}{radosavovic2020_regnet}
\abx@aux@segm{0}{0}{radosavovic2020_regnet}
\abx@aux@cite{0}{brock2021_nfnet}
\abx@aux@segm{0}{0}{brock2021_nfnet}
\abx@aux@cite{0}{brock2021_nfnet}
\abx@aux@segm{0}{0}{brock2021_nfnet}
\abx@aux@cite{0}{tan2019_efficientnet}
\abx@aux@segm{0}{0}{tan2019_efficientnet}
\abx@aux@cite{0}{tan2019_efficientnet}
\abx@aux@segm{0}{0}{tan2019_efficientnet}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\abx@aux@cite{0}{brock2021_nfnet}
\abx@aux@segm{0}{0}{brock2021_nfnet}
\abx@aux@cite{0}{tan2021_efficientnetv2}
\abx@aux@segm{0}{0}{tan2021_efficientnetv2}
\abx@aux@cite{0}{brock2021_nfnet}
\abx@aux@segm{0}{0}{brock2021_nfnet}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.9.5}NFNets Architecture and ResNet-D}{430}{subsection.11.9.5}\protected@file@percent }
\newlabel{subsubsec:nfnets_resnetd}{{11.9.5}{430}{NFNets Architecture and ResNet-D}{subsection.11.9.5}{}}
\abx@aux@backref{299}{he2018_resnetd}{0}{430}{430}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.9.6}Comparison Across Diverse Architectures}{430}{subsection.11.9.6}\protected@file@percent }
\newlabel{subsubsec:nfnets_comparison_more}{{11.9.6}{430}{Comparison Across Diverse Architectures}{subsection.11.9.6}{}}
\abx@aux@backref{300}{brock2021_nfnet}{0}{430}{430}
\abx@aux@backref{301}{vit2020_transformers}{0}{430}{430}
\abx@aux@backref{302}{radosavovic2020_regnet}{0}{430}{430}
\abx@aux@backref{303}{tan2021_efficientnetv2}{0}{430}{430}
\abx@aux@backref{304}{touvron2021_deit}{0}{430}{430}
\abx@aux@backref{305}{zhang2020_resnest}{0}{430}{430}
\abx@aux@backref{306}{he2016_resnet}{0}{430}{430}
\abx@aux@backref{307}{radosavovic2020_regnet}{0}{430}{430}
\abx@aux@backref{308}{brock2021_nfnet}{0}{430}{430}
\abx@aux@backref{309}{brock2021_nfnet}{0}{430}{430}
\abx@aux@backref{310}{tan2019_efficientnet}{0}{430}{430}
\abx@aux@backref{311}{tan2019_efficientnet}{0}{430}{430}
\abx@aux@backref{312}{tan2021_efficientnetv2}{0}{430}{430}
\abx@aux@backref{313}{tan2021_efficientnetv2}{0}{430}{430}
\abx@aux@backref{314}{tan2021_efficientnetv2}{0}{430}{430}
\abx@aux@backref{315}{vit2020_transformers}{0}{430}{430}
\abx@aux@backref{316}{touvron2021_deit}{0}{430}{430}
\abx@aux@backref{317}{touvron2021_deit}{0}{430}{430}
\@writefile{lot}{\contentsline {table}{\numberline {11.11}{\ignorespaces \textbf  {Comparison of NFNets with leading architectures}, including RegNet, EfficientNetV2, and Vision Transformers on ImageNet. \textbf  {All models were pre-trained on ImageNet.} The \textit  {inference time} refers to single-batch inference on an NVIDIA V100 GPU in FP16 precision with batch size 16, as reported in \blx@tocontentsinit {0}\cite {tan2021_efficientnetv2,brock2021_nfnet}. The \textit  {train time} (hrs) assumes a standard TPUv3 configuration (32–64 cores). $^\dagger $ViT-B/16 at 384 input can vary in accuracy (77--79\%) depending on hyperparameters.}}{430}{table.caption.844}\protected@file@percent }
\abx@aux@backref{320}{brock2021_nfnet}{0}{430}{430}
\abx@aux@backref{321}{tan2021_efficientnetv2}{0}{430}{430}
\newlabel{tab:compare_effnetv2_main}{{11.11}{430}{\textbf {Comparison of NFNets with leading architectures}, including RegNet, EfficientNetV2, and Vision Transformers on ImageNet. \textbf {All models were pre-trained on ImageNet.} The \textit {inference time} refers to single-batch inference on an NVIDIA V100 GPU in FP16 precision with batch size 16, as reported in \cite {tan2021_efficientnetv2,brock2021_nfnet}. The \textit {train time} (hrs) assumes a standard TPUv3 configuration (32–64 cores). $^\dagger $ViT-B/16 at 384 input can vary in accuracy (77--79\%) depending on hyperparameters}{table.caption.844}{}}
\@writefile{toc}{\contentsline {paragraph}{Key Takeaways}{430}{section*.845}\protected@file@percent }
\BKM@entry{id=449,dest={73756273656374696F6E2E31312E392E37},srcline={1827}}{5C3337365C3337375C303030465C303030755C303030725C303030745C303030685C303030655C303030725C3030305C3034305C303030525C303030655C303030615C303030645C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030525C303030655C303030735C3030306F5C303030755C303030725C303030635C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.9.7}Further Reading and Resources}{431}{subsection.11.9.7}\protected@file@percent }
\newlabel{subsubsec:nfnets_further_reading}{{11.9.7}{431}{Further Reading and Resources}{subsection.11.9.7}{}}
\BKM@entry{id=450,dest={73656374696F6E2E31312E3130},srcline={1839}}{5C3337365C3337375C303030525C303030655C303030765C303030695C303030735C303030695C303030745C303030695C3030306E5C303030675C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C303030745C303030735C3030303A5C3030305C3034305C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030655C303030645C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030535C303030635C303030615C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030535C303030745C303030725C303030615C303030745C303030655C303030675C303030695C303030655C30303073}
\abx@aux@cite{0}{bello2021_revisitingresnets}
\abx@aux@segm{0}{0}{bello2021_revisitingresnets}
\BKM@entry{id=451,dest={73756273656374696F6E2E31312E31302E31},srcline={1844}}{5C3337365C3337375C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030455C3030306E5C303030685C303030615C3030306E5C303030635C303030655C3030306D5C303030655C3030306E5C303030745C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C303030745C30303073}
\abx@aux@cite{0}{bello2021_revisitingresnets}
\abx@aux@segm{0}{0}{bello2021_revisitingresnets}
\abx@aux@cite{0}{bello2021_revisitingresnets}
\abx@aux@segm{0}{0}{bello2021_revisitingresnets}
\BKM@entry{id=452,dest={73756273656374696F6E2E31312E31302E32},srcline={1883}}{5C3337365C3337375C303030535C303030635C303030615C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C303030745C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {section}{\numberline {11.10}Revisiting ResNets: Improved Training and Scaling Strategies}{432}{section.11.10}\protected@file@percent }
\newlabel{subsec:revisiting_resnets}{{11.10}{432}{Revisiting ResNets: Improved Training and Scaling Strategies}{section.11.10}{}}
\abx@aux@backref{322}{bello2021_revisitingresnets}{0}{432}{432}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.10.1}Training Enhancements for ResNets}{432}{subsection.11.10.1}\protected@file@percent }
\newlabel{subsubsec:resnet_training_improvements}{{11.10.1}{432}{Training Enhancements for ResNets}{subsection.11.10.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11.12}{\ignorespaces \textbf  {Training improvements for ResNet-200} \blx@tocontentsinit {0}\cite {bello2021_revisitingresnets}. Applying these modifications systematically increases accuracy, demonstrating the potential of ResNet-style architectures when trained properly.}}{432}{table.caption.846}\protected@file@percent }
\abx@aux@backref{324}{bello2021_revisitingresnets}{0}{432}{432}
\newlabel{tab:resnet_training_improvements}{{11.12}{432}{\textbf {Training improvements for ResNet-200} \cite {bello2021_revisitingresnets}. Applying these modifications systematically increases accuracy, demonstrating the potential of ResNet-style architectures when trained properly}{table.caption.846}{}}
\@writefile{toc}{\contentsline {paragraph}{Key Enhancements}{432}{section*.847}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.10.2}Scaling ResNets for Efficient Training}{432}{subsection.11.10.2}\protected@file@percent }
\newlabel{subsubsec:resnet_scaling}{{11.10.2}{432}{Scaling ResNets for Efficient Training}{subsection.11.10.2}{}}
\BKM@entry{id=453,dest={73756273656374696F6E2E31312E31302E33},srcline={1894}}{5C3337365C3337375C303030525C303030655C303030735C3030304E5C303030655C303030745C3030302D5C303030525C303030535C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030304E5C303030655C303030745C3030303A5C3030305C3034305C303030415C3030305C3034305C303030525C303030655C3030302D5C303030455C303030765C303030615C3030306C5C303030755C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{bello2021_revisitingresnets}
\abx@aux@segm{0}{0}{bello2021_revisitingresnets}
\abx@aux@cite{0}{bello2021_revisitingresnets}
\abx@aux@segm{0}{0}{bello2021_revisitingresnets}
\abx@aux@cite{0}{bello2021_revisitingresnets}
\abx@aux@segm{0}{0}{bello2021_revisitingresnets}
\abx@aux@cite{0}{bello2021_revisitingresnets}
\abx@aux@segm{0}{0}{bello2021_revisitingresnets}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.10.3}ResNet-RS vs. EfficientNet: A Re-Evaluation}{433}{subsection.11.10.3}\protected@file@percent }
\newlabel{subsubsec:chapter11_resnetrs_vs_efficientnet}{{11.10.3}{433}{ResNet-RS vs. EfficientNet: A Re-Evaluation}{subsection.11.10.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.37}{\ignorespaces \textbf  {Training time comparison:} Optimized ResNets train significantly faster than EfficientNets at the same accuracy level \blx@tocontentsinit {0}\cite {bello2021_revisitingresnets}.}}{433}{figure.caption.848}\protected@file@percent }
\abx@aux@backref{326}{bello2021_revisitingresnets}{0}{433}{433}
\newlabel{fig:chapter11_resnet_vs_efficientnet}{{11.37}{433}{\textbf {Training time comparison:} Optimized ResNets train significantly faster than EfficientNets at the same accuracy level \cite {bello2021_revisitingresnets}}{figure.caption.848}{}}
\@writefile{toc}{\contentsline {paragraph}{Comparison of ResNet-RS and EfficientNet}{433}{section*.849}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {11.13}{\ignorespaces \textbf  {Comparison of ResNet-RS and EfficientNet} \blx@tocontentsinit {0}\cite {bello2021_revisitingresnets}. Despite having more parameters and FLOPs, ResNet-RS models are significantly faster in both training and inference. TPU latency is measured per training step for 1024 images on 8 TPUv3 cores, while memory usage is reported for 32 images per core, using bfloat16 precision without fusion or rematerialization.}}{433}{table.caption.850}\protected@file@percent }
\abx@aux@backref{328}{bello2021_revisitingresnets}{0}{433}{433}
\newlabel{tab:chapter11_resnetrs_vs_efficientnet}{{11.13}{433}{\textbf {Comparison of ResNet-RS and EfficientNet} \cite {bello2021_revisitingresnets}. Despite having more parameters and FLOPs, ResNet-RS models are significantly faster in both training and inference. TPU latency is measured per training step for 1024 images on 8 TPUv3 cores, while memory usage is reported for 32 images per core, using bfloat16 precision without fusion or rematerialization}{table.caption.850}{}}
\@writefile{toc}{\contentsline {paragraph}{Key Observations}{433}{section*.851}\protected@file@percent }
\abx@aux@cite{0}{bello2021_revisitingresnets}
\abx@aux@segm{0}{0}{bello2021_revisitingresnets}
\BKM@entry{id=454,dest={73656374696F6E2E31312E3131},srcline={1937}}{5C3337365C3337375C303030525C303030655C303030675C3030304E5C303030655C303030745C303030735C3030303A5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C3030305C3034305C303030445C303030655C303030735C303030695C303030675C3030306E5C3030305C3034305C303030535C303030705C303030615C303030635C303030655C30303073}
\abx@aux@cite{0}{radosavovic2020_regnet}
\abx@aux@segm{0}{0}{radosavovic2020_regnet}
\BKM@entry{id=455,dest={73756273656374696F6E2E31312E31312E31},srcline={1944}}{5C3337365C3337375C303030525C303030655C303030675C3030304E5C303030655C303030745C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C30303065}
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{434}{section*.852}\protected@file@percent }
\abx@aux@backref{329}{bello2021_revisitingresnets}{0}{434}{434}
\@writefile{toc}{\contentsline {section}{\numberline {11.11}RegNets: Network Design Spaces}{434}{section.11.11}\protected@file@percent }
\newlabel{subsec:regnets}{{11.11}{434}{RegNets: Network Design Spaces}{section.11.11}{}}
\abx@aux@backref{330}{radosavovic2020_regnet}{0}{434}{434}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.11.1}RegNet Architecture}{434}{subsection.11.11.1}\protected@file@percent }
\newlabel{subsubsec:regnet_architecture}{{11.11.1}{434}{RegNet Architecture}{subsection.11.11.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.38}{\ignorespaces \textbf  {RegNet architecture}: A simple backbone with a stem, four-stage body, and head. Each stage consists of multiple blocks with defined parameters. These building blocks will allows us to construct architectures at different sizes that are efficient \& producing competitive accuracy.}}{434}{figure.caption.853}\protected@file@percent }
\newlabel{fig:chapter11_regnet_architecture}{{11.38}{434}{\textbf {RegNet architecture}: A simple backbone with a stem, four-stage body, and head. Each stage consists of multiple blocks with defined parameters. These building blocks will allows us to construct architectures at different sizes that are efficient \& producing competitive accuracy}{figure.caption.853}{}}
\BKM@entry{id=456,dest={73756273656374696F6E2E31312E31312E32},srcline={1980}}{5C3337365C3337375C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030445C303030655C303030735C303030695C303030675C3030306E5C3030305C3034305C303030535C303030705C303030615C303030635C30303065}
\abx@aux@cite{0}{radosavovic2020_regnet}
\abx@aux@segm{0}{0}{radosavovic2020_regnet}
\@writefile{toc}{\contentsline {paragraph}{Block Design: Generalizing ResNeXt}{435}{section*.854}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.39}{\ignorespaces \textbf  {RegNet block structure}: A generalization of ResNeXt, where each stage is defined by four parameters only.}}{435}{figure.caption.855}\protected@file@percent }
\newlabel{fig:chapter11_regnet_block}{{11.39}{435}{\textbf {RegNet block structure}: A generalization of ResNeXt, where each stage is defined by four parameters only}{figure.caption.855}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.11.2}Optimizing the Design Space}{435}{subsection.11.11.2}\protected@file@percent }
\newlabel{subsubsec:regnet_optimization}{{11.11.2}{435}{Optimizing the Design Space}{subsection.11.11.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Random Sampling and Performance Trends}{435}{section*.856}\protected@file@percent }
\abx@aux@backref{331}{radosavovic2020_regnet}{0}{435}{435}
\@writefile{toc}{\contentsline {paragraph}{Reducing the Design Space}{436}{section*.857}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Final Six Parameters}{436}{section*.858}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.40}{\ignorespaces \textbf  {RegNet design space optimization}: The initial 16 parameters were reduced to 6, enabling efficient architecture search.}}{436}{figure.caption.859}\protected@file@percent }
\newlabel{fig:chapter11_regnet_design_space}{{11.40}{436}{\textbf {RegNet design space optimization}: The initial 16 parameters were reduced to 6, enabling efficient architecture search}{figure.caption.859}{}}
\@writefile{toc}{\contentsline {paragraph}{Why This Works}{436}{section*.860}\protected@file@percent }
\BKM@entry{id=457,dest={73756273656374696F6E2E31312E31312E33},srcline={2039}}{5C3337365C3337375C303030505C303030655C303030725C303030665C3030306F5C303030725C3030306D5C303030615C3030306E5C303030635C303030655C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030415C303030705C303030705C3030306C5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{437}{section*.861}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.11.3}Performance and Applications}{437}{subsection.11.11.3}\protected@file@percent }
\newlabel{subsubsec:regnet_performance}{{11.11.3}{437}{Performance and Applications}{subsection.11.11.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.41}{\ignorespaces RegNet models match EfficientNet accuracy but train up to 5$\times $ faster per iteration.}}{437}{figure.caption.862}\protected@file@percent }
\newlabel{fig:chapter11_regnet_vs_efficientnet}{{11.41}{437}{RegNet models match EfficientNet accuracy but train up to 5$\times $ faster per iteration}{figure.caption.862}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.42}{\ignorespaces \textbf  {RegNet in real-world deployment}: Tesla uses RegNet-based architectures for processing inputs from multiple cameras.}}{438}{figure.caption.863}\protected@file@percent }
\newlabel{fig:chapter11_regnet_tesla}{{11.42}{438}{\textbf {RegNet in real-world deployment}: Tesla uses RegNet-based architectures for processing inputs from multiple cameras}{figure.caption.863}{}}
\@writefile{toc}{\contentsline {paragraph}{Key Takeaways}{438}{section*.864}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{438}{section*.865}\protected@file@percent }
\BKM@entry{id=458,dest={73656374696F6E2E31312E3132},srcline={2076}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030305C3034305C3030306F5C303030665C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {11.12}Summary of Efficient Network Architectures}{439}{section.11.12}\protected@file@percent }
\newlabel{subsec:chapter11_summary}{{11.12}{439}{Summary of Efficient Network Architectures}{section.11.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{Grouped Convolutions and ResNeXt}{439}{section*.866}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Squeeze-and-Excitation (SE) Blocks}{439}{section*.867}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{MobileNet and ShuffleNet: Depthwise Separable Convolutions and Channel Mixing}{439}{section*.868}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{MobileNetV2: Inverted Residual Blocks}{439}{section*.869}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{NAS and MobileNetV3, ShuffleNetV2 Insights}{439}{section*.870}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{EfficientNet: Compound Scaling}{439}{section*.871}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{EfficientNet-Lite and EfficientNetV2}{440}{section*.872}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{NFNets: BN-Free Training}{440}{section*.873}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Revisiting ResNets: Scaling and Training Recipes}{440}{section*.874}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{RegNets: Optimizing the Design Space}{440}{section*.875}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Key Takeaways}{440}{section*.876}\protected@file@percent }
\BKM@entry{id=459,dest={636861707465722E3132},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030315C303030325C3030303A5C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030535C3030306F5C303030665C303030745C303030775C303030615C303030725C30303065}
\BKM@entry{id=460,dest={73656374696F6E2E31322E31},srcline={10}}{5C3337365C3337375C303030445C303030655C303030655C303030705C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030465C303030725C303030615C3030306D5C303030655C303030775C3030306F5C303030725C3030306B5C303030735C3030303A5C3030305C3034305C303030455C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C303030615C3030306E5C303030645C303030735C303030635C303030615C303030705C30303065}
\@writefile{toc}{\contentsline {chapter}{\numberline {12}Lecture 12: Deep Learning Software}{441}{chapter.12}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@11}}
\ttl@writefile{ptc}{\ttl@starttoc{default@12}}
\pgfsyspdfmark {pgfid33}{0}{52099153}
\pgfsyspdfmark {pgfid32}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {12.1}Deep Learning Frameworks: Evolution and Landscape}{441}{section.12.1}\protected@file@percent }
\newlabel{sec:chapter12_frameworks}{{12.1}{441}{Deep Learning Frameworks: Evolution and Landscape}{section.12.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.1}{\ignorespaces Overview of major deep learning frameworks and their affiliations.}}{441}{figure.caption.877}\protected@file@percent }
\newlabel{fig:chapter12_frameworks}{{12.1}{441}{Overview of major deep learning frameworks and their affiliations}{figure.caption.877}{}}
\BKM@entry{id=461,dest={73756273656374696F6E2E31322E312E31},srcline={35}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030505C303030755C303030725C303030705C3030306F5C303030735C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030465C303030725C303030615C3030306D5C303030655C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=462,dest={73756273656374696F6E2E31322E312E32},srcline={46}}{5C3337365C3337375C303030525C303030655C303030635C303030615C3030306C5C3030306C5C3030303A5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030725C303030615C303030705C303030685C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1.1}The Purpose of Deep Learning Frameworks}{442}{subsection.12.1.1}\protected@file@percent }
\newlabel{subsec:chapter12_purpose}{{12.1.1}{442}{The Purpose of Deep Learning Frameworks}{subsection.12.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1.2}Recall: Computational Graphs}{442}{subsection.12.1.2}\protected@file@percent }
\newlabel{subsubsec:chapter12_computational_graphs}{{12.1.2}{442}{Recall: Computational Graphs}{subsection.12.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.2}{\ignorespaces \textbf  {Computational graphs in deep learning.} These graphs define the sequence of operations for training and inference, enabling automatic differentiation and optimization.}}{442}{figure.caption.878}\protected@file@percent }
\newlabel{fig:chapter12_computational_graphs}{{12.2}{442}{\textbf {Computational graphs in deep learning.} These graphs define the sequence of operations for training and inference, enabling automatic differentiation and optimization}{figure.caption.878}{}}
\BKM@entry{id=463,dest={73656374696F6E2E31322E32},srcline={72}}{5C3337365C3337375C303030505C303030795C303030545C3030306F5C303030725C303030635C303030685C3030303A5C3030305C3034305C303030465C303030755C3030306E5C303030645C303030615C3030306D5C303030655C3030306E5C303030745C303030615C3030306C5C3030305C3034305C303030435C3030306F5C3030306E5C303030635C303030655C303030705C303030745C30303073}
\BKM@entry{id=464,dest={73756273656374696F6E2E31322E322E31},srcline={83}}{5C3337365C3337375C303030545C303030655C3030306E5C303030735C3030306F5C303030725C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030425C303030615C303030735C303030695C303030635C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {12.2}PyTorch: Fundamental Concepts}{443}{section.12.2}\protected@file@percent }
\newlabel{subsec:chapter12_pytorch}{{12.2}{443}{PyTorch: Fundamental Concepts}{section.12.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.1}Tensors and Basic Computation}{443}{subsection.12.2.1}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_tensors}{{12.2.1}{443}{Tensors and Basic Computation}{subsection.12.2.1}{}}
\BKM@entry{id=465,dest={73756273656374696F6E2E31322E322E32},srcline={123}}{5C3337365C3337375C303030415C303030755C303030745C3030306F5C303030675C303030725C303030615C303030645C3030303A5C3030305C3034305C303030415C303030755C303030745C3030306F5C3030306D5C303030615C303030745C303030695C303030635C3030305C3034305C303030445C303030695C303030665C303030665C303030655C303030725C303030655C3030306E5C303030745C303030695C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=466,dest={73756273656374696F6E2E31322E322E33},srcline={163}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030725C303030615C303030705C303030685C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304D5C3030306F5C303030645C303030755C3030306C5C303030615C303030725C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.2}Autograd: Automatic Differentiation}{444}{subsection.12.2.2}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_autograd}{{12.2.2}{444}{Autograd: Automatic Differentiation}{subsection.12.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.3}Computational Graphs and Modular Computation}{445}{subsection.12.2.3}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_graph}{{12.2.3}{445}{Computational Graphs and Modular Computation}{subsection.12.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{Building the Computational Graph}{445}{section*.879}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_graph_building}{{12.2.3}{445}{Building the Computational Graph}{section*.879}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.3}{\ignorespaces \textbf  {First computational node in the graph.} The matrix multiplication \texttt  {x.mm(w1)} creates the first node in the computational graph.}}{445}{figure.caption.880}\protected@file@percent }
\newlabel{fig:chapter12_pytorch_mm_graph}{{12.3}{445}{\textbf {First computational node in the graph.} The matrix multiplication \texttt {x.mm(w1)} creates the first node in the computational graph}{figure.caption.880}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.4}{\ignorespaces \textbf  {ReLU activation node.} The ReLU function introduces a non-linearity while maintaining the computational graph structure.}}{446}{figure.caption.881}\protected@file@percent }
\newlabel{fig:chapter12_pytorch_relu_graph}{{12.4}{446}{\textbf {ReLU activation node.} The ReLU function introduces a non-linearity while maintaining the computational graph structure}{figure.caption.881}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.5}{\ignorespaces \textbf  {Final matrix multiplication node.} The output prediction \texttt  {y\_pred} is produced by matrix multiplication with \texttt  {w2}.}}{446}{figure.caption.882}\protected@file@percent }
\newlabel{fig:chapter12_pytorch_mm2_graph}{{12.5}{446}{\textbf {Final matrix multiplication node.} The output prediction \texttt {y\_pred} is produced by matrix multiplication with \texttt {w2}}{figure.caption.882}{}}
\@writefile{toc}{\contentsline {subsubsection}{Loss Computation and Backpropagation}{446}{section*.883}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_loss_backprop}{{12.2.3}{446}{Loss Computation and Backpropagation}{section*.883}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.6}{\ignorespaces \textbf  {Loss computation node.} The final loss is computed as a scalar output in the computational graph, allowing backpropagation to all inputs requiring gradients.}}{447}{figure.caption.884}\protected@file@percent }
\newlabel{fig:chapter12_pytorch_loss_graph}{{12.6}{447}{\textbf {Loss computation node.} The final loss is computed as a scalar output in the computational graph, allowing backpropagation to all inputs requiring gradients}{figure.caption.884}{}}
\@writefile{toc}{\contentsline {subsubsection}{Extending Computational Graphs with Python Functions}{447}{section*.885}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_modular}{{12.2.3}{447}{Extending Computational Graphs with Python Functions}{section*.885}{}}
\@writefile{toc}{\contentsline {subsubsection}{Custom Autograd Functions}{448}{section*.886}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_custom_autograd}{{12.2.3}{448}{Custom Autograd Functions}{section*.886}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.7}{\ignorespaces \textbf  {Custom autograd function for sigmoid.} This method creates a single node in the computational graph (on the right) instead of multiple primitive operations (on the left).}}{448}{figure.caption.887}\protected@file@percent }
\newlabel{fig:chapter12_pytorch_custom_autograd}{{12.7}{448}{\textbf {Custom autograd function for sigmoid.} This method creates a single node in the computational graph (on the right) instead of multiple primitive operations (on the left)}{figure.caption.887}{}}
\BKM@entry{id=467,dest={73756273656374696F6E2E31322E322E34},srcline={331}}{5C3337365C3337375C303030485C303030695C303030675C303030685C3030302D5C3030304C5C303030655C303030765C303030655C3030306C5C3030305C3034305C303030415C303030625C303030735C303030745C303030725C303030615C303030635C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030505C303030795C303030545C3030306F5C303030725C303030635C303030685C3030303A5C3030305C3034305C303030745C3030306F5C303030725C303030635C303030685C3030302E5C3030306E5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {subsubsection}{Summary: Backpropagation and Graph Optimization}{449}{section*.888}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_graph_summary}{{12.2.3}{449}{Summary: Backpropagation and Graph Optimization}{section*.888}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.4}High-Level Abstractions in PyTorch: \texttt  {torch.nn} and Optimizers}{449}{subsection.12.2.4}\protected@file@percent }
\newlabel{subsec:chapter12_pytorch_nn}{{12.2.4}{449}{High-Level Abstractions in PyTorch: \texttt {torch.nn} and Optimizers}{subsection.12.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Using \texttt  {torch.nn.Sequential}}{449}{section*.889}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_sequential}{{12.2.4}{449}{Using \texttt {torch.nn.Sequential}}{section*.889}{}}
\@writefile{toc}{\contentsline {subsubsection}{Using Optimizers: Automating Gradient Descent}{450}{section*.890}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_optimizers}{{12.2.4}{450}{Using Optimizers: Automating Gradient Descent}{section*.890}{}}
\BKM@entry{id=468,dest={73756273656374696F6E2E31322E322E35},srcline={466}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030625C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030435C303030755C303030735C303030745C3030306F5C3030306D5C3030305C3034305C3030304D5C3030306F5C303030645C303030755C3030306C5C303030655C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030535C303030655C303030715C303030755C303030655C3030306E5C303030745C303030695C303030615C3030306C5C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\@writefile{toc}{\contentsline {subsubsection}{Defining Custom \texttt  {nn.Module} Subclasses}{451}{section*.891}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_nn_module}{{12.2.4}{451}{Defining Custom \texttt {nn.Module} Subclasses}{section*.891}{}}
\@writefile{toc}{\contentsline {subsubsection}{Key Takeaways}{451}{section*.892}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.5}Combining Custom Modules with Sequential Models}{451}{subsection.12.2.5}\protected@file@percent }
\newlabel{subsec:chapter12_pytorch_custom_sequential}{{12.2.5}{451}{Combining Custom Modules with Sequential Models}{subsection.12.2.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{Example: Parallel Block}{452}{section*.893}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_parallel_block}{{12.2.5}{452}{Example: Parallel Block}{section*.893}{}}
\BKM@entry{id=469,dest={73756273656374696F6E2E31322E322E36},srcline={533}}{5C3337365C3337375C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C303030445C303030615C303030745C303030615C3030305C3034305C3030304C5C3030306F5C303030615C303030645C303030695C3030306E5C303030675C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030745C3030306F5C303030725C303030635C303030685C3030302E5C303030755C303030745C303030695C3030306C5C303030735C3030302E5C303030645C303030615C303030745C30303061}
\@writefile{lof}{\contentsline {figure}{\numberline {12.8}{\ignorespaces \textbf  {ParallelBlock module design:} The implementation of the \texttt  {ParallelBlock} and its corresponding computational graph visualization.}}{453}{figure.caption.894}\protected@file@percent }
\newlabel{fig:chapter12_parallel_block}{{12.8}{453}{\textbf {ParallelBlock module design:} The implementation of the \texttt {ParallelBlock} and its corresponding computational graph visualization}{figure.caption.894}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.9}{\ignorespaces \textbf  {Stacking multiple \texttt  {ParallelBlock} instances in a Sequential model.} The left side of the figure shows the computational graph produced.}}{453}{figure.caption.895}\protected@file@percent }
\newlabel{fig:chapter12_parallel_block_graph}{{12.9}{453}{\textbf {Stacking multiple \texttt {ParallelBlock} instances in a Sequential model.} The left side of the figure shows the computational graph produced}{figure.caption.895}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.6}Efficient Data Loading with \texttt  {torch.utils.data}}{453}{subsection.12.2.6}\protected@file@percent }
\newlabel{subsec:chapter12_pytorch_dataloader}{{12.2.6}{453}{Efficient Data Loading with \texttt {torch.utils.data}}{subsection.12.2.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{Example: Using \texttt  {DataLoader} for Mini-batching}{453}{section*.896}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_dataloader_example}{{12.2.6}{453}{Example: Using \texttt {DataLoader} for Mini-batching}{section*.896}{}}
\BKM@entry{id=470,dest={73756273656374696F6E2E31322E322E37},srcline={573}}{5C3337365C3337375C303030555C303030735C303030695C3030306E5C303030675C3030305C3034305C303030505C303030725C303030655C303030745C303030725C303030615C303030695C3030306E5C303030655C303030645C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030545C3030306F5C303030725C303030635C303030685C303030565C303030695C303030735C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.7}Using Pretrained Models with TorchVision}{454}{subsection.12.2.7}\protected@file@percent }
\newlabel{subsec:chapter12_pytorch_torchvision}{{12.2.7}{454}{Using Pretrained Models with TorchVision}{subsection.12.2.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{Key Takeaways}{454}{section*.897}\protected@file@percent }
\BKM@entry{id=471,dest={73656374696F6E2E31322E33},srcline={601}}{5C3337365C3337375C303030445C303030795C3030306E5C303030615C3030306D5C303030695C303030635C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C303030535C303030745C303030615C303030745C303030695C303030635C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030725C303030615C303030705C303030685C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030505C303030795C303030545C3030306F5C303030725C303030635C30303068}
\BKM@entry{id=472,dest={73756273656374696F6E2E31322E332E31},srcline={622}}{5C3337365C3337375C303030535C303030745C303030615C303030745C303030695C303030635C3030305C3034305C303030475C303030725C303030615C303030705C303030685C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304A5C303030755C303030735C303030745C3030302D5C303030695C3030306E5C3030302D5C303030545C303030695C3030306D5C303030655C3030305C3034305C3030305C3035305C3030304A5C303030495C303030545C3030305C3035315C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030695C3030306C5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {12.3}Dynamic vs. Static Computational Graphs in PyTorch}{455}{section.12.3}\protected@file@percent }
\newlabel{subsec:chapter12_pytorch_dynamic_vs_static}{{12.3}{455}{Dynamic vs. Static Computational Graphs in PyTorch}{section.12.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{Example: Dynamic Graph Construction}{455}{section*.898}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_dynamic_example}{{12.3}{455}{Example: Dynamic Graph Construction}{section*.898}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.10}{\ignorespaces \textbf  {Example of a dynamically constructed graph:} The model structure changes at each iteration based on previous loss values.}}{455}{figure.caption.899}\protected@file@percent }
\newlabel{fig:chapter12_dynamic_graph}{{12.10}{455}{\textbf {Example of a dynamically constructed graph:} The model structure changes at each iteration based on previous loss values}{figure.caption.899}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3.1}Static Graphs and Just-in-Time (JIT) Compilation}{455}{subsection.12.3.1}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_static_graphs}{{12.3.1}{455}{Static Graphs and Just-in-Time (JIT) Compilation}{subsection.12.3.1}{}}
\BKM@entry{id=473,dest={73756273656374696F6E2E31322E332E32},srcline={633}}{5C3337365C3337375C303030555C303030735C303030695C3030306E5C303030675C3030305C3034305C3030304A5C303030495C303030545C3030305C3034305C303030745C3030306F5C3030305C3034305C303030435C303030725C303030655C303030615C303030745C303030655C3030305C3034305C303030535C303030745C303030615C303030745C303030695C303030635C3030305C3034305C303030475C303030725C303030615C303030705C303030685C30303073}
\BKM@entry{id=474,dest={73756273656374696F6E2E31322E332E33},srcline={664}}{5C3337365C3337375C303030485C303030615C3030306E5C303030645C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306E5C303030645C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030535C303030745C303030615C303030745C303030695C303030635C3030305C3034305C303030475C303030725C303030615C303030705C303030685C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3.2}Using JIT to Create Static Graphs}{456}{subsection.12.3.2}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_jit}{{12.3.2}{456}{Using JIT to Create Static Graphs}{subsection.12.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.11}{\ignorespaces \textbf  {TorchScript:} Using JIT compilation to convert PyTorch models into static graphs for optimization.}}{456}{figure.caption.900}\protected@file@percent }
\newlabel{fig:chapter12_pytorch_jit}{{12.11}{456}{\textbf {TorchScript:} Using JIT compilation to convert PyTorch models into static graphs for optimization}{figure.caption.900}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3.3}Handling Conditionals in Static Graphs}{456}{subsection.12.3.3}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_jit_conditionals}{{12.3.3}{456}{Handling Conditionals in Static Graphs}{subsection.12.3.3}{}}
\BKM@entry{id=475,dest={73756273656374696F6E2E31322E332E34},srcline={678}}{5C3337365C3337375C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030475C303030725C303030615C303030705C303030685C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304A5C303030495C30303054}
\BKM@entry{id=476,dest={73756273656374696F6E2E31322E332E35},srcline={692}}{5C3337365C3337375C303030425C303030655C3030306E5C303030655C303030665C303030695C303030745C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030535C303030745C303030615C303030745C303030695C303030635C3030305C3034305C303030475C303030725C303030615C303030705C303030685C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {12.12}{\ignorespaces \textbf  {Conditionals in static graphs:} JIT inserts a conditional node to handle different execution paths.}}{457}{figure.caption.901}\protected@file@percent }
\newlabel{fig:chapter12_pytorch_jit_conditionals}{{12.12}{457}{\textbf {Conditionals in static graphs:} JIT inserts a conditional node to handle different execution paths}{figure.caption.901}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3.4}Optimizing Computation Graphs with JIT}{457}{subsection.12.3.4}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_graph_optimization}{{12.3.4}{457}{Optimizing Computation Graphs with JIT}{subsection.12.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.13}{\ignorespaces \textbf  {Operation fusion in static graphs:} Layers such as Conv + ReLU are combined into a single operation to improve efficiency.}}{457}{figure.caption.902}\protected@file@percent }
\newlabel{fig:chapter12_pytorch_fusion}{{12.13}{457}{\textbf {Operation fusion in static graphs:} Layers such as Conv + ReLU are combined into a single operation to improve efficiency}{figure.caption.902}{}}
\BKM@entry{id=477,dest={73756273656374696F6E2E31322E332E36},srcline={709}}{5C3337365C3337375C303030575C303030685C303030655C3030306E5C3030305C3034305C303030415C303030725C303030655C3030305C3034305C303030445C303030795C3030306E5C303030615C3030306D5C303030695C303030635C3030305C3034305C303030475C303030725C303030615C303030705C303030685C303030735C3030305C3034305C3030304E5C303030655C303030635C303030655C303030735C303030735C303030615C303030725C303030795C3030303F}
\abx@aux@cite{0}{johnson2017_infering}
\abx@aux@segm{0}{0}{johnson2017_infering}
\BKM@entry{id=478,dest={73656374696F6E2E31322E34},srcline={722}}{5C3337365C3337375C303030545C303030655C3030306E5C303030735C3030306F5C303030725C303030465C3030306C5C3030306F5C303030775C3030303A5C3030305C3034305C303030445C303030795C3030306E5C303030615C3030306D5C303030695C303030635C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030535C303030745C303030615C303030745C303030695C303030635C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030725C303030615C303030705C303030685C30303073}
\BKM@entry{id=479,dest={73756273656374696F6E2E31322E342E31},srcline={727}}{5C3337365C3337375C303030445C303030655C303030665C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030725C303030615C303030705C303030685C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030545C303030655C3030306E5C303030735C3030306F5C303030725C303030465C3030306C5C3030306F5C303030775C3030305C3034305C303030325C3030302E5C30303030}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3.5}Benefits and Limitations of Static Graphs}{458}{subsection.12.3.5}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_static_benefits_challenges}{{12.3.5}{458}{Benefits and Limitations of Static Graphs}{subsection.12.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3.6}When Are Dynamic Graphs Necessary?}{458}{subsection.12.3.6}\protected@file@percent }
\newlabel{subsubsec:chapter12_pytorch_dynamic_needed}{{12.3.6}{458}{When Are Dynamic Graphs Necessary?}{subsection.12.3.6}{}}
\abx@aux@backref{332}{johnson2017_infering}{0}{458}{458}
\@writefile{toc}{\contentsline {section}{\numberline {12.4}TensorFlow: Dynamic and Static Computational Graphs}{458}{section.12.4}\protected@file@percent }
\newlabel{subsec:chapter12_tensorflow}{{12.4}{458}{TensorFlow: Dynamic and Static Computational Graphs}{section.12.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.4.1}Defining Computational Graphs in TensorFlow 2.0}{458}{subsection.12.4.1}\protected@file@percent }
\newlabel{subsubsec:chapter12_tensorflow_dynamic}{{12.4.1}{458}{Defining Computational Graphs in TensorFlow 2.0}{subsection.12.4.1}{}}
\BKM@entry{id=480,dest={73756273656374696F6E2E31322E342E32},srcline={757}}{5C3337365C3337375C303030535C303030745C303030615C303030745C303030695C303030635C3030305C3034305C303030475C303030725C303030615C303030705C303030685C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030745C303030665C3030302E5C303030665C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=481,dest={73656374696F6E2E31322E35},srcline={781}}{5C3337365C3337375C3030304B5C303030655C303030725C303030615C303030735C3030303A5C3030305C3034305C303030485C303030695C303030675C303030685C3030302D5C3030304C5C303030655C303030765C303030655C3030306C5C3030305C3034305C303030415C303030505C303030495C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030545C303030655C3030306E5C303030735C3030306F5C303030725C303030465C3030306C5C3030306F5C30303077}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.4.2}Static Graphs with \texttt  {tf.function}}{459}{subsection.12.4.2}\protected@file@percent }
\newlabel{subsubsec:chapter12_tensorflow_static}{{12.4.2}{459}{Static Graphs with \texttt {tf.function}}{subsection.12.4.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {12.5}Keras: High-Level API for TensorFlow}{459}{section.12.5}\protected@file@percent }
\newlabel{subsec:chapter12_keras}{{12.5}{459}{Keras: High-Level API for TensorFlow}{section.12.5}{}}
\BKM@entry{id=482,dest={73656374696F6E2E31322E36},srcline={834}}{5C3337365C3337375C303030545C303030655C3030306E5C303030735C3030306F5C303030725C303030425C3030306F5C303030615C303030725C303030645C3030303A5C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030304D5C303030655C303030745C303030725C303030695C303030635C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {12.6}TensorBoard: Visualizing Training Metrics}{460}{section.12.6}\protected@file@percent }
\newlabel{subsec:chapter12_tensorboard}{{12.6}{460}{TensorBoard: Visualizing Training Metrics}{section.12.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.14}{\ignorespaces \textbf  {TensorBoard visualization:} Loss curves and weight distributions during training.}}{460}{figure.caption.903}\protected@file@percent }
\newlabel{fig:chapter12_tensorboard}{{12.14}{460}{\textbf {TensorBoard visualization:} Loss curves and weight distributions during training}{figure.caption.903}{}}
\BKM@entry{id=483,dest={73656374696F6E2E31322E37},srcline={855}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C303030735C3030306F5C3030306E5C3030303A5C3030305C3034305C303030505C303030795C303030545C3030306F5C303030725C303030635C303030685C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C303030545C303030655C3030306E5C303030735C3030306F5C303030725C303030465C3030306C5C3030306F5C30303077}
\@writefile{toc}{\contentsline {section}{\numberline {12.7}Comparison: PyTorch vs. TensorFlow}{461}{section.12.7}\protected@file@percent }
\newlabel{subsec:chapter12_comparison}{{12.7}{461}{Comparison: PyTorch vs. TensorFlow}{section.12.7}{}}
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{461}{section*.904}\protected@file@percent }
\BKM@entry{id=484,dest={636861707465722E3133},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030315C303030335C3030303A5C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=485,dest={73656374696F6E2E31332E31},srcline={10}}{5C3337365C3337375C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=486,dest={73756273656374696F6E2E31332E312E31},srcline={17}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030655C303030725C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030615C303030735C3030306B5C303030735C3030303A5C3030305C3034305C303030425C303030655C303030795C3030306F5C3030306E5C303030645C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {chapter}{\numberline {13}Lecture 13: Object Detection}{462}{chapter.13}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@12}}
\ttl@writefile{ptc}{\ttl@starttoc{default@13}}
\pgfsyspdfmark {pgfid63}{0}{52099153}
\pgfsyspdfmark {pgfid62}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {13.1}Object Detection: Introduction}{462}{section.13.1}\protected@file@percent }
\newlabel{subsec:chapter13_intro}{{13.1}{462}{Object Detection: Introduction}{section.13.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.1}Computer Vision Tasks: Beyond Classification}{462}{subsection.13.1.1}\protected@file@percent }
\newlabel{subsubsec:chapter13_cv_tasks}{{13.1.1}{462}{Computer Vision Tasks: Beyond Classification}{subsection.13.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.1}{\ignorespaces \textbf  {Comparison of common computer vision tasks.}}}{462}{figure.caption.905}\protected@file@percent }
\newlabel{fig:chapter13_cv_tasks}{{13.1}{462}{\textbf {Comparison of common computer vision tasks.}}{figure.caption.905}{}}
\BKM@entry{id=487,dest={73756273656374696F6E2E31332E312E32},srcline={31}}{5C3337365C3337375C303030575C303030685C303030615C303030745C3030305C3034305C303030695C303030735C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E5C3030303F}
\BKM@entry{id=488,dest={73756273656374696F6E2E31332E312E33},srcline={40}}{5C3337365C3337375C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=489,dest={73756273656374696F6E2E31332E312E34},srcline={52}}{5C3337365C3337375C303030425C3030306F5C303030755C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030425C3030306F5C303030785C303030655C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030495C3030306E5C303030745C303030655C303030725C303030735C303030655C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C303030765C303030655C303030725C3030305C3034305C303030555C3030306E5C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030495C3030306F5C303030555C3030305C303531}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.2}What is Object Detection?}{463}{subsection.13.1.2}\protected@file@percent }
\newlabel{subsubsec:chapter13_what_is_detection}{{13.1.2}{463}{What is Object Detection?}{subsection.13.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.3}Challenges in Object Detection}{463}{subsection.13.1.3}\protected@file@percent }
\newlabel{subsubsec:chapter13_detection_challenges}{{13.1.3}{463}{Challenges in Object Detection}{subsection.13.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.4}Bounding Boxes and Intersection over Union (IoU)}{463}{subsection.13.1.4}\protected@file@percent }
\newlabel{subsubsec:chapter13_bboxes_iou}{{13.1.4}{463}{Bounding Boxes and Intersection over Union (IoU)}{subsection.13.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.2}{\ignorespaces \textbf  {Axis-aligned vs. Oriented bounding boxes.} Although an oriented box provides a more accurate fit, most models predict axis-aligned boxes for simplicity.}}{463}{figure.caption.906}\protected@file@percent }
\newlabel{fig:chapter13_bbox_orientation}{{13.2}{463}{\textbf {Axis-aligned vs. Oriented bounding boxes.} Although an oriented box provides a more accurate fit, most models predict axis-aligned boxes for simplicity}{figure.caption.906}{}}
\BKM@entry{id=490,dest={73756273656374696F6E2E31332E312E35},srcline={77}}{5C3337365C3337375C303030455C303030765C303030615C3030306C5C303030755C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030425C3030306F5C303030755C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030425C3030306F5C303030785C303030655C303030735C3030303A5C3030305C3034305C303030495C3030306E5C303030745C303030655C303030725C303030735C303030655C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C303030765C303030655C303030725C3030305C3034305C303030555C3030306E5C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030495C3030306F5C303030555C3030305C303531}
\@writefile{lof}{\contentsline {figure}{\numberline {13.3}{\ignorespaces \textbf  {Modal vs. Amodal bounding boxes.} The blue box (modal) covers only the visible portion of the cat, while the green box (amodal) extends to include occluded regions.}}{464}{figure.caption.907}\protected@file@percent }
\newlabel{fig:chapter13_modal_amodal}{{13.3}{464}{\textbf {Modal vs. Amodal bounding boxes.} The blue box (modal) covers only the visible portion of the cat, while the green box (amodal) extends to include occluded regions}{figure.caption.907}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.5}Evaluating Bounding Boxes: Intersection over Union (IoU)}{464}{subsection.13.1.5}\protected@file@percent }
\newlabel{subsubsec:chapter13_iou}{{13.1.5}{464}{Evaluating Bounding Boxes: Intersection over Union (IoU)}{subsection.13.1.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.4}{\ignorespaces \textbf  {Intersection over Union (IoU).} Left: Intersection of predicted and ground-truth bounding boxes (orange). Right: Union of predicted and ground-truth boxes (purple).}}{464}{figure.caption.908}\protected@file@percent }
\newlabel{fig:chapter13_iou_definition}{{13.4}{464}{\textbf {Intersection over Union (IoU).} Left: Intersection of predicted and ground-truth bounding boxes (orange). Right: Union of predicted and ground-truth boxes (purple)}{figure.caption.908}{}}
\BKM@entry{id=491,dest={73756273656374696F6E2E31332E312E36},srcline={112}}{5C3337365C3337375C3030304D5C303030755C3030306C5C303030745C303030695C303030745C303030615C303030735C3030306B5C3030305C3034305C3030304C5C3030306F5C303030735C303030735C3030303A5C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030525C303030655C303030675C303030725C303030655C303030735C303030735C303030695C3030306F5C3030306E}
\BKM@entry{id=492,dest={73656374696F6E2E31332E32},srcline={128}}{5C3337365C3337375C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030535C303030695C3030306E5C303030675C3030306C5C303030655C3030302D5C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030745C3030306F5C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C3030302D5C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {13.5}{\ignorespaces \textbf  {Intersection over Union (IoU) Examples.} Left: IoU \(\sim 0.5\) (acceptable). Middle: IoU \(\sim 0.7\) (good). Right: IoU \(\sim 0.9\) (almost perfect).}}{465}{figure.caption.909}\protected@file@percent }
\newlabel{fig:chapter13_iou_examples}{{13.5}{465}{\textbf {Intersection over Union (IoU) Examples.} Left: IoU \(\sim 0.5\) (acceptable). Middle: IoU \(\sim 0.7\) (good). Right: IoU \(\sim 0.9\) (almost perfect)}{figure.caption.909}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.6}Multitask Loss: Classification and Regression}{465}{subsection.13.1.6}\protected@file@percent }
\newlabel{subsubsec:chapter13_multitask_loss}{{13.1.6}{465}{Multitask Loss: Classification and Regression}{subsection.13.1.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {13.2}From Single-Object to Multi-Object Detection}{465}{section.13.2}\protected@file@percent }
\newlabel{subsubsec:chapter13_single_vs_multi}{{13.2}{465}{From Single-Object to Multi-Object Detection}{section.13.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.6}{\ignorespaces \textbf  {Single-object localization pipeline.} A CNN extracts a feature vector that is used for both classification (predicting the category) and regression (predicting bounding box coordinates). This simple approach works well for a single object but does not generalize well.}}{465}{figure.caption.910}\protected@file@percent }
\newlabel{fig:chapter13_single_object}{{13.6}{465}{\textbf {Single-object localization pipeline.} A CNN extracts a feature vector that is used for both classification (predicting the category) and regression (predicting bounding box coordinates). This simple approach works well for a single object but does not generalize well}{figure.caption.910}{}}
\BKM@entry{id=493,dest={73756273656374696F6E2E31332E322E31},srcline={140}}{5C3337365C3337375C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306E5C303030675C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030655C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C30303073}
\BKM@entry{id=494,dest={73756273656374696F6E2E31332E322E32},srcline={153}}{5C3337365C3337375C303030535C3030306C5C303030695C303030645C303030695C3030306E5C303030675C3030305C3034305C303030575C303030695C3030306E5C303030645C3030306F5C303030775C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030615C303030635C30303068}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2.1}Challenges in Detecting Multiple Objects}{466}{subsection.13.2.1}\protected@file@percent }
\newlabel{subsec:chapter13_multiple_objects}{{13.2.1}{466}{Challenges in Detecting Multiple Objects}{subsection.13.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2.2}Sliding Window Approach}{466}{subsection.13.2.2}\protected@file@percent }
\newlabel{subsubsec:chapter13_sliding_window}{{13.2.2}{466}{Sliding Window Approach}{subsection.13.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.7}{\ignorespaces \textbf  {Sliding window classification.} Each image crop is classified as either an object (e.g., dog, cat) or background.}}{466}{figure.caption.911}\protected@file@percent }
\newlabel{fig:chapter13_sliding_window}{{13.7}{466}{\textbf {Sliding window classification.} Each image crop is classified as either an object (e.g., dog, cat) or background}{figure.caption.911}{}}
\BKM@entry{id=495,dest={73756273656374696F6E2E31332E322E33},srcline={187}}{5C3337365C3337375C303030525C303030655C303030675C303030695C3030306F5C3030306E5C3030305C3034305C303030505C303030725C3030306F5C303030705C3030306F5C303030735C303030615C3030306C5C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {13.8}{\ignorespaces \textbf  {Positive detection:} The classifier correctly identifies the presence of a dog in the selected region.}}{467}{figure.caption.912}\protected@file@percent }
\newlabel{fig:chapter13_sliding_window_positive}{{13.8}{467}{\textbf {Positive detection:} The classifier correctly identifies the presence of a dog in the selected region}{figure.caption.912}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2.3}Region Proposal Methods}{467}{subsection.13.2.3}\protected@file@percent }
\newlabel{subsubsec:chapter13_region_proposals}{{13.2.3}{467}{Region Proposal Methods}{subsection.13.2.3}{}}
\abx@aux@cite{0}{alexe2012_objectness}
\abx@aux@segm{0}{0}{alexe2012_objectness}
\abx@aux@cite{0}{uijlings2013_selective}
\abx@aux@segm{0}{0}{uijlings2013_selective}
\abx@aux@cite{0}{cheng2014_bing}
\abx@aux@segm{0}{0}{cheng2014_bing}
\abx@aux@cite{0}{zitnick2014_edgeboxes}
\abx@aux@segm{0}{0}{zitnick2014_edgeboxes}
\BKM@entry{id=496,dest={73656374696F6E2E31332E33},srcline={218}}{5C3337365C3337375C3030304E5C303030615C303030695C303030765C303030655C3030305C3034305C303030535C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030525C303030655C303030675C303030695C3030306F5C3030306E5C3030302D5C303030425C303030615C303030735C303030655C303030645C3030305C3034305C303030435C3030304E5C3030304E5C3030305C3034305C3030305C3035305C303030525C3030302D5C303030435C3030304E5C3030304E5C3030305C303531}
\abx@aux@cite{0}{girshick2014_rcnn}
\abx@aux@segm{0}{0}{girshick2014_rcnn}
\@writefile{lof}{\contentsline {figure}{\numberline {13.9}{\ignorespaces \textbf  {Region Proposal Example.} Selective Search identifies potential object locations before classification, significantly reducing the number of regions to evaluate.}}{468}{figure.caption.913}\protected@file@percent }
\newlabel{fig:chapter13_region_proposals}{{13.9}{468}{\textbf {Region Proposal Example.} Selective Search identifies potential object locations before classification, significantly reducing the number of regions to evaluate}{figure.caption.913}{}}
\abx@aux@backref{333}{alexe2012_objectness}{0}{468}{468}
\abx@aux@backref{334}{uijlings2013_selective}{0}{468}{468}
\abx@aux@backref{335}{cheng2014_bing}{0}{468}{468}
\abx@aux@backref{336}{zitnick2014_edgeboxes}{0}{468}{468}
\@writefile{toc}{\contentsline {section}{\numberline {13.3}Naive Solution: Region-Based CNN (R-CNN)}{468}{section.13.3}\protected@file@percent }
\newlabel{subsubsec:chapter13_rcnn}{{13.3}{468}{Naive Solution: Region-Based CNN (R-CNN)}{section.13.3}{}}
\abx@aux@backref{337}{girshick2014_rcnn}{0}{468}{468}
\BKM@entry{id=497,dest={73756273656374696F6E2E31332E332E31},srcline={241}}{5C3337365C3337375C303030425C3030306F5C303030755C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030425C3030306F5C303030785C3030305C3034305C303030525C303030655C303030675C303030725C303030655C303030735C303030735C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030525C303030655C303030665C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C3030304C5C3030306F5C303030635C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {13.10}{\ignorespaces \textbf  {R-CNN Pipeline.} Each proposed region is resized and classified independently. A single CNN is used to extract features for classification and regression, making it more efficient than sliding windows but still computationally expensive.}}{469}{figure.caption.914}\protected@file@percent }
\newlabel{fig:chapter13_rcnn}{{13.10}{469}{\textbf {R-CNN Pipeline.} Each proposed region is resized and classified independently. A single CNN is used to extract features for classification and regression, making it more efficient than sliding windows but still computationally expensive}{figure.caption.914}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.3.1}Bounding Box Regression: Refining Object Localization}{469}{subsection.13.3.1}\protected@file@percent }
\newlabel{subsubsec:chapter13_bbox_regression}{{13.3.1}{469}{Bounding Box Regression: Refining Object Localization}{subsection.13.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.11}{\ignorespaces \textbf  {Bounding Box Regression.} A transformation adjusts the region proposal (blue) to improve alignment. We can see the resultant output box after the transformation as well (orange).}}{470}{figure.caption.915}\protected@file@percent }
\newlabel{fig:chapter13_bbox_regression}{{13.11}{470}{\textbf {Bounding Box Regression.} A transformation adjusts the region proposal (blue) to improve alignment. We can see the resultant output box after the transformation as well (orange)}{figure.caption.915}{}}
\@writefile{toc}{\contentsline {paragraph}{Why a Logarithmic Transformation?}{470}{section*.916}\protected@file@percent }
\BKM@entry{id=498,dest={73756273656374696F6E2E31332E332E32},srcline={298}}{5C3337365C3337375C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030525C3030302D5C303030435C3030304E5C3030304E}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.3.2}Training R-CNN}{471}{subsection.13.3.2}\protected@file@percent }
\newlabel{subsubsec:training_rcnn}{{13.3.2}{471}{Training R-CNN}{subsection.13.3.2}{}}
\@writefile{toc}{\contentsline {paragraph}{1) Collecting Positive and Negative Examples}{471}{section*.917}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13.12}{\ignorespaces \textbf  {Positive vs.\ Negative vs.\ Neutral Region Proposals.} An example image (green bounding boxes are ground-truth). Each region proposal is categorized as: \textbf  {Positive} (blue) if its IoU with a ground-truth box is above 0.5, \textbf  {Negative} (red) if its IoU is below 0.3, \textbf  {Neutral} (gray) otherwise. Neutral proposals are typically ignored when training SVMs or fine-tuning, thus avoiding ambiguous overlap ranges.}}{471}{figure.caption.918}\protected@file@percent }
\newlabel{fig:chapter13_slide73}{{13.12}{471}{\textbf {Positive vs.\ Negative vs.\ Neutral Region Proposals.} An example image (green bounding boxes are ground-truth). Each region proposal is categorized as: \textbf {Positive} (blue) if its IoU with a ground-truth box is above 0.5, \textbf {Negative} (red) if its IoU is below 0.3, \textbf {Neutral} (gray) otherwise. Neutral proposals are typically ignored when training SVMs or fine-tuning, thus avoiding ambiguous overlap ranges}{figure.caption.918}{}}
\@writefile{toc}{\contentsline {paragraph}{2) Fine-Tuning the CNN on Region Proposals (Classification Only)}{471}{section*.919}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3) Training the Bounding Box Regressors}{472}{section*.920}\protected@file@percent }
\newlabel{paragraph:training_bbox_regressors}{{13.3.2}{472}{3) Training the Bounding Box Regressors}{section*.920}{}}
\@writefile{toc}{\contentsline {paragraph}{4) Forming the Final Detector}{473}{section*.921}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Training Considerations for Object Detection}{473}{section*.922}\protected@file@percent }
\BKM@entry{id=499,dest={73756273656374696F6E2E31332E332E33},srcline={414}}{5C3337365C3337375C303030535C303030655C3030306C5C303030655C303030635C303030745C303030695C3030306E5C303030675C3030305C3034305C303030465C303030695C3030306E5C303030615C3030306C5C3030305C3034305C303030505C303030725C303030655C303030645C303030695C303030635C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=500,dest={73656374696F6E2E31332E34},srcline={428}}{5C3337365C3337375C3030304E5C3030306F5C3030306E5C3030302D5C3030304D5C303030615C303030785C303030695C3030306D5C303030755C3030306D5C3030305C3034305C303030535C303030755C303030705C303030705C303030725C303030655C303030735C303030735C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C3030304E5C3030304D5C303030535C3030305C303531}
\BKM@entry{id=501,dest={73756273656374696F6E2E31332E342E31},srcline={431}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C3030304E5C303030655C303030655C303030645C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304E5C3030304D5C30303053}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.3.3}Selecting Final Predictions for Object Detection}{474}{subsection.13.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {13.4}Non-Maximum Suppression (NMS)}{474}{section.13.4}\protected@file@percent }
\newlabel{subsec:chapter13_nms}{{13.4}{474}{Non-Maximum Suppression (NMS)}{section.13.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.4.1}Motivation: The Need for NMS}{474}{subsection.13.4.1}\protected@file@percent }
\newlabel{subsec:nms_motivation}{{13.4.1}{474}{Motivation: The Need for NMS}{subsection.13.4.1}{}}
\BKM@entry{id=502,dest={73756273656374696F6E2E31332E342E32},srcline={440}}{5C3337365C3337375C3030304E5C3030304D5C303030535C3030305C3034305C303030415C3030306C5C303030675C3030306F5C303030725C303030695C303030745C303030685C3030306D}
\BKM@entry{id=503,dest={73756273656374696F6E2E31332E342E33},srcline={453}}{5C3337365C3337375C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030303A5C3030305C3034305C303030535C303030745C303030655C303030705C3030302D5C303030625C303030795C3030302D5C303030535C303030745C303030655C303030705C3030305C3034305C303030455C303030785C303030655C303030635C303030755C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.4.2}NMS Algorithm}{475}{subsection.13.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.4.3}Example: Step-by-Step Execution}{475}{subsection.13.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13.13}{\ignorespaces \textbf  {Step 1: Selecting the highest-scoring bounding box and comparing IoUs.} The blue box (\(P(\text  {dog})=0.9\)) is selected first. The orange box (\(P(\text  {dog})=0.8\)) has an \(\text  {IoU} = 0.78\), which exceeds the threshold (0.7), so it is removed. Other boxes with lower IoU remain.}}{475}{figure.caption.923}\protected@file@percent }
\newlabel{fig:chapter13_nms_step1}{{13.13}{475}{\textbf {Step 1: Selecting the highest-scoring bounding box and comparing IoUs.} The blue box (\(P(\text {dog})=0.9\)) is selected first. The orange box (\(P(\text {dog})=0.8\)) has an \(\text {IoU} = 0.78\), which exceeds the threshold (0.7), so it is removed. Other boxes with lower IoU remain}{figure.caption.923}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.14}{\ignorespaces \textbf  {Step 2: Processing the next highest-scoring box.} The purple box is selected next. The yellow box has \(\text  {IoU} = 0.74\) with the purple box, which exceeds the threshold, so it is removed.}}{475}{figure.caption.924}\protected@file@percent }
\newlabel{fig:chapter13_nms_step2}{{13.14}{475}{\textbf {Step 2: Processing the next highest-scoring box.} The purple box is selected next. The yellow box has \(\text {IoU} = 0.74\) with the purple box, which exceeds the threshold, so it is removed}{figure.caption.924}{}}
\BKM@entry{id=504,dest={73756273656374696F6E2E31332E342E34},srcline={475}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304E5C3030304D5C30303053}
\BKM@entry{id=505,dest={73756273656374696F6E2E31332E342E35},srcline={486}}{5C3337365C3337375C303030525C303030655C303030665C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030304E5C3030304D5C303030535C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304F5C303030765C303030655C303030725C3030306C5C303030615C303030705C303030705C303030695C3030306E5C303030675C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C30303073}
\BKM@entry{id=506,dest={73656374696F6E2E31332E35},srcline={497}}{5C3337365C3337375C303030455C303030765C303030615C3030306C5C303030755C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C3030306F5C303030725C303030735C3030303A5C3030305C3034305C3030304D5C303030655C303030615C3030306E5C3030305C3034305C303030415C303030765C303030655C303030725C303030615C303030675C303030655C3030305C3034305C303030505C303030725C303030655C303030635C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C3030306D5C303030415C303030505C3030305C303531}
\BKM@entry{id=507,dest={73756273656374696F6E2E31332E352E31},srcline={502}}{5C3337365C3337375C3030304B5C303030655C303030795C3030305C3034305C303030455C303030765C303030615C3030306C5C303030755C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C303030655C303030745C303030725C303030695C303030635C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.4.4}Limitations of NMS}{476}{subsection.13.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13.15}{\ignorespaces \textbf  {NMS Failure Case: Highly Overlapping Objects.} When objects are close together, NMS may incorrectly eliminate valid detections, reducing detection accuracy.}}{476}{figure.caption.925}\protected@file@percent }
\newlabel{fig:chapter13_nms_failure}{{13.15}{476}{\textbf {NMS Failure Case: Highly Overlapping Objects.} When objects are close together, NMS may incorrectly eliminate valid detections, reducing detection accuracy}{figure.caption.925}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.4.5}Refining NMS for Overlapping Objects}{476}{subsection.13.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {13.5}Evaluating Object Detectors: Mean Average Precision (mAP)}{476}{section.13.5}\protected@file@percent }
\newlabel{sec:chapter13_map}{{13.5}{476}{Evaluating Object Detectors: Mean Average Precision (mAP)}{section.13.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.5.1}Key Evaluation Metrics}{477}{subsection.13.5.1}\protected@file@percent }
\newlabel{subsec:chapter13_eval_metrics}{{13.5.1}{477}{Key Evaluation Metrics}{subsection.13.5.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Precision and Recall}{477}{section*.926}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Trade-offs Between Precision and Recall}}{477}{section*.927}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Isn't F1 Score Suffice?}}{477}{section*.928}\protected@file@percent }
\BKM@entry{id=508,dest={73756273656374696F6E2E31332E352E32},srcline={567}}{5C3337365C3337375C303030535C303030745C303030655C303030705C3030302D5C303030625C303030795C3030302D5C303030535C303030745C303030655C303030705C3030305C3034305C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030303A5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030695C3030306E5C303030675C3030305C3034305C303030415C303030505C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030615C3030305C3034305C303030535C303030695C3030306E5C303030675C3030306C5C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C30303073}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Precision-Recall (PR) Curve and Average Precision (AP)}}{478}{section*.929}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Why the 0.5 IoU Threshold?}}{478}{section*.930}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Why AP is Preferable to the F1 Score:}}{478}{section*.931}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.5.2}Step-by-Step Example: Computing AP for a Single Class}{479}{subsection.13.5.2}\protected@file@percent }
\newlabel{subsec:chapter13_ap_example}{{13.5.2}{479}{Step-by-Step Example: Computing AP for a Single Class}{subsection.13.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.16}{\ignorespaces \textbf  {Step 1: First match.} The highest-scoring detection correctly matches a ground-truth box.}}{479}{figure.caption.932}\protected@file@percent }
\newlabel{fig:chapter13_slide86}{{13.16}{479}{\textbf {Step 1: First match.} The highest-scoring detection correctly matches a ground-truth box}{figure.caption.932}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.17}{\ignorespaces \textbf  {Step 2: Second match.} The second highest detection correctly matches another ground-truth box.}}{479}{figure.caption.933}\protected@file@percent }
\newlabel{fig:chapter13_slide87}{{13.17}{479}{\textbf {Step 2: Second match.} The second highest detection correctly matches another ground-truth box}{figure.caption.933}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.18}{\ignorespaces \textbf  {Step 3: False Positive.} A detection fails to match any ground-truth box.}}{480}{figure.caption.934}\protected@file@percent }
\newlabel{fig:chapter13_slide88}{{13.18}{480}{\textbf {Step 3: False Positive.} A detection fails to match any ground-truth box}{figure.caption.934}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.19}{\ignorespaces \textbf  {Step 4: Another False Positive.} More false detections further lower precision.}}{480}{figure.caption.935}\protected@file@percent }
\newlabel{fig:chapter13_slide89}{{13.19}{480}{\textbf {Step 4: Another False Positive.} More false detections further lower precision}{figure.caption.935}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.20}{\ignorespaces \textbf  {Final Step: All ground-truth boxes matched.} Recall reaches 1.0, while the precision ends up being $3/5=0.6$.}}{481}{figure.caption.936}\protected@file@percent }
\newlabel{fig:chapter13_slide90}{{13.20}{481}{\textbf {Final Step: All ground-truth boxes matched.} Recall reaches 1.0, while the precision ends up being $3/5=0.6$}{figure.caption.936}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.21}{\ignorespaces The final AP score for the dog class is the area under the precision-recall curve, summing up to 0.87 in this case, which is pretty decent but can be further improved if we'll the reduce false-positives.}}{481}{figure.caption.937}\protected@file@percent }
\newlabel{fig:chapter13_slide91}{{13.21}{481}{The final AP score for the dog class is the area under the precision-recall curve, summing up to 0.87 in this case, which is pretty decent but can be further improved if we'll the reduce false-positives}{figure.caption.937}{}}
\BKM@entry{id=509,dest={73756273656374696F6E2E31332E352E33},srcline={647}}{5C3337365C3337375C3030304D5C303030655C303030615C3030306E5C3030305C3034305C303030415C303030765C303030655C303030725C303030615C303030675C303030655C3030305C3034305C303030505C303030725C303030655C303030635C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C3030306D5C303030415C303030505C3030305C303531}
\BKM@entry{id=510,dest={73756273656374696F6E2E31332E352E34},srcline={665}}{5C3337365C3337375C303030435C3030304F5C303030435C3030304F5C3030305C3034305C3030306D5C303030415C303030505C3030303A5C3030305C3034305C303030415C3030305C3034305C303030535C303030745C303030725C303030695C303030635C303030745C303030655C303030725C3030305C3034305C3030304D5C303030655C303030615C303030735C303030755C303030725C30303065}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.5.3}Mean Average Precision (mAP)}{482}{subsection.13.5.3}\protected@file@percent }
\newlabel{subsec:chapter13_map}{{13.5.3}{482}{Mean Average Precision (mAP)}{subsection.13.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.5.4}COCO mAP: A Stricter Measure}{482}{subsection.13.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{COCO mAP for Different Object Sizes}{482}{section*.938}\protected@file@percent }
\newlabel{subsec:chapter13_coco_map_sizes}{{13.5.4}{482}{COCO mAP for Different Object Sizes}{section*.938}{}}
\@writefile{lot}{\contentsline {table}{\numberline {13.1}{\ignorespaces COCO mAP computed for different object size categories. The choice of which category to prioritize depends on the specific use case of the detection system.}}{482}{table.caption.939}\protected@file@percent }
\newlabel{tab:coco_ap_object_sizes}{{13.1}{482}{COCO mAP computed for different object size categories. The choice of which category to prioritize depends on the specific use case of the detection system}{table.caption.939}{}}
\BKM@entry{id=511,dest={73756273656374696F6E2E31332E352E35},srcline={767}}{5C3337365C3337375C303030455C303030765C303030615C3030306C5C303030755C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C3030306F5C303030725C303030735C3030303A5C3030305C3034305C3030304B5C303030655C303030795C3030305C3034305C303030545C303030615C3030306B5C303030655C303030615C303030775C303030615C303030795C30303073}
\@writefile{toc}{\contentsline {paragraph}{When and Why Object Size-Specific mAP Matters}{483}{section*.940}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Prioritizing Specific Classes in Object Detection}{483}{section*.941}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.5.5}Evaluating Object Detectors: Key Takeaways}{484}{subsection.13.5.5}\protected@file@percent }
\BKM@entry{id=512,dest={73656374696F6E2A2E393432},srcline={782}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030335C3030302E5C303030355C3030302E5C303030365C3030303A5C3030305C3034305C3030304D5C3030306F5C303030735C303030615C303030695C303030635C3030305C3034305C303030415C303030755C303030675C3030306D5C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{bochkovskiy2020_yolov4}
\abx@aux@segm{0}{0}{bochkovskiy2020_yolov4}
\abx@aux@cite{0}{chen2020_gpt_pixels}
\abx@aux@segm{0}{0}{chen2020_gpt_pixels}
\abx@aux@cite{0}{bochkovskiy2020_yolov4}
\abx@aux@segm{0}{0}{bochkovskiy2020_yolov4}
\abx@aux@cite{0}{bochkovskiy2020_yolov4}
\abx@aux@segm{0}{0}{bochkovskiy2020_yolov4}
\@writefile{toc}{\contentsline {subsection}{Enrichment 13.5.6: Mosaic Augmentation for Object Detection}{485}{section*.942}\protected@file@percent }
\newlabel{enrichment:mosaic_for_object_detection}{{13.5.6}{485}{\color {ocre}Enrichment \thesubsection : Mosaic Augmentation for Object Detection}{section*.942}{}}
\abx@aux@backref{338}{bochkovskiy2020_yolov4}{0}{485}{485}
\abx@aux@backref{339}{chen2020_gpt_pixels}{0}{485}{485}
\@writefile{toc}{\contentsline {paragraph}{Motivation and Advantages}{485}{section*.943}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13.22}{\ignorespaces Mosaic augmentation in YOLOv4. Source: \blx@tocontentsinit {0}\cite {bochkovskiy2020_yolov4}. Four input images are stitched into a \(2 \times 2\) grid, allowing object instances to appear in unfamiliar contexts and at smaller scales. Unlike CutMix, which blends only two images, Mosaic mixes four scenes simultaneously, encouraging robustness to background variation and spatial layout. Notably, this design enables BatchNorm to compute statistics over four different images per batch, reducing the need for large batch sizes.}}{485}{figure.caption.944}\protected@file@percent }
\abx@aux@backref{341}{bochkovskiy2020_yolov4}{0}{485}{485}
\newlabel{fig:chapter13_yolo_mosaic}{{13.22}{485}{Mosaic augmentation in YOLOv4. Source: \cite {bochkovskiy2020_yolov4}. Four input images are stitched into a \(2 \times 2\) grid, allowing object instances to appear in unfamiliar contexts and at smaller scales. Unlike CutMix, which blends only two images, Mosaic mixes four scenes simultaneously, encouraging robustness to background variation and spatial layout. Notably, this design enables BatchNorm to compute statistics over four different images per batch, reducing the need for large batch sizes}{figure.caption.944}{}}
\@writefile{toc}{\contentsline {paragraph}{Implementation Considerations}{485}{section*.945}\protected@file@percent }
\abx@aux@cite{0}{bochkovskiy2020_yolov4}
\abx@aux@segm{0}{0}{bochkovskiy2020_yolov4}
\abx@aux@cite{0}{bochkovskiy2020_yolov4}
\abx@aux@segm{0}{0}{bochkovskiy2020_yolov4}
\@writefile{lof}{\contentsline {figure}{\numberline {13.23}{\ignorespaces Data augmentation strategies used in YOLOv4. Source: \blx@tocontentsinit {0}\cite {bochkovskiy2020_yolov4}. These include \textbf  {bilateral blurring}, \textbf  {MixUp}, \textbf  {CutMix}, and \textbf  {Mosaic}. These augmentations enhance the model’s robustness to varying object scales, positions, and backgrounds.}}{486}{figure.caption.946}\protected@file@percent }
\abx@aux@backref{343}{bochkovskiy2020_yolov4}{0}{486}{486}
\newlabel{fig:chapter13_yolo_augmentations}{{13.23}{486}{Data augmentation strategies used in YOLOv4. Source: \cite {bochkovskiy2020_yolov4}. These include \textbf {bilateral blurring}, \textbf {MixUp}, \textbf {CutMix}, and \textbf {Mosaic}. These augmentations enhance the model’s robustness to varying object scales, positions, and backgrounds}{figure.caption.946}{}}
\@writefile{toc}{\contentsline {paragraph}{Domain-Dependent Utility}{486}{section*.947}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{486}{section*.948}\protected@file@percent }
\BKM@entry{id=513,dest={636861707465722E3134},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030315C303030345C3030303A5C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C3030306F5C303030725C30303073}
\BKM@entry{id=514,dest={73656374696F6E2E31342E31},srcline={10}}{5C3337365C3337375C303030425C303030655C303030795C3030306F5C3030306E5C303030645C3030305C3034305C303030525C3030302D5C303030435C3030304E5C3030304E5C3030303A5C3030305C3034305C303030415C303030645C303030765C303030615C3030306E5C303030635C303030695C3030306E5C303030675C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=515,dest={73756273656374696F6E2E31342E312E31},srcline={17}}{5C3337365C3337375C3030304C5C3030306F5C3030306F5C3030306B5C303030695C3030306E5C303030675C3030305C3034305C303030415C303030685C303030655C303030615C303030645C3030303A5C3030305C3034305C303030425C303030655C303030795C3030306F5C3030306E5C303030645C3030305C3034305C303030435C3030304E5C3030304E5C3030302D5C303030425C303030615C303030735C303030655C303030645C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C3030306F5C303030725C30303073}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\abx@aux@cite{0}{zhang2022_dino}
\abx@aux@segm{0}{0}{zhang2022_dino}
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\@writefile{toc}{\contentsline {chapter}{\numberline {14}Lecture 14: Object Detectors}{487}{chapter.14}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@13}}
\ttl@writefile{ptc}{\ttl@starttoc{default@14}}
\pgfsyspdfmark {pgfid65}{0}{52099153}
\pgfsyspdfmark {pgfid64}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {14.1}Beyond R-CNN: Advancing Object Detection}{487}{section.14.1}\protected@file@percent }
\newlabel{sec:chapter14_intro}{{14.1}{487}{Beyond R-CNN: Advancing Object Detection}{section.14.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.1.1}Looking Ahead: Beyond CNN-Based Object Detectors}{487}{subsection.14.1.1}\protected@file@percent }
\newlabel{subsec:chapter14_future_object_detection}{{14.1.1}{487}{Looking Ahead: Beyond CNN-Based Object Detectors}{subsection.14.1.1}{}}
\abx@aux@backref{344}{carion2020_detr}{0}{487}{487}
\abx@aux@backref{345}{zhang2022_dino}{0}{487}{487}
\abx@aux@backref{346}{oquab2023_dinov2}{0}{487}{487}
\BKM@entry{id=516,dest={73656374696F6E2E31342E32},srcline={40}}{5C3337365C3337375C303030465C303030615C303030735C303030745C3030305C3034305C303030525C3030302D5C303030435C3030304E5C3030304E5C3030303A5C3030305C3034305C303030415C303030635C303030635C303030655C3030306C5C303030655C303030725C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{girshick2015_fastrcnn}
\abx@aux@segm{0}{0}{girshick2015_fastrcnn}
\BKM@entry{id=517,dest={73756273656374696F6E2E31342E322E31},srcline={47}}{5C3337365C3337375C3030304B5C303030655C303030795C3030305C3034305C303030495C303030645C303030655C303030615C3030303A5C3030305C3034305C303030535C303030685C303030615C303030725C303030655C303030645C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030455C303030785C303030745C303030725C303030615C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {14.2}Fast R-CNN: Accelerating Object Detection}{488}{section.14.2}\protected@file@percent }
\newlabel{sec:chapter14_fast_rcnn}{{14.2}{488}{Fast R-CNN: Accelerating Object Detection}{section.14.2}{}}
\abx@aux@backref{347}{girshick2015_fastrcnn}{0}{488}{488}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.2.1}Key Idea: Shared Feature Extraction}{488}{subsection.14.2.1}\protected@file@percent }
\newlabel{subsec:chapter14_fast_rcnn_idea}{{14.2.1}{488}{Key Idea: Shared Feature Extraction}{subsection.14.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.1}{\ignorespaces \textbf  {Fast R-CNN architecture:} A backbone CNN processes the full image, generating a feature map. RoI Pooling extracts regions from this shared representation, followed by classification and bounding box refinement. This significantly improves efficiency while maintaining detection accuracy.}}{488}{figure.caption.949}\protected@file@percent }
\newlabel{fig:chapter14_fast_rcnn}{{14.1}{488}{\textbf {Fast R-CNN architecture:} A backbone CNN processes the full image, generating a feature map. RoI Pooling extracts regions from this shared representation, followed by classification and bounding box refinement. This significantly improves efficiency while maintaining detection accuracy}{figure.caption.949}{}}
\BKM@entry{id=518,dest={73756273656374696F6E2E31342E322E32},srcline={62}}{5C3337365C3337375C303030555C303030735C303030695C3030306E5C303030675C3030305C3034305C303030465C303030755C3030306C5C3030306C5C303030795C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C303030425C303030615C303030635C3030306B5C303030625C3030306F5C3030306E5C303030655C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030455C303030785C303030745C303030725C303030615C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.2.2}Using Fully Convolutional Deep Backbones for Feature Extraction}{489}{subsection.14.2.2}\protected@file@percent }
\newlabel{subsec:chapter14_fast_rcnn_backbone}{{14.2.2}{489}{Using Fully Convolutional Deep Backbones for Feature Extraction}{subsection.14.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.2}{\ignorespaces \textbf  {AlexNet as a backbone:} Early implementations of Fast R-CNN explored the use of AlexNet for feature extraction. Only the last two FC layers were used for the per-region network.}}{489}{figure.caption.950}\protected@file@percent }
\newlabel{fig:chapter14_alexnet}{{14.2}{489}{\textbf {AlexNet as a backbone:} Early implementations of Fast R-CNN explored the use of AlexNet for feature extraction. Only the last two FC layers were used for the per-region network}{figure.caption.950}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.3}{\ignorespaces \textbf  {ResNet as a backbone:} More modern implementations utilize ResNet for feature extraction, leveraging deeper architectures for improved accuracy. In this case, only the last stage of the network was used for the per-region network, while the rest of the network was used as a backbone deriving features from the entire image.}}{489}{figure.caption.951}\protected@file@percent }
\newlabel{fig:chapter14_resnet}{{14.3}{489}{\textbf {ResNet as a backbone:} More modern implementations utilize ResNet for feature extraction, leveraging deeper architectures for improved accuracy. In this case, only the last stage of the network was used for the per-region network, while the rest of the network was used as a backbone deriving features from the entire image}{figure.caption.951}{}}
\BKM@entry{id=519,dest={73756273656374696F6E2E31342E322E33},srcline={87}}{5C3337365C3337375C303030525C303030655C303030675C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C303030495C3030306E5C303030745C303030655C303030725C303030655C303030735C303030745C3030305C3034305C3030305C3035305C303030525C3030306F5C303030495C3030305C3035315C3030305C3034305C303030505C3030306F5C3030306F5C3030306C5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.2.3}Region of Interest (RoI) Pooling}{490}{subsection.14.2.3}\protected@file@percent }
\newlabel{subsec:chapter14_roi_pooling}{{14.2.3}{490}{Region of Interest (RoI) Pooling}{subsection.14.2.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Mapping Region Proposals onto the Feature Map}{490}{section*.952}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Dividing the Region into Fixed Bins}{490}{section*.953}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Max Pooling within Each Bin}{490}{section*.954}\protected@file@percent }
\abx@aux@cite{0}{patnaik2020_roi_pool}
\abx@aux@segm{0}{0}{patnaik2020_roi_pool}
\abx@aux@cite{0}{erdem2020_RoIAlign}
\abx@aux@segm{0}{0}{erdem2020_RoIAlign}
\abx@aux@cite{0}{erdem2020_RoIAlign}
\abx@aux@segm{0}{0}{erdem2020_RoIAlign}
\@writefile{lof}{\contentsline {figure}{\numberline {14.4}{\ignorespaces \textbf  {RoI Pooling Process.} Each region proposal is mapped onto the feature map, divided into fixed bins, and max-pooled to a fixed output size for classification and bounding box refinement.}}{491}{figure.caption.955}\protected@file@percent }
\newlabel{fig:chapter14_roi_pooling}{{14.4}{491}{\textbf {RoI Pooling Process.} Each region proposal is mapped onto the feature map, divided into fixed bins, and max-pooled to a fixed output size for classification and bounding box refinement}{figure.caption.955}{}}
\@writefile{toc}{\contentsline {paragraph}{Summary: Key Steps in RoI Pooling}{491}{section*.956}\protected@file@percent }
\abx@aux@backref{348}{patnaik2020_roi_pool}{0}{491}{491}
\@writefile{toc}{\contentsline {paragraph}{Limitations of RoI Pooling}{491}{section*.957}\protected@file@percent }
\BKM@entry{id=520,dest={73756273656374696F6E2E31342E322E34},srcline={145}}{5C3337365C3337375C303030525C3030306F5C303030495C303030415C3030306C5C303030695C303030675C3030306E}
\abx@aux@cite{0}{patnaik2020_roi_pool}
\abx@aux@segm{0}{0}{patnaik2020_roi_pool}
\@writefile{lof}{\contentsline {figure}{\numberline {14.5}{\ignorespaces Impact of quantization in RoI Pooling. When mapping a region proposal onto the feature map (red), quantization (orange) can result in the loss of relevant object information (highlighted in \emph  {dark blue}) while also introducing unwanted features from adjacent areas (\emph  {green}). This misalignment reduces localization precision, as certain parts of the object may be omitted, while non-object features may be included in the pooled representation. Figure taken from \blx@tocontentsinit {0}\cite {erdem2020_RoIAlign}.}}{492}{figure.caption.958}\protected@file@percent }
\abx@aux@backref{350}{erdem2020_RoIAlign}{0}{492}{492}
\newlabel{fig:chapter14_roi_pooling_downside}{{14.5}{492}{Impact of quantization in RoI Pooling. When mapping a region proposal onto the feature map (red), quantization (orange) can result in the loss of relevant object information (highlighted in \emph {dark blue}) while also introducing unwanted features from adjacent areas (\emph {green}). This misalignment reduces localization precision, as certain parts of the object may be omitted, while non-object features may be included in the pooled representation. Figure taken from \cite {erdem2020_RoIAlign}}{figure.caption.958}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.2.4}RoIAlign}{492}{subsection.14.2.4}\protected@file@percent }
\newlabel{subsubsec:roi_align_intro}{{14.2.4}{492}{RoIAlign}{subsection.14.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{RoIAlign: A Visual Example}{493}{section*.959}\protected@file@percent }
\newlabel{subsubsec:roi_align_example}{{14.2.4}{493}{RoIAlign: A Visual Example}{section*.959}{}}
\abx@aux@backref{351}{patnaik2020_roi_pool}{0}{493}{493}
\@writefile{toc}{\contentsline {paragraph}{Step 1: Projection of Region Proposal onto the Feature Map}{493}{section*.960}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.6}{\ignorespaces Projection of the region proposal onto the feature map, dividing it into $2 \times 2$ bins.}}{493}{figure.caption.961}\protected@file@percent }
\newlabel{fig:chapter14_roi_align_projection}{{14.6}{493}{Projection of the region proposal onto the feature map, dividing it into $2 \times 2$ bins}{figure.caption.961}{}}
\@writefile{toc}{\contentsline {paragraph}{Step 2: Selecting Interpolation Points in Each Bin}{493}{section*.962}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.7}{\ignorespaces Selection of four interpolation points in each sub-region for bilinear interpolation.}}{494}{figure.caption.963}\protected@file@percent }
\newlabel{fig:chapter14_roi_align_interpolation_points}{{14.7}{494}{Selection of four interpolation points in each sub-region for bilinear interpolation}{figure.caption.963}{}}
\@writefile{toc}{\contentsline {subparagraph}{Why Choose 0.25 and 0.75 for Sampling?}{494}{subparagraph*.964}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 3: Mapping Sampled Points onto the Feature Grid}{495}{section*.965}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.8}{\ignorespaces Mapping of the selected interpolation points onto the discrete grid of the feature map. Each sampled point is enclosed by four neighboring grid points, which will be used in bilinear interpolation.}}{495}{figure.caption.966}\protected@file@percent }
\newlabel{fig:chapter14_roi_align_interpolation_grid}{{14.8}{495}{Mapping of the selected interpolation points onto the discrete grid of the feature map. Each sampled point is enclosed by four neighboring grid points, which will be used in bilinear interpolation}{figure.caption.966}{}}
\@writefile{toc}{\contentsline {paragraph}{Step 4: Computing Bilinear Interpolation Weights}{496}{section*.967}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Normalization Constant and Its Interpretation}{496}{subparagraph*.968}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Weight Computation for Each Corner}{496}{subparagraph*.969}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.9}{\ignorespaces Computing interpolation weight for the top-left corner ($w_a$). Since the sampled point is far from this corner, its weight is relatively low: ($w_a=0.1$).}}{497}{figure.caption.970}\protected@file@percent }
\newlabel{fig:chapter14_roi_align_interpolation_weight_a}{{14.9}{497}{Computing interpolation weight for the top-left corner ($w_a$). Since the sampled point is far from this corner, its weight is relatively low: ($w_a=0.1$)}{figure.caption.970}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.10}{\ignorespaces Computing interpolation weight for the top-right corner ($w_c$). Since this point is equidistant from $w_a$, the weights are equal ($w_a = w_c = 0.1$).}}{498}{figure.caption.971}\protected@file@percent }
\newlabel{fig:chapter14_roi_align_interpolation_weight_c}{{14.10}{498}{Computing interpolation weight for the top-right corner ($w_c$). Since this point is equidistant from $w_a$, the weights are equal ($w_a = w_c = 0.1$)}{figure.caption.971}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.11}{\ignorespaces Computing interpolation weight for the bottom-left corner ($w_b$). Since the sampled point is much closer to this corner, its weight is significantly higher: ($w_b=0.4$).}}{498}{figure.caption.972}\protected@file@percent }
\newlabel{fig:chapter14_roi_align_interpolation_weight_b}{{14.11}{498}{Computing interpolation weight for the bottom-left corner ($w_b$). Since the sampled point is much closer to this corner, its weight is significantly higher: ($w_b=0.4$)}{figure.caption.972}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.12}{\ignorespaces Computing interpolation weight for the bottom-right corner ($w_d$). This weight is identical to $w_b$, because the sampled point $(x,y)$ is symmetrically placed between $b, d$.}}{499}{figure.caption.973}\protected@file@percent }
\newlabel{fig:chapter14_roi_align_interpolation_weight_d}{{14.12}{499}{Computing interpolation weight for the bottom-right corner ($w_d$). This weight is identical to $w_b$, because the sampled point $(x,y)$ is symmetrically placed between $b, d$}{figure.caption.973}{}}
\@writefile{toc}{\contentsline {paragraph}{Step 5: Computing the Interpolated Feature Value}{499}{section*.974}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{\textbf  {Example Computation}}{499}{subparagraph*.975}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 6: Aggregating Interpolated Values}{500}{section*.976}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{\textbf  {Final Output}}{500}{subparagraph*.977}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.13}{\ignorespaces Final RoIAlign result: Each bin's value is determined via bilinear interpolation and pooling.}}{500}{figure.caption.978}\protected@file@percent }
\newlabel{fig:chapter14_roi_align_final}{{14.13}{500}{Final RoIAlign result: Each bin's value is determined via bilinear interpolation and pooling}{figure.caption.978}{}}
\@writefile{toc}{\contentsline {paragraph}{Key Takeaways}{500}{section*.979}\protected@file@percent }
\abx@aux@cite{0}{patnaik2020_roi_pool}
\abx@aux@segm{0}{0}{patnaik2020_roi_pool}
\@writefile{toc}{\contentsline {paragraph}{RoIAlign Important Implementation Parts in PyTorch}{501}{section*.980}\protected@file@percent }
\abx@aux@backref{352}{patnaik2020_roi_pool}{0}{501}{501}
\BKM@entry{id=521,dest={73656374696F6E2E31342E33},srcline={542}}{5C3337365C3337375C303030465C303030615C303030735C303030745C303030655C303030725C3030305C3034305C303030525C3030302D5C303030435C3030304E5C3030304E5C3030303A5C3030305C3034305C303030465C303030615C303030735C303030745C303030655C303030725C3030305C3034305C303030505C303030725C3030306F5C303030705C3030306F5C303030735C303030615C3030306C5C303030735C3030305C3034305C303030555C303030735C303030695C3030306E5C303030675C3030305C3034305C303030525C303030505C3030304E5C30303073}
\BKM@entry{id=522,dest={73756273656374696F6E2E31342E332E31},srcline={545}}{5C3337365C3337375C303030465C303030615C303030735C303030745C3030305C3034305C303030525C3030302D5C303030435C3030304E5C3030304E5C3030305C3034305C303030425C3030306F5C303030745C303030745C3030306C5C303030655C3030306E5C303030655C303030635C3030306B5C3030303A5C3030305C3034305C303030525C303030655C303030675C303030695C3030306F5C3030306E5C3030305C3034305C303030505C303030725C3030306F5C303030705C3030306F5C303030735C303030615C3030306C5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=523,dest={73756273656374696F6E2E31342E332E32},srcline={557}}{5C3337365C3337375C303030545C3030306F5C303030775C303030615C303030725C303030645C303030735C3030305C3034305C303030465C303030615C303030735C303030745C303030655C303030725C3030305C3034305C303030525C303030655C303030675C303030695C3030306F5C3030306E5C3030305C3034305C303030505C303030725C3030306F5C303030705C3030306F5C303030735C303030615C3030306C5C303030735C3030303A5C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030505C303030725C3030306F5C303030705C3030306F5C303030735C303030615C3030306C5C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030435C3030304E5C3030304E5C30303073}
\abx@aux@cite{0}{ren2016_fasterrcnn}
\abx@aux@segm{0}{0}{ren2016_fasterrcnn}
\BKM@entry{id=524,dest={73756273656374696F6E2E31342E332E33},srcline={569}}{5C3337365C3337375C303030525C303030655C303030675C303030695C3030306F5C3030306E5C3030305C3034305C303030505C303030725C3030306F5C303030705C3030306F5C303030735C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030525C303030505C3030304E5C303030735C3030305C303531}
\@writefile{toc}{\contentsline {section}{\numberline {14.3}Faster R-CNN: Faster Proposals Using RPNs}{504}{section.14.3}\protected@file@percent }
\newlabel{sec:chapter14_faster_rcnn}{{14.3}{504}{Faster R-CNN: Faster Proposals Using RPNs}{section.14.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.3.1}Fast R-CNN Bottleneck: Region Proposal Computation}{504}{subsection.14.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.14}{\ignorespaces Problem: Despite Fast R-CNN’s optimizations, runtime is still dominated by region proposal computation. Selective Search runs on the CPU and remains the slowest part of the pipeline.}}{504}{figure.caption.981}\protected@file@percent }
\newlabel{fig:chapter14_runtime_bottleneck}{{14.14}{504}{Problem: Despite Fast R-CNN’s optimizations, runtime is still dominated by region proposal computation. Selective Search runs on the CPU and remains the slowest part of the pipeline}{figure.caption.981}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.3.2}Towards Faster Region Proposals: Learning Proposals with CNNs}{504}{subsection.14.3.2}\protected@file@percent }
\abx@aux@backref{353}{ren2016_fasterrcnn}{0}{504}{504}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.3.3}Region Proposal Networks (RPNs)}{505}{subsection.14.3.3}\protected@file@percent }
\newlabel{subsec:chapter14_rpn}{{14.3.3}{505}{Region Proposal Networks (RPNs)}{subsection.14.3.3}{}}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {How RPNs Work}}{505}{section*.982}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Anchor Boxes: Handling Scale and Aspect Ratio Variations}}{505}{section*.983}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.15}{\ignorespaces Anchor boxes and their classification: Positive (green) anchors contain objects, while negative (red) anchors do not.}}{505}{figure.caption.984}\protected@file@percent }
\newlabel{fig:chapter14_rpn_anchor_classification}{{14.15}{505}{Anchor boxes and their classification: Positive (green) anchors contain objects, while negative (red) anchors do not}{figure.caption.984}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.16}{\ignorespaces Examples of $K$ anchor boxes at a single location, illustrating different sizes and aspect ratios.}}{506}{figure.caption.985}\protected@file@percent }
\newlabel{fig:chapter14_rpn_anchors_sizes}{{14.16}{506}{Examples of $K$ anchor boxes at a single location, illustrating different sizes and aspect ratios}{figure.caption.985}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.17}{\ignorespaces RPN predicting objectness scores and bounding box transforms for each anchor.}}{506}{figure.caption.986}\protected@file@percent }
\newlabel{fig:chapter14_rpn_predictions}{{14.17}{506}{RPN predicting objectness scores and bounding box transforms for each anchor}{figure.caption.986}{}}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Bounding Box Refinement: Aligning Anchors to Objects}}{507}{section*.987}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.18}{\ignorespaces For positive anchors (green), the RPN predicts a transformation (orange) that converts the anchor to the ground-truth bounding box (gold).}}{507}{figure.caption.988}\protected@file@percent }
\newlabel{fig:chapter14_rpn_box_transform}{{14.18}{507}{For positive anchors (green), the RPN predicts a transformation (orange) that converts the anchor to the ground-truth bounding box (gold)}{figure.caption.988}{}}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Training RPNs: Assigning Labels to Anchors}}{507}{section*.989}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Loss Function for RPN Training}}{508}{section*.990}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{\textbf  {Assigning Ground-Truth Bounding Boxes to Anchors}}{508}{subparagraph*.991}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Smooth \( L_1 \) Loss for Bounding Box Regression}}{508}{section*.992}\protected@file@percent }
\abx@aux@cite{0}{ren2015_fasterrcnn}
\abx@aux@segm{0}{0}{ren2015_fasterrcnn}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Why Use Negative Anchors?}}{509}{section*.993}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 14.3.3.1: Training Region Proposal Networks (RPNs)}{509}{section*.994}\protected@file@percent }
\newlabel{enrichment:rpn_training_pipeline}{{14.3.3.1}{509}{\color {ocre}Enrichment \thesubsubsection : Training Region Proposal Networks (RPNs)}{section*.994}{}}
\abx@aux@backref{354}{ren2015_fasterrcnn}{0}{509}{509}
\@writefile{toc}{\contentsline {paragraph}{1. Input Feature Map}{509}{section*.995}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. Sliding Window: Shared 3\(\times \)3 Conv}{509}{section*.996}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3. RPN Heads: Anchor-wise Classification and Regression}{509}{section*.997}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{4. Anchor Labeling and Ground Truth Assignment}{510}{section*.998}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{5. Bounding-Box Regression Targets}{510}{section*.999}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{6. Loss Computation}{510}{section*.1000}\protected@file@percent }
\abx@aux@cite{0}{ren2015_fasterrcnn}
\abx@aux@segm{0}{0}{ren2015_fasterrcnn}
\abx@aux@cite{0}{ren2015_fasterrcnn}
\abx@aux@segm{0}{0}{ren2015_fasterrcnn}
\@writefile{lof}{\contentsline {figure}{\numberline {14.19}{\ignorespaces Region Proposal Network and Example Detections}}{511}{figure.caption.1001}\protected@file@percent }
\abx@aux@backref{356}{ren2015_fasterrcnn}{0}{511}{511}
\newlabel{fig:training_rpn_and_rpn_detections}{{14.19}{511}{Region Proposal Network and Example Detections}{figure.caption.1001}{}}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Inference: Generating Region Proposals}}{511}{section*.1002}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\textbf  {RPNs Improve Region Proposal Generation}}{511}{section*.1003}\protected@file@percent }
\BKM@entry{id=525,dest={73756273656374696F6E2E31342E332E34},srcline={854}}{5C3337365C3337375C303030465C303030615C303030735C303030745C303030655C303030725C3030305C3034305C303030525C3030302D5C303030435C3030304E5C3030304E5C3030305C3034305C3030304C5C3030306F5C303030735C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030505C303030725C303030615C303030635C303030745C303030695C303030635C303030655C3030303A5C3030305C3034305C3030304A5C3030306F5C303030695C3030306E5C303030745C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030465C3030306F5C303030755C303030725C3030305C3034305C3030304C5C3030306F5C303030735C303030735C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.3.4}Faster R-CNN Loss in Practice: Joint Training with Four Losses}{512}{subsection.14.3.4}\protected@file@percent }
\newlabel{subsec:chapter14_faster_rcnn_loss}{{14.3.4}{512}{Faster R-CNN Loss in Practice: Joint Training with Four Losses}{subsection.14.3.4}{}}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {Joint Training in Faster R-CNN}}{512}{section*.1004}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\textbf  {How RPN Improves Inference Speed}}{512}{section*.1005}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.20}{\ignorespaces Comparison of inference time between R-CNN, SPP-Net, Fast R-CNN, and Faster R-CNN. RPN reduces the test-time speed from 2.3s in Fast R-CNN to 0.2s in Faster R-CNN.}}{512}{figure.caption.1006}\protected@file@percent }
\newlabel{fig:chapter14_faster_rcnn_speed_comparison}{{14.20}{512}{Comparison of inference time between R-CNN, SPP-Net, Fast R-CNN, and Faster R-CNN. RPN reduces the test-time speed from 2.3s in Fast R-CNN to 0.2s in Faster R-CNN}{figure.caption.1006}{}}
\BKM@entry{id=526,dest={73756273656374696F6E2E31342E332E35},srcline={893}}{5C3337365C3337375C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030505C303030795C303030725C303030615C3030306D5C303030695C303030645C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030465C303030505C3030304E5C303030735C3030305C3035315C3030303A5C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C3030302D5C303030535C303030635C303030615C3030306C5C303030655C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\abx@aux@cite{0}{lin2017_fpn}
\abx@aux@segm{0}{0}{lin2017_fpn}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.3.5}Feature Pyramid Networks (FPNs): Multi-Scale Feature Learning}{513}{subsection.14.3.5}\protected@file@percent }
\newlabel{subsec:chapter14_fpn}{{14.3.5}{513}{Feature Pyramid Networks (FPNs): Multi-Scale Feature Learning}{subsection.14.3.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.21}{\ignorespaces Illustration of the classic image pyramid approach, where the detector is applied to multiple resized versions of the image to improve small-object detection. However, this method is computationally expensive.}}{513}{figure.caption.1007}\protected@file@percent }
\newlabel{fig:chapter14_image_pyramid}{{14.21}{513}{Illustration of the classic image pyramid approach, where the detector is applied to multiple resized versions of the image to improve small-object detection. However, this method is computationally expensive}{figure.caption.1007}{}}
\@writefile{toc}{\contentsline {subsubsection}{Feature Pyramid Networks: A More Efficient Approach}{513}{section*.1008}\protected@file@percent }
\abx@aux@backref{357}{lin2017_fpn}{0}{513}{513}
\@writefile{lof}{\contentsline {figure}{\numberline {14.22}{\ignorespaces Applying object detectors at different stages of a CNN backbone. However, early-stage features suffer from limited receptive fields and lack access to high-level semantic information, reducing detection performance.}}{514}{figure.caption.1009}\protected@file@percent }
\newlabel{fig:chapter14_fpn_early_stages}{{14.22}{514}{Applying object detectors at different stages of a CNN backbone. However, early-stage features suffer from limited receptive fields and lack access to high-level semantic information, reducing detection performance}{figure.caption.1009}{}}
\@writefile{toc}{\contentsline {subsubsection}{Enhancing Low-Level Features with High-Level Semantics}{514}{section*.1010}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.23}{\ignorespaces Top-down feature fusion in Feature Pyramid Networks. High-level features are progressively upsampled and combined with low-level features to enhance their semantic richness before detection.}}{514}{figure.caption.1011}\protected@file@percent }
\newlabel{fig:chapter14_fpn_topdown}{{14.23}{514}{Top-down feature fusion in Feature Pyramid Networks. High-level features are progressively upsampled and combined with low-level features to enhance their semantic richness before detection}{figure.caption.1011}{}}
\@writefile{toc}{\contentsline {paragraph}{How Upsampling Works in FPNs}{515}{section*.1012}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Combining Results from Multiple Feature Levels}{515}{section*.1013}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Advantages of FPNs}{515}{section*.1014}\protected@file@percent }
\abx@aux@cite{0}{lin2018_focalloss}
\abx@aux@segm{0}{0}{lin2018_focalloss}
\abx@aux@cite{0}{tian2019_fcos}
\abx@aux@segm{0}{0}{tian2019_fcos}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\BKM@entry{id=527,dest={73656374696F6E2E31342E34},srcline={999}}{5C3337365C3337375C303030525C303030655C303030745C303030695C3030306E5C303030615C3030304E5C303030655C303030745C3030303A5C3030305C3034305C303030415C3030305C3034305C303030425C303030725C303030655C303030615C3030306B5C303030745C303030685C303030725C3030306F5C303030755C303030675C303030685C3030305C3034305C303030695C3030306E5C3030305C3034305C303030535C303030695C3030306E5C303030675C3030306C5C303030655C3030302D5C303030535C303030745C303030615C303030675C303030655C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{lin2018_focalloss}
\abx@aux@segm{0}{0}{lin2018_focalloss}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {The Two-Stage Object Detection Pipeline}}{516}{section*.1015}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.24}{\ignorespaces Visualization of Faster R-CNN as a two-stage object detector. The \textbf  {first stage} (blue) generates region proposals, while the \textbf  {second stage} (green) classifies objects and refines the proposals.}}{516}{figure.caption.1016}\protected@file@percent }
\newlabel{fig:chapter14_faster_rcnn_pipeline}{{14.24}{516}{Visualization of Faster R-CNN as a two-stage object detector. The \textbf {first stage} (blue) generates region proposals, while the \textbf {second stage} (green) classifies objects and refines the proposals}{figure.caption.1016}{}}
\abx@aux@backref{358}{lin2018_focalloss}{0}{516}{516}
\abx@aux@backref{359}{tian2019_fcos}{0}{516}{516}
\abx@aux@backref{360}{carion2020_detr}{0}{516}{516}
\BKM@entry{id=528,dest={73756273656374696F6E2E31342E342E31},srcline={1004}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030535C303030695C3030306E5C303030675C3030306C5C303030655C3030302D5C303030535C303030745C303030615C303030675C303030655C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C3030306F5C303030725C303030735C3030305C3034305C303030435C303030615C3030306E5C3030305C3034305C303030425C303030655C3030305C3034305C303030465C303030615C303030735C303030745C303030655C30303072}
\BKM@entry{id=529,dest={73756273656374696F6E2E31342E342E32},srcline={1021}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C3030305C3034305C303030495C3030306D5C303030625C303030615C3030306C5C303030615C3030306E5C303030635C303030655C3030305C3034305C303030505C303030725C3030306F5C303030625C3030306C5C303030655C3030306D5C3030305C3034305C303030695C3030306E5C3030305C3034305C303030445C303030655C3030306E5C303030735C303030655C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {14.4}RetinaNet: A Breakthrough in Single-Stage Object Detection}{517}{section.14.4}\protected@file@percent }
\newlabel{subsec:chapter14_retinanet}{{14.4}{517}{RetinaNet: A Breakthrough in Single-Stage Object Detection}{section.14.4}{}}
\abx@aux@backref{361}{lin2018_focalloss}{0}{517}{517}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.4.1}Why Single-Stage Detectors Can Be Faster}{517}{subsection.14.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.25}{\ignorespaces Inference speed comparison of RetinaNet and other detectors. Single-stage detectors like RetinaNet are significantly faster than two-stage detectors, such as FPN Faster R-CNN.}}{517}{figure.caption.1017}\protected@file@percent }
\newlabel{fig:chapter14_retinanet_inference}{{14.25}{517}{Inference speed comparison of RetinaNet and other detectors. Single-stage detectors like RetinaNet are significantly faster than two-stage detectors, such as FPN Faster R-CNN}{figure.caption.1017}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.4.2}The Class Imbalance Problem in Dense Detection}{517}{subsection.14.4.2}\protected@file@percent }
\BKM@entry{id=530,dest={73756273656374696F6E2E31342E342E33},srcline={1033}}{5C3337365C3337375C303030465C3030306F5C303030635C303030615C3030306C5C3030305C3034305C3030304C5C3030306F5C303030735C303030735C3030303A5C3030305C3034305C303030415C303030645C303030645C303030725C303030655C303030735C303030735C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C3030305C3034305C303030495C3030306D5C303030625C303030615C3030306C5C303030615C3030306E5C303030635C30303065}
\abx@aux@cite{0}{lin2018_focalloss}
\abx@aux@segm{0}{0}{lin2018_focalloss}
\abx@aux@cite{0}{lin2018_focalloss}
\abx@aux@segm{0}{0}{lin2018_focalloss}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.4.3}Focal Loss: Addressing Class Imbalance}{518}{subsection.14.4.3}\protected@file@percent }
\abx@aux@cite{0}{lin2018_focalloss}
\abx@aux@segm{0}{0}{lin2018_focalloss}
\abx@aux@cite{0}{lin2018_focalloss}
\abx@aux@segm{0}{0}{lin2018_focalloss}
\@writefile{lof}{\contentsline {figure}{\numberline {14.26}{\ignorespaces Focal loss modifies the standard cross-entropy loss by incorporating a modulating factor \((1-p_t)^\gamma \). This factor down-weights the loss for well-classified examples. For instance, when \(\gamma =2\), the loss for examples with high confidence (e.g., \(p_t \approx 0.9\)) is significantly reduced, while the loss for moderately difficult examples (e.g., \(p_t \approx 0.5\) or \(p_t \approx 0.2\)) remains similar to that of the standard cross-entropy loss. Setting \(\gamma \) too high (such as \(\gamma =5\)) can overly suppress the loss even for examples that are not trivial, potentially eliminating valuable learning signals. Thus, \(\gamma =2\) is often chosen as a good compromise, effectively reducing the loss from very easy examples while preserving enough gradient for harder examples. Source: \blx@tocontentsinit {0}\cite {lin2018_focalloss}.}}{519}{figure.caption.1018}\protected@file@percent }
\abx@aux@backref{363}{lin2018_focalloss}{0}{519}{519}
\newlabel{fig:chapter14_focal_loss}{{14.26}{519}{Focal loss modifies the standard cross-entropy loss by incorporating a modulating factor \((1-p_t)^\gamma \). This factor down-weights the loss for well-classified examples. For instance, when \(\gamma =2\), the loss for examples with high confidence (e.g., \(p_t \approx 0.9\)) is significantly reduced, while the loss for moderately difficult examples (e.g., \(p_t \approx 0.5\) or \(p_t \approx 0.2\)) remains similar to that of the standard cross-entropy loss. Setting \(\gamma \) too high (such as \(\gamma =5\)) can overly suppress the loss even for examples that are not trivial, potentially eliminating valuable learning signals. Thus, \(\gamma =2\) is often chosen as a good compromise, effectively reducing the loss from very easy examples while preserving enough gradient for harder examples. Source: \cite {lin2018_focalloss}}{figure.caption.1018}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.27}{\ignorespaces Cumulative distribution functions (CDFs) of the normalized loss for background (negative) and foreground (positive) examples under different values of \(\gamma \). As \(\gamma \) increases, the loss contribution from easy negatives is dramatically reduced, which flattens the loss distribution for background examples. Importantly, with \(\gamma =2\), the loss for foreground examples remains nearly unchanged, ensuring that the model still learns effectively from the scarce positive examples. This selective down-weighting is crucial for mitigating class imbalance. Source: \blx@tocontentsinit {0}\cite {lin2018_focalloss}.}}{519}{figure.caption.1019}\protected@file@percent }
\abx@aux@backref{365}{lin2018_focalloss}{0}{519}{519}
\newlabel{fig:chapter14_focal_loss_distribution}{{14.27}{519}{Cumulative distribution functions (CDFs) of the normalized loss for background (negative) and foreground (positive) examples under different values of \(\gamma \). As \(\gamma \) increases, the loss contribution from easy negatives is dramatically reduced, which flattens the loss distribution for background examples. Importantly, with \(\gamma =2\), the loss for foreground examples remains nearly unchanged, ensuring that the model still learns effectively from the scarce positive examples. This selective down-weighting is crucial for mitigating class imbalance. Source: \cite {lin2018_focalloss}}{figure.caption.1019}{}}
\BKM@entry{id=531,dest={73756273656374696F6E2E31342E342E34},srcline={1088}}{5C3337365C3337375C303030525C303030655C303030745C303030695C3030306E5C303030615C3030304E5C303030655C303030745C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030505C303030695C303030705C303030655C3030306C5C303030695C3030306E5C30303065}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.4.4}RetinaNet Architecture and Pipeline}{520}{subsection.14.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.28}{\ignorespaces RetinaNet pipeline. Like RPN, it predicts object categories and bounding box refinements, but in a single stage.}}{520}{figure.caption.1020}\protected@file@percent }
\newlabel{fig:chapter14_retinanet_pipeline}{{14.28}{520}{RetinaNet pipeline. Like RPN, it predicts object categories and bounding box refinements, but in a single stage}{figure.caption.1020}{}}
\BKM@entry{id=532,dest={73656374696F6E2E31342E35},srcline={1107}}{5C3337365C3337375C303030465C303030435C3030304F5C303030535C3030303A5C3030305C3034305C303030415C3030306E5C3030305C3034305C303030415C3030306E5C303030635C303030685C3030306F5C303030725C3030302D5C303030465C303030725C303030655C303030655C3030302C5C3030305C3034305C303030465C303030755C3030306C5C3030306C5C303030795C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C3030306F5C30303072}
\abx@aux@cite{0}{tian2019_fcos}
\abx@aux@segm{0}{0}{tian2019_fcos}
\abx@aux@cite{0}{law2019_cornernet}
\abx@aux@segm{0}{0}{law2019_cornernet}
\BKM@entry{id=533,dest={73756273656374696F6E2E31342E352E31},srcline={1114}}{5C3337365C3337375C303030435C3030306F5C303030725C303030655C3030305C3034305C303030505C303030695C303030705C303030655C3030306C5C303030695C3030306E5C303030655C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C3030304D5C303030615C303030705C3030305C3034305C303030495C3030306E5C303030745C303030655C303030725C303030705C303030725C303030655C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {14.5}FCOS: An Anchor-Free, Fully Convolutional Detector}{521}{section.14.5}\protected@file@percent }
\newlabel{subsec:chapter14_fcos}{{14.5}{521}{FCOS: An Anchor-Free, Fully Convolutional Detector}{section.14.5}{}}
\abx@aux@backref{366}{tian2019_fcos}{0}{521}{521}
\abx@aux@backref{367}{law2019_cornernet}{0}{521}{521}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.5.1}Core Pipeline and Feature Map Interpretation}{521}{subsection.14.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.29}{\ignorespaces On the left: an example prediction of the 4D localization output of FCOS, fitting a bounding box surrounding the object, according to its predicted distances from the edges. On the right: an example of an ambiguous case in FCOS. When a feature map location falls within multiple ground-truth boxes, to which bounding box shall we assign it? The authors proposed solution is to assign it to the box with the smallest area. This makes sense as smaller objects are harder to detect, so we'll want to give it more weight, and improve our chances to detect it well.}}{522}{figure.caption.1021}\protected@file@percent }
\newlabel{fig:chapter14_fcos_edge_case}{{14.29}{522}{On the left: an example prediction of the 4D localization output of FCOS, fitting a bounding box surrounding the object, according to its predicted distances from the edges. On the right: an example of an ambiguous case in FCOS. When a feature map location falls within multiple ground-truth boxes, to which bounding box shall we assign it? The authors proposed solution is to assign it to the box with the smallest area. This makes sense as smaller objects are harder to detect, so we'll want to give it more weight, and improve our chances to detect it well}{figure.caption.1021}{}}
\BKM@entry{id=534,dest={73756273656374696F6E2E31342E352E32},srcline={1163}}{5C3337365C3337375C303030425C3030306F5C303030755C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030425C3030306F5C303030785C3030305C3034305C303030525C303030655C303030675C303030725C303030655C303030735C303030735C303030695C3030306F5C3030306E}
\BKM@entry{id=535,dest={73756273656374696F6E2E31342E352E33},srcline={1180}}{5C3337365C3337375C303030435C303030655C3030306E5C303030745C303030655C303030725C3030306E5C303030655C303030735C303030735C3030303A5C3030305C3034305C303030465C303030695C3030306C5C303030745C303030655C303030725C303030695C3030306E5C303030675C3030305C3034305C3030304C5C3030306F5C303030775C3030302D5C303030515C303030755C303030615C3030306C5C303030695C303030745C303030795C3030305C3034305C303030505C303030725C303030655C303030645C303030695C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.5.2}Bounding Box Regression}{523}{subsection.14.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.5.3}Centerness: Filtering Low-Quality Predictions}{523}{subsection.14.5.3}\protected@file@percent }
\BKM@entry{id=536,dest={73756273656374696F6E2E31342E352E34},srcline={1202}}{5C3337365C3337375C3030304D5C303030755C3030306C5C303030745C303030695C3030302D5C3030304C5C303030655C303030765C303030655C3030306C5C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030505C303030725C303030655C303030645C303030695C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030465C303030505C3030304E}
\BKM@entry{id=537,dest={73756273656374696F6E2E31342E352E35},srcline={1217}}{5C3337365C3337375C3030304C5C3030306F5C303030735C303030735C3030305C3034305C303030465C303030755C3030306E5C303030635C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030465C3030306F5C303030635C303030615C3030306C5C3030305C3034305C3030304C5C3030306F5C303030735C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030495C3030306F5C303030555C3030305C3034305C3030304C5C3030306F5C303030735C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {14.30}{\ignorespaces FCOS pipeline: The backbone CNN extracts a multi-scale feature map from the input image. For each spatial location, FCOS predicts class scores, bounding box offsets \((l,t,r,b)\), and a centerness score that reflects the reliability of the prediction based on its distance from the center.}}{524}{figure.caption.1022}\protected@file@percent }
\newlabel{fig:chapter14_fcos_pipeline}{{14.30}{524}{FCOS pipeline: The backbone CNN extracts a multi-scale feature map from the input image. For each spatial location, FCOS predicts class scores, bounding box offsets \((l,t,r,b)\), and a centerness score that reflects the reliability of the prediction based on its distance from the center}{figure.caption.1022}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.5.4}Multi-Level Feature Prediction with FPN}{524}{subsection.14.5.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.31}{\ignorespaces FCOS utilizes an FPN for multi-scale detection. Each feature level is responsible for detecting objects within a specific size range.}}{524}{figure.caption.1023}\protected@file@percent }
\newlabel{fig:chapter14_fcos_fpn}{{14.31}{524}{FCOS utilizes an FPN for multi-scale detection. Each feature level is responsible for detecting objects within a specific size range}{figure.caption.1023}{}}
\abx@aux@cite{0}{lin2018_focalloss}
\abx@aux@segm{0}{0}{lin2018_focalloss}
\BKM@entry{id=538,dest={73756273656374696F6E2E31342E352E36},srcline={1231}}{5C3337365C3337375C303030495C3030306E5C303030665C303030655C303030725C303030655C3030306E5C303030635C303030655C3030303A5C3030305C3034305C303030535C303030655C3030306C5C303030655C303030635C303030745C303030695C3030306E5C303030675C3030305C3034305C303030465C303030695C3030306E5C303030615C3030306C5C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=539,dest={73756273656374696F6E2E31342E352E37},srcline={1241}}{5C3337365C3337375C303030415C303030645C303030765C303030615C3030306E5C303030745C303030615C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030465C303030435C3030304F5C30303053}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.5.5}Loss Function: Focal Loss and IoU Loss}{525}{subsection.14.5.5}\protected@file@percent }
\abx@aux@backref{368}{lin2018_focalloss}{0}{525}{525}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.5.6}Inference: Selecting Final Detections}{525}{subsection.14.5.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.5.7}Advantages of FCOS}{525}{subsection.14.5.7}\protected@file@percent }
\BKM@entry{id=540,dest={73656374696F6E2A2E31303234},srcline={1253}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030345C3030302E5C303030365C3030303A5C3030305C3034305C303030595C3030304F5C3030304C5C3030304F5C3030305C3034305C3030302D5C3030305C3034305C303030595C3030306F5C303030755C3030305C3034305C3030304F5C3030306E5C3030306C5C303030795C3030305C3034305C3030304C5C3030306F5C3030306F5C3030306B5C3030305C3034305C3030304F5C3030306E5C303030635C30303065}
\BKM@entry{id=541,dest={73656374696F6E2A2E31303235},srcline={1254}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030345C3030302E5C303030365C3030302E5C303030315C3030303A5C3030305C3034305C303030425C303030615C303030635C3030306B5C303030675C303030725C3030306F5C303030755C3030306E5C30303064}
\abx@aux@cite{0}{redmon2016_yolo}
\abx@aux@segm{0}{0}{redmon2016_yolo}
\BKM@entry{id=542,dest={73656374696F6E2A2E31303236},srcline={1265}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030345C3030302E5C303030365C3030302E5C303030325C3030303A5C3030305C3034305C303030535C303030745C303030655C303030705C3030302D5C303030625C303030795C3030302D5C303030535C303030745C303030655C303030705C3030303A5C3030305C3034305C303030485C3030306F5C303030775C3030305C3034305C303030595C3030304F5C3030304C5C3030304F5C303030765C303030315C3030305C3034305C303030505C303030725C3030306F5C303030635C303030655C303030735C303030735C303030655C303030735C3030305C3034305C303030615C3030306E5C3030305C3034305C303030495C3030306E5C303030705C303030755C303030745C3030305C3034305C303030495C3030306D5C303030615C303030675C30303065}
\@writefile{toc}{\contentsline {section}{Enrichment 14.6: YOLO - You Only Look Once}{526}{section*.1024}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 14.6.1: Background}{526}{section*.1025}\protected@file@percent }
\abx@aux@backref{369}{redmon2016_yolo}{0}{526}{526}
\@writefile{toc}{\contentsline {subsection}{Enrichment 14.6.2: Step-by-Step: How YOLOv1 Processes an Input Image}{526}{section*.1026}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{1. Input Image and Preprocessing}{526}{section*.1027}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. Feature Extraction (DarkNet + Additional Convolution Layers)}{526}{section*.1028}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3. Flattening and Fully Connected Layers}{526}{section*.1029}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{4. Understanding the Output Format}{527}{section*.1030}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{5. Why a Sigmoid?}{527}{section*.1031}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{6. Converting Predictions to Actual Bounding Boxes}{527}{section*.1032}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{7. Loss and Training (High Level)}{528}{section*.1033}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{8. Why It Works (and Its Trade-offs)}{528}{section*.1034}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{9. Final Detections and NMS}{528}{section*.1035}\protected@file@percent }
\abx@aux@cite{0}{redmon2016_yolo}
\abx@aux@segm{0}{0}{redmon2016_yolo}
\abx@aux@cite{0}{redmon2016_yolo}
\abx@aux@segm{0}{0}{redmon2016_yolo}
\BKM@entry{id=543,dest={73656374696F6E2A2E31303338},srcline={1416}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030345C3030302E5C303030365C3030302E5C303030335C3030303A5C3030305C3034305C303030455C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C303030595C3030304F5C3030304C5C3030304F}
\abx@aux@cite{0}{redmon2017_yolo9000}
\abx@aux@segm{0}{0}{redmon2017_yolo9000}
\abx@aux@cite{0}{redmon2018_yolov3}
\abx@aux@segm{0}{0}{redmon2018_yolov3}
\abx@aux@cite{0}{bochkovskiy2020_yolov4}
\abx@aux@segm{0}{0}{bochkovskiy2020_yolov4}
\@writefile{toc}{\contentsline {paragraph}{Summary}{529}{section*.1036}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.32}{\ignorespaces YOLO pipeline: A single CNN processes the entire image, predicts bounding boxes and class probabilities, and applies NMS to refine detections. Source: \blx@tocontentsinit {0}\cite {redmon2016_yolo}.}}{529}{figure.caption.1037}\protected@file@percent }
\abx@aux@backref{371}{redmon2016_yolo}{0}{529}{529}
\newlabel{fig:chapter14_yolo_pipeline}{{14.32}{529}{YOLO pipeline: A single CNN processes the entire image, predicts bounding boxes and class probabilities, and applies NMS to refine detections. Source: \cite {redmon2016_yolo}}{figure.caption.1037}{}}
\@writefile{toc}{\contentsline {subsection}{Enrichment 14.6.3: Evolution of YOLO}{529}{section*.1038}\protected@file@percent }
\abx@aux@backref{372}{redmon2017_yolo9000}{0}{529}{529}
\abx@aux@backref{373}{redmon2018_yolov3}{0}{529}{529}
\abx@aux@backref{374}{bochkovskiy2020_yolov4}{0}{529}{529}
\BKM@entry{id=544,dest={73656374696F6E2E31342E37},srcline={1430}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030455C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {14.7}Conclusion: The Evolution of Object Detection}{530}{section.14.7}\protected@file@percent }
\newlabel{sec:chapter14_conclusion}{{14.7}{530}{Conclusion: The Evolution of Object Detection}{section.14.7}{}}
\@writefile{toc}{\contentsline {paragraph}{From R-CNN to Faster R-CNN: Learning Region Proposals}{530}{section*.1039}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Improving Multi-Scale Detection: Feature Pyramid Networks (FPN)}{530}{section*.1040}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{RetinaNet: A Breakthrough for One-Stage Detectors}{530}{section*.1041}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{FCOS: Moving Toward Anchor-Free Detection}{530}{section*.1042}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{YOLO: A Widely Used Real-Time Detector}{531}{section*.1043}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Looking Ahead: Transformers and SOTA Detectors}{531}{section*.1044}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{531}{section*.1045}\protected@file@percent }
\BKM@entry{id=545,dest={636861707465722E3135},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030315C303030355C3030303A5C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030535C303030655C303030675C3030306D5C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=546,dest={73656374696F6E2E31352E31},srcline={10}}{5C3337365C3337375C303030465C303030725C3030306F5C3030306D5C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030535C303030655C303030675C3030306D5C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{ren2016_fasterrcnn}
\abx@aux@segm{0}{0}{ren2016_fasterrcnn}
\abx@aux@cite{0}{redmon2016_yolo}
\abx@aux@segm{0}{0}{redmon2016_yolo}
\@writefile{toc}{\contentsline {chapter}{\numberline {15}Lecture 15: Image Segmentation}{532}{chapter.15}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@14}}
\ttl@writefile{ptc}{\ttl@starttoc{default@15}}
\pgfsyspdfmark {pgfid79}{0}{52099153}
\pgfsyspdfmark {pgfid78}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {15.1}From Object Detection to Segmentation}{532}{section.15.1}\protected@file@percent }
\abx@aux@backref{375}{ren2016_fasterrcnn}{0}{532}{532}
\abx@aux@backref{376}{redmon2016_yolo}{0}{532}{532}
\@writefile{lof}{\contentsline {figure}{\numberline {15.1}{\ignorespaces Comparison of different computer vision tasks: classification, object detection, and semantic and instance segmentation. We begin with Semantic Segmentation.}}{532}{figure.caption.1046}\protected@file@percent }
\newlabel{fig:chapter15_cv_tasks}{{15.1}{532}{Comparison of different computer vision tasks: classification, object detection, and semantic and instance segmentation. We begin with Semantic Segmentation}{figure.caption.1046}{}}
\BKM@entry{id=547,dest={73656374696F6E2A2E31303437},srcline={32}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030355C3030302E5C303030325C3030303A5C3030305C3034305C303030575C303030685C303030795C3030305C3034305C303030695C303030735C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304E5C3030306F5C303030745C3030305C3034305C303030455C3030306E5C3030306F5C303030755C303030675C303030685C3030303F}
\@writefile{toc}{\contentsline {section}{Enrichment 15.2: Why is Object Detection Not Enough?}{533}{section*.1047}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15.2}{\ignorespaces Segmentation differentiates between \textit  {things} (discrete objects like cars, people) and \textit  {stuff} (amorphous regions like sky, road).}}{533}{figure.caption.1048}\protected@file@percent }
\newlabel{fig:chapter15_things_stuff}{{15.2}{533}{Segmentation differentiates between \textit {things} (discrete objects like cars, people) and \textit {stuff} (amorphous regions like sky, road)}{figure.caption.1048}{}}
\BKM@entry{id=548,dest={73656374696F6E2E31352E33},srcline={56}}{5C3337365C3337375C303030415C303030645C303030765C303030615C3030306E5C303030635C303030655C3030306D5C303030655C3030306E5C303030745C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030535C303030655C3030306D5C303030615C3030306E5C303030745C303030695C303030635C3030305C3034305C303030535C303030655C303030675C3030306D5C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=549,dest={73756273656374696F6E2E31352E332E31},srcline={60}}{5C3337365C3337375C303030455C303030615C303030725C3030306C5C303030795C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030615C303030635C303030685C303030655C303030735C3030303A5C3030305C3034305C303030535C3030306C5C303030695C303030645C303030695C3030306E5C303030675C3030305C3034305C303030575C303030695C3030306E5C303030645C3030306F5C303030775C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C30303064}
\BKM@entry{id=550,dest={73756273656374696F6E2E31352E332E32},srcline={73}}{5C3337365C3337375C303030465C303030755C3030306C5C3030306C5C303030795C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030465C303030435C3030304E5C303030735C3030305C303531}
\abx@aux@cite{0}{long2015_fcn}
\abx@aux@segm{0}{0}{long2015_fcn}
\@writefile{toc}{\contentsline {section}{\numberline {15.3}Advancements in Semantic Segmentation}{534}{section.15.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.3.1}Early Approaches: Sliding Window Method}{534}{subsection.15.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15.3}{\ignorespaces Sliding window approach for semantic segmentation, illustrating the inefficiency due to redundant computations over overlapping patches.}}{534}{figure.caption.1049}\protected@file@percent }
\newlabel{fig:chapter15_sliding_window}{{15.3}{534}{Sliding window approach for semantic segmentation, illustrating the inefficiency due to redundant computations over overlapping patches}{figure.caption.1049}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.3.2}Fully Convolutional Networks (FCNs)}{534}{subsection.15.3.2}\protected@file@percent }
\abx@aux@backref{377}{long2015_fcn}{0}{534}{534}
\BKM@entry{id=551,dest={73756273656374696F6E2E31352E332E33},srcline={86}}{5C3337365C3337375C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030465C303030435C3030304E5C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030535C303030655C3030306D5C303030615C3030306E5C303030745C303030695C303030635C3030305C3034305C303030535C303030655C303030675C3030306D5C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=552,dest={73756273656374696F6E2E31352E332E34},srcline={95}}{5C3337365C3337375C303030455C3030306E5C303030635C3030306F5C303030645C303030655C303030725C3030302D5C303030445C303030655C303030635C3030306F5C303030645C303030655C303030725C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C30303073}
\abx@aux@cite{0}{noh2015_deconvnet}
\abx@aux@segm{0}{0}{noh2015_deconvnet}
\@writefile{lof}{\contentsline {figure}{\numberline {15.4}{\ignorespaces Architecture of a Fully Convolutional Network maintaining input spatial dimensions, producing a feature map with $C \times H \times W$, where $C$ is the number of classes.}}{535}{figure.caption.1050}\protected@file@percent }
\newlabel{fig:chapter15_fcn_architecture}{{15.4}{535}{Architecture of a Fully Convolutional Network maintaining input spatial dimensions, producing a feature map with $C \times H \times W$, where $C$ is the number of classes}{figure.caption.1050}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.3.3}Challenges in FCNs for Semantic Segmentation}{535}{subsection.15.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.3.4}Encoder-Decoder Architectures}{535}{subsection.15.3.4}\protected@file@percent }
\abx@aux@backref{378}{noh2015_deconvnet}{0}{535}{535}
\abx@aux@cite{0}{raffel2020_t5}
\abx@aux@segm{0}{0}{raffel2020_t5}
\abx@aux@cite{0}{lewis2020_bart}
\abx@aux@segm{0}{0}{lewis2020_bart}
\abx@aux@cite{0}{ronneberger2015_unet}
\abx@aux@segm{0}{0}{ronneberger2015_unet}
\abx@aux@cite{0}{ledig2017_srgan}
\abx@aux@segm{0}{0}{ledig2017_srgan}
\BKM@entry{id=553,dest={73656374696F6E2E31352E34},srcline={124}}{5C3337365C3337375C303030555C303030705C303030735C303030615C3030306D5C303030705C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030555C3030306E5C303030705C3030306F5C3030306F5C3030306C5C303030695C3030306E5C30303067}
\abx@aux@backref{379}{raffel2020_t5}{0}{536}{536}
\abx@aux@backref{380}{lewis2020_bart}{0}{536}{536}
\abx@aux@backref{381}{ronneberger2015_unet}{0}{536}{536}
\abx@aux@backref{382}{ledig2017_srgan}{0}{536}{536}
\@writefile{lof}{\contentsline {figure}{\numberline {15.5}{\ignorespaces Encoder-decoder architecture for semantic segmentation, featuring downsampling in the encoder and upsampling in the decoder to achieve pixel-wise classification.}}{536}{figure.caption.1051}\protected@file@percent }
\newlabel{fig:chapter15_encoder_decoder}{{15.5}{536}{Encoder-decoder architecture for semantic segmentation, featuring downsampling in the encoder and upsampling in the decoder to achieve pixel-wise classification}{figure.caption.1051}{}}
\@writefile{toc}{\contentsline {section}{\numberline {15.4}Upsampling and Unpooling}{536}{section.15.4}\protected@file@percent }
\BKM@entry{id=554,dest={73756273656374696F6E2E31352E342E31},srcline={136}}{5C3337365C3337375C303030425C303030655C303030645C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304E5C303030615C303030695C3030306C5C303030735C3030305C3034305C303030555C3030306E5C303030705C3030306F5C3030306F5C3030306C5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.4.1}Bed of Nails Unpooling}{537}{subsection.15.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Limitations of Bed of Nails Unpooling}{537}{section*.1052}\protected@file@percent }
\abx@aux@cite{0}{wiki_Aliasing}
\abx@aux@segm{0}{0}{wiki_Aliasing}
\abx@aux@cite{0}{wiki_Aliasing}
\abx@aux@segm{0}{0}{wiki_Aliasing}
\BKM@entry{id=555,dest={73756273656374696F6E2E31352E342E32},srcline={172}}{5C3337365C3337375C3030304E5C303030655C303030615C303030725C303030655C303030735C303030745C3030302D5C3030304E5C303030655C303030695C303030675C303030685C303030625C3030306F5C303030725C3030305C3034305C303030555C3030306E5C303030705C3030306F5C3030306F5C3030306C5C303030695C3030306E5C30303067}
\@writefile{lof}{\contentsline {figure}{\numberline {15.6}{\ignorespaces Comparison of a well-sampled image (left) versus one affected by aliasing (right). The right image exhibits moiré patterns due to insufficient sampling, a phenomenon similar to the high-frequency distortions introduced by Bed of Nails unpooling. Source: \blx@tocontentsinit {0}\cite {wiki_Aliasing}.}}{538}{figure.caption.1053}\protected@file@percent }
\abx@aux@backref{384}{wiki_Aliasing}{0}{538}{538}
\newlabel{fig:chapter15_bed_of_nails_artifacts}{{15.6}{538}{Comparison of a well-sampled image (left) versus one affected by aliasing (right). The right image exhibits moiré patterns due to insufficient sampling, a phenomenon similar to the high-frequency distortions introduced by Bed of Nails unpooling. Source: \cite {wiki_Aliasing}}{figure.caption.1053}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.4.2}Nearest-Neighbor Unpooling}{538}{subsection.15.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15.7}{\ignorespaces Comparison of Bed of Nails unpooling (left) and Nearest-Neighbor unpooling (right).}}{538}{figure.caption.1054}\protected@file@percent }
\newlabel{fig:chapter15_unpooling}{{15.7}{538}{Comparison of Bed of Nails unpooling (left) and Nearest-Neighbor unpooling (right)}{figure.caption.1054}{}}
\BKM@entry{id=556,dest={73756273656374696F6E2E31352E342E33},srcline={205}}{5C3337365C3337375C303030425C303030695C3030306C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030495C3030306E5C303030745C303030655C303030725C303030705C3030306F5C3030306C5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030555C303030705C303030735C303030615C3030306D5C303030705C3030306C5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.4.3}Bilinear Interpolation for Upsampling}{539}{subsection.15.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Bilinear Interpolation: Generalized Case}{539}{section*.1055}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15.8}{\ignorespaces Bilinear interpolation applied to a $C \times 2 \times 2$ input tensor, producing a $C \times 4 \times 4$ output. Each upsampled value is computed as a weighted sum of its four nearest neighbors in the original feature map.}}{540}{figure.caption.1056}\protected@file@percent }
\newlabel{fig:chapter15_bilinear_interpolation}{{15.8}{540}{Bilinear interpolation applied to a $C \times 2 \times 2$ input tensor, producing a $C \times 4 \times 4$ output. Each upsampled value is computed as a weighted sum of its four nearest neighbors in the original feature map}{figure.caption.1056}{}}
\BKM@entry{id=557,dest={73756273656374696F6E2E31352E342E34},srcline={213}}{5C3337365C3337375C303030425C303030695C303030635C303030755C303030625C303030695C303030635C3030305C3034305C303030495C3030306E5C303030745C303030655C303030725C303030705C3030306F5C3030306C5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030555C303030705C303030735C303030615C3030306D5C303030705C3030306C5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {subsubsection}{Advantages of Bilinear Interpolation}{541}{section*.1057}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Limitations and Transition to Bicubic Interpolation}{541}{section*.1058}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.4.4}Bicubic Interpolation for Upsampling}{541}{subsection.15.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Why Bicubic Interpolation?}{541}{section*.1059}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Mathematical Reasoning}{541}{section*.1060}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Bicubic Interpolation: Generalized Case}{542}{section*.1061}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15.9}{\ignorespaces Bicubic interpolation demonstrated on a \(C \times 2 \times 2\) feature map, generating a \(C \times 4 \times 4\) output. Each interpolated value is computed by applying a cubic weighting to the nearest sixteen pixels.}}{542}{figure.caption.1062}\protected@file@percent }
\newlabel{fig:chapter15_bicubic_interpolation}{{15.9}{542}{Bicubic interpolation demonstrated on a \(C \times 2 \times 2\) feature map, generating a \(C \times 4 \times 4\) output. Each interpolated value is computed by applying a cubic weighting to the nearest sixteen pixels}{figure.caption.1062}{}}
\BKM@entry{id=558,dest={73756273656374696F6E2E31352E342E35},srcline={282}}{5C3337365C3337375C3030304D5C303030615C303030785C3030305C3034305C303030555C3030306E5C303030705C3030306F5C3030306F5C3030306C5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {subsubsection}{Advantages and Limitations}{543}{section*.1063}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.4.5}Max Unpooling}{543}{subsection.15.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Max Unpooling in the Context of Noh et al. (ICCV 2015)}{543}{section*.1064}\protected@file@percent }
\BKM@entry{id=559,dest={73756273656374696F6E2E31352E342E36},srcline={343}}{5C3337365C3337375C303030545C303030725C303030615C3030306E5C303030735C303030705C3030306F5C303030735C303030655C303030645C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {15.10}{\ignorespaces Illustration of \textbf  {max unpooling} using recorded pooling indices to restore spatial activations. Unlike interpolation-based methods, \textbf  {max unpooling} reinstates feature activations at their original locations.}}{544}{figure.caption.1065}\protected@file@percent }
\newlabel{fig:chapter15_max_unpooling}{{15.10}{544}{Illustration of \textbf {max unpooling} using recorded pooling indices to restore spatial activations. Unlike interpolation-based methods, \textbf {max unpooling} reinstates feature activations at their original locations}{figure.caption.1065}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why Max Unpooling is More Effective Than Bed of Nails Unpooling}{544}{section*.1066}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Bridging to Transposed Convolution}{544}{section*.1067}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.4.6}Transposed Convolution}{545}{subsection.15.4.6}\protected@file@percent }
\newlabel{chapter15_subsec:transposed_convolution}{{15.4.6}{545}{Transposed Convolution}{subsection.15.4.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{Understanding the Similarity to Standard Convolution}{545}{section*.1068}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Step-by-Step Process of Transposed Convolution}{545}{section*.1069}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15.11}{\ignorespaces Illustration of the first step in transposed convolution: applying the filter to the first input element.}}{546}{figure.caption.1070}\protected@file@percent }
\newlabel{fig:chapter15_transposed_conv_first_step}{{15.11}{546}{Illustration of the first step in transposed convolution: applying the filter to the first input element}{figure.caption.1070}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.12}{\ignorespaces The second input element is processed: its weighted filter values are placed in the output grid, with overlapping values summed.}}{546}{figure.caption.1071}\protected@file@percent }
\newlabel{fig:chapter15_transposed_conv_second_step}{{15.12}{546}{The second input element is processed: its weighted filter values are placed in the output grid, with overlapping values summed}{figure.caption.1071}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.13}{\ignorespaces Final constructed output after processing all input elements.}}{547}{figure.caption.1072}\protected@file@percent }
\newlabel{fig:chapter15_transposed_conv_final_output}{{15.13}{547}{Final constructed output after processing all input elements}{figure.caption.1072}{}}
\@writefile{toc}{\contentsline {subsubsection}{1D Transposed Convolution}{547}{section*.1073}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15.14}{\ignorespaces Illustration of 1D transposed convolution: a 2-element input and a 3-element filter produce a 5-element output.}}{547}{figure.caption.1074}\protected@file@percent }
\newlabel{fig:chapter15_transposed_conv_1D}{{15.14}{547}{Illustration of 1D transposed convolution: a 2-element input and a 3-element filter produce a 5-element output}{figure.caption.1074}{}}
\BKM@entry{id=560,dest={73756273656374696F6E2E31352E342E37},srcline={457}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030705C3030306F5C303030735C303030655C303030645C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C303030735C3030305C3034305C3030304D5C303030615C303030745C303030725C303030695C303030785C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsubsection}{Advantages of Transposed Convolution}{548}{section*.1075}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Connection to Standard Convolution}{548}{section*.1076}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.4.7}Convolution and Transposed Convolution as Matrix Multiplication}{548}{subsection.15.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Convolution via Matrix Multiplication}{548}{section*.1077}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15.15}{\ignorespaces 1D convolution represented as matrix multiplication.}}{549}{figure.caption.1078}\protected@file@percent }
\newlabel{fig:conv_matrix_mul}{{15.15}{549}{1D convolution represented as matrix multiplication}{figure.caption.1078}{}}
\@writefile{toc}{\contentsline {subsubsection}{Transposed Convolution via Matrix Multiplication (Stride = 1)}{549}{section*.1079}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15.16}{\ignorespaces Transposed convolution as the transpose of the convolution matrix (for stride=1).}}{550}{figure.caption.1080}\protected@file@percent }
\newlabel{fig:trans_conv_matrix_mul}{{15.16}{550}{Transposed convolution as the transpose of the convolution matrix (for stride=1)}{figure.caption.1080}{}}
\@writefile{toc}{\contentsline {subsubsection}{Transposed Convolution and Gradient Derivation}{550}{section*.1081}\protected@file@percent }
\BKM@entry{id=561,dest={73756273656374696F6E2E31352E342E38},srcline={572}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030435C303030685C3030306F5C3030306F5C303030735C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030525C303030695C303030675C303030685C303030745C3030305C3034305C303030555C303030705C303030735C303030615C3030306D5C303030705C3030306C5C303030695C3030306E5C303030675C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C30303064}
\@writefile{toc}{\contentsline {subsubsection}{Advantages of Transposed Convolution}{551}{section*.1082}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Challenges and Considerations}{551}{section*.1083}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.4.8}Conclusion: Choosing the Right Upsampling Method}{551}{subsection.15.4.8}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {15.1}{\ignorespaces Comparison of upsampling methods based on their properties.}}{551}{table.caption.1084}\protected@file@percent }
\newlabel{tab:upsampling_comparison}{{15.1}{551}{Comparison of upsampling methods based on their properties}{table.caption.1084}{}}
\@writefile{toc}{\contentsline {subsubsection}{Guidelines for Choosing an Upsampling Method}{551}{section*.1085}\protected@file@percent }
\BKM@entry{id=562,dest={73656374696F6E2E31352E35},srcline={631}}{5C3337365C3337375C303030495C3030306E5C303030735C303030745C303030615C3030306E5C303030635C303030655C3030305C3034305C303030535C303030655C303030675C3030306D5C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsubsection}{Final Thoughts}{552}{section*.1086}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {15.5}Instance Segmentation}{552}{section.15.5}\protected@file@percent }
\BKM@entry{id=563,dest={73756273656374696F6E2E31352E352E31},srcline={648}}{5C3337365C3337375C3030304D5C303030615C303030735C3030306B5C3030305C3034305C303030525C3030302D5C303030435C3030304E5C3030304E5C3030303A5C3030305C3034305C303030415C3030305C3034305C303030545C303030775C3030306F5C3030302D5C303030535C303030745C303030615C303030675C303030655C3030305C3034305C303030465C303030725C303030615C3030306D5C303030655C303030775C3030306F5C303030725C3030306B5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030495C3030306E5C303030735C303030745C303030615C3030306E5C303030635C303030655C3030305C3034305C303030535C303030655C303030675C3030306D5C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.5.1}Mask R-CNN: A Two-Stage Framework for Instance Segmentation}{553}{subsection.15.5.1}\protected@file@percent }
\newlabel{subsec:chapter15_mask_rcnn}{{15.5.1}{553}{Mask R-CNN: A Two-Stage Framework for Instance Segmentation}{subsection.15.5.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Faster R-CNN Backbone}{553}{section*.1087}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Key Additions in Mask R-CNN}{553}{section*.1088}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Segmentation Mask Prediction: Fixed Size Output}{553}{section*.1089}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Training Mask R-CNN and Loss Functions}{553}{section*.1090}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Bilinear Interpolation vs. Bicubic Interpolation}{554}{section*.1091}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Class-Aware Mask Selection}{554}{section*.1092}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Gradient Flow in Mask R-CNN}{554}{section*.1093}\protected@file@percent }
\BKM@entry{id=564,dest={73756273656374696F6E2E31352E352E32},srcline={747}}{5C3337365C3337375C303030455C303030785C303030745C303030655C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030505C303030615C303030725C303030615C303030645C303030695C303030675C3030306D}
\abx@aux@cite{0}{johnson2015_densecap}
\abx@aux@segm{0}{0}{johnson2015_densecap}
\@writefile{toc}{\contentsline {subsubsection}{Summary}{555}{section*.1094}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.5.2}Extending the Object Detection Paradigm}{555}{subsection.15.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15.17}{\ignorespaces Mask R-CNN extended for keypoint estimation, predicting key locations such as joints for human pose estimation.}}{555}{figure.caption.1095}\protected@file@percent }
\newlabel{fig:chapter15_keypoints}{{15.17}{555}{Mask R-CNN extended for keypoint estimation, predicting key locations such as joints for human pose estimation}{figure.caption.1095}{}}
\abx@aux@backref{385}{johnson2015_densecap}{0}{555}{555}
\abx@aux@cite{0}{gkioxari2020_meshrcnn}
\abx@aux@segm{0}{0}{gkioxari2020_meshrcnn}
\@writefile{lof}{\contentsline {figure}{\numberline {15.18}{\ignorespaces Dense Captioning (DenseCap) extends object detection by adding a captioning head, enabling textual descriptions of detected objects.}}{556}{figure.caption.1096}\protected@file@percent }
\newlabel{fig:chapter15_densecap}{{15.18}{556}{Dense Captioning (DenseCap) extends object detection by adding a captioning head, enabling textual descriptions of detected objects}{figure.caption.1096}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.19}{\ignorespaces Example output of DenseCap: Generated captions describe detected regions with natural language.}}{556}{figure.caption.1097}\protected@file@percent }
\newlabel{fig:chapter15_densecap_example}{{15.19}{556}{Example output of DenseCap: Generated captions describe detected regions with natural language}{figure.caption.1097}{}}
\abx@aux@backref{386}{gkioxari2020_meshrcnn}{0}{556}{556}
\@writefile{lof}{\contentsline {figure}{\numberline {15.20}{\ignorespaces Mesh R-CNN extends Mask R-CNN with a mesh prediction head, enabling 3D shape reconstruction from 2D images.}}{557}{figure.caption.1098}\protected@file@percent }
\newlabel{fig:chapter15_mesh_rcnn}{{15.20}{557}{Mesh R-CNN extends Mask R-CNN with a mesh prediction head, enabling 3D shape reconstruction from 2D images}{figure.caption.1098}{}}
\BKM@entry{id=565,dest={73656374696F6E2A2E31303939},srcline={790}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030355C3030302E5C303030365C3030303A5C3030305C3034305C303030555C3030302D5C3030304E5C303030655C303030745C3030303A5C3030305C3034305C303030415C3030305C3034305C303030465C303030755C3030306C5C3030306C5C303030795C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030535C303030655C303030675C3030306D5C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=566,dest={73656374696F6E2A2E31313030},srcline={793}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030355C3030302E5C303030365C3030302E5C303030315C3030303A5C3030305C3034305C3030304F5C303030765C303030655C303030725C303030765C303030695C303030655C30303077}
\abx@aux@cite{0}{ronneberger2015_unet}
\abx@aux@segm{0}{0}{ronneberger2015_unet}
\BKM@entry{id=567,dest={73656374696F6E2A2E31313031},srcline={799}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030355C3030302E5C303030365C3030302E5C303030325C3030303A5C3030305C3034305C303030555C3030302D5C3030304E5C303030655C303030745C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C30303065}
\abx@aux@cite{0}{ronneberger2015_unet}
\abx@aux@segm{0}{0}{ronneberger2015_unet}
\abx@aux@cite{0}{ronneberger2015_unet}
\abx@aux@segm{0}{0}{ronneberger2015_unet}
\BKM@entry{id=568,dest={73656374696F6E2A2E31313033},srcline={827}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030355C3030302E5C303030365C3030302E5C303030335C3030303A5C3030305C3034305C303030535C3030306B5C303030695C303030705C3030305C3034305C303030435C3030306F5C3030306E5C3030306E5C303030655C303030635C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030435C3030306F5C3030306E5C303030635C303030615C303030745C303030655C3030306E5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{Enrichment 15.6: U-Net: A Fully Conv Architecture for Segmentation}{558}{section*.1099}\protected@file@percent }
\newlabel{enr:chapter15_unet}{{15.6}{558}{\color {ocre}Enrichment \thesection : U-Net: A Fully Conv Architecture for Segmentation}{section*.1099}{}}
\@writefile{toc}{\contentsline {subsection}{Enrichment 15.6.1: Overview}{558}{section*.1100}\protected@file@percent }
\abx@aux@backref{387}{ronneberger2015_unet}{0}{558}{558}
\@writefile{toc}{\contentsline {subsection}{Enrichment 15.6.2: U-Net Architecture}{558}{section*.1101}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15.21}{\ignorespaces U-Net architecture: The encoder (left) captures context, while the decoder (right) restores details using transposed convolutions and skip connections. Source: \blx@tocontentsinit {0}\cite {ronneberger2015_unet}.}}{558}{figure.caption.1102}\protected@file@percent }
\abx@aux@backref{389}{ronneberger2015_unet}{0}{558}{558}
\newlabel{fig:chapter15_unet_architecture}{{15.21}{558}{U-Net architecture: The encoder (left) captures context, while the decoder (right) restores details using transposed convolutions and skip connections. Source: \cite {ronneberger2015_unet}}{figure.caption.1102}{}}
\BKM@entry{id=569,dest={73656374696F6E2A2E31313034},srcline={855}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030355C3030302E5C303030365C3030302E5C303030345C3030303A5C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030555C3030302D5C3030304E5C303030655C30303074}
\BKM@entry{id=570,dest={73656374696F6E2A2E31313035},srcline={888}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030355C3030302E5C303030365C3030302E5C303030355C3030303A5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C303030735C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304D5C303030615C303030735C3030306B5C3030305C3034305C303030525C3030302D5C303030435C3030304E5C3030304E}
\BKM@entry{id=571,dest={73656374696F6E2A2E31313036},srcline={900}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030355C3030302E5C303030365C3030302E5C303030365C3030303A5C3030305C3034305C303030495C3030306D5C303030705C303030615C303030635C303030745C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030455C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C303030555C3030302D5C3030304E5C303030655C30303074}
\@writefile{toc}{\contentsline {subsection}{Enrichment 15.6.3: Skip Connections and Concatenation}{559}{section*.1103}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 15.6.4: Training U-Net}{559}{section*.1104}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 15.6.5: Comparison with Mask R-CNN}{559}{section*.1105}\protected@file@percent }
\abx@aux@cite{0}{zhou2018_unetpp}
\abx@aux@segm{0}{0}{zhou2018_unetpp}
\abx@aux@cite{0}{cciccek2016_3dunet}
\abx@aux@segm{0}{0}{cciccek2016_3dunet}
\abx@aux@cite{0}{zhang2018_resunet}
\abx@aux@segm{0}{0}{zhang2018_resunet}
\abx@aux@cite{0}{oktay2018_attentionunet}
\abx@aux@segm{0}{0}{oktay2018_attentionunet}
\@writefile{toc}{\contentsline {subsection}{Enrichment 15.6.6: Impact and Evolution of U-Net}{560}{section*.1106}\protected@file@percent }
\abx@aux@backref{390}{zhou2018_unetpp}{0}{560}{560}
\abx@aux@backref{391}{cciccek2016_3dunet}{0}{560}{560}
\abx@aux@backref{392}{zhang2018_resunet}{0}{560}{560}
\abx@aux@backref{393}{oktay2018_attentionunet}{0}{560}{560}
\BKM@entry{id=572,dest={73656374696F6E2A2E31313037},srcline={917}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030355C3030302E5C303030375C3030303A5C3030305C3034305C303030535C303030745C303030725C303030695C303030645C303030695C3030306E5C303030675C3030305C3034305C303030545C3030306F5C303030775C303030615C303030725C303030645C303030735C3030305C3034305C303030535C3030304F5C303030545C303030415C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030535C303030655C303030675C3030306D5C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{ravi2024_sam2}
\abx@aux@segm{0}{0}{ravi2024_sam2}
\abx@aux@cite{0}{cheng2022_mask2former}
\abx@aux@segm{0}{0}{cheng2022_mask2former}
\abx@aux@cite{0}{li2022_maskdino}
\abx@aux@segm{0}{0}{li2022_maskdino}
\abx@aux@cite{0}{liu2023_groundingdino}
\abx@aux@segm{0}{0}{liu2023_groundingdino}
\abx@aux@cite{0}{ren2024_groundedsam}
\abx@aux@segm{0}{0}{ren2024_groundedsam}
\abx@aux@cite{0}{jain2023_oneformer}
\abx@aux@segm{0}{0}{jain2023_oneformer}
\abx@aux@cite{0}{zou2022_xdecoder}
\abx@aux@segm{0}{0}{zou2022_xdecoder}
\abx@aux@cite{0}{ke2023_hqsam}
\abx@aux@segm{0}{0}{ke2023_hqsam}
\@writefile{toc}{\contentsline {section}{Enrichment 15.7: Striding Towards SOTA Image Segmentation}{561}{section*.1107}\protected@file@percent }
\newlabel{enr:sec_chapter15_towards_sota_segmentation}{{15.7}{561}{\color {ocre}Enrichment \thesection : Striding Towards SOTA Image Segmentation}{section*.1107}{}}
\abx@aux@backref{394}{kirillov2023_sam}{0}{561}{561}
\abx@aux@backref{395}{ravi2024_sam2}{0}{561}{561}
\abx@aux@backref{396}{cheng2022_mask2former}{0}{561}{561}
\abx@aux@backref{397}{li2022_maskdino}{0}{561}{561}
\abx@aux@backref{398}{liu2023_groundingdino}{0}{561}{561}
\abx@aux@backref{399}{ren2024_groundedsam}{0}{561}{561}
\abx@aux@backref{400}{jain2023_oneformer}{0}{561}{561}
\abx@aux@backref{401}{ke2023_hqsam}{0}{561}{561}
\abx@aux@backref{402}{zou2022_xdecoder}{0}{561}{561}
\BKM@entry{id=573,dest={73656374696F6E2A2E31313038},srcline={939}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030355C3030302E5C303030375C3030302E5C303030315C3030303A5C3030305C3034305C303030535C303030415C3030304D5C3030303A5C3030305C3034305C303030535C303030655C303030675C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030415C3030306E5C303030795C303030745C303030685C303030695C3030306E5C303030675C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C}
\abx@aux@cite{0}{ronneberger2015_unet}
\abx@aux@segm{0}{0}{ronneberger2015_unet}
\abx@aux@cite{0}{he2017_maskrcnn}
\abx@aux@segm{0}{0}{he2017_maskrcnn}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\@writefile{toc}{\contentsline {subsection}{Enrichment 15.7.1: SAM: Segment Anything Model}{562}{section*.1108}\protected@file@percent }
\newlabel{enr:subsec_chapter15_sam}{{15.7.1}{562}{\color {ocre}Enrichment \thesubsection : SAM: Segment Anything Model}{section*.1108}{}}
\@writefile{toc}{\contentsline {paragraph}{Background}{562}{section*.1109}\protected@file@percent }
\abx@aux@backref{403}{ronneberger2015_unet}{0}{562}{562}
\abx@aux@backref{404}{he2017_maskrcnn}{0}{562}{562}
\abx@aux@backref{405}{kirillov2023_sam}{0}{562}{562}
\@writefile{lof}{\contentsline {figure}{\numberline {15.22}{\ignorespaces \textbf  {Task, model, and data engine}. A promptable segmentation task, a model (SAM) supporting interactive and zero-shot use, and a data engine that scales mask collection to SA-1B; credit: Kirillov \emph  {et~al.}~\blx@tocontentsinit {0}\cite {kirillov2023_sam}.}}{562}{figure.caption.1110}\protected@file@percent }
\abx@aux@backref{407}{kirillov2023_sam}{0}{562}{562}
\newlabel{fig:chapter15_sam_idea}{{15.22}{562}{\textbf {Task, model, and data engine}. A promptable segmentation task, a model (SAM) supporting interactive and zero-shot use, and a data engine that scales mask collection to SA-1B; credit: Kirillov \emph {et~al.}~\cite {kirillov2023_sam}}{figure.caption.1110}{}}
\@writefile{toc}{\contentsline {paragraph}{Contribution and innovation}{562}{section*.1111}\protected@file@percent }
\abx@aux@backref{408}{kirillov2023_sam}{0}{562}{562}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\@writefile{toc}{\contentsline {paragraph}{Zero-shot segmentation by prompting (and ambiguity-aware decoding)}{563}{section*.1112}\protected@file@percent }
\abx@aux@backref{409}{kirillov2023_sam}{0}{563}{563}
\abx@aux@backref{410}{kirillov2023_sam}{0}{563}{563}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{he2022_mae}
\abx@aux@segm{0}{0}{he2022_mae}
\@writefile{lof}{\contentsline {figure}{\numberline {15.23}{\ignorespaces \textbf  {Ambiguity-aware outputs.} Each \emph  {column} shows three valid masks produced by SAM from a \emph  {single point prompt} (green dot). \emph  {Rows:} top = \textit  {whole} object, middle = \textit  {part}, bottom = \textit  {subpart}. The examples illustrate hierarchical ambiguity under the same cue (e.g., person$\rightarrow $backpack$\rightarrow $pocket; bird$\rightarrow $torso$\rightarrow $head). SAM proposes multiple hypotheses ranked by a predicted IoU, enabling the user or downstream code to select or refine the intended extent. Credit: Kirillov \emph  {et\,al.}~\blx@tocontentsinit {0}\cite {kirillov2023_sam}.}}{565}{figure.caption.1113}\protected@file@percent }
\abx@aux@backref{412}{kirillov2023_sam}{0}{565}{565}
\newlabel{fig:chapter15_sam_point_ambiguity}{{15.23}{565}{\textbf {Ambiguity-aware outputs.} Each \emph {column} shows three valid masks produced by SAM from a \emph {single point prompt} (green dot). \emph {Rows:} top = \textit {whole} object, middle = \textit {part}, bottom = \textit {subpart}. The examples illustrate hierarchical ambiguity under the same cue (e.g., person$\rightarrow $backpack$\rightarrow $pocket; bird$\rightarrow $torso$\rightarrow $head). SAM proposes multiple hypotheses ranked by a predicted IoU, enabling the user or downstream code to select or refine the intended extent. Credit: Kirillov \emph {et\,al.}~\cite {kirillov2023_sam}}{figure.caption.1113}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.24}{\ignorespaces \textbf  {Add/remove refinement.} Starting from a full bear mask, a negative click removes the torso to retain only the head, illustrating part-focused refinement. Example created by interacting with the official demo at \href  {https://segment-anything.com/}{segment-anything.com}.}}{565}{figure.caption.1114}\protected@file@percent }
\newlabel{fig:chapter15_sam_point_removal}{{15.24}{565}{\textbf {Add/remove refinement.} Starting from a full bear mask, a negative click removes the torso to retain only the head, illustrating part-focused refinement. Example created by interacting with the official demo at \href {https://segment-anything.com/}{segment-anything.com}}{figure.caption.1114}{}}
\abx@aux@backref{413}{he2022_mae}{0}{565}{565}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{he2022_mae}
\abx@aux@segm{0}{0}{he2022_mae}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\@writefile{toc}{\contentsline {subsubsection}{Method}{566}{section*.1115}\protected@file@percent }
\newlabel{enr:subsubsec_chapter15_sam_method}{{15.7.1}{566}{Method}{section*.1115}{}}
\@writefile{toc}{\contentsline {paragraph}{Model overview and data flow}{566}{section*.1116}\protected@file@percent }
\abx@aux@backref{414}{kirillov2023_sam}{0}{566}{566}
\@writefile{lof}{\contentsline {figure}{\numberline {15.25}{\ignorespaces \textbf  {SAM overview}. A heavyweight image encoder outputs a cached image embedding; a prompt encoder converts points/boxes/masks to tokens; a two-way transformer mask decoder fuses them to predict multiple candidate masks with IoU scores at interactive speed; credit: Kirillov \emph  {et\,al.}~\blx@tocontentsinit {0}\cite {kirillov2023_sam}.}}{566}{figure.caption.1117}\protected@file@percent }
\abx@aux@backref{416}{kirillov2023_sam}{0}{566}{566}
\newlabel{fig:chapter15_sam_overview}{{15.25}{566}{\textbf {SAM overview}. A heavyweight image encoder outputs a cached image embedding; a prompt encoder converts points/boxes/masks to tokens; a two-way transformer mask decoder fuses them to predict multiple candidate masks with IoU scores at interactive speed; credit: Kirillov \emph {et\,al.}~\cite {kirillov2023_sam}}{figure.caption.1117}{}}
\@writefile{toc}{\contentsline {paragraph}{Image encoder}{566}{section*.1118}\protected@file@percent }
\abx@aux@backref{417}{he2022_mae}{0}{566}{566}
\abx@aux@backref{418}{kirillov2023_sam}{0}{566}{566}
\@writefile{toc}{\contentsline {paragraph}{Prompt encoder}{566}{section*.1119}\protected@file@percent }
\abx@aux@backref{419}{kirillov2023_sam}{0}{566}{566}
\abx@aux@backref{420}{kirillov2023_sam}{0}{566}{566}
\abx@aux@backref{421}{radford2021_clip}{0}{566}{566}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\abx@aux@cite{0}{li2021_learnable_fourier}
\abx@aux@segm{0}{0}{li2021_learnable_fourier}
\abx@aux@cite{0}{li2021_learnable_fourier}
\abx@aux@segm{0}{0}{li2021_learnable_fourier}
\abx@aux@cite{0}{tancik2020_fourier}
\abx@aux@segm{0}{0}{tancik2020_fourier}
\@writefile{toc}{\contentsline {paragraph}{Positional encodings for 2D prompts}{567}{section*.1120}\protected@file@percent }
\newlabel{enr:par_chapter15_sam_posenc}{{15.7.1}{567}{Positional encodings for 2D prompts}{section*.1120}{}}
\abx@aux@backref{422}{vaswani2017_attention}{0}{567}{567}
\@writefile{lof}{\contentsline {figure}{\numberline {15.26}{\ignorespaces \textbf  {Positional similarity: separable 1D PE vs.\ 2D random Fourier features.} Each heatmap shows the dot-product between the embedding at the center (origin) and all other grid locations. With \emph  {separable 1D sinusoidal} PE (concatenating $x$-only and $y$-only sin/cos terms), iso-similarity contours are axis-aligned, producing anisotropy. We mark two points, $P_1$ (axis-aligned) and $P_2$ (diagonal), chosen so that $\|P_1\|\!\approx \!\|P_2\|$; nevertheless $\langle \mathrm  {PE}_{\text  {1D-sep}}(0),\,\mathrm  {PE}_{\text  {1D-sep}}(P_1)\rangle \gg \langle \mathrm  {PE}_{\text  {1D-sep}}(0),\,\mathrm  {PE}_{\text  {1D-sep}}(P_2)\rangle $, i.e., $d(0,P_1)\!\approx \!d(0,P_2)$ but the embedding similarity differs markedly—an undesirable bias. In contrast, \emph  {2D random Fourier features} (RFF) draw frequencies over the joint $(x,y)$ space, yielding near-isotropic similarity that decays primarily with Euclidean distance, so the center’s similarity to $P_1$ and $P_2$ is comparable. Inspired by~\blx@tocontentsinit {0}\cite {li2021_learnable_fourier}.}}{567}{figure.caption.1121}\protected@file@percent }
\abx@aux@backref{424}{li2021_learnable_fourier}{0}{567}{567}
\newlabel{fig:chapter15_sam_regular_vs_fourier}{{15.26}{567}{\textbf {Positional similarity: separable 1D PE vs.\ 2D random Fourier features.} Each heatmap shows the dot-product between the embedding at the center (origin) and all other grid locations. With \emph {separable 1D sinusoidal} PE (concatenating $x$-only and $y$-only sin/cos terms), iso-similarity contours are axis-aligned, producing anisotropy. We mark two points, $P_1$ (axis-aligned) and $P_2$ (diagonal), chosen so that $\|P_1\|\!\approx \!\|P_2\|$; nevertheless $\langle \mathrm {PE}_{\text {1D-sep}}(0),\,\mathrm {PE}_{\text {1D-sep}}(P_1)\rangle \gg \langle \mathrm {PE}_{\text {1D-sep}}(0),\,\mathrm {PE}_{\text {1D-sep}}(P_2)\rangle $, i.e., $d(0,P_1)\!\approx \!d(0,P_2)$ but the embedding similarity differs markedly—an undesirable bias. In contrast, \emph {2D random Fourier features} (RFF) draw frequencies over the joint $(x,y)$ space, yielding near-isotropic similarity that decays primarily with Euclidean distance, so the center’s similarity to $P_1$ and $P_2$ is comparable. Inspired by~\cite {li2021_learnable_fourier}}{figure.caption.1121}{}}
\abx@aux@backref{425}{tancik2020_fourier}{0}{567}{567}
\abx@aux@cite{0}{tancik2020_fourier}
\abx@aux@segm{0}{0}{tancik2020_fourier}
\abx@aux@cite{0}{jacot2018_ntk}
\abx@aux@segm{0}{0}{jacot2018_ntk}
\@writefile{lof}{\contentsline {figure}{\numberline {15.27}{\ignorespaces \textbf  {Fourier feature basis on a plane.} Rows of \(B\) induce oriented sinusoids at different spatial frequencies; their stack forms a rich 2D positional code. Credit: explanatory \href  {https://www.youtube.com/watch?v=iKyIJ_EtSkw}{video}.}}{568}{figure.caption.1122}\protected@file@percent }
\newlabel{fig:chapter15_sam_2d_ff_examples}{{15.27}{568}{\textbf {Fourier feature basis on a plane.} Rows of \(B\) induce oriented sinusoids at different spatial frequencies; their stack forms a rich 2D positional code. Credit: explanatory \href {https://www.youtube.com/watch?v=iKyIJ_EtSkw}{video}}{figure.caption.1122}{}}
\abx@aux@backref{426}{tancik2020_fourier}{0}{568}{568}
\abx@aux@backref{427}{jacot2018_ntk}{0}{568}{568}
\@writefile{lof}{\contentsline {figure}{\numberline {15.28}{\ignorespaces \textbf  {Kernel regression analogy.} A fit is a sum of local bumps; kernel width trades smoothness for detail. The NTK plays the same role for wide networks. Credit: explanatory \href  {https://www.youtube.com/watch?v=iKyIJ_EtSkw}{video}.}}{568}{figure.caption.1123}\protected@file@percent }
\newlabel{fig:chapter15_sam_kernel_regression}{{15.28}{568}{\textbf {Kernel regression analogy.} A fit is a sum of local bumps; kernel width trades smoothness for detail. The NTK plays the same role for wide networks. Credit: explanatory \href {https://www.youtube.com/watch?v=iKyIJ_EtSkw}{video}}{figure.caption.1123}{}}
\abx@aux@cite{0}{tancik2020_fourier}
\abx@aux@segm{0}{0}{tancik2020_fourier}
\@writefile{lof}{\contentsline {figure}{\numberline {15.29}{\ignorespaces \textbf  {Kernel width is critical.} Too wide \(\Rightarrow \) blurred structure (underfit). Too narrow \(\Rightarrow \) noisy/aliased (overfit). RFF exposes a single knob—\(\sigma \)—to dial the effective width via the scale of \(B\). Credit: explanatory \href  {https://www.youtube.com/watch?v=iKyIJ_EtSkw}{video}.}}{569}{figure.caption.1124}\protected@file@percent }
\newlabel{fig:chapter15_sam_kernel_width}{{15.29}{569}{\textbf {Kernel width is critical.} Too wide \(\Rightarrow \) blurred structure (underfit). Too narrow \(\Rightarrow \) noisy/aliased (overfit). RFF exposes a single knob—\(\sigma \)—to dial the effective width via the scale of \(B\). Credit: explanatory \href {https://www.youtube.com/watch?v=iKyIJ_EtSkw}{video}}{figure.caption.1124}{}}
\abx@aux@backref{428}{tancik2020_fourier}{0}{569}{569}
\@writefile{lof}{\contentsline {figure}{\numberline {15.30}{\ignorespaces \textbf  {NTK perspective.} RFF turns the network’s effective kernel into a stationary, radial form whose bandwidth is governed by \(\sigma \). Tuning \(\sigma \) navigates the bias–variance trade-off. Credit: explanatory \href  {https://www.youtube.com/watch?v=iKyIJ_EtSkw}{video}.}}{569}{figure.caption.1125}\protected@file@percent }
\newlabel{fig:chapter15_sam_ntk}{{15.30}{569}{\textbf {NTK perspective.} RFF turns the network’s effective kernel into a stationary, radial form whose bandwidth is governed by \(\sigma \). Tuning \(\sigma \) navigates the bias–variance trade-off. Credit: explanatory \href {https://www.youtube.com/watch?v=iKyIJ_EtSkw}{video}}{figure.caption.1125}{}}
\abx@aux@cite{0}{tancik2020_fourier}
\abx@aux@segm{0}{0}{tancik2020_fourier}
\abx@aux@cite{0}{tancik2020_fourier}
\abx@aux@segm{0}{0}{tancik2020_fourier}
\@writefile{lof}{\contentsline {figure}{\numberline {15.31}{\ignorespaces \textbf  {Too small \(\sigma \) (wide kernel).} High-frequency details are missed and outputs look over-smoothed/blurred. Credit: explanatory \href  {https://www.youtube.com/watch?v=iKyIJ_EtSkw}{video}.}}{570}{figure.caption.1126}\protected@file@percent }
\newlabel{fig:chapter15_sam_sigma_small}{{15.31}{570}{\textbf {Too small \(\sigma \) (wide kernel).} High-frequency details are missed and outputs look over-smoothed/blurred. Credit: explanatory \href {https://www.youtube.com/watch?v=iKyIJ_EtSkw}{video}}{figure.caption.1126}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.32}{\ignorespaces \textbf  {Near-optimal \(\sigma \).} Fine detail is preserved without a lot of aliasing; quality peaks near this region. Credit: explanatory \href  {https://www.youtube.com/watch?v=iKyIJ_EtSkw}{video}.}}{570}{figure.caption.1127}\protected@file@percent }
\newlabel{fig:chapter15_sam_sigma_opt}{{15.32}{570}{\textbf {Near-optimal \(\sigma \).} Fine detail is preserved without a lot of aliasing; quality peaks near this region. Credit: explanatory \href {https://www.youtube.com/watch?v=iKyIJ_EtSkw}{video}}{figure.caption.1127}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.33}{\ignorespaces \textbf  {Fourier features mitigate spectral bias.} A coordinate MLP remains blurry at equal iterations, whereas the same MLP with RFF recovers high-frequency detail much earlier. Credit: explanatory \href  {https://www.youtube.com/watch?v=iKyIJ_EtSkw}{video}; see also~\blx@tocontentsinit {0}\cite {tancik2020_fourier}.}}{570}{figure.caption.1128}\protected@file@percent }
\abx@aux@backref{430}{tancik2020_fourier}{0}{570}{570}
\newlabel{fig:chapter15_sam_ff_blur_vs_sharp}{{15.33}{570}{\textbf {Fourier features mitigate spectral bias.} A coordinate MLP remains blurry at equal iterations, whereas the same MLP with RFF recovers high-frequency detail much earlier. Credit: explanatory \href {https://www.youtube.com/watch?v=iKyIJ_EtSkw}{video}; see also~\cite {tancik2020_fourier}}{figure.caption.1128}{}}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\@writefile{toc}{\contentsline {paragraph}{Mask decoder (two-way attention and dynamic heads)}{571}{section*.1129}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15.34}{\ignorespaces \textbf  {SAM's lightweight mask decoder.} \textbf  {(a)} Inputs: prompt tokens plus four learned outputs (three mask tokens, one IoU token); if available, the previously accepted mask is injected as a \emph  {dense prompt} by adding its embedding to the image features. \textbf  {(b)} Two stacked two-way attention blocks: token self-attention fuses cues; token$\rightarrow $image retrieves spatial evidence; image$\rightarrow $token makes features prompt-aware (PE added on image attention; original prompt re-added to token queries/keys for stability). \textbf  {(c)} Upscaled features feed dynamic heads: mask tokens via MLP+dot yield multiple hypotheses; the IoU token scores them for ranking/selection. Adapted from ~\blx@tocontentsinit {0}\cite {kirillov2023_sam}.}}{571}{figure.caption.1130}\protected@file@percent }
\abx@aux@backref{432}{kirillov2023_sam}{0}{571}{571}
\newlabel{fig:chapter15_sam_decoder_details}{{15.34}{571}{\textbf {SAM's lightweight mask decoder.} \textbf {(a)} Inputs: prompt tokens plus four learned outputs (three mask tokens, one IoU token); if available, the previously accepted mask is injected as a \emph {dense prompt} by adding its embedding to the image features. \textbf {(b)} Two stacked two-way attention blocks: token self-attention fuses cues; token$\rightarrow $image retrieves spatial evidence; image$\rightarrow $token makes features prompt-aware (PE added on image attention; original prompt re-added to token queries/keys for stability). \textbf {(c)} Upscaled features feed dynamic heads: mask tokens via MLP+dot yield multiple hypotheses; the IoU token scores them for ranking/selection. Adapted from ~\cite {kirillov2023_sam}}{figure.caption.1130}{}}
\abx@aux@backref{433}{kirillov2023_sam}{0}{571}{571}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@backref{434}{kirillov2023_sam}{0}{572}{572}
\@writefile{toc}{\contentsline {paragraph}{Training objective and loss}{572}{section*.1131}\protected@file@percent }
\newlabel{enr:par_chapter15_sam_training}{{15.7.1}{572}{Training objective and loss}{section*.1131}{}}
\abx@aux@backref{435}{kirillov2023_sam}{0}{572}{572}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@backref{436}{kirillov2023_sam}{0}{573}{573}
\@writefile{lof}{\contentsline {figure}{\numberline {15.35}{\ignorespaces \textbf  {Dice loss intuition.} Dice complements focal by measuring region-level overlap: it decreases as symmetric set difference shrinks, and rises when FP/FN inflate the union. A high focal{:}dice weight in SAM targets boundary imbalance while preserving shape fidelity.}}{573}{figure.caption.1132}\protected@file@percent }
\newlabel{fig:chapter15_sam_dice}{{15.35}{573}{\textbf {Dice loss intuition.} Dice complements focal by measuring region-level overlap: it decreases as symmetric set difference shrinks, and rises when FP/FN inflate the union. A high focal{:}dice weight in SAM targets boundary imbalance while preserving shape fidelity}{figure.caption.1132}{}}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@backref{437}{kirillov2023_sam}{0}{574}{574}
\@writefile{toc}{\contentsline {paragraph}{Pseudo-code for interactive inference}{574}{section*.1133}\protected@file@percent }
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\@writefile{toc}{\contentsline {subsubsection}{Data engine and SA-1B}{575}{section*.1134}\protected@file@percent }
\newlabel{enr:subsubsec_chapter15_sam_data}{{15.7.1}{575}{Data engine and SA-1B}{section*.1134}{}}
\abx@aux@backref{438}{kirillov2023_sam}{0}{575}{575}
\@writefile{lof}{\contentsline {figure}{\numberline {15.36}{\ignorespaces \textbf  {SA-1B examples}. 11M licensed and privacy-protecting images and \(\sim \)1.1B masks; images grouped by masks-per-image to illustrate density and diversity; credit: Kirillov \emph  {et~al.}~\blx@tocontentsinit {0}\cite {kirillov2023_sam}.}}{575}{figure.caption.1135}\protected@file@percent }
\abx@aux@backref{440}{kirillov2023_sam}{0}{575}{575}
\newlabel{fig:chapter15_sa1b_examples}{{15.36}{575}{\textbf {SA-1B examples}. 11M licensed and privacy-protecting images and \(\sim \)1.1B masks; images grouped by masks-per-image to illustrate density and diversity; credit: Kirillov \emph {et~al.}~\cite {kirillov2023_sam}}{figure.caption.1135}{}}
\@writefile{toc}{\contentsline {paragraph}{Dataset properties and diversity}{575}{section*.1136}\protected@file@percent }
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\@writefile{lof}{\contentsline {figure}{\numberline {15.37}{\ignorespaces \textbf  {Normalized mask centers}. Heatmaps of mask centers across datasets indicate that SA-1B reduces strong center bias and covers corners/edges more uniformly; credit: Kirillov \emph  {et~al.}~\blx@tocontentsinit {0}\cite {kirillov2023_sam}.}}{576}{figure.caption.1137}\protected@file@percent }
\abx@aux@backref{442}{kirillov2023_sam}{0}{576}{576}
\newlabel{fig:chapter15_sam_centers}{{15.37}{576}{\textbf {Normalized mask centers}. Heatmaps of mask centers across datasets indicate that SA-1B reduces strong center bias and covers corners/edges more uniformly; credit: Kirillov \emph {et~al.}~\cite {kirillov2023_sam}}{figure.caption.1137}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.38}{\ignorespaces \textbf  {Mask properties}. SA-1B contains many images with high mask density, a broad distribution of mask sizes, and comparable or greater concavity diversity than prior datasets; credit: Kirillov \emph  {et~al.}~\blx@tocontentsinit {0}\cite {kirillov2023_sam}.}}{576}{figure.caption.1138}\protected@file@percent }
\abx@aux@backref{444}{kirillov2023_sam}{0}{576}{576}
\newlabel{fig:chapter15_sam_mask_props}{{15.38}{576}{\textbf {Mask properties}. SA-1B contains many images with high mask density, a broad distribution of mask sizes, and comparable or greater concavity diversity than prior datasets; credit: Kirillov \emph {et~al.}~\cite {kirillov2023_sam}}{figure.caption.1138}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.39}{\ignorespaces \textbf  {Geographic distribution}. Estimated distribution by country shows global coverage; the top three countries come from different regions; credit: Kirillov \emph  {et~al.}~\blx@tocontentsinit {0}\cite {kirillov2023_sam}.}}{576}{figure.caption.1139}\protected@file@percent }
\abx@aux@backref{446}{kirillov2023_sam}{0}{576}{576}
\newlabel{fig:chapter15_sam_geo}{{15.39}{576}{\textbf {Geographic distribution}. Estimated distribution by country shows global coverage; the top three countries come from different regions; credit: Kirillov \emph {et~al.}~\cite {kirillov2023_sam}}{figure.caption.1139}{}}
\@writefile{toc}{\contentsline {subsubsection}{Experiments and ablations}{576}{section*.1140}\protected@file@percent }
\newlabel{enr:subsubsec_chapter15_sam_experiments}{{15.7.1}{576}{Experiments and ablations}{section*.1140}{}}
\@writefile{toc}{\contentsline {paragraph}{Zero-shot samples across domains}{576}{section*.1141}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15.40}{\ignorespaces \textbf  {Zero-shot qualitative results}. Samples from 23 diverse datasets (autonomous driving, medical, aerial, egocentric, etc.) segmented by SAM without fine-tuning; credit: Kirillov \emph  {et~al.}~\blx@tocontentsinit {0}\cite {kirillov2023_sam}.}}{576}{figure.caption.1142}\protected@file@percent }
\abx@aux@backref{448}{kirillov2023_sam}{0}{576}{576}
\newlabel{fig:chapter15_sam_zeroshot}{{15.40}{576}{\textbf {Zero-shot qualitative results}. Samples from 23 diverse datasets (autonomous driving, medical, aerial, egocentric, etc.) segmented by SAM without fine-tuning; credit: Kirillov \emph {et~al.}~\cite {kirillov2023_sam}}{figure.caption.1142}{}}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{tancik2020_fourier}
\abx@aux@segm{0}{0}{tancik2020_fourier}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\@writefile{toc}{\contentsline {paragraph}{Interactive point-to-mask evaluation}{577}{section*.1143}\protected@file@percent }
\abx@aux@backref{449}{kirillov2023_sam}{0}{577}{577}
\abx@aux@backref{450}{kirillov2023_sam}{0}{577}{577}
\abx@aux@backref{451}{kirillov2023_sam}{0}{577}{577}
\abx@aux@backref{452}{kirillov2023_sam}{0}{577}{577}
\@writefile{lof}{\contentsline {figure}{\numberline {15.41}{\ignorespaces \textbf  {Zero-shot point-to-mask across 23 datasets.} \emph  {(a) One-click mIoU.} SAM (automatic selection via its IoU head) surpasses RITM on most datasets; the \emph  {SAM–oracle} bar (best-of-3 selection) is an upper bound, illustrating the benefit of ambiguity-aware decoding. \emph  {(b) Human study.} Mean mask-quality ratings place \emph  {SAM–oracle} near ground-truth and above prior interactive systems. \emph  {(c,d) Multi-click curves.} mIoU improves with additional corrective clicks (simulation places the next click at the largest error); gains taper after \textasciitilde 8 clicks, matching the training curriculum (up to 11 prompts). Panels adapted from Kirillov \emph  {et\,al.}~\blx@tocontentsinit {0}\cite {kirillov2023_sam}.}}{577}{figure.caption.1144}\protected@file@percent }
\abx@aux@backref{454}{kirillov2023_sam}{0}{577}{577}
\newlabel{fig:chapter15_sam_eval}{{15.41}{577}{\textbf {Zero-shot point-to-mask across 23 datasets.} \emph {(a) One-click mIoU.} SAM (automatic selection via its IoU head) surpasses RITM on most datasets; the \emph {SAM–oracle} bar (best-of-3 selection) is an upper bound, illustrating the benefit of ambiguity-aware decoding. \emph {(b) Human study.} Mean mask-quality ratings place \emph {SAM–oracle} near ground-truth and above prior interactive systems. \emph {(c,d) Multi-click curves.} mIoU improves with additional corrective clicks (simulation places the next click at the largest error); gains taper after \textasciitilde 8 clicks, matching the training curriculum (up to 11 prompts). Panels adapted from Kirillov \emph {et\,al.}~\cite {kirillov2023_sam}}{figure.caption.1144}{}}
\@writefile{toc}{\contentsline {paragraph}{Ablations (highlights)}{577}{section*.1145}\protected@file@percent }
\abx@aux@backref{455}{kirillov2023_sam}{0}{577}{577}
\abx@aux@backref{456}{kirillov2023_sam}{0}{577}{577}
\abx@aux@backref{457}{kirillov2023_sam}{0}{577}{577}
\abx@aux@backref{458}{tancik2020_fourier}{0}{577}{577}
\abx@aux@backref{459}{kirillov2023_sam}{0}{577}{577}
\abx@aux@backref{460}{kirillov2023_sam}{0}{577}{577}
\@writefile{toc}{\contentsline {subsubsection}{Limitations and future directions}{578}{section*.1146}\protected@file@percent }
\newlabel{enr:subsubsec_chapter15_sam_limits}{{15.7.1}{578}{Limitations and future directions}{section*.1146}{}}
\BKM@entry{id=574,dest={73656374696F6E2A2E31313437},srcline={1404}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030355C3030302E5C303030375C3030302E5C303030325C3030303A5C3030305C3034305C303030535C303030415C3030304D5C3030305C3034305C303030325C3030303A5C3030305C3034305C303030535C303030655C303030675C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030415C3030306E5C303030795C303030745C303030685C303030695C3030306E5C303030675C3030305C3034305C303030695C3030306E5C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030565C303030695C303030645C303030655C3030306F5C30303073}
\abx@aux@cite{0}{ravi2024_sam2}
\abx@aux@segm{0}{0}{ravi2024_sam2}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{ravi2024_sam2}
\abx@aux@segm{0}{0}{ravi2024_sam2}
\abx@aux@cite{0}{ravi2024_sam2}
\abx@aux@segm{0}{0}{ravi2024_sam2}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{ravi2024_sam2}
\abx@aux@segm{0}{0}{ravi2024_sam2}
\abx@aux@cite{0}{ravi2024_sam2}
\abx@aux@segm{0}{0}{ravi2024_sam2}
\abx@aux@cite{0}{ravi2024_sam2}
\abx@aux@segm{0}{0}{ravi2024_sam2}
\@writefile{toc}{\contentsline {subsection}{Enrichment 15.7.2: SAM 2: Segment Anything in Images and Videos}{579}{section*.1147}\protected@file@percent }
\newlabel{enr:subsec_chapter15_sam2}{{15.7.2}{579}{\color {ocre}Enrichment \thesubsection : SAM 2: Segment Anything in Images and Videos}{section*.1147}{}}
\abx@aux@backref{461}{ravi2024_sam2}{0}{579}{579}
\abx@aux@backref{462}{kirillov2023_sam}{0}{579}{579}
\@writefile{lof}{\contentsline {figure}{\numberline {15.42}{\ignorespaces \textbf  {SAM~2 overview.} The model extends promptable segmentation to images \emph  {and} videos by introducing a streaming memory that stores prompts and predictions from prior frames. The SA-V data engine scales training through human-in-the-loop collection and automatic propagation. Credit: SAM~2~\blx@tocontentsinit {0}\cite {ravi2024_sam2}.}}{579}{figure.caption.1148}\protected@file@percent }
\abx@aux@backref{464}{ravi2024_sam2}{0}{579}{579}
\newlabel{fig:chapter15_sam2_overview}{{15.42}{579}{\textbf {SAM~2 overview.} The model extends promptable segmentation to images \emph {and} videos by introducing a streaming memory that stores prompts and predictions from prior frames. The SA-V data engine scales training through human-in-the-loop collection and automatic propagation. Credit: SAM~2~\cite {ravi2024_sam2}}{figure.caption.1148}{}}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{579}{section*.1149}\protected@file@percent }
\newlabel{enr:subsubsec_chapter15_sam2_motivation}{{15.7.2}{579}{Motivation}{section*.1149}{}}
\abx@aux@backref{465}{kirillov2023_sam}{0}{579}{579}
\@writefile{lof}{\contentsline {figure}{\numberline {15.43}{\ignorespaces \textbf  {Interactive video segmentation with SAM~2.} An initial prompt on frame~1 yields a masklet that propagates forward. If tracking drifts, a single corrective click in a later frame recovers the object due to the streaming memory, avoiding re-annotation from scratch. Credit: SAM~2~\blx@tocontentsinit {0}\cite {ravi2024_sam2}.}}{579}{figure.caption.1150}\protected@file@percent }
\abx@aux@backref{467}{ravi2024_sam2}{0}{579}{579}
\newlabel{fig:chapter15_sam2_interactive}{{15.43}{579}{\textbf {Interactive video segmentation with SAM~2.} An initial prompt on frame~1 yields a masklet that propagates forward. If tracking drifts, a single corrective click in a later frame recovers the object due to the streaming memory, avoiding re-annotation from scratch. Credit: SAM~2~\cite {ravi2024_sam2}}{figure.caption.1150}{}}
\@writefile{toc}{\contentsline {subsubsection}{Method}{580}{section*.1151}\protected@file@percent }
\newlabel{enr:subsubsec_chapter15_sam2_method}{{15.7.2}{580}{Method}{section*.1151}{}}
\@writefile{toc}{\contentsline {paragraph}{Problem setup}{580}{section*.1152}\protected@file@percent }
\abx@aux@backref{468}{ravi2024_sam2}{0}{580}{580}
\@writefile{toc}{\contentsline {paragraph}{What is new compared to SAM}{580}{section*.1153}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why streaming memory}{580}{section*.1154}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{High-level data flow}{580}{section*.1155}\protected@file@percent }
\abx@aux@cite{0}{tancik2020_fourier}
\abx@aux@segm{0}{0}{tancik2020_fourier}
\@writefile{toc}{\contentsline {paragraph}{Streaming memory mechanics}{581}{section*.1156}\protected@file@percent }
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{kirillov2023_sam}
\abx@aux@segm{0}{0}{kirillov2023_sam}
\abx@aux@cite{0}{ravi2024_sam2}
\abx@aux@segm{0}{0}{ravi2024_sam2}
\@writefile{toc}{\contentsline {paragraph}{Prompt encoder}{582}{section*.1157}\protected@file@percent }
\abx@aux@backref{469}{tancik2020_fourier}{0}{582}{582}
\abx@aux@backref{470}{kirillov2023_sam}{0}{582}{582}
\@writefile{toc}{\contentsline {paragraph}{Mask decoder with memory conditioning}{582}{section*.1158}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Training objective and supervision}{582}{section*.1159}\protected@file@percent }
\abx@aux@backref{471}{kirillov2023_sam}{0}{582}{582}
\abx@aux@backref{472}{ravi2024_sam2}{0}{582}{582}
\abx@aux@cite{0}{ravi2024_sam2}
\abx@aux@segm{0}{0}{ravi2024_sam2}
\abx@aux@cite{0}{ravi2024_sam2}
\abx@aux@segm{0}{0}{ravi2024_sam2}
\abx@aux@cite{0}{sam2_repo}
\abx@aux@segm{0}{0}{sam2_repo}
\abx@aux@cite{0}{ravi2024_sam2}
\abx@aux@segm{0}{0}{ravi2024_sam2}
\abx@aux@cite{0}{ravi2024_sam2}
\abx@aux@segm{0}{0}{ravi2024_sam2}
\@writefile{toc}{\contentsline {paragraph}{Pseudo-code for streaming interactive inference}{583}{section*.1160}\protected@file@percent }
\newlabel{enr:par_chapter15_sam2_pseudocode}{{15.7.2}{583}{Pseudo-code for streaming interactive inference}{section*.1160}{}}
\@writefile{toc}{\contentsline {subsubsection}{Architecture \& Implementation Details}{583}{section*.1161}\protected@file@percent }
\newlabel{enr:subsubsec_chapter15_sam2_arch}{{15.7.2}{583}{Architecture \& Implementation Details}{section*.1161}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.44}{\ignorespaces \textbf  {Architecture.} Each frame is encoded once; memory tokens from prior frames are retrieved and fused with current features (and optional prompts) via a lightweight decoder to predict the mask. Predictions are transformed by a memory encoder for use in future frames. Credit: SAM~2~\blx@tocontentsinit {0}\cite {ravi2024_sam2}.}}{583}{figure.caption.1162}\protected@file@percent }
\abx@aux@backref{474}{ravi2024_sam2}{0}{583}{583}
\newlabel{fig:chapter15_sam2_arch}{{15.44}{583}{\textbf {Architecture.} Each frame is encoded once; memory tokens from prior frames are retrieved and fused with current features (and optional prompts) via a lightweight decoder to predict the mask. Predictions are transformed by a memory encoder for use in future frames. Credit: SAM~2~\cite {ravi2024_sam2}}{figure.caption.1162}{}}
\@writefile{toc}{\contentsline {subsubsection}{Experiments and Ablations}{583}{section*.1163}\protected@file@percent }
\newlabel{enr:subsubsec_chapter15_sam2_experiments}{{15.7.2}{583}{Experiments and Ablations}{section*.1163}{}}
\@writefile{toc}{\contentsline {paragraph}{SA-V dataset and data engine}{583}{section*.1164}\protected@file@percent }
\abx@aux@backref{475}{sam2_repo}{0}{583}{583}
\abx@aux@cite{0}{ravi2024_sam2}
\abx@aux@segm{0}{0}{ravi2024_sam2}
\abx@aux@cite{0}{ravi2024_sam2}
\abx@aux@segm{0}{0}{ravi2024_sam2}
\abx@aux@cite{0}{ravi2024_sam2}
\abx@aux@segm{0}{0}{ravi2024_sam2}
\abx@aux@cite{0}{ponttuset2017_davis}
\abx@aux@segm{0}{0}{ponttuset2017_davis}
\abx@aux@cite{0}{xu2018_youtubevos}
\abx@aux@segm{0}{0}{xu2018_youtubevos}
\abx@aux@cite{0}{wang2021_uvo}
\abx@aux@segm{0}{0}{wang2021_uvo}
\abx@aux@cite{0}{tokmakov2022_vost}
\abx@aux@segm{0}{0}{tokmakov2022_vost}
\abx@aux@cite{0}{athar2023_burst}
\abx@aux@segm{0}{0}{athar2023_burst}
\abx@aux@cite{0}{ding2023_mose}
\abx@aux@segm{0}{0}{ding2023_mose}
\abx@aux@cite{0}{ravi2024_sam2}
\abx@aux@segm{0}{0}{ravi2024_sam2}
\abx@aux@cite{0}{ponttuset2017_davis}
\abx@aux@segm{0}{0}{ponttuset2017_davis}
\abx@aux@cite{0}{xu2018_youtubevos}
\abx@aux@segm{0}{0}{xu2018_youtubevos}
\abx@aux@cite{0}{wang2021_uvo}
\abx@aux@segm{0}{0}{wang2021_uvo}
\abx@aux@cite{0}{tokmakov2022_vost}
\abx@aux@segm{0}{0}{tokmakov2022_vost}
\abx@aux@cite{0}{athar2023_burst}
\abx@aux@segm{0}{0}{athar2023_burst}
\abx@aux@cite{0}{ding2023_mose}
\abx@aux@segm{0}{0}{ding2023_mose}
\@writefile{lof}{\contentsline {figure}{\numberline {15.45}{\ignorespaces \textbf  {SA-V qualitative examples.} Masklets overlaid on sample videos; each color denotes a distinct masklet. Frames are sampled at 1-second intervals. Credit: SAM~2~\blx@tocontentsinit {0}\cite {ravi2024_sam2}.}}{584}{figure.caption.1165}\protected@file@percent }
\abx@aux@backref{477}{ravi2024_sam2}{0}{584}{584}
\newlabel{fig:chapter15_sam2_examples}{{15.45}{584}{\textbf {SA-V qualitative examples.} Masklets overlaid on sample videos; each color denotes a distinct masklet. Frames are sampled at 1-second intervals. Credit: SAM~2~\cite {ravi2024_sam2}}{figure.caption.1165}{}}
\@writefile{lot}{\contentsline {table}{\numberline {15.2}{\ignorespaces \textbf  {Data engine phases.} Average annotation time per frame, percent of edited frames per masklet, clicks per clicked frame, and mask alignment to Phase~1 by size. Credit: SAM~2~\blx@tocontentsinit {0}\cite {ravi2024_sam2}.}}{584}{table.caption.1166}\protected@file@percent }
\abx@aux@backref{479}{ravi2024_sam2}{0}{584}{584}
\newlabel{tab:chapter15_sam2_dataengine}{{15.2}{584}{\textbf {Data engine phases.} Average annotation time per frame, percent of edited frames per masklet, clicks per clicked frame, and mask alignment to Phase~1 by size. Credit: SAM~2~\cite {ravi2024_sam2}}{table.caption.1166}{}}
\abx@aux@cite{0}{ravi2024_sam2}
\abx@aux@segm{0}{0}{ravi2024_sam2}
\abx@aux@cite{0}{bekuzarov2023_xmempp}
\abx@aux@segm{0}{0}{bekuzarov2023_xmempp}
\abx@aux@cite{0}{cheng2024_cutie}
\abx@aux@segm{0}{0}{cheng2024_cutie}
\abx@aux@cite{0}{ravi2024_sam2}
\abx@aux@segm{0}{0}{ravi2024_sam2}
\abx@aux@cite{0}{bekuzarov2023_xmempp}
\abx@aux@segm{0}{0}{bekuzarov2023_xmempp}
\abx@aux@cite{0}{cheng2024_cutie}
\abx@aux@segm{0}{0}{cheng2024_cutie}
\abx@aux@cite{0}{ravi2024_sam2}
\abx@aux@segm{0}{0}{ravi2024_sam2}
\abx@aux@cite{0}{ravi2024_sam2}
\abx@aux@segm{0}{0}{ravi2024_sam2}
\@writefile{lot}{\contentsline {table}{\numberline {15.3}{\ignorespaces \textbf  {Dataset comparison.} SA-V versus common VOS datasets. Disappearance rate indicates the fraction of frames where the object is absent. Credit: SAM~2~\blx@tocontentsinit {0}\cite {ravi2024_sam2}; benchmarks include DAVIS~\blx@tocontentsinit {0}\cite {ponttuset2017_davis}, YouTube-VOS~\blx@tocontentsinit {0}\cite {xu2018_youtubevos}, UVO~\blx@tocontentsinit {0}\cite {wang2021_uvo}, VOST~\blx@tocontentsinit {0}\cite {tokmakov2022_vost}, BURST~\blx@tocontentsinit {0}\cite {athar2023_burst}, and MOSE~\blx@tocontentsinit {0}\cite {ding2023_mose}.}}{585}{table.caption.1167}\protected@file@percent }
\abx@aux@backref{487}{ravi2024_sam2}{0}{585}{585}
\abx@aux@backref{488}{ponttuset2017_davis}{0}{585}{585}
\abx@aux@backref{489}{xu2018_youtubevos}{0}{585}{585}
\abx@aux@backref{490}{wang2021_uvo}{0}{585}{585}
\abx@aux@backref{491}{tokmakov2022_vost}{0}{585}{585}
\abx@aux@backref{492}{athar2023_burst}{0}{585}{585}
\abx@aux@backref{493}{ding2023_mose}{0}{585}{585}
\newlabel{tab:chapter15_sam2_dataset_compare}{{15.3}{585}{\textbf {Dataset comparison.} SA-V versus common VOS datasets. Disappearance rate indicates the fraction of frames where the object is absent. Credit: SAM~2~\cite {ravi2024_sam2}; benchmarks include DAVIS~\cite {ponttuset2017_davis}, YouTube-VOS~\cite {xu2018_youtubevos}, UVO~\cite {wang2021_uvo}, VOST~\cite {tokmakov2022_vost}, BURST~\cite {athar2023_burst}, and MOSE~\cite {ding2023_mose}}{table.caption.1167}{}}
\@writefile{toc}{\contentsline {paragraph}{Zero-shot semi-supervised VOS}{585}{section*.1168}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {15.4}{\ignorespaces \textbf  {Semi-supervised VOS: zero-shot accuracy across 17 video datasets.} Average accuracy for different first-frame prompts. In the ``ground-truth mask'' case, masks are passed directly to XMem++/Cutie without SAM. Credit: SAM~2~\blx@tocontentsinit {0}\cite {ravi2024_sam2}; baselines from XMem++~\blx@tocontentsinit {0}\cite {bekuzarov2023_xmempp} and Cutie~\blx@tocontentsinit {0}\cite {cheng2024_cutie}.}}{585}{table.caption.1169}\protected@file@percent }
\abx@aux@backref{497}{ravi2024_sam2}{0}{585}{585}
\abx@aux@backref{498}{bekuzarov2023_xmempp}{0}{585}{585}
\abx@aux@backref{499}{cheng2024_cutie}{0}{585}{585}
\newlabel{tab:chapter15_sam2_vos}{{15.4}{585}{\textbf {Semi-supervised VOS: zero-shot accuracy across 17 video datasets.} Average accuracy for different first-frame prompts. In the ``ground-truth mask'' case, masks are passed directly to XMem++/Cutie without SAM. Credit: SAM~2~\cite {ravi2024_sam2}; baselines from XMem++~\cite {bekuzarov2023_xmempp} and Cutie~\cite {cheng2024_cutie}}{table.caption.1169}{}}
\@writefile{toc}{\contentsline {paragraph}{Segment Anything across 37 datasets}{585}{section*.1170}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {15.5}{\ignorespaces \textbf  {Segment Anything task across 37 datasets.} Average 1- and 5-click mIoU for SAM and SAM~2 on SA-23 and additional video datasets; FPS from the optimized video predictor. Credit: SAM~2~\blx@tocontentsinit {0}\cite {ravi2024_sam2}.}}{585}{table.caption.1171}\protected@file@percent }
\abx@aux@backref{501}{ravi2024_sam2}{0}{585}{585}
\newlabel{tab:chapter15_sam2_sa37}{{15.5}{585}{\textbf {Segment Anything task across 37 datasets.} Average 1- and 5-click mIoU for SAM and SAM~2 on SA-23 and additional video datasets; FPS from the optimized video predictor. Credit: SAM~2~\cite {ravi2024_sam2}}{table.caption.1171}{}}
\@writefile{toc}{\contentsline {paragraph}{Ablations}{586}{section*.1172}\protected@file@percent }
\abx@aux@cite{0}{yang2024_samurai}
\abx@aux@segm{0}{0}{yang2024_samurai}
\abx@aux@cite{0}{ren2024_groundedsam}
\abx@aux@segm{0}{0}{ren2024_groundedsam}
\abx@aux@cite{0}{groundedsam2_repo}
\abx@aux@segm{0}{0}{groundedsam2_repo}
\abx@aux@cite{0}{ding2024_sam2long}
\abx@aux@segm{0}{0}{ding2024_sam2long}
\abx@aux@cite{0}{ding2024_sam2long}
\abx@aux@segm{0}{0}{ding2024_sam2long}
\abx@aux@cite{0}{yang2024_samurai}
\abx@aux@segm{0}{0}{yang2024_samurai}
\abx@aux@cite{0}{ren2024_groundedsam}
\abx@aux@segm{0}{0}{ren2024_groundedsam}
\abx@aux@cite{0}{groundedsam2_repo}
\abx@aux@segm{0}{0}{groundedsam2_repo}
\abx@aux@cite{0}{ren2024_groundedsam}
\abx@aux@segm{0}{0}{ren2024_groundedsam}
\@writefile{toc}{\contentsline {subsubsection}{Limitations and Future Directions}{587}{section*.1173}\protected@file@percent }
\newlabel{enr:subsubsec_chapter15_sam2_limits}{{15.7.2}{587}{Limitations and Future Directions}{section*.1173}{}}
\abx@aux@backref{502}{yang2024_samurai}{0}{587}{587}
\abx@aux@backref{503}{ren2024_groundedsam}{0}{587}{587}
\abx@aux@backref{504}{groundedsam2_repo}{0}{587}{587}
\abx@aux@backref{505}{ding2024_sam2long}{0}{587}{587}
\abx@aux@backref{506}{ding2024_sam2long}{0}{587}{587}
\abx@aux@backref{507}{yang2024_samurai}{0}{587}{587}
\abx@aux@backref{508}{groundedsam2_repo}{0}{587}{587}
\abx@aux@backref{509}{ren2024_groundedsam}{0}{587}{587}
\abx@aux@backref{510}{ren2024_groundedsam}{0}{587}{587}
\BKM@entry{id=575,dest={636861707465722E3136},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030315C303030365C3030303A5C3030305C3034305C303030525C303030655C303030635C303030755C303030725C303030725C303030655C3030306E5C303030745C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=576,dest={73656374696F6E2E31362E31},srcline={9}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030525C303030655C303030635C303030755C303030725C303030725C303030655C3030306E5C303030745C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030525C3030304E5C3030304E5C303030735C3030305C303531}
\BKM@entry{id=577,dest={73756273656374696F6E2E31362E312E31},srcline={13}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030535C303030745C303030755C303030645C303030795C3030305C3034305C303030535C303030655C303030715C303030755C303030655C3030306E5C303030745C303030695C303030615C3030306C5C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030735C3030303F}
\abx@aux@cite{0}{vinyals2015_showtell}
\abx@aux@segm{0}{0}{vinyals2015_showtell}
\abx@aux@cite{0}{karpathy2014_largevideo}
\abx@aux@segm{0}{0}{karpathy2014_largevideo}
\abx@aux@cite{0}{sutskever2014_seq2seq}
\abx@aux@segm{0}{0}{sutskever2014_seq2seq}
\@writefile{toc}{\contentsline {chapter}{\numberline {16}Lecture 16: Recurrent Networks}{588}{chapter.16}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@15}}
\ttl@writefile{ptc}{\ttl@starttoc{default@16}}
\pgfsyspdfmark {pgfid81}{0}{52099153}
\pgfsyspdfmark {pgfid80}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {16.1}Introduction to Recurrent Neural Networks (RNNs)}{588}{section.16.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.1.1}Why Study Sequential Models?}{588}{subsection.16.1.1}\protected@file@percent }
\abx@aux@backref{511}{vinyals2015_showtell}{0}{588}{588}
\abx@aux@backref{512}{karpathy2014_largevideo}{0}{588}{588}
\abx@aux@backref{513}{sutskever2014_seq2seq}{0}{588}{588}
\BKM@entry{id=578,dest={73756273656374696F6E2E31362E312E32},srcline={33}}{5C3337365C3337375C303030525C3030304E5C3030304E5C303030735C3030305C3034305C303030615C303030735C3030305C3034305C303030615C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C3030306C5C3030302D5C303030505C303030755C303030725C303030705C3030306F5C303030735C303030655C3030305C3034305C303030535C303030655C303030715C303030755C303030655C3030306E5C303030635C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C}
\BKM@entry{id=579,dest={73756273656374696F6E2E31362E312E33},srcline={39}}{5C3337365C3337375C303030525C3030304E5C3030304E5C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{ba2015_attention}
\abx@aux@segm{0}{0}{ba2015_attention}
\@writefile{lof}{\contentsline {figure}{\numberline {16.1}{\ignorespaces Illustration of different sequence modeling problems and their RNN structures: One-to-One, One-to-Many, Many-to-One, and Many-to-Many.}}{589}{figure.caption.1174}\protected@file@percent }
\newlabel{fig:chapter16_rnn_types}{{16.1}{589}{Illustration of different sequence modeling problems and their RNN structures: One-to-One, One-to-Many, Many-to-One, and Many-to-Many}{figure.caption.1174}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.1.2}RNNs as a General-Purpose Sequence Model}{589}{subsection.16.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.1.3}RNNs for Visual Attention and Image Generation}{589}{subsection.16.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Visual Attention: Sequential Image Processing}{589}{section*.1175}\protected@file@percent }
\abx@aux@backref{514}{ba2015_attention}{0}{589}{589}
\abx@aux@cite{0}{gregor2015_draw}
\abx@aux@segm{0}{0}{gregor2015_draw}
\abx@aux@cite{0}{gregor2015_draw}
\abx@aux@segm{0}{0}{gregor2015_draw}
\BKM@entry{id=580,dest={73756273656374696F6E2E31362E312E34},srcline={72}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030545C303030725C303030615C303030645C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030535C303030655C303030715C303030755C303030655C3030306E5C303030745C303030695C303030615C3030306C5C3030305C3034305C303030445C303030615C303030745C30303061}
\BKM@entry{id=581,dest={73756273656374696F6E2E31362E312E35},srcline={94}}{5C3337365C3337375C3030304F5C303030765C303030655C303030725C303030765C303030695C303030655C303030775C3030305C3034305C3030306F5C303030665C3030305C3034305C303030525C303030655C303030635C303030755C303030725C303030725C303030655C3030306E5C303030745C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030525C3030304E5C3030304E5C303030735C3030305C3035315C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030545C303030685C303030655C303030695C303030725C3030305C3034305C303030455C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{bengio1994_learning}
\abx@aux@segm{0}{0}{bengio1994_learning}
\abx@aux@cite{0}{pascanu2013_difficulty}
\abx@aux@segm{0}{0}{pascanu2013_difficulty}
\abx@aux@cite{0}{hochreiter1997_lstm}
\abx@aux@segm{0}{0}{hochreiter1997_lstm}
\@writefile{toc}{\contentsline {subsubsection}{Autoregressive Image Generation with RNNs}{590}{section*.1176}\protected@file@percent }
\abx@aux@backref{515}{gregor2015_draw}{0}{590}{590}
\abx@aux@backref{516}{gregor2015_draw}{0}{590}{590}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.1.4}Limitations of Traditional Neural Networks for Sequential Data}{590}{subsection.16.1.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {16.1}{\ignorespaces Comparison of RNNs with Fully Connected and Convolutional Networks.}}{590}{table.caption.1177}\protected@file@percent }
\newlabel{tab:rnn_vs_fc_cnn}{{16.1}{590}{Comparison of RNNs with Fully Connected and Convolutional Networks}{table.caption.1177}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.1.5}Overview of Recurrent Neural Networks (RNNs) and Their Evolution}{590}{subsection.16.1.5}\protected@file@percent }
\newlabel{sec:rnn_overview}{{16.1.5}{590}{Overview of Recurrent Neural Networks (RNNs) and Their Evolution}{subsection.16.1.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{RNN Progression: From Vanilla to Gated Units}{590}{section*.1178}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Vanilla RNNs}{590}{section*.1179}\protected@file@percent }
\abx@aux@backref{517}{bengio1994_learning}{0}{590}{590}
\abx@aux@backref{518}{pascanu2013_difficulty}{0}{590}{590}
\@writefile{toc}{\contentsline {paragraph}{Long Short-Term Memory (LSTM)}{590}{section*.1180}\protected@file@percent }
\abx@aux@backref{519}{hochreiter1997_lstm}{0}{590}{590}
\abx@aux@cite{0}{cho2014_gru}
\abx@aux@segm{0}{0}{cho2014_gru}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\abx@aux@cite{0}{yao2022_improving}
\abx@aux@segm{0}{0}{yao2022_improving}
\abx@aux@cite{0}{gu2018_nonautoregressive}
\abx@aux@segm{0}{0}{gu2018_nonautoregressive}
\BKM@entry{id=582,dest={73656374696F6E2E31362E32},srcline={142}}{5C3337365C3337375C303030525C303030655C303030635C303030755C303030725C303030725C303030655C3030306E5C303030745C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030525C3030304E5C3030304E5C303030735C3030305C3035315C3030305C3034305C3030302D5C3030305C3034305C303030485C3030306F5C303030775C3030305C3034305C303030545C303030685C303030655C303030795C3030305C3034305C303030575C3030306F5C303030725C3030306B}
\@writefile{toc}{\contentsline {paragraph}{Gated Recurrent Units (GRUs)}{591}{section*.1181}\protected@file@percent }
\abx@aux@backref{520}{cho2014_gru}{0}{591}{591}
\@writefile{toc}{\contentsline {paragraph}{Bidirectional RNNs}{591}{section*.1182}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Motivation Toward Transformers}{591}{section*.1183}\protected@file@percent }
\abx@aux@backref{521}{vaswani2017_attention}{0}{591}{591}
\abx@aux@backref{522}{yao2022_improving}{0}{591}{591}
\abx@aux@backref{523}{gu2018_nonautoregressive}{0}{591}{591}
\@writefile{toc}{\contentsline {subsubsection}{Bridging to Detailed Explanations}{591}{section*.1184}\protected@file@percent }
\BKM@entry{id=583,dest={73756273656374696F6E2E31362E322E31},srcline={162}}{5C3337365C3337375C303030525C3030304E5C3030304E5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030725C303030615C303030705C30303068}
\@writefile{toc}{\contentsline {section}{\numberline {16.2}Recurrent Neural Networks (RNNs) - How They Work}{592}{section.16.2}\protected@file@percent }
\newlabel{sec:chapter16_rnn_how_it_works}{{16.2}{592}{Recurrent Neural Networks (RNNs) - How They Work}{section.16.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.2.1}RNN Computational Graph}{592}{subsection.16.2.1}\protected@file@percent }
\newlabel{sec:chapter16_rnn_computational_graph}{{16.2.1}{592}{RNN Computational Graph}{subsection.16.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Many-to-Many}{592}{section*.1185}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16.2}{\ignorespaces RNN Computational Graph for Many-to-Many Processing.}}{593}{figure.caption.1186}\protected@file@percent }
\newlabel{fig:chapter16_rnn_many_to_many}{{16.2}{593}{RNN Computational Graph for Many-to-Many Processing}{figure.caption.1186}{}}
\@writefile{toc}{\contentsline {subsubsection}{Many-to-One}{593}{section*.1187}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16.3}{\ignorespaces RNN Computational Graph for Many-to-One Processing.}}{593}{figure.caption.1188}\protected@file@percent }
\newlabel{fig:chapter16_rnn_many_to_one}{{16.3}{593}{RNN Computational Graph for Many-to-One Processing}{figure.caption.1188}{}}
\BKM@entry{id=584,dest={73756273656374696F6E2E31362E322E32},srcline={241}}{5C3337365C3337375C303030535C303030655C303030715C303030325C303030535C303030655C303030715C3030303A5C3030305C3034305C303030535C303030655C303030715C303030755C303030655C3030306E5C303030635C303030655C3030302D5C303030745C3030306F5C3030302D5C303030535C303030655C303030715C303030755C303030655C3030306E5C303030635C303030655C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\abx@aux@cite{0}{sutskever2014_seq2seq}
\abx@aux@segm{0}{0}{sutskever2014_seq2seq}
\@writefile{toc}{\contentsline {subsubsection}{One-to-Many}{594}{section*.1189}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16.4}{\ignorespaces RNN Computational Graph for One-to-Many Processing.}}{594}{figure.caption.1190}\protected@file@percent }
\newlabel{fig:chapter16_rnn_one_to_many}{{16.4}{594}{RNN Computational Graph for One-to-Many Processing}{figure.caption.1190}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.2.2}Seq2Seq: Sequence-to-Sequence Learning}{594}{subsection.16.2.2}\protected@file@percent }
\newlabel{sec:chapter16_seq2seq}{{16.2.2}{594}{Seq2Seq: Sequence-to-Sequence Learning}{subsection.16.2.2}{}}
\abx@aux@backref{524}{sutskever2014_seq2seq}{0}{594}{594}
\@writefile{lof}{\contentsline {figure}{\numberline {16.5}{\ignorespaces Computational graph of the Sequence-to-Sequence model. The encoder processes the input sequence, producing a final hidden state \( h_T \), which initializes the decoder. The decoder then generates the output sequence step by step.}}{595}{figure.caption.1191}\protected@file@percent }
\newlabel{fig:chapter16_seq2seq_computational_graph}{{16.5}{595}{Computational graph of the Sequence-to-Sequence model. The encoder processes the input sequence, producing a final hidden state \( h_T \), which initializes the decoder. The decoder then generates the output sequence step by step}{figure.caption.1191}{}}
\BKM@entry{id=585,dest={73656374696F6E2E31362E33},srcline={298}}{5C3337365C3337375C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030305C3034305C303030555C303030735C303030615C303030675C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C303030535C303030655C303030715C303030325C303030535C303030655C303030715C3030303A5C3030305C3034305C3030304C5C303030615C3030306E5C303030675C303030755C303030615C303030675C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030695C3030306E5C30303067}
\BKM@entry{id=586,dest={73756273656374696F6E2E31362E332E31},srcline={305}}{5C3337365C3337375C303030465C3030306F5C303030725C3030306D5C303030755C3030306C5C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030505C303030725C3030306F5C303030625C3030306C5C303030655C3030306D}
\BKM@entry{id=587,dest={73756273656374696F6E2E31362E332E32},srcline={326}}{5C3337365C3337375C3030304F5C3030306E5C303030655C3030302D5C303030485C3030306F5C303030745C3030305C3034305C303030455C3030306E5C303030635C3030306F5C303030645C303030695C3030306E5C303030675C3030305C3034305C3030306F5C303030665C3030305C3034305C303030495C3030306E5C303030705C303030755C303030745C3030305C3034305C303030435C303030685C303030615C303030725C303030615C303030635C303030745C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {subsubsection}{Significance of Seq2Seq Models}{596}{section*.1192}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {16.3}Example Usage of Seq2Seq: Language Modeling}{596}{section.16.3}\protected@file@percent }
\newlabel{sec:chapter16_seq2seq_language_model}{{16.3}{596}{Example Usage of Seq2Seq: Language Modeling}{section.16.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.3.1}Formulating the Problem}{596}{subsection.16.3.1}\protected@file@percent }
\BKM@entry{id=588,dest={73756273656374696F6E2E31362E332E33},srcline={347}}{5C3337365C3337375C303030505C303030725C3030306F5C303030635C303030655C303030735C303030735C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030465C303030695C303030725C303030735C303030745C3030305C3034305C303030435C303030685C303030615C303030725C303030615C303030635C303030745C303030655C30303072}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.3.2}One-Hot Encoding of Input Characters}{597}{subsection.16.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Advantages of One-Hot Encoding}{597}{section*.1193}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.3.3}Processing the First Character}{597}{subsection.16.3.3}\protected@file@percent }
\BKM@entry{id=589,dest={73756273656374696F6E2E31362E332E34},srcline={374}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030695C3030306E5C303030675C3030305C3034305C3030304C5C3030306F5C303030735C303030735C3030305C3034305C303030415C303030635C303030725C3030306F5C303030735C303030735C3030305C3034305C303030545C303030695C3030306D5C303030655C3030305C3034305C303030535C303030745C303030655C303030705C30303073}
\BKM@entry{id=590,dest={73756273656374696F6E2E31362E332E35},srcline={391}}{5C3337365C3337375C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030545C303030655C303030785C303030745C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030615C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030655C303030645C3030305C3034305C303030525C3030304E5C3030304E}
\@writefile{lof}{\contentsline {figure}{\numberline {16.6}{\ignorespaces Processing the first letter in "hello" and predicting "e" as the next character.}}{598}{figure.caption.1194}\protected@file@percent }
\newlabel{fig:chapter16_rnn_language_model_step}{{16.6}{598}{Processing the first letter in "hello" and predicting "e" as the next character}{figure.caption.1194}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.3.4}Computing Loss Across Time Steps}{598}{subsection.16.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.3.5}Generating Text with a Trained RNN}{598}{subsection.16.3.5}\protected@file@percent }
\BKM@entry{id=591,dest={73756273656374696F6E2E31362E332E36},srcline={413}}{5C3337365C3337375C303030555C303030735C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C3030305C3034305C303030455C3030306D5C303030625C303030655C303030645C303030645C303030695C3030306E5C303030675C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030435C303030685C303030615C303030725C303030615C303030635C303030745C303030655C303030725C3030305C3034305C303030495C3030306E5C303030705C303030755C303030745C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {16.7}{\ignorespaces Generating text by feeding back each predicted character as input.}}{599}{figure.caption.1195}\protected@file@percent }
\newlabel{fig:chapter16_rnn_text_generation}{{16.7}{599}{Generating text by feeding back each predicted character as input}{figure.caption.1195}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.3.6}Using an Embedding Layer for Character Inputs}{599}{subsection.16.3.6}\protected@file@percent }
\BKM@entry{id=592,dest={73756273656374696F6E2E31362E332E37},srcline={438}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304E5C303030655C303030785C303030745C3030305C3034305C303030535C303030745C303030655C303030705C30303073}
\BKM@entry{id=593,dest={73656374696F6E2E31362E34},srcline={444}}{5C3337365C3337375C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030685C303030725C3030306F5C303030755C303030675C303030685C3030305C3034305C303030545C303030695C3030306D5C303030655C3030305C3034305C3030305C3035305C303030425C303030505C303030545C303030545C3030305C303531}
\BKM@entry{id=594,dest={73756273656374696F6E2E31362E342E31},srcline={450}}{5C3337365C3337375C3030304D5C303030615C303030745C303030685C303030655C3030306D5C303030615C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C303030465C3030306F5C303030725C3030306D5C303030755C3030306C5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C303030425C303030505C303030545C303030545C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304D5C303030655C3030306D5C3030306F5C303030725C303030795C3030305C3034305C303030435C3030306F5C3030306E5C303030735C303030745C303030725C303030615C303030695C3030306E5C303030745C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {16.8}{\ignorespaces RNN architecture incorporating an embedding layer for character inputs.}}{600}{figure.caption.1196}\protected@file@percent }
\newlabel{fig:chapter16_rnn_embedding_layer}{{16.8}{600}{RNN architecture incorporating an embedding layer for character inputs}{figure.caption.1196}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.3.7}Conclusion and Next Steps}{600}{subsection.16.3.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {16.4}Backpropagation Through Time (BPTT)}{600}{section.16.4}\protected@file@percent }
\newlabel{sec:chapter16_bptt}{{16.4}{600}{Backpropagation Through Time (BPTT)}{section.16.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.4.1}Mathematical Formulation of BPTT and Memory Constraints}{600}{subsection.16.4.1}\protected@file@percent }
\newlabel{sec:chapter16_bptt_math}{{16.4.1}{600}{Mathematical Formulation of BPTT and Memory Constraints}{subsection.16.4.1}{}}
\abx@aux@cite{0}{bengio1994_learning}
\abx@aux@segm{0}{0}{bengio1994_learning}
\abx@aux@cite{0}{pascanu2013_difficulty}
\abx@aux@segm{0}{0}{pascanu2013_difficulty}
\BKM@entry{id=595,dest={73756273656374696F6E2E31362E342E32},srcline={498}}{5C3337365C3337375C303030545C303030725C303030755C3030306E5C303030635C303030615C303030745C303030655C303030645C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030685C303030725C3030306F5C303030755C303030675C303030685C3030305C3034305C303030545C303030695C3030306D5C30303065}
\abx@aux@backref{525}{bengio1994_learning}{0}{601}{601}
\abx@aux@backref{526}{pascanu2013_difficulty}{0}{601}{601}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.4.2}Truncated Backpropagation Through Time}{601}{subsection.16.4.2}\protected@file@percent }
\newlabel{sec:chapter16_truncated_bptt}{{16.4.2}{601}{Truncated Backpropagation Through Time}{subsection.16.4.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Loss Processing in Truncated BPTT}{601}{section*.1197}\protected@file@percent }
\newlabel{sec:chapter16_truncated_loss}{{16.4.2}{601}{Loss Processing in Truncated BPTT}{section*.1197}{}}
\BKM@entry{id=596,dest={73756273656374696F6E2E31362E342E33},srcline={531}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030425C303030505C303030545C303030545C3030305C3034305C303030465C303030615C303030695C3030306C5C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304C5C3030306F5C3030306E5C303030675C3030305C3034305C303030535C303030655C303030715C303030755C303030655C3030306E5C303030635C303030655C30303073}
\BKM@entry{id=597,dest={73656374696F6E2E31362E35},srcline={544}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030525C3030304E5C3030304E5C303030735C3030305C3034305C303030555C303030735C303030655C3030305C3034305C303030745C303030615C3030306E5C303030685C3030305C3034305C303030495C3030306E5C303030735C303030745C303030655C303030615C303030645C3030305C3034305C3030306F5C303030665C3030305C3034305C303030525C303030655C3030304C5C30303055}
\BKM@entry{id=598,dest={73756273656374696F6E2E31362E352E31},srcline={550}}{5C3337365C3337375C303030525C303030655C303030635C303030755C303030725C303030725C303030655C3030306E5C303030745C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030425C303030655C303030685C303030615C303030765C303030695C3030306F5C30303072}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.4.3}Why BPTT Fails for Long Sequences}{602}{subsection.16.4.3}\protected@file@percent }
\newlabel{sec:chapter16_bptt_failures}{{16.4.3}{602}{Why BPTT Fails for Long Sequences}{subsection.16.4.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.5}Why RNNs Use \textit  {tanh} Instead of ReLU}{602}{section.16.5}\protected@file@percent }
\newlabel{sec:chapter16_why_tanh_rnns}{{16.5}{602}{Why RNNs Use \textit {tanh} Instead of ReLU}{section.16.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.5.1}Recurrent Computation and Gradient Behavior}{602}{subsection.16.5.1}\protected@file@percent }
\newlabel{sec:chapter16_single_layer_rnn}{{16.5.1}{602}{Recurrent Computation and Gradient Behavior}{subsection.16.5.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Repeated Multiplication and the Hidden State}{602}{section*.1198}\protected@file@percent }
\BKM@entry{id=599,dest={73756273656374696F6E2E31362E352E32},srcline={645}}{5C3337365C3337375C3030304D5C303030615C303030745C303030685C303030655C3030306D5C303030615C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C303030525C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C303030655C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030745C303030615C3030306E5C303030685C3030305C3034305C303030695C3030306E5C3030305C3034305C303030525C3030304E5C3030304E5C30303073}
\@writefile{toc}{\contentsline {paragraph}{Spectral Properties of \(\mathbf  {W}_{hh}\).}{603}{section*.1199}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{How Large or Small States Affect Gradients}{603}{section*.1200}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Effect of Activation Function}{603}{section*.1201}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.5.2}Mathematical Rationale for \textit  {tanh} in RNNs}{603}{subsection.16.5.2}\protected@file@percent }
\newlabel{sec:chapter16_tanh_stability}{{16.5.2}{603}{Mathematical Rationale for \textit {tanh} in RNNs}{subsection.16.5.2}{}}
\abx@aux@cite{0}{pascanu2013_difficulty}
\abx@aux@segm{0}{0}{pascanu2013_difficulty}
\abx@aux@cite{0}{hochreiter1997_lstm}
\abx@aux@segm{0}{0}{hochreiter1997_lstm}
\abx@aux@cite{0}{cho2014_gru}
\abx@aux@segm{0}{0}{cho2014_gru}
\@writefile{toc}{\contentsline {subsubsection}{How \textit  {tanh} Curbs Exploding Gradients}{604}{section*.1202}\protected@file@percent }
\newlabel{sec:how_tanh_prevents_explosion}{{16.5.2}{604}{How \textit {tanh} Curbs Exploding Gradients}{section*.1202}{}}
\@writefile{toc}{\contentsline {paragraph}{1. Bounded Outputs:}{604}{section*.1203}\protected@file@percent }
\abx@aux@backref{527}{pascanu2013_difficulty}{0}{604}{604}
\@writefile{toc}{\contentsline {paragraph}{2. Derivative Control:}{604}{section*.1204}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3. Zero-Centered Activation:}{604}{section*.1205}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{A Caveat: Vanishing Gradients Still Remain}{604}{section*.1206}\protected@file@percent }
\abx@aux@backref{528}{cho2014_gru}{0}{604}{604}
\abx@aux@backref{529}{hochreiter1997_lstm}{0}{604}{604}
\BKM@entry{id=600,dest={73756273656374696F6E2E31362E352E33},srcline={682}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030525C303030655C3030304C5C303030555C303030365C3030305C3034305C3030306F5C303030725C3030305C3034305C3030304C5C303030655C303030615C3030306B5C303030795C3030305C3034305C303030525C303030655C3030304C5C303030555C3030305C3034305C303030415C303030725C303030655C3030305C3034305C3030304E5C3030306F5C303030745C3030305C3034305C303030615C3030305C3034305C303030465C303030755C3030306C5C3030306C5C3030305C3034305C303030525C303030655C3030306D5C303030655C303030645C30303079}
\BKM@entry{id=601,dest={73756273656374696F6E2E31362E352E34},srcline={710}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030435C3030306C5C303030695C303030705C303030705C303030695C3030306E5C303030675C3030305C3034305C303030415C3030306C5C3030306F5C3030306E5C303030655C3030305C3034305C303030695C303030735C3030305C3034305C303030495C3030306E5C303030735C303030755C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C30303074}
\abx@aux@cite{0}{pascanu2013_difficulty}
\abx@aux@segm{0}{0}{pascanu2013_difficulty}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.5.3}Why ReLU6 or Leaky ReLU Are Not a Full Remedy}{605}{subsection.16.5.3}\protected@file@percent }
\newlabel{sec:chapter16_relu_variants_rnn_issues}{{16.5.3}{605}{Why ReLU6 or Leaky ReLU Are Not a Full Remedy}{subsection.16.5.3}{}}
\@writefile{toc}{\contentsline {paragraph}{ReLU6: The Saturation Issue}{605}{section*.1207}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Leaky ReLU: A Partial Fix with Remaining Instability}{605}{section*.1208}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.5.4}Why Gradient Clipping Alone is Insufficient}{605}{subsection.16.5.4}\protected@file@percent }
\newlabel{sec:chapter16_gradient_clipping_limitations}{{16.5.4}{605}{Why Gradient Clipping Alone is Insufficient}{subsection.16.5.4}{}}
\abx@aux@backref{530}{pascanu2013_difficulty}{0}{605}{605}
\@writefile{toc}{\contentsline {paragraph}{Clipping Does Not Prevent Hidden State Growth}{606}{section*.1209}\protected@file@percent }
\BKM@entry{id=602,dest={73656374696F6E2E31362E36},srcline={731}}{5C3337365C3337375C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030305C3034305C303030555C303030735C303030615C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030525C303030655C303030635C303030755C303030725C303030725C303030655C3030306E5C303030745C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=603,dest={73756273656374696F6E2E31362E362E31},srcline={736}}{5C3337365C3337375C303030525C3030304E5C3030304E5C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030545C303030655C303030785C303030745C3030302D5C303030425C303030615C303030735C303030655C303030645C3030305C3034305C303030545C303030615C303030735C3030306B5C30303073}
\BKM@entry{id=604,dest={73756273656374696F6E2E31362E362E32},srcline={755}}{5C3337365C3337375C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030575C303030685C303030615C303030745C3030305C3034305C303030525C3030304E5C3030304E5C303030735C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E}
\abx@aux@cite{0}{karpathy2015_visualizing_rnns}
\abx@aux@segm{0}{0}{karpathy2015_visualizing_rnns}
\@writefile{toc}{\contentsline {section}{\numberline {16.6}Example Usages of Recurrent Neural Networks}{607}{section.16.6}\protected@file@percent }
\newlabel{sec:chapter16_rnn_examples}{{16.6}{607}{Example Usages of Recurrent Neural Networks}{section.16.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.6.1}RNNs for Text-Based Tasks}{607}{subsection.16.6.1}\protected@file@percent }
\newlabel{sec:chapter16_rnn_text_tasks}{{16.6.1}{607}{RNNs for Text-Based Tasks}{subsection.16.6.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Generating Text with RNNs}{607}{section*.1210}\protected@file@percent }
\newlabel{sec:chapter16_rnn_text_generation}{{16.6.1}{607}{Generating Text with RNNs}{section*.1210}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.6.2}Understanding What RNNs Learn}{607}{subsection.16.6.2}\protected@file@percent }
\newlabel{sec:chapter16_rnn_representations}{{16.6.2}{607}{Understanding What RNNs Learn}{subsection.16.6.2}{}}
\abx@aux@backref{531}{karpathy2015_visualizing_rnns}{0}{607}{607}
\@writefile{toc}{\contentsline {paragraph}{Visualization of Hidden State Activations}{607}{section*.1211}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16.9}{\ignorespaces Some hidden units do not exhibit clearly explainable patterns, making interpretation difficult.}}{608}{figure.caption.1212}\protected@file@percent }
\newlabel{fig:chapter16_uninterpretable_cells}{{16.9}{608}{Some hidden units do not exhibit clearly explainable patterns, making interpretation difficult}{figure.caption.1212}{}}
\@writefile{toc}{\contentsline {subsubsection}{Interpretable Hidden Units}{608}{section*.1213}\protected@file@percent }
\newlabel{sec:chapter16_rnn_interpretable_cells}{{16.6.2}{608}{Interpretable Hidden Units}{section*.1213}{}}
\@writefile{toc}{\contentsline {paragraph}{Quote Detection Cell}{608}{section*.1214}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16.10}{\ignorespaces An RNN hidden unit detecting quoted text. The activations shift significantly at the beginning and end of quotes.}}{608}{figure.caption.1215}\protected@file@percent }
\newlabel{fig:chapter16_quote_cell}{{16.10}{608}{An RNN hidden unit detecting quoted text. The activations shift significantly at the beginning and end of quotes}{figure.caption.1215}{}}
\BKM@entry{id=605,dest={73756273656374696F6E2E31362E362E33},srcline={826}}{5C3337365C3337375C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C303030615C303030705C303030745C303030695C3030306F5C3030306E5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {paragraph}{Line Length Tracking Cell}{609}{section*.1216}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16.11}{\ignorespaces An RNN hidden unit tracking line length, moving from blue (short lines) to red (long lines).}}{609}{figure.caption.1217}\protected@file@percent }
\newlabel{fig:chapter16_line_length}{{16.11}{609}{An RNN hidden unit tracking line length, moving from blue (short lines) to red (long lines)}{figure.caption.1217}{}}
\@writefile{toc}{\contentsline {paragraph}{Other Interpretable Hidden Units}{609}{section*.1218}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Key Takeaways from Interpretable Units}{609}{section*.1219}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.6.3}Image Captioning}{610}{subsection.16.6.3}\protected@file@percent }
\newlabel{sec:chapter16_image_captioning}{{16.6.3}{610}{Image Captioning}{subsection.16.6.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16.12}{\ignorespaces An RNN-based image captioning model stops generating text after producing an \texttt  {<END>} token.}}{610}{figure.caption.1220}\protected@file@percent }
\newlabel{fig:chapter16_image_captioning_pipeline}{{16.12}{610}{An RNN-based image captioning model stops generating text after producing an \texttt {<END>} token}{figure.caption.1220}{}}
\BKM@entry{id=606,dest={73756273656374696F6E2E31362E362E34},srcline={863}}{5C3337365C3337375C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C303030615C303030705C303030745C303030695C3030306F5C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030525C303030655C303030735C303030755C3030306C5C303030745C30303073}
\BKM@entry{id=607,dest={73756273656374696F6E2E31362E362E35},srcline={884}}{5C3337365C3337375C303030465C303030615C303030695C3030306C5C303030755C303030725C303030655C3030305C3034305C303030435C303030615C303030735C303030655C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C303030615C303030705C303030745C303030695C3030306F5C3030306E5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.6.4}Image Captioning Results}{611}{subsection.16.6.4}\protected@file@percent }
\newlabel{sec:chapter16_image_captioning_results}{{16.6.4}{611}{Image Captioning Results}{subsection.16.6.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16.13}{\ignorespaces Success cases of RNN-based image captioning: (Left) "A cat sitting on a suitcase on the floor." (Right) "Two giraffes standing in a grassy field."}}{611}{figure.caption.1221}\protected@file@percent }
\newlabel{fig:chapter16_captioning_success}{{16.13}{611}{Success cases of RNN-based image captioning: (Left) "A cat sitting on a suitcase on the floor." (Right) "Two giraffes standing in a grassy field."}{figure.caption.1221}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.6.5}Failure Cases in Image Captioning}{611}{subsection.16.6.5}\protected@file@percent }
\newlabel{sec:chapter16_image_captioning_failures}{{16.6.5}{611}{Failure Cases in Image Captioning}{subsection.16.6.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16.14}{\ignorespaces Failure cases of RNN-based image captioning, where captions reflect dataset biases rather than true understanding.}}{611}{figure.caption.1222}\protected@file@percent }
\newlabel{fig:chapter16_captioning_failures}{{16.14}{611}{Failure cases of RNN-based image captioning, where captions reflect dataset biases rather than true understanding}{figure.caption.1222}{}}
\BKM@entry{id=608,dest={73756273656374696F6E2E31362E362E36},srcline={921}}{5C3337365C3337375C303030425C303030725C303030695C303030645C303030675C303030695C3030306E5C303030675C3030305C3034305C303030745C3030306F5C3030305C3034305C3030304C5C303030535C303030545C3030304D5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030475C303030525C303030555C303030735C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C3030304E5C303030655C303030655C303030645C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030475C303030615C303030745C303030655C303030645C3030305C3034305C3030304D5C303030655C3030306D5C3030306F5C303030725C30303079}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.6.6}Bridging to LSTMs and GRUs: The Need for Gated Memory}{612}{subsection.16.6.6}\protected@file@percent }
\newlabel{sec:chapter16_bridging_to_lstm_gru}{{16.6.6}{612}{Bridging to LSTMs and GRUs: The Need for Gated Memory}{subsection.16.6.6}{}}
\BKM@entry{id=609,dest={73656374696F6E2E31362E37},srcline={946}}{5C3337365C3337375C3030304C5C3030306F5C3030306E5C303030675C3030305C3034305C303030535C303030685C3030306F5C303030725C303030745C3030302D5C303030545C303030655C303030725C3030306D5C3030305C3034305C3030304D5C303030655C3030306D5C3030306F5C303030725C303030795C3030305C3034305C3030305C3035305C3030304C5C303030535C303030545C3030304D5C3030305C3035315C3030305C3034305C3030304F5C303030765C303030655C303030725C303030765C303030695C303030655C30303077}
\abx@aux@cite{0}{hochreiter1997_lstm}
\abx@aux@segm{0}{0}{hochreiter1997_lstm}
\BKM@entry{id=610,dest={73756273656374696F6E2E31362E372E31},srcline={953}}{5C3337365C3337375C3030304C5C303030535C303030545C3030304D5C3030305C3034305C303030475C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C3030304D5C303030655C303030635C303030685C303030615C3030306E5C303030695C303030735C3030306D}
\BKM@entry{id=611,dest={73756273656374696F6E2E31362E372E32},srcline={965}}{5C3337365C3337375C3030304C5C303030535C303030545C3030304D5C3030305C3034305C303030475C303030615C303030745C303030655C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {16.7}Long Short-Term Memory (LSTM) Overview}{613}{section.16.7}\protected@file@percent }
\newlabel{sec:chapter16_lstm_overview}{{16.7}{613}{Long Short-Term Memory (LSTM) Overview}{section.16.7}{}}
\abx@aux@backref{532}{hochreiter1997_lstm}{0}{613}{613}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.7.1}LSTM Gating Mechanism}{613}{subsection.16.7.1}\protected@file@percent }
\newlabel{sec:chapter16_lstm_gating}{{16.7.1}{613}{LSTM Gating Mechanism}{subsection.16.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.7.2}LSTM Gate Computation}{613}{subsection.16.7.2}\protected@file@percent }
\newlabel{sec:chapter16_lstm_gates}{{16.7.2}{613}{LSTM Gate Computation}{subsection.16.7.2}{}}
\BKM@entry{id=612,dest={73756273656374696F6E2E31362E372E33},srcline={1034}}{5C3337365C3337375C3030304C5C303030535C303030545C3030304D5C3030305C3034305C303030535C303030745C303030615C303030745C303030655C3030305C3034305C303030555C303030705C303030645C303030615C303030745C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.7.3}LSTM State Updates}{614}{subsection.16.7.3}\protected@file@percent }
\newlabel{sec:chapter16_lstm_updates}{{16.7.3}{614}{LSTM State Updates}{subsection.16.7.3}{}}
\BKM@entry{id=613,dest={73756273656374696F6E2E31362E372E34},srcline={1068}}{5C3337365C3337375C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030465C3030306C5C3030306F5C303030775C3030305C3034305C303030695C3030306E5C3030305C3034305C3030304C5C303030535C303030545C3030304D5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {16.15}{\ignorespaces Long Short-Term Memory (LSTM) architecture. All gates are computed from the concatenated input \( [h_{t-1}, x_t] \) via a single matrix multiplication and then split. Each gate plays a role in regulating the memory and hidden state updates.}}{615}{figure.caption.1223}\protected@file@percent }
\newlabel{fig:chapter16_lstm_architecture}{{16.15}{615}{Long Short-Term Memory (LSTM) architecture. All gates are computed from the concatenated input \( [h_{t-1}, x_t] \) via a single matrix multiplication and then split. Each gate plays a role in regulating the memory and hidden state updates}{figure.caption.1223}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.7.4}Gradient Flow in LSTMs}{615}{subsection.16.7.4}\protected@file@percent }
\newlabel{sec:chapter16_gradient_flow_lstm}{{16.7.4}{615}{Gradient Flow in LSTMs}{subsection.16.7.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why the Cell State $\mathbf  {c}_t$ Preserves Long-Term Information}{615}{section*.1224}\protected@file@percent }
\newlabel{subsec:chapter16_lstm_cell_state}{{16.7.4}{615}{Why the Cell State \texorpdfstring {$\mathbf {c}_t$}{c\_t} Preserves Long-Term Information}{section*.1224}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why $\mathbf  {f}_t$ Prevents Vanishing Gradients}{616}{section*.1225}\protected@file@percent }
\newlabel{subsec:chapter16_lstm_forget_gate}{{16.7.4}{616}{Why \texorpdfstring {$\mathbf {f}_t$}{f\_t} Prevents Vanishing Gradients}{section*.1225}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why $\mathbf  {f}_t$ Can Be Learned to Stay Near 1}{616}{section*.1226}\protected@file@percent }
\newlabel{subsec:chapter16_lstm_why_f_near_1}{{16.7.4}{616}{Why \texorpdfstring {$\mathbf {f}_t$}{f\_t} Can Be Learned to Stay Near 1}{section*.1226}{}}
\abx@aux@cite{0}{srivastava2015_training}
\abx@aux@segm{0}{0}{srivastava2015_training}
\abx@aux@cite{0}{medium_lstm_vanishing}
\abx@aux@segm{0}{0}{medium_lstm_vanishing}
\abx@aux@cite{0}{hochreiter1997_lstm}
\abx@aux@segm{0}{0}{hochreiter1997_lstm}
\@writefile{toc}{\contentsline {subsubsection}{Why Forget Gates Do Not Cause Exponential Decay Like RNN Activations}{617}{section*.1227}\protected@file@percent }
\newlabel{subsec:chapter16_lstm_forget_gate_stability}{{16.7.4}{617}{Why Forget Gates Do Not Cause Exponential Decay Like RNN Activations}{section*.1227}{}}
\@writefile{toc}{\contentsline {subsubsection}{How Hidden-State Gradients Differ}{617}{section*.1228}\protected@file@percent }
\newlabel{subsec:chapter16_hidden_grad_vanilla}{{16.7.4}{617}{How Hidden-State Gradients Differ}{section*.1228}{}}
\abx@aux@backref{533}{medium_lstm_vanishing}{0}{617}{617}
\abx@aux@backref{534}{srivastava2015_training}{0}{617}{617}
\@writefile{toc}{\contentsline {paragraph}{Consequence for Training}{617}{section*.1229}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Why Weight-Gradient Vanishing Is Less Critical}{617}{section*.1230}\protected@file@percent }
\newlabel{subsec:chapter16_weights_gradients_lstm}{{16.7.4}{617}{Why Weight-Gradient Vanishing Is Less Critical}{section*.1230}{}}
\abx@aux@backref{535}{hochreiter1997_lstm}{0}{617}{617}
\abx@aux@cite{0}{pascanu2013_difficulty}
\abx@aux@segm{0}{0}{pascanu2013_difficulty}
\BKM@entry{id=614,dest={73656374696F6E2E31362E38},srcline={1231}}{5C3337365C3337375C303030525C303030655C303030735C303030655C3030306D5C303030625C3030306C5C303030615C3030306E5C303030635C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304C5C303030535C303030545C3030304D5C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C303030485C303030695C303030675C303030685C303030775C303030615C303030795C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C303030745C30303073}
\abx@aux@cite{0}{srivastava2015_training}
\abx@aux@segm{0}{0}{srivastava2015_training}
\abx@aux@cite{0}{he2016_resnet}
\abx@aux@segm{0}{0}{he2016_resnet}
\BKM@entry{id=615,dest={73756273656374696F6E2E31362E382E31},srcline={1236}}{5C3337365C3337375C303030485C303030695C303030675C303030685C303030775C303030615C303030795C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C303030535C303030545C3030304D5C30303073}
\@writefile{toc}{\contentsline {subsubsection}{Mitigating Exploding Gradients}{618}{section*.1231}\protected@file@percent }
\newlabel{subsec:chapter16_exploding_gradients}{{16.7.4}{618}{Mitigating Exploding Gradients}{section*.1231}{}}
\abx@aux@backref{536}{pascanu2013_difficulty}{0}{618}{618}
\@writefile{lof}{\contentsline {figure}{\numberline {16.16}{\ignorespaces  LSTM gradient flow: the primary error path is \emph  {additive} in the cell state $\mathbf  {c}_t$, with each step scaled by the forget gate $\mathbf  {f}_t \approx 1$ (in many cases). Other gate-derivative channels exist but often contribute smaller factors. }}{618}{figure.caption.1232}\protected@file@percent }
\newlabel{fig:chapter16_lstm_gradient_flow}{{16.16}{618}{LSTM gradient flow: the primary error path is \emph {additive} in the cell state $\mathbf {c}_t$, with each step scaled by the forget gate $\mathbf {f}_t \approx 1$ (in many cases). Other gate-derivative channels exist but often contribute smaller factors}{figure.caption.1232}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.8}Resemblance of LSTMs to Highway Networks and ResNets}{618}{section.16.8}\protected@file@percent }
\newlabel{subsec:chapter16_lstm_highway_resnets}{{16.8}{618}{Resemblance of LSTMs to Highway Networks and ResNets}{section.16.8}{}}
\abx@aux@backref{537}{srivastava2015_training}{0}{618}{618}
\abx@aux@backref{538}{he2016_resnet}{0}{618}{618}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.8.1}Highway Networks and LSTMs}{618}{subsection.16.8.1}\protected@file@percent }
\newlabel{sec:chapter16_highway_networks}{{16.8.1}{618}{Highway Networks and LSTMs}{subsection.16.8.1}{}}
\BKM@entry{id=616,dest={73756273656374696F6E2E31362E382E32},srcline={1247}}{5C3337365C3337375C303030525C303030655C303030735C3030304E5C303030655C303030745C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C303030535C303030545C3030304D5C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.8.2}ResNets and LSTMs}{619}{subsection.16.8.2}\protected@file@percent }
\newlabel{sec:chapter16_resnets}{{16.8.2}{619}{ResNets and LSTMs}{subsection.16.8.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Differences Between ResNets and Highway Networks}{619}{section*.1233}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16.17}{\ignorespaces Comparison between ResNets and LSTMs: Both employ additive connections to improve gradient flow, but LSTMs introduce gating mechanisms to dynamically control information retention across time.}}{619}{figure.caption.1234}\protected@file@percent }
\newlabel{fig:chapter16_resnets_lstm_similarity}{{16.17}{619}{Comparison between ResNets and LSTMs: Both employ additive connections to improve gradient flow, but LSTMs introduce gating mechanisms to dynamically control information retention across time}{figure.caption.1234}{}}
\BKM@entry{id=617,dest={73756273656374696F6E2E31362E382E33},srcline={1274}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030305C3034305C3030306F5C303030665C3030305C3034305C3030304C5C303030535C303030545C3030304D5C3030302C5C3030305C3034305C303030485C303030695C303030675C303030685C303030775C303030615C303030795C3030302C5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030525C303030655C303030735C3030304E5C303030655C303030745C3030305C3034305C303030435C3030306F5C3030306E5C3030306E5C303030655C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=618,dest={73656374696F6E2E31362E39},srcline={1278}}{5C3337365C3337375C303030535C303030745C303030615C303030635C3030306B5C303030695C3030306E5C303030675C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030525C3030304E5C3030304E5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C303030535C303030545C3030304D5C30303073}
\BKM@entry{id=619,dest={73756273656374696F6E2E31362E392E31},srcline={1284}}{5C3337365C3337375C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C303030535C303030745C303030615C303030635C3030306B5C303030655C303030645C3030305C3034305C303030525C3030304E5C3030304E5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C303030535C303030545C3030304D5C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.8.3}Summary of LSTM, Highway, and ResNet Connections}{620}{subsection.16.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {16.9}Stacking Layers in RNNs and LSTMs}{620}{section.16.9}\protected@file@percent }
\newlabel{sec:chapter16_stacking_rnn_lstm}{{16.9}{620}{Stacking Layers in RNNs and LSTMs}{section.16.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.9.1}Architecture of Stacked RNNs and LSTMs}{620}{subsection.16.9.1}\protected@file@percent }
\newlabel{sec:chapter16_stacked_rnn_architecture}{{16.9.1}{620}{Architecture of Stacked RNNs and LSTMs}{subsection.16.9.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16.18}{\ignorespaces A two-layer stacked RNN. The first layer reads the input sequence; its hidden states feed into the second layer, refining representations at each timestep.}}{620}{figure.caption.1235}\protected@file@percent }
\newlabel{fig:chapter16_two_layer_rnn}{{16.18}{620}{A two-layer stacked RNN. The first layer reads the input sequence; its hidden states feed into the second layer, refining representations at each timestep}{figure.caption.1235}{}}
\BKM@entry{id=620,dest={73756273656374696F6E2E31362E392E32},srcline={1322}}{5C3337365C3337375C303030505C303030725C303030615C303030635C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C303030525C3030304E5C3030304E5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C30303073}
\BKM@entry{id=621,dest={73756273656374696F6E2E31362E392E33},srcline={1334}}{5C3337365C3337375C303030445C303030655C303030655C303030705C3030305C3034305C303030525C3030304E5C3030304E5C303030735C3030303A5C3030305C3034305C303030425C303030615C3030306C5C303030615C3030306E5C303030635C303030695C3030306E5C303030675C3030305C3034305C303030445C303030655C303030705C303030745C303030685C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030635C30303079}
\BKM@entry{id=622,dest={73656374696F6E2A2E31323337},srcline={1340}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030365C3030302E5C303030315C303030305C3030303A5C3030305C3034305C3030304F5C303030745C303030685C303030655C303030725C3030305C3034305C303030525C3030304E5C3030304E5C3030305C3034305C303030565C303030615C303030725C303030695C303030615C3030306E5C303030745C303030735C3030303A5C3030305C3034305C303030475C303030525C30303055}
\abx@aux@cite{0}{cho2014_gru}
\abx@aux@segm{0}{0}{cho2014_gru}
\@writefile{lof}{\contentsline {figure}{\numberline {16.19}{\ignorespaces A three-layer stacked RNN. Each layer takes the hidden states from the layer below, thereby learning progressively more abstract temporal features.}}{621}{figure.caption.1236}\protected@file@percent }
\newlabel{fig:chapter16_three_layer_rnn}{{16.19}{621}{A three-layer stacked RNN. Each layer takes the hidden states from the layer below, thereby learning progressively more abstract temporal features}{figure.caption.1236}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.9.2}Practical Limitations of Deep RNN Architectures}{621}{subsection.16.9.2}\protected@file@percent }
\newlabel{subsec:chapter16_deep_rnn_limitations}{{16.9.2}{621}{Practical Limitations of Deep RNN Architectures}{subsection.16.9.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.9.3}Deep RNNs: Balancing Depth and Efficiency}{621}{subsection.16.9.3}\protected@file@percent }
\newlabel{sec:chapter16_summary_stacking}{{16.9.3}{621}{Deep RNNs: Balancing Depth and Efficiency}{subsection.16.9.3}{}}
\BKM@entry{id=623,dest={73656374696F6E2A2E31323338},srcline={1344}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030365C3030302E5C303030315C303030305C3030302E5C303030315C3030303A5C3030305C3034305C303030475C303030525C303030555C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C30303065}
\abx@aux@cite{0}{Mahadi2024_GRU_Plasmonic}
\abx@aux@segm{0}{0}{Mahadi2024_GRU_Plasmonic}
\abx@aux@cite{0}{Mahadi2024_GRU_Plasmonic}
\abx@aux@segm{0}{0}{Mahadi2024_GRU_Plasmonic}
\@writefile{toc}{\contentsline {section}{Enrichment 16.10: Other RNN Variants: GRU}{622}{section*.1237}\protected@file@percent }
\abx@aux@backref{539}{cho2014_gru}{0}{622}{622}
\@writefile{toc}{\contentsline {subsection}{Enrichment 16.10.1: GRU Architecture}{622}{section*.1238}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16.20}{\ignorespaces Visualization of GRU architecture, illustrating the reset and update gates. Adapted from \blx@tocontentsinit {0}\cite {Mahadi2024_GRU_Plasmonic}.}}{622}{figure.caption.1239}\protected@file@percent }
\abx@aux@backref{541}{Mahadi2024_GRU_Plasmonic}{0}{622}{622}
\newlabel{fig:chapter16_gru_architecture}{{16.20}{622}{Visualization of GRU architecture, illustrating the reset and update gates. Adapted from \cite {Mahadi2024_GRU_Plasmonic}}{figure.caption.1239}{}}
\abx@aux@cite{0}{cho2014_gru}
\abx@aux@segm{0}{0}{cho2014_gru}
\BKM@entry{id=624,dest={73656374696F6E2A2E31323431},srcline={1381}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030365C3030302E5C303030315C303030305C3030302E5C303030325C3030303A5C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030465C3030306C5C3030306F5C303030775C3030305C3034305C303030695C3030306E5C3030305C3034305C303030475C303030525C303030555C30303073}
\abx@aux@cite{0}{cho2014_gru}
\abx@aux@segm{0}{0}{cho2014_gru}
\@writefile{toc}{\contentsline {paragraph}{Key Observations:}{623}{section*.1240}\protected@file@percent }
\abx@aux@backref{542}{cho2014_gru}{0}{623}{623}
\@writefile{toc}{\contentsline {subsection}{Enrichment 16.10.2: Gradient Flow in GRUs}{623}{section*.1241}\protected@file@percent }
\abx@aux@backref{543}{cho2014_gru}{0}{623}{623}
\BKM@entry{id=625,dest={73656374696F6E2A2E31323435},srcline={1431}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030365C3030302E5C303030315C303030305C3030302E5C303030335C3030303A5C3030305C3034305C303030415C303030645C303030765C303030615C3030306E5C303030745C303030615C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030475C303030525C303030555C303030735C3030305C3034305C3030306F5C303030765C303030655C303030725C3030305C3034305C3030304C5C303030535C303030545C3030304D5C30303073}
\abx@aux@cite{0}{cho2014_gru}
\abx@aux@segm{0}{0}{cho2014_gru}
\BKM@entry{id=626,dest={73656374696F6E2A2E31323436},srcline={1443}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030365C3030302E5C303030315C303030305C3030302E5C303030345C3030303A5C3030305C3034305C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030475C303030525C303030555C30303073}
\BKM@entry{id=627,dest={73656374696F6E2A2E31323437},srcline={1455}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030365C3030302E5C303030315C303030305C3030302E5C303030355C3030303A5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C303030735C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304C5C303030535C303030545C3030304D5C30303073}
\BKM@entry{id=628,dest={73656374696F6E2A2E31323438},srcline={1470}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030315C303030365C3030302E5C303030315C303030305C3030302E5C303030365C3030303A5C3030305C3034305C303030425C303030725C303030695C303030645C303030675C303030695C3030306E5C303030675C3030305C3034305C303030745C3030306F5C3030305C3034305C303030415C303030645C303030765C303030615C3030306E5C303030635C303030655C303030645C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{Enrichment 16.10.3: Advantages of GRUs over LSTMs}{624}{section*.1245}\protected@file@percent }
\abx@aux@backref{544}{cho2014_gru}{0}{624}{624}
\@writefile{toc}{\contentsline {subsection}{Enrichment 16.10.4: Limitations of GRUs}{624}{section*.1246}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 16.10.5: Comparison with LSTMs}{624}{section*.1247}\protected@file@percent }
\BKM@entry{id=629,dest={73656374696F6E2E31362E3131},srcline={1480}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030465C303030755C303030745C303030755C303030725C303030655C3030305C3034305C303030445C303030695C303030725C303030655C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=630,dest={73756273656374696F6E2E31362E31312E31},srcline={1483}}{5C3337365C3337375C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030535C303030655C303030615C303030725C303030635C303030685C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030655C303030645C3030305C3034305C303030525C3030304E5C3030304E5C30303073}
\abx@aux@cite{0}{zoph2017_nas}
\abx@aux@segm{0}{0}{zoph2017_nas}
\abx@aux@cite{0}{zoph2017_nas}
\abx@aux@segm{0}{0}{zoph2017_nas}
\abx@aux@cite{0}{zoph2017_nas}
\abx@aux@segm{0}{0}{zoph2017_nas}
\@writefile{toc}{\contentsline {subsection}{Enrichment 16.10.6: Bridging to Advanced Architectures}{625}{section*.1248}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {16.11}Summary and Future Directions}{625}{section.16.11}\protected@file@percent }
\newlabel{sec:chapter16_summary_future}{{16.11}{625}{Summary and Future Directions}{section.16.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.11.1}Neural Architecture Search for Improved RNNs}{625}{subsection.16.11.1}\protected@file@percent }
\abx@aux@backref{545}{zoph2017_nas}{0}{625}{625}
\@writefile{lof}{\contentsline {figure}{\numberline {16.21}{\ignorespaces Neural Architecture Search (NAS) applied to recurrent neural network architectures, showcasing evolutionary exploration of candidate designs (adapted from \blx@tocontentsinit {0}\cite {zoph2017_nas}).}}{625}{figure.caption.1249}\protected@file@percent }
\abx@aux@backref{547}{zoph2017_nas}{0}{625}{625}
\newlabel{fig:chapter16_nas_rnn}{{16.21}{625}{Neural Architecture Search (NAS) applied to recurrent neural network architectures, showcasing evolutionary exploration of candidate designs (adapted from \cite {zoph2017_nas})}{figure.caption.1249}{}}
\BKM@entry{id=631,dest={73756273656374696F6E2E31362E31312E32},srcline={1497}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030305C3034305C3030306F5C303030665C3030305C3034305C303030525C3030304E5C3030304E5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C30303073}
\abx@aux@cite{0}{Mahadi2024_GRU_Plasmonic}
\abx@aux@segm{0}{0}{Mahadi2024_GRU_Plasmonic}
\abx@aux@cite{0}{Mahadi2024_GRU_Plasmonic}
\abx@aux@segm{0}{0}{Mahadi2024_GRU_Plasmonic}
\BKM@entry{id=632,dest={73756273656374696F6E2E31362E31312E33},srcline={1518}}{5C3337365C3337375C303030425C303030655C303030795C3030306F5C3030306E5C303030645C3030305C3034305C303030525C3030304E5C3030304E5C303030735C3030303A5C3030305C3034305C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030525C303030655C303030635C303030755C303030725C303030725C303030655C3030306E5C303030635C303030655C3030305C3034305C303030745C3030306F5C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.11.2}Summary of RNN Architectures}{626}{subsection.16.11.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16.22}{\ignorespaces Comparison of Vanilla RNN, LSTM, and GRU architectures. Source: \blx@tocontentsinit {0}\cite {Mahadi2024_GRU_Plasmonic}.}}{626}{figure.caption.1250}\protected@file@percent }
\abx@aux@backref{549}{Mahadi2024_GRU_Plasmonic}{0}{626}{626}
\newlabel{fig:chapter16_rnn_lstm_gru_comparison}{{16.22}{626}{Comparison of Vanilla RNN, LSTM, and GRU architectures. Source: \cite {Mahadi2024_GRU_Plasmonic}}{figure.caption.1250}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.11.3}Beyond RNNs: From Recurrence to Attention}{626}{subsection.16.11.3}\protected@file@percent }
\abx@aux@backref{550}{vaswani2017_attention}{0}{626}{626}
\BKM@entry{id=633,dest={636861707465722E3137},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030315C303030375C3030303A5C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=634,dest={73656374696F6E2E31372E31},srcline={10}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030535C303030655C303030715C303030755C303030655C3030306E5C303030635C303030655C3030302D5C303030745C3030306F5C3030302D5C303030535C303030655C303030715C303030755C303030655C3030306E5C303030635C303030655C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030525C3030304E5C3030304E5C30303073}
\@writefile{toc}{\contentsline {chapter}{\numberline {17}Lecture 17: Attention}{627}{chapter.17}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@16}}
\ttl@writefile{ptc}{\ttl@starttoc{default@17}}
\pgfsyspdfmark {pgfid83}{0}{52099153}
\pgfsyspdfmark {pgfid82}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {17.1}Limitations of Sequence-to-Sequence with RNNs}{627}{section.17.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.1}{\ignorespaces Sequence-to-sequence with RNNs.}}{627}{figure.caption.1251}\protected@file@percent }
\newlabel{fig:chapter17_seq2seq_rnn}{{17.1}{627}{Sequence-to-sequence with RNNs}{figure.caption.1251}{}}
\BKM@entry{id=635,dest={73656374696F6E2E31372E32},srcline={40}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C303030655C303030635C303030685C303030615C3030306E5C303030695C303030735C3030306D}
\@writefile{lof}{\contentsline {figure}{\numberline {17.2}{\ignorespaces The bottleneck problem in sequence-to-sequence RNNs.}}{628}{figure.caption.1252}\protected@file@percent }
\newlabel{fig:chapter17_bottleneck}{{17.2}{628}{The bottleneck problem in sequence-to-sequence RNNs}{figure.caption.1252}{}}
\@writefile{toc}{\contentsline {section}{\numberline {17.2}Introducing the Attention Mechanism}{628}{section.17.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.3}{\ignorespaces Attention mechanism in sequence-to-sequence models.}}{629}{figure.caption.1253}\protected@file@percent }
\newlabel{fig:chapter17_attention}{{17.3}{629}{Attention mechanism in sequence-to-sequence models}{figure.caption.1253}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17.4}{\ignorespaces Illustration of attention weights for translating \texttt  {"we are eating bread"} to \texttt  {"estamos comiendo pan"}.}}{629}{figure.caption.1254}\protected@file@percent }
\newlabel{fig:chapter17_attention_example}{{17.4}{629}{Illustration of attention weights for translating \texttt {"we are eating bread"} to \texttt {"estamos comiendo pan"}}{figure.caption.1254}{}}
\BKM@entry{id=636,dest={73756273656374696F6E2E31372E322E31},srcline={92}}{5C3337365C3337375C303030425C303030655C3030306E5C303030655C303030665C303030695C303030745C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=637,dest={73756273656374696F6E2E31372E322E32},srcline={104}}{5C3337365C3337375C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030495C3030306E5C303030745C303030655C303030725C303030705C303030725C303030655C303030745C303030615C303030625C303030695C3030306C5C303030695C303030745C30303079}
\abx@aux@cite{0}{bahdanau2016_neural}
\abx@aux@segm{0}{0}{bahdanau2016_neural}
\@writefile{toc}{\contentsline {subsubsection}{Intuition Behind Attention}{630}{section*.1255}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.2.1}Benefits of Attention}{630}{subsection.17.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.2.2}Attention Interpretability}{630}{subsection.17.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Attention Maps: Visualizing Model Decisions}{630}{section*.1256}\protected@file@percent }
\abx@aux@backref{551}{bahdanau2016_neural}{0}{630}{630}
\@writefile{lof}{\contentsline {figure}{\numberline {17.5}{\ignorespaces Visualization of attention weights in the form of an attention map, offering insight into the seq2seq model's alignment process.}}{631}{figure.caption.1257}\protected@file@percent }
\newlabel{fig:chapter17_attention_map}{{17.5}{631}{Visualization of attention weights in the form of an attention map, offering insight into the seq2seq model's alignment process}{figure.caption.1257}{}}
\@writefile{toc}{\contentsline {subsubsection}{Understanding Attention Patterns}{631}{section*.1258}\protected@file@percent }
\BKM@entry{id=638,dest={73656374696F6E2E31372E33},srcline={162}}{5C3337365C3337375C303030415C303030705C303030705C3030306C5C303030795C303030695C3030306E5C303030675C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C303030615C303030705C303030745C303030695C3030306F5C3030306E5C303030695C3030306E5C30303067}
\abx@aux@cite{0}{xu2015_showattend}
\abx@aux@segm{0}{0}{xu2015_showattend}
\BKM@entry{id=639,dest={73756273656374696F6E2E31372E332E31},srcline={173}}{5C3337365C3337375C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030525C303030655C303030705C303030725C303030655C303030735C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsubsection}{Why Attention Interpretability Matters}{632}{section*.1259}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {17.3}Applying Attention to Image Captioning}{632}{section.17.3}\protected@file@percent }
\abx@aux@backref{552}{xu2015_showattend}{0}{632}{632}
\@writefile{lof}{\contentsline {figure}{\numberline {17.6}{\ignorespaces Image captioning using RNNs and attention. A CNN extracts spatial feature vectors, and the decoder dynamically attends to different image regions at each timestep.}}{632}{figure.caption.1260}\protected@file@percent }
\newlabel{fig:chapter17_image_captioning}{{17.6}{632}{Image captioning using RNNs and attention. A CNN extracts spatial feature vectors, and the decoder dynamically attends to different image regions at each timestep}{figure.caption.1260}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.3.1}Feature Representation and Attention Computation}{632}{subsection.17.3.1}\protected@file@percent }
\BKM@entry{id=640,dest={73756273656374696F6E2E31372E332E32},srcline={223}}{5C3337365C3337375C303030475C303030655C3030306E5C303030655C303030725C303030615C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030745C3030306F5C3030305C3034305C303030415C3030306E5C303030795C3030305C3034305C303030545C303030695C3030306D5C303030655C303030735C303030745C303030655C303030705C3030305C3034305C3030305C3034305C303030745C3030305C303430}
\BKM@entry{id=641,dest={73756273656374696F6E2E31372E332E33},srcline={242}}{5C3337365C3337375C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C3030303A5C3030305C3034305C303030435C303030615C303030705C303030745C303030695C3030306F5C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C3030306F5C303030665C3030305C3034305C303030615C3030305C3034305C303030435C303030615C30303074}
\BKM@entry{id=642,dest={73756273656374696F6E2E31372E332E34},srcline={274}}{5C3337365C3337375C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030695C3030306E5C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030435C303030615C303030705C303030745C303030695C3030306F5C3030306E5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.3.2}Generalizing to Any Timestep \( t \)}{634}{subsection.17.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.3.3}Example: Captioning an Image of a Cat}{634}{subsection.17.3.3}\protected@file@percent }
\abx@aux@cite{0}{xu2015_showattend}
\abx@aux@segm{0}{0}{xu2015_showattend}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.3.4}Visualizing Attention in Image Captioning}{635}{subsection.17.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.7}{\ignorespaces Attention maps corresponding to different caption tokens for an image of a bird flying above water. Brighter regions indicate higher attention weights.}}{635}{figure.caption.1261}\protected@file@percent }
\newlabel{fig:chapter17_attention_map_visual_example}{{17.7}{635}{Attention maps corresponding to different caption tokens for an image of a bird flying above water. Brighter regions indicate higher attention weights}{figure.caption.1261}{}}
\@writefile{toc}{\contentsline {subsubsection}{Hard vs. Soft Attention}{635}{section*.1262}\protected@file@percent }
\abx@aux@backref{553}{xu2015_showattend}{0}{635}{635}
\BKM@entry{id=643,dest={73756273656374696F6E2E31372E332E35},srcline={296}}{5C3337365C3337375C303030425C303030695C3030306F5C3030306C5C3030306F5C303030675C303030695C303030635C303030615C3030306C5C3030305C3034305C303030495C3030306E5C303030735C303030705C303030695C303030725C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030535C303030615C303030635C303030635C303030615C303030645C303030655C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030485C303030755C3030306D5C303030615C3030306E5C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.3.5}Biological Inspiration: Saccades in Human Vision}{636}{subsection.17.3.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.8}{\ignorespaces Illustration of the retina (left) and a graph of visual acuity across different retinal positions (right). The x-axis represents retinal position, while the y-axis represents acuity (ranging from 0 to 1).}}{636}{figure.caption.1263}\protected@file@percent }
\newlabel{fig:chapter17_retina_acuity}{{17.8}{636}{Illustration of the retina (left) and a graph of visual acuity across different retinal positions (right). The x-axis represents retinal position, while the y-axis represents acuity (ranging from 0 to 1)}{figure.caption.1263}{}}
\BKM@entry{id=644,dest={73756273656374696F6E2E31372E332E36},srcline={331}}{5C3337365C3337375C303030425C303030655C303030795C3030306F5C3030306E5C303030645C3030305C3034305C303030435C303030615C303030705C303030745C303030695C3030306F5C3030306E5C303030695C3030306E5C303030675C3030303A5C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C303030655C303030635C303030685C303030615C3030306E5C303030695C303030735C3030306D5C30303073}
\abx@aux@cite{0}{xu2016_askattend}
\abx@aux@segm{0}{0}{xu2016_askattend}
\abx@aux@cite{0}{chan2016_listenattend}
\abx@aux@segm{0}{0}{chan2016_listenattend}
\abx@aux@cite{0}{mei2016_listenwalk}
\abx@aux@segm{0}{0}{mei2016_listenwalk}
\BKM@entry{id=645,dest={73656374696F6E2E31372E34},srcline={343}}{5C3337365C3337375C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304C5C303030615C303030795C303030655C30303072}
\@writefile{lof}{\contentsline {figure}{\numberline {17.9}{\ignorespaces Illustration of saccades in human vision and their relation to attention-based image captioning.}}{637}{figure.caption.1264}\protected@file@percent }
\newlabel{fig:chapter17_saccades_captioning}{{17.9}{637}{Illustration of saccades in human vision and their relation to attention-based image captioning}{figure.caption.1264}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.3.6}Beyond Captioning: Generalizing Attention Mechanisms}{637}{subsection.17.3.6}\protected@file@percent }
\abx@aux@backref{554}{xu2016_askattend}{0}{637}{637}
\abx@aux@backref{555}{chan2016_listenattend}{0}{637}{637}
\abx@aux@backref{556}{mei2016_listenwalk}{0}{637}{637}
\BKM@entry{id=646,dest={73756273656374696F6E2E31372E342E31},srcline={376}}{5C3337365C3337375C303030535C303030635C303030615C3030306C5C303030655C303030645C3030305C3034305C303030445C3030306F5C303030745C3030302D5C303030505C303030725C3030306F5C303030645C303030755C303030635C303030745C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {17.4}Attention Layer}{638}{section.17.4}\protected@file@percent }
\newlabel{sec:chapter17_attention_layer}{{17.4}{638}{Attention Layer}{section.17.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.4.1}Scaled Dot-Product Attention}{638}{subsection.17.4.1}\protected@file@percent }
\newlabel{sec:chapter17_scaled_dot_product}{{17.4.1}{638}{Scaled Dot-Product Attention}{subsection.17.4.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Scale by \(\sqrt  {D_Q}\)?}{639}{section*.1265}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Scaling and Softmax Temperature}{639}{section*.1266}\protected@file@percent }
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\abx@aux@cite{0}{bahdanau2016_neural}
\abx@aux@segm{0}{0}{bahdanau2016_neural}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\BKM@entry{id=647,dest={73756273656374696F6E2E31372E342E32},srcline={456}}{5C3337365C3337375C303030455C303030785C303030745C303030655C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030745C3030306F5C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C303030705C3030306C5C303030655C3030305C3034305C303030515C303030755C303030655C303030725C303030795C3030305C3034305C303030565C303030655C303030635C303030745C3030306F5C303030725C30303073}
\@writefile{toc}{\contentsline {paragraph}{Why Dot Product?}{640}{section*.1267}\protected@file@percent }
\abx@aux@backref{557}{vaswani2017_attention}{0}{640}{640}
\abx@aux@backref{558}{bahdanau2016_neural}{0}{640}{640}
\abx@aux@backref{559}{vaswani2017_attention}{0}{640}{640}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.4.2}Extending to Multiple Query Vectors}{640}{subsection.17.4.2}\protected@file@percent }
\newlabel{sec:chapter17_multiple_queries}{{17.4.2}{640}{Extending to Multiple Query Vectors}{subsection.17.4.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Benefits of Multiple Queries:}{640}{section*.1268}\protected@file@percent }
\BKM@entry{id=648,dest={73756273656374696F6E2E31372E342E33},srcline={484}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030695C3030306E5C303030675C3030305C3034305C3030304B5C303030655C303030795C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030565C303030615C3030306C5C303030755C303030655C3030305C3034305C303030565C303030655C303030635C303030745C3030306F5C303030725C30303073}
\BKM@entry{id=649,dest={73756273656374696F6E2E31372E342E34},srcline={512}}{5C3337365C3337375C303030415C3030306E5C3030305C3034305C303030415C3030306E5C303030615C3030306C5C3030306F5C303030675C303030795C3030303A5C3030305C3034305C303030535C303030655C303030615C303030725C303030635C303030685C3030305C3034305C303030455C3030306E5C303030675C303030695C3030306E5C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.4.3}Introducing Key and Value Vectors}{641}{subsection.17.4.3}\protected@file@percent }
\newlabel{sec:chapter17_keys_values}{{17.4.3}{641}{Introducing Key and Value Vectors}{subsection.17.4.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Separate Keys and Values?}{641}{section*.1269}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.4.4}An Analogy: Search Engines}{641}{subsection.17.4.4}\protected@file@percent }
\newlabel{sec:chapter17_search_engine_analogy}{{17.4.4}{641}{An Analogy: Search Engines}{subsection.17.4.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Empire State Building Example}{641}{section*.1270}\protected@file@percent }
\BKM@entry{id=650,dest={73756273656374696F6E2E31372E342E35},srcline={553}}{5C3337365C3337375C303030425C303030725C303030695C303030645C303030675C303030695C3030306E5C303030675C3030305C3034305C303030745C3030306F5C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030465C303030755C303030725C303030745C303030685C303030655C303030725C3030305C3034305C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {subsubsection}{Why This Separation Matters}{642}{section*.1271}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.4.5}Bridging to Visualization and Further Understanding}{642}{subsection.17.4.5}\protected@file@percent }
\newlabel{sec:chapter17_attention_visualization}{{17.4.5}{642}{Bridging to Visualization and Further Understanding}{subsection.17.4.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{Overview of the Attention Layer Steps}{642}{section*.1272}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.10}{\ignorespaces Visualization of the Attention Layer. The input vectors \(\textcolor [rgb]{0.267,0.447,0.769}{X}\) and query vectors \(\textcolor [rgb]{0.471,0.694,0.318}{Q}\) are transformed into key vectors \(\textcolor [rgb]{0.929,0.502,0.212}{K}\) and value vectors \(\textcolor [rgb]{0.769,0.369,0.800}{V}\). The similarity scores \(\textcolor [rgb]{0.236,0.236,0.236}{E}\), attention weights \(\textcolor [rgb]{0.236,0.236,0.236}{A}\), and final outputs \(\textcolor [rgb]{1.0,0.816,0.267}{Y}\) illustrate the attention process. Each column in \(\textcolor [rgb]{0.236,0.236,0.236}{E^T}\) and \(\textcolor [rgb]{0.236,0.236,0.236}{A^T}\) corresponds to a query vector, aligning visually with its associated output.}}{643}{figure.caption.1273}\protected@file@percent }
\newlabel{fig:chapter17_attention_visualization}{{17.10}{643}{Visualization of the Attention Layer. The input vectors \(\textcolor [rgb]{0.267,0.447,0.769}{X}\) and query vectors \(\textcolor [rgb]{0.471,0.694,0.318}{Q}\) are transformed into key vectors \(\textcolor [rgb]{0.929,0.502,0.212}{K}\) and value vectors \(\textcolor [rgb]{0.769,0.369,0.800}{V}\). The similarity scores \(\textcolor [rgb]{0.236,0.236,0.236}{E}\), attention weights \(\textcolor [rgb]{0.236,0.236,0.236}{A}\), and final outputs \(\textcolor [rgb]{1.0,0.816,0.267}{Y}\) illustrate the attention process. Each column in \(\textcolor [rgb]{0.236,0.236,0.236}{E^T}\) and \(\textcolor [rgb]{0.236,0.236,0.236}{A^T}\) corresponds to a query vector, aligning visually with its associated output}{figure.caption.1273}{}}
\BKM@entry{id=651,dest={73756273656374696F6E2E31372E342E36},srcline={667}}{5C3337365C3337375C303030545C3030306F5C303030775C303030615C303030725C303030645C303030735C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.4.6}Towards Self-Attention}{644}{subsection.17.4.6}\protected@file@percent }
\abx@aux@backref{560}{vaswani2017_attention}{0}{644}{644}
\BKM@entry{id=652,dest={73656374696F6E2E31372E35},srcline={683}}{5C3337365C3337375C303030535C303030655C3030306C5C303030665C3030302D5C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=653,dest={73756273656374696F6E2E31372E352E31},srcline={690}}{5C3337365C3337375C3030304D5C303030615C303030745C303030685C303030655C3030306D5C303030615C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C303030465C3030306F5C303030725C3030306D5C303030755C3030306C5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=654,dest={73756273656374696F6E2E31372E352E32},srcline={721}}{5C3337365C3337375C3030304E5C3030306F5C3030306E5C3030302D5C3030304C5C303030695C3030306E5C303030655C303030615C303030725C303030695C303030745C303030795C3030305C3034305C303030695C3030306E5C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {17.5}Self-Attention}{645}{section.17.5}\protected@file@percent }
\newlabel{sec:chapter17_self_attention}{{17.5}{645}{Self-Attention}{section.17.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.5.1}Mathematical Formulation of Self-Attention}{645}{subsection.17.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.11}{\ignorespaces Visualization of the Self-Attention Layer without positional encoding. The input vectors \(\textcolor [rgb]{0.267,0.447,0.769}{X}\) are transformed into \(\textcolor [rgb]{0.471,0.694,0.318}{Q}\), \(\textcolor [rgb]{0.929,0.502,0.212}{K}\), and \(\textcolor [rgb]{0.769,0.369,0.800}{V}\), and the final outputs are computed via weighted summation.}}{645}{figure.caption.1274}\protected@file@percent }
\newlabel{fig:chapter17_self_attention}{{17.11}{645}{Visualization of the Self-Attention Layer without positional encoding. The input vectors \(\textcolor [rgb]{0.267,0.447,0.769}{X}\) are transformed into \(\textcolor [rgb]{0.471,0.694,0.318}{Q}\), \(\textcolor [rgb]{0.929,0.502,0.212}{K}\), and \(\textcolor [rgb]{0.769,0.369,0.800}{V}\), and the final outputs are computed via weighted summation}{figure.caption.1274}{}}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\abx@aux@cite{0}{bahdanau2016_neural}
\abx@aux@segm{0}{0}{bahdanau2016_neural}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.5.2}Non-Linearity in Self-Attention}{646}{subsection.17.5.2}\protected@file@percent }
\newlabel{sec:chapter17_non_linearity_self_attention}{{17.5.2}{646}{Non-Linearity in Self-Attention}{subsection.17.5.2}{}}
\abx@aux@backref{561}{vaswani2017_attention}{0}{646}{646}
\abx@aux@backref{562}{bahdanau2016_neural}{0}{646}{646}
\abx@aux@backref{563}{vaswani2017_attention}{0}{646}{646}
\abx@aux@backref{564}{vaswani2017_attention}{0}{646}{646}
\BKM@entry{id=655,dest={73756273656374696F6E2E31372E352E33},srcline={744}}{5C3337365C3337375C303030505C303030655C303030725C3030306D5C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030455C303030715C303030755C303030695C303030765C303030615C303030725C303030695C303030615C3030306E5C303030635C303030655C3030305C3034305C303030695C3030306E5C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.5.3}Permutation Equivariance in Self-Attention}{647}{subsection.17.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.12}{\ignorespaces Self-Attention is permutation equivariant: permuting the input vectors results in permuted outputs, demonstrating that the layer treats inputs as an unordered set.}}{647}{figure.caption.1275}\protected@file@percent }
\newlabel{fig:chapter17_permutation_equivariance}{{17.12}{647}{Self-Attention is permutation equivariant: permuting the input vectors results in permuted outputs, demonstrating that the layer treats inputs as an unordered set}{figure.caption.1275}{}}
\@writefile{toc}{\contentsline {subsubsection}{When is Permutation Equivariance a Problem?}{647}{section*.1276}\protected@file@percent }
\BKM@entry{id=656,dest={73756273656374696F6E2E31372E352E34},srcline={782}}{5C3337365C3337375C303030505C3030306F5C303030735C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030455C3030306E5C303030635C3030306F5C303030645C303030695C3030306E5C303030675C303030735C3030303A5C3030305C3034305C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=657,dest={73756273656374696F6E2E31372E352E35},srcline={820}}{5C3337365C3337375C303030535C303030695C3030306E5C303030755C303030735C3030306F5C303030695C303030645C303030615C3030306C5C3030305C3034305C303030505C3030306F5C303030735C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030455C3030306E5C303030635C3030306F5C303030645C303030695C3030306E5C30303067}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.5.4}Positional Encodings: Introduction}{648}{subsection.17.5.4}\protected@file@percent }
\newlabel{sec:chapter17_positional_encodings}{{17.5.4}{648}{Positional Encodings: Introduction}{subsection.17.5.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Not Use Simple Positional Indices?}{648}{section*.1277}\protected@file@percent }
\newlabel{par:chapter17_why_not_positional_indices}{{17.5.4}{648}{Why Not Use Simple Positional Indices?}{section*.1277}{}}
\newlabel{eq:simple_index}{{17.44}{648}{Why Not Use Simple Positional Indices?}{equation.17.44}{}}
\newlabel{eq:normalized_index}{{17.45}{648}{Why Not Use Simple Positional Indices?}{equation.17.45}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.5.5}Sinusoidal Positional Encoding}{648}{subsection.17.5.5}\protected@file@percent }
\newlabel{sec:chapter17_sinusoidal_encoding}{{17.5.5}{648}{Sinusoidal Positional Encoding}{subsection.17.5.5}{}}
\abx@aux@backref{565}{vaswani2017_attention}{0}{648}{648}
\@writefile{toc}{\contentsline {subsubsection}{Mathematical Definition}{649}{section*.1278}\protected@file@percent }
\newlabel{eq:chapter17_sinusoidal_encoding}{{17.46}{649}{Mathematical Definition}{equation.17.46}{}}
\@writefile{toc}{\contentsline {paragraph}{Intuition: Multiple Frequencies for Local \& Global Positioning}{649}{section*.1279}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.13}{\ignorespaces Sinusoidal positional encoding visualization where \(t\) denotes the position index and \(d\) the embedding dimension. The encoding reflects multiple frequency scales, facilitating both local and global dependency modeling.}}{649}{figure.caption.1280}\protected@file@percent }
\newlabel{fig:chapter17_positional_encoding}{{17.13}{649}{Sinusoidal positional encoding visualization where \(t\) denotes the position index and \(d\) the embedding dimension. The encoding reflects multiple frequency scales, facilitating both local and global dependency modeling}{figure.caption.1280}{}}
\@writefile{toc}{\contentsline {subsubsection}{Removing Ambiguity with Sine and Cosine}{649}{section*.1281}\protected@file@percent }
\abx@aux@cite{0}{wiki_sine_cosine}
\abx@aux@segm{0}{0}{wiki_sine_cosine}
\abx@aux@cite{0}{wiki_sine_cosine}
\abx@aux@segm{0}{0}{wiki_sine_cosine}
\@writefile{lof}{\contentsline {figure}{\numberline {17.14}{\ignorespaces Sine and cosine functions over one period. The phase shift of 90° ensures that when the sine value repeats, the cosine value differs, reducing the chance of ambiguity in positional encoding. Image source: \blx@tocontentsinit {0}\cite {wiki_sine_cosine}.}}{650}{figure.caption.1282}\protected@file@percent }
\abx@aux@backref{567}{wiki_sine_cosine}{0}{650}{650}
\newlabel{fig:sine_cosine_period}{{17.14}{650}{Sine and cosine functions over one period. The phase shift of 90° ensures that when the sine value repeats, the cosine value differs, reducing the chance of ambiguity in positional encoding. Image source: \cite {wiki_sine_cosine}}{figure.caption.1282}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why Use \(\displaystyle 10000\) in the Denominator?}{650}{section*.1283}\protected@file@percent }
\newlabel{par:why_10000_sin_encoding}{{17.5.5}{650}{Why Use \(\displaystyle 10000\) in the Denominator?}{section*.1283}{}}
\@writefile{toc}{\contentsline {subsubsection}{Frequency Variation and Intuition}{650}{section*.1284}\protected@file@percent }
\newlabel{sec:chapter17_freq_var_intuition}{{17.5.5}{650}{Frequency Variation and Intuition}{section*.1284}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17.15}{\ignorespaces Comparison of sinusoidal positional encodings for positions \(t=1\), \(t=2\), \(t=50\), and \(t=500\). The x-axis represents the embedding dimension index \(i\) (ranging from \(0\) to \(511\) for an embedding dimension \(d=512\)). The y-axis shows the corresponding encoded value for each \(p_t(i)\). For each \(i\), if \(i=2k\), then \(p_t(i) = \sin (\omega _k \cdot t)\), and if \(i=2k+1\), then \(p_t(i) = \cos (\omega _k \cdot t)\), where \(k = \frac  {i}{2}\) and \(\omega _k = \frac  {1}{10000^{2k/d}}\). Positions that are close (\(1\) and \(2\)) primarily differ in their high-frequency dimensions, while distant positions (\(50\) and \(500\)) show more pronounced differences in the lower-frequency dimensions in comparison.}}{651}{figure.caption.1285}\protected@file@percent }
\newlabel{fig:chapter17_positional_encoding_comparison_no_t1000}{{17.15}{651}{Comparison of sinusoidal positional encodings for positions \(t=1\), \(t=2\), \(t=50\), and \(t=500\). The x-axis represents the embedding dimension index \(i\) (ranging from \(0\) to \(511\) for an embedding dimension \(d=512\)). The y-axis shows the corresponding encoded value for each \(p_t(i)\). For each \(i\), if \(i=2k\), then \(p_t(i) = \sin (\omega _k \cdot t)\), and if \(i=2k+1\), then \(p_t(i) = \cos (\omega _k \cdot t)\), where \(k = \frac {i}{2}\) and \(\omega _k = \frac {1}{10000^{2k/d}}\). Positions that are close (\(1\) and \(2\)) primarily differ in their high-frequency dimensions, while distant positions (\(50\) and \(500\)) show more pronounced differences in the lower-frequency dimensions in comparison}{figure.caption.1285}{}}
\@writefile{toc}{\contentsline {paragraph}{Concrete Example:}{652}{section*.1286}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{How Relative Position Awareness Emerges}{652}{section*.1287}\protected@file@percent }
\newlabel{sec:chapter17_relpos_sinusoids}{{17.5.5}{652}{How Relative Position Awareness Emerges}{section*.1287}{}}
\@writefile{toc}{\contentsline {paragraph}{Why This Matters for Relative Positioning}{652}{section*.1288}\protected@file@percent }
\BKM@entry{id=658,dest={73756273656374696F6E2E31372E352E36},srcline={1032}}{5C3337365C3337375C3030304C5C303030655C303030615C303030725C3030306E5C303030655C303030645C3030305C3034305C303030505C3030306F5C303030735C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030455C3030306E5C303030635C3030306F5C303030645C303030695C3030306E5C303030675C303030735C3030303A5C3030305C3034305C303030415C3030306E5C3030305C3034305C303030415C3030306C5C303030745C303030655C303030725C3030306E5C303030615C303030745C303030695C303030765C303030655C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030615C303030635C30303068}
\@writefile{lof}{\contentsline {figure}{\numberline {17.16}{\ignorespaces Sinusoidal positional encoding applied to the sentence "I Can Buy Myself Flowers." The encoding ensures consistent relative distances between tokens while allowing generalization to longer sequences.}}{653}{figure.caption.1289}\protected@file@percent }
\newlabel{fig:chapter17_positional_encoding_example}{{17.16}{653}{Sinusoidal positional encoding applied to the sentence "I Can Buy Myself Flowers." The encoding ensures consistent relative distances between tokens while allowing generalization to longer sequences}{figure.caption.1289}{}}
\@writefile{toc}{\contentsline {subsubsection}{Does Positional Information Vanish in Deeper Layers?}{653}{section*.1290}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Why Sinusoidal Encoding Solves Previous Limitations}{653}{section*.1291}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Conclusion on Sinusoidal Positional Encoding}{653}{section*.1292}\protected@file@percent }
\abx@aux@cite{0}{devlin2019_bert}
\abx@aux@segm{0}{0}{devlin2019_bert}
\abx@aux@cite{0}{radford2019_language}
\abx@aux@segm{0}{0}{radford2019_language}
\abx@aux@cite{0}{raffel2020_t5}
\abx@aux@segm{0}{0}{raffel2020_t5}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.5.6}Learned Positional Encodings: An Alternative Approach}{654}{subsection.17.5.6}\protected@file@percent }
\newlabel{sec:chapter17_learned_pe}{{17.5.6}{654}{Learned Positional Encodings: An Alternative Approach}{subsection.17.5.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Definition and Mechanics}{654}{section*.1293}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Examples of Learned Positional Encodings}{654}{section*.1294}\protected@file@percent }
\abx@aux@backref{568}{devlin2019_bert}{0}{654}{654}
\abx@aux@backref{569}{radford2019_language}{0}{654}{654}
\abx@aux@backref{570}{raffel2020_t5}{0}{654}{654}
\@writefile{toc}{\contentsline {subparagraph}{Highlighting Crucial Positions or Transitions}{654}{subparagraph*.1295}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Task-Specific Optimization of Position is Useful}{654}{section*.1296}\protected@file@percent }
\abx@aux@cite{0}{ke2021rethinking_position}
\abx@aux@segm{0}{0}{ke2021rethinking_position}
\abx@aux@cite{0}{shaw2018selfrelative_pos}
\abx@aux@segm{0}{0}{shaw2018selfrelative_pos}
\@writefile{toc}{\contentsline {subsubsection}{Pros \& Cons of Learned Positional Embeddings}{655}{section*.1297}\protected@file@percent }
\abx@aux@backref{571}{ke2021rethinking_position}{0}{655}{655}
\abx@aux@backref{572}{shaw2018selfrelative_pos}{0}{655}{655}
\abx@aux@cite{0}{kazemnejad2019_pencoding}
\abx@aux@segm{0}{0}{kazemnejad2019_pencoding}
\@writefile{toc}{\contentsline {paragraph}{Conclusion on Learned Positional Embeddings}{656}{section*.1298}\protected@file@percent }
\abx@aux@backref{573}{kazemnejad2019_pencoding}{0}{656}{656}
\BKM@entry{id=659,dest={73756273656374696F6E2E31372E352E37},srcline={1112}}{5C3337365C3337375C3030304D5C303030615C303030735C3030306B5C303030655C303030645C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304C5C303030615C303030795C303030655C30303072}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.5.7}Masked Self-Attention Layer}{657}{subsection.17.5.7}\protected@file@percent }
\newlabel{sec:chapter17_masked_self_attention}{{17.5.7}{657}{Masked Self-Attention Layer}{subsection.17.5.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why Do We Need Masking?}{657}{section*.1299}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Applying the Mask in Attention Computation}{657}{section*.1300}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{How Masking Affects the Attention Weights}{657}{section*.1301}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.17}{\ignorespaces Masked Self-Attention Layer. The mask prevents tokens from attending to future positions, ensuring that each prediction depends only on past inputs.}}{658}{figure.caption.1302}\protected@file@percent }
\newlabel{fig:chapter17_masked_self_attention}{{17.17}{658}{Masked Self-Attention Layer. The mask prevents tokens from attending to future positions, ensuring that each prediction depends only on past inputs}{figure.caption.1302}{}}
\@writefile{toc}{\contentsline {subsubsection}{Example of Masking in a Short Sequence}{658}{section*.1303}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Handling Batches with Variable-Length Sequences}{658}{section*.1304}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why is Padding Necessary?}{658}{section*.1305}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Moving on to Input Processing with Self-Attention}{659}{section*.1306}\protected@file@percent }
\BKM@entry{id=660,dest={73756273656374696F6E2E31372E352E38},srcline={1217}}{5C3337365C3337375C303030505C303030725C3030306F5C303030635C303030655C303030735C303030735C303030695C3030306E5C303030675C3030305C3034305C303030495C3030306E5C303030705C303030755C303030745C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.5.8}Processing Inputs with Self-Attention}{660}{subsection.17.5.8}\protected@file@percent }
\newlabel{sec:chapter17_processing_inputs}{{17.5.8}{660}{Processing Inputs with Self-Attention}{subsection.17.5.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{Parallelization in Self-Attention}{660}{section*.1307}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Handling Batches of Sequences with Different Lengths}{661}{section*.1308}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Why is Self-Attention Parallelizable?}{662}{section*.1309}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Computational Complexity of Self-Attention, RNNs, and Convolutions}{662}{section*.1310}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{When is Self-Attention Computationally Efficient?}{662}{section*.1311}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Conclusion: When to Use Self-Attention?}{663}{section*.1312}\protected@file@percent }
\BKM@entry{id=661,dest={73756273656374696F6E2E31372E352E39},srcline={1381}}{5C3337365C3337375C3030304D5C303030755C3030306C5C303030745C303030695C3030302D5C303030485C303030655C303030615C303030645C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304C5C303030615C303030795C303030655C30303072}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\abx@aux@cite{0}{voita2019_analyzing_heads}
\abx@aux@segm{0}{0}{voita2019_analyzing_heads}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.5.9}Multi-Head Self-Attention Layer}{664}{subsection.17.5.9}\protected@file@percent }
\newlabel{sec:chapter17_multihead_self_attention}{{17.5.9}{664}{Multi-Head Self-Attention Layer}{subsection.17.5.9}{}}
\abx@aux@backref{574}{vaswani2017_attention}{0}{664}{664}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{664}{section*.1313}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Analogy with Convolutional Kernels}{664}{section*.1314}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Diversity in Attention Patterns}{664}{section*.1315}\protected@file@percent }
\abx@aux@backref{575}{voita2019_analyzing_heads}{0}{664}{664}
\@writefile{toc}{\contentsline {subsubsection}{How Multi-Head Attention Works}{664}{section*.1316}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Splitting Dimensions}{664}{section*.1317}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Computing Multi-Head Attention}{665}{section*.1318}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Concatenation and Output Projection}{665}{section*.1319}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.18}{\ignorespaces Multi-Head Self-Attention. Each head learns a different attention pattern, capturing diverse sequence-level relationships.}}{665}{figure.caption.1320}\protected@file@percent }
\newlabel{fig:chapter17_multihead_self_attention}{{17.18}{665}{Multi-Head Self-Attention. Each head learns a different attention pattern, capturing diverse sequence-level relationships}{figure.caption.1320}{}}
\@writefile{toc}{\contentsline {subsubsection}{Optimized Implementation and Linear Projection}{666}{section*.1321}\protected@file@percent }
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\BKM@entry{id=662,dest={73756273656374696F6E2E31372E352E3130},srcline={1520}}{5C3337365C3337375C303030535C303030655C3030306C5C303030665C3030302D5C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030415C303030705C303030705C3030306C5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {subsubsection}{PyTorch Implementation of Multi-Head Attention}{667}{section*.1322}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Stepping Stone to Transformers and Vision Applications}{667}{section*.1323}\protected@file@percent }
\abx@aux@backref{576}{vaswani2017_attention}{0}{667}{667}
\abx@aux@cite{0}{zhang2019_self_attention_gan}
\abx@aux@segm{0}{0}{zhang2019_self_attention_gan}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.5.10}Self-Attention for Vision Applications}{668}{subsection.17.5.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Generating Queries, Keys, and Values}{668}{section*.1324}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Reshaping for Attention Computation}{668}{section*.1325}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Computing Attention Scores}{668}{section*.1326}\protected@file@percent }
\abx@aux@backref{577}{zhang2019_self_attention_gan}{0}{669}{669}
\@writefile{toc}{\contentsline {paragraph}{Normalizing Attention Weights}{669}{section*.1327}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Computing the Attention Output}{669}{section*.1328}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Reshaping, Final Projection, and Residual Connection}{669}{section*.1329}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.19}{\ignorespaces Self-Attention Module integrated within a CNN framework. The figure illustrates the generation of \(Q\), \(K\), \(V\), computation of attention, and the final residual connection.}}{670}{figure.caption.1330}\protected@file@percent }
\newlabel{fig:chapter17_self_attention_module}{{17.19}{670}{Self-Attention Module integrated within a CNN framework. The figure illustrates the generation of \(Q\), \(K\), \(V\), computation of attention, and the final residual connection}{figure.caption.1330}{}}
\@writefile{toc}{\contentsline {paragraph}{Summary}{670}{section*.1331}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Bridging Towards Transformers}{670}{section*.1332}\protected@file@percent }
\BKM@entry{id=663,dest={73656374696F6E2E31372E36},srcline={1626}}{5C3337365C3337375C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C30303072}
\BKM@entry{id=664,dest={73756273656374696F6E2E31372E362E31},srcline={1627}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\@writefile{toc}{\contentsline {section}{\numberline {17.6}Transformer}{671}{section.17.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.6.1}Motivation and Introduction}{671}{subsection.17.6.1}\protected@file@percent }
\newlabel{sec:chapter17_transformer_motivation}{{17.6.1}{671}{Motivation and Introduction}{subsection.17.6.1}{}}
\abx@aux@backref{578}{vaswani2017_attention}{0}{671}{671}
\@writefile{toc}{\contentsline {subsubsection}{Three Ways of Processing Sequences}{671}{section*.1333}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Recurrent Neural Networks (RNNs)}{671}{section*.1334}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{1D Convolution for Sequence Processing}{671}{section*.1335}\protected@file@percent }
\BKM@entry{id=665,dest={73756273656374696F6E2E31372E362E32},srcline={1679}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C3030303F}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\abx@aux@cite{0}{devlin2019_bert}
\abx@aux@segm{0}{0}{devlin2019_bert}
\abx@aux@cite{0}{radford2019_language}
\abx@aux@segm{0}{0}{radford2019_language}
\@writefile{toc}{\contentsline {paragraph}{Self-Attention Mechanism}{672}{section*.1336}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.20}{\ignorespaces Comparison of sequence processing methods: RNNs, 1D Convolutions, and Self-Attention. Each approach has distinct advantages and drawbacks concerning long-range dependencies, parallelization, and computational efficiency.}}{672}{figure.caption.1337}\protected@file@percent }
\newlabel{fig:chapter17_sequence_processing_comparison}{{17.20}{672}{Comparison of sequence processing methods: RNNs, 1D Convolutions, and Self-Attention. Each approach has distinct advantages and drawbacks concerning long-range dependencies, parallelization, and computational efficiency}{figure.caption.1337}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.6.2}Why the Transformer?}{672}{subsection.17.6.2}\protected@file@percent }
\abx@aux@backref{579}{vaswani2017_attention}{0}{672}{672}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\BKM@entry{id=666,dest={73756273656374696F6E2E31372E362E33},srcline={1696}}{5C3337365C3337375C303030535C303030655C303030715C303030325C303030535C303030655C303030715C3030305C3034305C3030304F5C303030725C303030695C303030675C303030695C3030306E5C303030615C3030306C5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C3030305C3034305C303030575C3030306F5C303030725C3030306B5C303030665C3030306C5C3030306F5C30303077}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\abx@aux@backref{580}{devlin2019_bert}{0}{673}{673}
\abx@aux@backref{581}{radford2019_language}{0}{673}{673}
\@writefile{lof}{\contentsline {figure}{\numberline {17.21}{\ignorespaces The original transformer architecture adapted from \blx@tocontentsinit {0}\cite {vaswani2017_attention}.}}{673}{figure.caption.1338}\protected@file@percent }
\abx@aux@backref{583}{vaswani2017_attention}{0}{673}{673}
\newlabel{fig:chapter17_transformer_architecture}{{17.21}{673}{The original transformer architecture adapted from \cite {vaswani2017_attention}}{figure.caption.1338}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.6.3}Seq2Seq Original Transformer Workflow}{674}{subsection.17.6.3}\protected@file@percent }
\newlabel{sec:chapter17_seq2seq_workflow}{{17.6.3}{674}{Seq2Seq Original Transformer Workflow}{subsection.17.6.3}{}}
\abx@aux@backref{584}{vaswani2017_attention}{0}{674}{674}
\abx@aux@cite{0}{jalammar2018_illustrated}
\abx@aux@segm{0}{0}{jalammar2018_illustrated}
\abx@aux@cite{0}{jalammar2018_illustrated}
\abx@aux@segm{0}{0}{jalammar2018_illustrated}
\@writefile{lof}{\contentsline {figure}{\numberline {17.22}{\ignorespaces  Illustration of the final step in 'Machine Translation' task using the encoder-decoder transformer. The input French sentence is fully encoded, and the decoder autoregressively generates the English translation token by token. Each generated token is embedded and combined with positional encoding before being fed into the next decoder step. This process repeats until an EOS token is reached, signaling completion. For the full animation of the decoding process, refer to \blx@tocontentsinit {0}\cite {jalammar2018_illustrated}. }}{675}{figure.caption.1339}\protected@file@percent }
\abx@aux@backref{586}{jalammar2018_illustrated}{0}{675}{675}
\newlabel{fig:chapter17_transformer_decoding_process}{{17.22}{675}{Illustration of the final step in 'Machine Translation' task using the encoder-decoder transformer. The input French sentence is fully encoded, and the decoder autoregressively generates the English translation token by token. Each generated token is embedded and combined with positional encoding before being fed into the next decoder step. This process repeats until an EOS token is reached, signaling completion. For the full animation of the decoding process, refer to \cite {jalammar2018_illustrated}}{figure.caption.1339}{}}
\@writefile{toc}{\contentsline {subsubsection}{Transformer Encoder Block: Structure and Reasoning}{676}{section*.1340}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Transformer Decoder Block: Structure and Reasoning}{677}{section*.1341}\protected@file@percent }
\BKM@entry{id=667,dest={73756273656374696F6E2E31372E362E34},srcline={1909}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C303030725C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C3030305C3034305C303030425C3030306C5C3030306F5C303030635C3030306B}
\@writefile{toc}{\contentsline {subsubsection}{Transitioning to Unified Transformer Blocks}{678}{section*.1342}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.6.4}The Modern Transformer Block}{679}{subsection.17.6.4}\protected@file@percent }
\newlabel{sec:chapter17_modern_transformer_block}{{17.6.4}{679}{The Modern Transformer Block}{subsection.17.6.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17.23}{\ignorespaces Modern Transformer Block: The input is a set of vectors \(X\), and the output is a refined set of vectors \(Y\). Self-attention is the only component enabling interaction between vectors. All other operations (layer normalization and MLP) are applied independently to each vector. This design ensures high scalability and parallelizability.}}{679}{figure.caption.1343}\protected@file@percent }
\newlabel{fig:chapter17_modern_transformer_block}{{17.23}{679}{Modern Transformer Block: The input is a set of vectors \(X\), and the output is a refined set of vectors \(Y\). Self-attention is the only component enabling interaction between vectors. All other operations (layer normalization and MLP) are applied independently to each vector. This design ensures high scalability and parallelizability}{figure.caption.1343}{}}
\@writefile{toc}{\contentsline {subsubsection}{Structure of the Modern Transformer Block}{679}{section*.1344}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{PyTorch Implementation}{680}{section*.1345}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Why the Modern Transformer Block?}{681}{section*.1346}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.24}{\ignorespaces The Transformer architecture revolutionized NLP by introducing a unified block structure. Models like BERT exemplify how stacking multiple Transformer blocks facilitates transfer learning across a wide range of language understanding tasks.}}{681}{figure.caption.1347}\protected@file@percent }
\newlabel{fig:chapter17_transformer_transfer_learning}{{17.24}{681}{The Transformer architecture revolutionized NLP by introducing a unified block structure. Models like BERT exemplify how stacking multiple Transformer blocks facilitates transfer learning across a wide range of language understanding tasks}{figure.caption.1347}{}}
\@writefile{toc}{\contentsline {subsubsection}{Key Benefits of the Modern Transformer Block}{681}{section*.1348}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.25}{\ignorespaces Overview of modern transformer architectures, their training data, resources, and computational costs. For instance, Gopher (Rae et al., 2021) reportedly cost around \$3,768,320 to train on Google Cloud (evaluation pricing).}}{682}{figure.caption.1349}\protected@file@percent }
\newlabel{fig:chapter17_transformer_architectures}{{17.25}{682}{Overview of modern transformer architectures, their training data, resources, and computational costs. For instance, Gopher (Rae et al., 2021) reportedly cost around \$3,768,320 to train on Google Cloud (evaluation pricing)}{figure.caption.1349}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17.26}{\ignorespaces GPT-3 demonstrates style transfer by emulating various literary styles—including an Ernest Hemingway-style parody of the Harry Potter series—showcasing complex, stylistic language generation.}}{682}{figure.caption.1350}\protected@file@percent }
\newlabel{fig:chapter17_gpt3_style_transfer}{{17.26}{682}{GPT-3 demonstrates style transfer by emulating various literary styles—including an Ernest Hemingway-style parody of the Harry Potter series—showcasing complex, stylistic language generation}{figure.caption.1350}{}}
\@writefile{toc}{\contentsline {subsubsection}{Further Reading and Resources}{683}{section*.1351}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Bridging Towards Vision Transformers}{683}{section*.1352}\protected@file@percent }
\BKM@entry{id=668,dest={636861707465722E3138},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030315C303030385C3030303A5C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C30303073}
\BKM@entry{id=669,dest={73656374696F6E2E31382E31},srcline={10}}{5C3337365C3337375C303030425C303030725C303030695C3030306E5C303030675C303030695C3030306E5C303030675C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030615C303030735C3030306B5C30303073}
\@writefile{toc}{\contentsline {chapter}{\numberline {18}Lecture 18: Vision Transformers}{684}{chapter.18}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@17}}
\ttl@writefile{ptc}{\ttl@starttoc{default@18}}
\pgfsyspdfmark {pgfid89}{0}{52099153}
\pgfsyspdfmark {pgfid88}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {18.1}Bringing Transformers to Vision Tasks}{684}{section.18.1}\protected@file@percent }
\BKM@entry{id=670,dest={73656374696F6E2E31382E32},srcline={30}}{5C3337365C3337375C303030495C3030306E5C303030745C303030655C303030675C303030725C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030695C3030306E5C303030745C3030306F5C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030435C3030304E5C3030304E5C303030735C3030305C303531}
\abx@aux@cite{0}{zhang2019_self_attention_gan}
\abx@aux@segm{0}{0}{zhang2019_self_attention_gan}
\abx@aux@cite{0}{wang2018_nonlocal_nn}
\abx@aux@segm{0}{0}{wang2018_nonlocal_nn}
\BKM@entry{id=671,dest={73756273656374696F6E2E31382E322E31},srcline={47}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030445C3030306F5C303030655C303030735C3030305C3034305C303030495C303030745C3030305C3034305C303030575C3030306F5C303030725C3030306B5C3030303F}
\BKM@entry{id=672,dest={73756273656374696F6E2E31382E322E32},srcline={51}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030415C303030645C303030645C303030695C3030306E5C303030675C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030435C3030304E5C3030304E5C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {18.2}Integrating Attention into Convolutional Neural Networks (CNNs)}{685}{section.18.2}\protected@file@percent }
\abx@aux@backref{587}{zhang2019_self_attention_gan}{0}{685}{685}
\abx@aux@backref{588}{wang2018_nonlocal_nn}{0}{685}{685}
\@writefile{lof}{\contentsline {figure}{\numberline {18.1}{\ignorespaces Illustration of integrating self-attention into CNN architectures.}}{685}{figure.caption.1353}\protected@file@percent }
\newlabel{fig:chapter18_attention_in_cnn}{{18.1}{685}{Illustration of integrating self-attention into CNN architectures}{figure.caption.1353}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.2.1}How Does It Work?}{685}{subsection.18.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {18.2.2}Limitations of Adding Attention to CNNs}{685}{subsection.18.2.2}\protected@file@percent }
\BKM@entry{id=673,dest={73656374696F6E2E31382E33},srcline={66}}{5C3337365C3337375C303030525C303030655C303030705C3030306C5C303030615C303030635C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304C5C3030306F5C303030635C303030615C3030306C5C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C303030655C303030635C303030685C303030615C3030306E5C303030695C303030735C3030306D5C30303073}
\BKM@entry{id=674,dest={73756273656374696F6E2E31382E332E31},srcline={74}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030445C3030306F5C303030655C303030735C3030305C3034305C3030304C5C3030306F5C303030635C303030615C3030306C5C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030575C3030306F5C303030725C3030306B5C3030303F}
\@writefile{toc}{\contentsline {section}{\numberline {18.3}Replacing Convolution with Local Attention Mechanisms}{687}{section.18.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {18.3.1}How Does Local Attention Work?}{687}{subsection.18.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.2}{\ignorespaces Replacing convolution with local self-attention. While this method offers dynamic feature aggregation, it introduces tricky implementation details and achieves only marginal performance improvements over ResNet architectures.}}{687}{figure.caption.1354}\protected@file@percent }
\newlabel{fig:chapter18_local_attention}{{18.2}{687}{Replacing convolution with local self-attention. While this method offers dynamic feature aggregation, it introduces tricky implementation details and achieves only marginal performance improvements over ResNet architectures}{figure.caption.1354}{}}
\BKM@entry{id=675,dest={73756273656374696F6E2E31382E332E32},srcline={102}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030695C303030735C3030305C3034305C3030304C5C3030306F5C303030635C303030615C3030306C5C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C3030306F5C303030725C303030655C3030305C3034305C303030465C3030306C5C303030655C303030785C303030695C303030625C3030306C5C303030655C3030305C3034305C303030745C303030685C303030615C3030306E5C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030735C3030303F}
\BKM@entry{id=676,dest={73756273656374696F6E2E31382E332E33},srcline={125}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C3030306C5C303030655C303030785C303030695C303030745C303030795C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C303030735C3030306F5C3030306E5C3030303A5C3030305C3034305C3030304C5C3030306F5C303030635C303030615C3030306C5C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.3.2}Why is Local Attention More Flexible than Convolutions?}{688}{subsection.18.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {18.3.3}Computational Complexity Comparison: Local Attention vs.\ Convolution}{688}{subsection.18.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Convolutional Complexity}{688}{section*.1355}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Local Attention Complexity}{688}{section*.1356}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why is Local Attention More Expensive?}{689}{section*.1357}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{689}{section*.1358}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{From Local Attention to ViTs}{689}{section*.1359}\protected@file@percent }
\BKM@entry{id=677,dest={73656374696F6E2E31382E34},srcline={194}}{5C3337365C3337375C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C303030735C3030305C3034305C3030305C3035305C303030565C303030695C303030545C303030735C3030305C3035315C3030303A5C3030305C3034305C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030505C303030695C303030785C303030655C3030306C5C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C303030505C303030615C303030745C303030635C303030685C303030655C30303073}
\abx@aux@cite{0}{chen2020_gpt_pixels}
\abx@aux@segm{0}{0}{chen2020_gpt_pixels}
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\BKM@entry{id=678,dest={73756273656374696F6E2E31382E342E31},srcline={210}}{5C3337365C3337375C303030535C303030705C3030306C5C303030695C303030745C303030745C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030695C3030306E5C303030745C3030306F5C3030305C3034305C303030505C303030615C303030745C303030635C303030685C303030655C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {18.4}Vision Transformers (ViTs): From Pixels to Patches}{690}{section.18.4}\protected@file@percent }
\newlabel{sec:chapter18_vit}{{18.4}{690}{Vision Transformers (ViTs): From Pixels to Patches}{section.18.4}{}}
\abx@aux@backref{589}{chen2020_gpt_pixels}{0}{690}{690}
\@writefile{lof}{\contentsline {figure}{\numberline {18.3}{\ignorespaces Applying standard transformers to pixels leads to an intractable \( O(R^4) \) memory complexity, motivating a more efficient patch-based approach.}}{690}{figure.caption.1360}\protected@file@percent }
\newlabel{fig:chapter18_pixel_transformer_memory}{{18.3}{690}{Applying standard transformers to pixels leads to an intractable \( O(R^4) \) memory complexity, motivating a more efficient patch-based approach}{figure.caption.1360}{}}
\abx@aux@backref{590}{vit2020_transformers}{0}{690}{690}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.4.1}Splitting an Image into Patches}{690}{subsection.18.4.1}\protected@file@percent }
\newlabel{sec:chapter18_vit_patching}{{18.4.1}{690}{Splitting an Image into Patches}{subsection.18.4.1}{}}
\BKM@entry{id=679,dest={73756273656374696F6E2E31382E342E32},srcline={238}}{5C3337365C3337375C303030435C3030306C5C303030615C303030735C303030735C3030305C3034305C303030545C3030306F5C3030306B5C303030655C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030505C3030306F5C303030735C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030455C3030306E5C303030635C3030306F5C303030645C303030695C3030306E5C30303067}
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\BKM@entry{id=680,dest={73756273656374696F6E2E31382E342E33},srcline={270}}{5C3337365C3337375C303030465C303030695C3030306E5C303030615C3030306C5C3030305C3034305C303030505C303030725C3030306F5C303030635C303030655C303030735C303030735C303030695C3030306E5C303030675C3030303A5C3030305C3034305C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030655C303030785C303030745C3030305C3034305C303030545C3030306F5C3030306B5C303030655C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.4.2}Class Token and Positional Encoding}{691}{subsection.18.4.2}\protected@file@percent }
\newlabel{sec:chapter18_vit_class_token}{{18.4.2}{691}{Class Token and Positional Encoding}{subsection.18.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18.4}{\ignorespaces ViT Model Overview: Images are split into fixed-size patches, linearly embedded, and passed through a Transformer encoder. The \texttt  {[CLS]} token is used for classification. Source: \blx@tocontentsinit {0}\cite {vit2020_transformers}.}}{691}{figure.caption.1361}\protected@file@percent }
\abx@aux@backref{592}{vit2020_transformers}{0}{691}{691}
\newlabel{fig:chapter18_vit_overview}{{18.4}{691}{ViT Model Overview: Images are split into fixed-size patches, linearly embedded, and passed through a Transformer encoder. The \texttt {[CLS]} token is used for classification. Source: \cite {vit2020_transformers}}{figure.caption.1361}{}}
\BKM@entry{id=681,dest={73756273656374696F6E2E31382E342E34},srcline={279}}{5C3337365C3337375C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C3030303A5C3030305C3034305C303030505C303030725C3030306F5C303030635C303030655C303030735C303030735C3030305C3034305C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030495C3030306D5C303030705C3030306C5C303030655C3030306D5C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.4.3}Final Processing: From Context Token to Classification}{692}{subsection.18.4.3}\protected@file@percent }
\newlabel{sec:chapter18_vit_output}{{18.4.3}{692}{Final Processing: From Context Token to Classification}{subsection.18.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.4.4}Vision Transformer: Process Summary and Implementation}{692}{subsection.18.4.4}\protected@file@percent }
\newlabel{sec:chapter18_vit_process}{{18.4.4}{692}{Vision Transformer: Process Summary and Implementation}{subsection.18.4.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Vision Transformer Processing Steps}{692}{section*.1362}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{PyTorch Implementation of a Vision Transformer}{693}{section*.1363}\protected@file@percent }
\BKM@entry{id=682,dest={73756273656374696F6E2E31382E342E35},srcline={509}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030755C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030435C3030306F5C3030306D5C303030705C3030306C5C303030655C303030785C303030695C303030745C303030795C3030303A5C3030305C3034305C303030565C303030695C303030545C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C303030505C303030695C303030785C303030655C3030306C5C3030302D5C3030304C5C303030655C303030765C303030655C3030306C5C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{chen2020_gpt_pixels}
\abx@aux@segm{0}{0}{chen2020_gpt_pixels}
\BKM@entry{id=683,dest={73756273656374696F6E2E31382E342E36},srcline={557}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030445C303030615C303030745C303030615C3030305C3034305C303030525C303030655C303030715C303030755C303030695C303030725C303030655C3030306D5C303030655C3030306E5C303030745C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C30303073}
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.4.5}Computational Complexity: ViT vs.\ Pixel-Level Self-Attention}{697}{subsection.18.4.5}\protected@file@percent }
\newlabel{sec:chapter18_vit_vs_pixels}{{18.4.5}{697}{Computational Complexity: ViT vs.\ Pixel-Level Self-Attention}{subsection.18.4.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{Pixel-Level Self-Attention}{697}{section*.1364}\protected@file@percent }
\abx@aux@backref{593}{chen2020_gpt_pixels}{0}{697}{697}
\@writefile{toc}{\contentsline {subsubsection}{Patch-Based Self-Attention (ViT)}{697}{section*.1365}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Key Takeaways}{697}{section*.1366}\protected@file@percent }
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.4.6}Limitations and Data Requirements of Vision Transformers}{698}{subsection.18.4.6}\protected@file@percent }
\newlabel{sec:vit_downsides}{{18.4.6}{698}{Limitations and Data Requirements of Vision Transformers}{subsection.18.4.6}{}}
\abx@aux@backref{594}{vit2020_transformers}{0}{698}{698}
\@writefile{toc}{\contentsline {subsubsection}{Large-Scale Pretraining is Critical}{698}{section*.1367}\protected@file@percent }
\abx@aux@backref{595}{vit2020_transformers}{0}{698}{698}
\@writefile{lof}{\contentsline {figure}{\numberline {18.5}{\ignorespaces ImageNet Top-1 accuracy comparison between CNNs (ResNets) and ViT models. ViTs struggle on smaller datasets but outperform ResNets when pre-trained on large-scale datasets such as JFT-300M.}}{698}{figure.caption.1368}\protected@file@percent }
\newlabel{fig:chapter18_imagenet_top1_accuracy}{{18.5}{698}{ImageNet Top-1 accuracy comparison between CNNs (ResNets) and ViT models. ViTs struggle on smaller datasets but outperform ResNets when pre-trained on large-scale datasets such as JFT-300M}{figure.caption.1368}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why Do ViTs Require More Data?}{698}{section*.1369}\protected@file@percent }
\newlabel{sec:vit_data_hungry}{{18.4.6}{698}{Why Do ViTs Require More Data?}{section*.1369}{}}
\@writefile{toc}{\contentsline {paragraph}{1. No Built-in Locality or Weight Sharing}{699}{section*.1370}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. Higher Parameter Count and Capacity}{699}{section*.1371}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3. Less Implicit Regularization}{699}{section*.1372}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{4. Absence of Hierarchical Representations}{699}{section*.1373}\protected@file@percent }
\BKM@entry{id=684,dest={73756273656374696F6E2E31382E342E37},srcline={616}}{5C3337365C3337375C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030565C303030695C303030545C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C3030305C3034305C303030565C303030615C303030725C303030695C303030615C3030306E5C303030745C30303073}
\@writefile{toc}{\contentsline {paragraph}{A Note on Inductive Bias}{700}{section*.1374}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {18.4.7}Understanding ViT Model Variants}{700}{subsection.18.4.7}\protected@file@percent }
\newlabel{sec:chapter18_vit_variants}{{18.4.7}{700}{Understanding ViT Model Variants}{subsection.18.4.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{Model Configurations}{700}{section*.1375}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {18.1}{\ignorespaces Typical ViT configurations. The number of heads (\(h\)) is such that the per-head dimension \(d = D / h\) remains constant (usually 64).}}{700}{table.caption.1376}\protected@file@percent }
\newlabel{tab:chapter18_vit_model_configurations}{{18.1}{700}{Typical ViT configurations. The number of heads (\(h\)) is such that the per-head dimension \(d = D / h\) remains constant (usually 64)}{table.caption.1376}{}}
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\BKM@entry{id=685,dest={73756273656374696F6E2E31382E342E38},srcline={676}}{5C3337365C3337375C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030695C3030306E5C303030675C3030305C3034305C303030565C303030695C303030545C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030635C30303079}
\abx@aux@cite{0}{steiner2021_how_to_train_vit}
\abx@aux@segm{0}{0}{steiner2021_how_to_train_vit}
\@writefile{toc}{\contentsline {subsubsection}{Transfer Performance Across Datasets}{701}{section*.1377}\protected@file@percent }
\abx@aux@backref{596}{vit2020_transformers}{0}{701}{701}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.4.8}Improving ViT Training Efficiency}{701}{subsection.18.4.8}\protected@file@percent }
\abx@aux@backref{597}{steiner2021_how_to_train_vit}{0}{701}{701}
\@writefile{lof}{\contentsline {figure}{\numberline {18.6}{\ignorespaces Impact of regularization and augmentation on ViT models. More augmentation and regularization generally lead to better performance.}}{701}{figure.caption.1378}\protected@file@percent }
\newlabel{fig:vit_regularization}{{18.6}{701}{Impact of regularization and augmentation on ViT models. More augmentation and regularization generally lead to better performance}{figure.caption.1378}{}}
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\@writefile{toc}{\contentsline {paragraph}{Regularization Techniques:}{702}{section*.1379}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Data Augmentation Strategies:}{702}{section*.1380}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Towards Data-Efficient Vision Transformers: Introducing DeiT}{702}{section*.1381}\protected@file@percent }
\abx@aux@backref{598}{touvron2021_deit}{0}{702}{702}
\BKM@entry{id=686,dest={73656374696F6E2E31382E35},srcline={714}}{5C3337365C3337375C303030445C303030615C303030745C303030615C3030302D5C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C303030735C3030305C3034305C3030305C3035305C303030445C303030655C303030695C303030545C303030735C3030305C303531}
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\BKM@entry{id=687,dest={73756273656374696F6E2E31382E352E31},srcline={735}}{5C3337365C3337375C303030435C303030725C3030306F5C303030735C303030735C3030302D5C303030455C3030306E5C303030745C303030725C3030306F5C303030705C303030795C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304B5C3030304C5C3030305C3034305C303030445C303030695C303030765C303030655C303030725C303030675C303030655C3030306E5C303030635C303030655C3030303A5C3030305C3034305C303030545C303030685C303030655C3030306F5C303030725C303030795C3030302C5C3030305C3034305C303030495C3030306E5C303030745C303030755C303030695C303030745C303030695C3030306F5C3030306E5C3030302C5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030525C3030306F5C3030306C5C303030655C3030305C3034305C303030695C3030306E5C3030305C3034305C303030445C303030695C303030735C303030745C303030695C3030306C5C3030306C5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {section}{\numberline {18.5}Data-Efficient Image Transformers (DeiTs)}{703}{section.18.5}\protected@file@percent }
\newlabel{sec:chapter18_deit}{{18.5}{703}{Data-Efficient Image Transformers (DeiTs)}{section.18.5}{}}
\abx@aux@backref{599}{touvron2021_deit}{0}{703}{703}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.5.1}Cross-Entropy and KL Divergence: Theory, Intuition, and Role in Distillation}{703}{subsection.18.5.1}\protected@file@percent }
\newlabel{sec:chapter18_deit_kl_ce}{{18.5.1}{703}{Cross-Entropy and KL Divergence: Theory, Intuition, and Role in Distillation}{subsection.18.5.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Cross-Entropy Loss}{703}{section*.1382}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.7}{\ignorespaces The function \( -\log (x) \). Cross-entropy loss penalizes incorrect predictions more harshly as the predicted probability approaches zero.}}{703}{figure.caption.1383}\protected@file@percent }
\newlabel{fig:chapter18_log_function}{{18.7}{703}{The function \( -\log (x) \). Cross-entropy loss penalizes incorrect predictions more harshly as the predicted probability approaches zero}{figure.caption.1383}{}}
\@writefile{toc}{\contentsline {paragraph}{KL Divergence: Full Distribution Matching}{704}{section*.1384}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Illustrative Example: CE vs KL}{704}{section*.1385}\protected@file@percent }
\BKM@entry{id=688,dest={73756273656374696F6E2E31382E352E32},srcline={857}}{5C3337365C3337375C303030445C303030655C303030695C303030545C3030305C3034305C303030445C303030695C303030735C303030745C303030695C3030306C5C3030306C5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030545C3030306F5C3030306B5C303030655C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030535C303030745C303030725C303030615C303030745C303030655C303030675C30303079}
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\@writefile{toc}{\contentsline {paragraph}{Hard vs.\ Soft Distillation: Choosing the Right Signal}{705}{section*.1386}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {18.5.2}DeiT Distillation Token and Training Strategy}{705}{subsection.18.5.2}\protected@file@percent }
\newlabel{sec:chapter18_deit_token}{{18.5.2}{705}{DeiT Distillation Token and Training Strategy}{subsection.18.5.2}{}}
\abx@aux@backref{600}{touvron2021_deit}{0}{705}{705}
\@writefile{toc}{\contentsline {subsubsection}{Distillation via Tokens: Setup}{705}{section*.1387}\protected@file@percent }
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\@writefile{toc}{\contentsline {paragraph}{Hard Distillation in Practice.}{706}{section*.1388}\protected@file@percent }
\newlabel{eq:deit_hard_distillation}{{18.5}{706}{Hard Distillation in Practice}{equation.18.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18.8}{\ignorespaces Comparison of distillation strategies on ImageNet. Hard distillation outperforms soft distillation. Late fusion of both tokens further improves results, confirming their complementarity. Source: \blx@tocontentsinit {0}\cite {touvron2021_deit}.}}{706}{figure.caption.1389}\protected@file@percent }
\abx@aux@backref{602}{touvron2021_deit}{0}{706}{706}
\newlabel{fig:chapter18_deit_distillation_experiments}{{18.8}{706}{Comparison of distillation strategies on ImageNet. Hard distillation outperforms soft distillation. Late fusion of both tokens further improves results, confirming their complementarity. Source: \cite {touvron2021_deit}}{figure.caption.1389}{}}
\@writefile{toc}{\contentsline {subsubsection}{Soft Distillation: Temperature and KL Loss}{706}{section*.1390}\protected@file@percent }
\newlabel{eq:deit_soft_distillation}{{18.6}{706}{Soft Distillation: Temperature and KL Loss}{equation.18.6}{}}
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\@writefile{toc}{\contentsline {subsubsection}{Why Use a CNN Teacher?}{707}{section*.1391}\protected@file@percent }
\newlabel{sec:chapter18_deit_teacher_choice}{{18.5.2}{707}{Why Use a CNN Teacher?}{section*.1391}{}}
\@writefile{toc}{\contentsline {subsubsection}{Learned Token Behavior}{707}{section*.1392}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.9}{\ignorespaces DeiT distillation architecture. The \texttt  {[CLS]} token is trained with the ground-truth label, while the \texttt  {[DIST]} token matches the teacher’s prediction (top-1 label). Source: \blx@tocontentsinit {0}\cite {touvron2021_deit}.}}{707}{figure.caption.1393}\protected@file@percent }
\abx@aux@backref{604}{touvron2021_deit}{0}{707}{707}
\newlabel{fig:chapter18_deit_distillation_token}{{18.9}{707}{DeiT distillation architecture. The \texttt {[CLS]} token is trained with the ground-truth label, while the \texttt {[DIST]} token matches the teacher’s prediction (top-1 label). Source: \cite {touvron2021_deit}}{figure.caption.1393}{}}
\abx@aux@cite{0}{touvron2019_fixres}
\abx@aux@segm{0}{0}{touvron2019_fixres}
\@writefile{toc}{\contentsline {subsubsection}{Fine-Tuning: High Resolution and Distillation Retention}{708}{section*.1394}\protected@file@percent }
\newlabel{sec:chapter18_deit_finetuning}{{18.5.2}{708}{Fine-Tuning: High Resolution and Distillation Retention}{section*.1394}{}}
\@writefile{toc}{\contentsline {paragraph}{Two-Phase Training Rationale}{708}{section*.1395}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Higher Resolution Helps}{708}{section*.1396}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Upscaling and L2-Norm Preservation}{708}{section*.1397}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Teacher Adaptation with FixRes}{708}{section*.1398}\protected@file@percent }
\abx@aux@backref{605}{touvron2019_fixres}{0}{708}{708}
\@writefile{toc}{\contentsline {paragraph}{Dual Supervision in Fine-Tuning}{708}{section*.1399}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Why This Works in Data-Limited Settings}{708}{section*.1400}\protected@file@percent }
\BKM@entry{id=689,dest={73756273656374696F6E2E31382E352E33},srcline={1024}}{5C3337365C3337375C3030304D5C3030306F5C303030645C303030655C3030306C5C3030305C3034305C303030565C303030615C303030725C303030695C303030615C3030306E5C303030745C30303073}
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\abx@aux@cite{0}{touvron2021_deit}
\abx@aux@segm{0}{0}{touvron2021_deit}
\BKM@entry{id=690,dest={73756273656374696F6E2E31382E352E34},srcline={1043}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304F5C303030755C303030745C3030306C5C3030306F5C3030306F5C3030306B5C3030303A5C3030305C3034305C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030445C303030655C303030695C303030545C3030305C3034305C303030745C3030306F5C3030305C3034305C303030445C303030655C303030695C303030545C3030305C3034305C303030495C303030495C303030495C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030425C303030655C303030795C3030306F5C3030306E5C30303064}
\abx@aux@cite{0}{touvron2022_deitiii}
\abx@aux@segm{0}{0}{touvron2022_deitiii}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.5.3}Model Variants}{709}{subsection.18.5.3}\protected@file@percent }
\newlabel{sec:chapter18_deit_variants}{{18.5.3}{709}{Model Variants}{subsection.18.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18.10}{\ignorespaces Comparison of DeiT model variants by throughput and accuracy. Higher embedding dimensions and head counts yield better performance. Source: \blx@tocontentsinit {0}\cite {touvron2021_deit}.}}{709}{figure.caption.1401}\protected@file@percent }
\abx@aux@backref{607}{touvron2021_deit}{0}{709}{709}
\newlabel{fig:chapter18_deit_variants}{{18.10}{709}{Comparison of DeiT model variants by throughput and accuracy. Higher embedding dimensions and head counts yield better performance. Source: \cite {touvron2021_deit}}{figure.caption.1401}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.5.4}Conclusion and Outlook: From DeiT to DeiT III and Beyond}{709}{subsection.18.5.4}\protected@file@percent }
\newlabel{sec:chapter18_deit_conclusion}{{18.5.4}{709}{Conclusion and Outlook: From DeiT to DeiT III and Beyond}{subsection.18.5.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18.11}{\ignorespaces Performance of ViT-B/16 with key improvements introduced by DeiT: distillation, longer training (300 \(\rightarrow \) 1000 epochs), and fine-tuning at higher resolution (224\(^2\) \(\rightarrow \) 384\(^2\)).}}{709}{figure.caption.1402}\protected@file@percent }
\newlabel{fig:chapter18_deit_improvements}{{18.11}{709}{Performance of ViT-B/16 with key improvements introduced by DeiT: distillation, longer training (300 \(\rightarrow \) 1000 epochs), and fine-tuning at higher resolution (224\(^2\) \(\rightarrow \) 384\(^2\))}{figure.caption.1402}{}}
\@writefile{toc}{\contentsline {subsubsection}{DeiT III: Revenge of the ViT}{710}{section*.1403}\protected@file@percent }
\newlabel{sec:chapter18_deit3}{{18.5.4}{710}{DeiT III: Revenge of the ViT}{section*.1403}{}}
\abx@aux@backref{608}{touvron2022_deitiii}{0}{710}{710}
\@writefile{toc}{\contentsline {paragraph}{Open Questions Raised by DeiT}{710}{section*.1404}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Toward Hierarchical Vision Transformers}{710}{section*.1405}\protected@file@percent }
\newlabel{sec:chapter18_transition_swin}{{18.5.4}{710}{Toward Hierarchical Vision Transformers}{section*.1405}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18.12}{\ignorespaces CNNs exhibit a hierarchical architecture (resolution decreases, channel width increases). In contrast, ViTs like DeiT use a flat structure with constant embedding size, token length.}}{710}{figure.caption.1406}\protected@file@percent }
\newlabel{fig:chapter18_cnn_vs_vit}{{18.12}{710}{CNNs exhibit a hierarchical architecture (resolution decreases, channel width increases). In contrast, ViTs like DeiT use a flat structure with constant embedding size, token length}{figure.caption.1406}{}}
\BKM@entry{id=691,dest={73656374696F6E2E31382E36},srcline={1107}}{5C3337365C3337375C303030535C303030775C303030695C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C3030303A5C3030305C3034305C303030485C303030695C303030655C303030725C303030615C303030725C303030635C303030685C303030695C303030635C303030615C3030306C5C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030535C303030685C303030695C303030665C303030745C303030655C303030645C3030305C3034305C303030575C303030695C3030306E5C303030645C3030306F5C303030775C30303073}
\abx@aux@cite{0}{liu2021_swin}
\abx@aux@segm{0}{0}{liu2021_swin}
\BKM@entry{id=692,dest={73756273656374696F6E2E31382E362E31},srcline={1127}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030535C303030775C303030695C3030306E5C3030305C3034305C303030575C3030306F5C303030725C3030306B5C30303073}
\BKM@entry{id=693,dest={73756273656374696F6E2E31382E362E32},srcline={1149}}{5C3337365C3337375C303030575C303030695C3030306E5C303030645C3030306F5C303030775C3030302D5C303030425C303030615C303030735C303030655C303030645C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030575C3030302D5C3030304D5C303030535C303030415C3030305C303531}
\@writefile{toc}{\contentsline {section}{\numberline {18.6}Swin Transformer: Hierarchical Vision Transformers with Shifted Windows}{712}{section.18.6}\protected@file@percent }
\newlabel{sec:chapter18_swin_intro}{{18.6}{712}{Swin Transformer: Hierarchical Vision Transformers with Shifted Windows}{section.18.6}{}}
\abx@aux@backref{609}{liu2021_swin}{0}{712}{712}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.6.1}How Swin Works}{712}{subsection.18.6.1}\protected@file@percent }
\newlabel{subsec:chapter18_swin_working}{{18.6.1}{712}{How Swin Works}{subsection.18.6.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Patch Tokenization}{712}{section*.1407}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.13}{\ignorespaces Patch partitioning and linear embedding in Swin Transformer. The input image is first processed with a convolutional layer using \(C\) kernels of size \(4 \times 4 \times 3\) and stride 4. This operation generates non-overlapping \(4 \times 4\) patches, each projected to an embedding of dimension \(C\). Thus, the convolutional layer performs both patch partitioning and linear projection in a single step, preparing the input sequence for the first Swin Transformer block.}}{712}{figure.caption.1408}\protected@file@percent }
\newlabel{fig:chapter18_swin_patch_embedding}{{18.13}{712}{Patch partitioning and linear embedding in Swin Transformer. The input image is first processed with a convolutional layer using \(C\) kernels of size \(4 \times 4 \times 3\) and stride 4. This operation generates non-overlapping \(4 \times 4\) patches, each projected to an embedding of dimension \(C\). Thus, the convolutional layer performs both patch partitioning and linear projection in a single step, preparing the input sequence for the first Swin Transformer block}{figure.caption.1408}{}}
\BKM@entry{id=694,dest={73756273656374696F6E2E31382E362E33},srcline={1179}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C3030304E5C3030306F5C3030305C3034305C303030435C303030725C3030306F5C303030735C303030735C3030302D5C303030575C303030695C3030306E5C303030645C3030306F5C303030775C3030305C3034305C303030435C3030306F5C3030306D5C3030306D5C303030755C3030306E5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.6.2}Window-Based Self-Attention (W-MSA)}{713}{subsection.18.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.14}{\ignorespaces  Visualization of Window-based Multi-Head Self-Attention (W-MSA). In Swin Transformers, self-attention is computed independently within each non-overlapping window. This design dramatically reduces computational complexity while capturing strong local context. For many vision tasks, local interactions are sufficient—e.g., background patches generally don’t benefit from attending to unrelated regions. However, in cases where understanding an object requires aggregating non-local features (e.g., recognizing a bird that spans multiple windows), purely local attention becomes limiting. This motivates the introduction of Shifted Window MSA (SW-MSA), which enables cross-window interactions to improve global understanding while maintaining efficiency. }}{713}{figure.caption.1409}\protected@file@percent }
\newlabel{fig:chapter18_wmsa}{{18.14}{713}{Visualization of Window-based Multi-Head Self-Attention (W-MSA). In Swin Transformers, self-attention is computed independently within each non-overlapping window. This design dramatically reduces computational complexity while capturing strong local context. For many vision tasks, local interactions are sufficient—e.g., background patches generally don’t benefit from attending to unrelated regions. However, in cases where understanding an object requires aggregating non-local features (e.g., recognizing a bird that spans multiple windows), purely local attention becomes limiting. This motivates the introduction of Shifted Window MSA (SW-MSA), which enables cross-window interactions to improve global understanding while maintaining efficiency}{figure.caption.1409}{}}
\BKM@entry{id=695,dest={73756273656374696F6E2E31382E362E34},srcline={1197}}{5C3337365C3337375C303030535C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030535C303030685C303030695C303030665C303030745C303030655C303030645C3030305C3034305C303030575C303030695C3030306E5C303030645C3030306F5C303030775C303030735C3030305C3034305C3030305C3035305C303030535C303030575C3030302D5C3030304D5C303030535C303030415C3030305C303531}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.6.3}Limitation: No Cross-Window Communication}{714}{subsection.18.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {18.6.4}Solution: Shifted Windows (SW-MSA)}{714}{subsection.18.6.4}\protected@file@percent }
\newlabel{subsec:chapter18_swin_shifted}{{18.6.4}{714}{Solution: Shifted Windows (SW-MSA)}{subsection.18.6.4}{}}
\@writefile{toc}{\contentsline {paragraph}{How it works}{714}{section*.1410}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Benefits of SW-MSA}{714}{section*.1411}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.15}{\ignorespaces Benefits of SW-MSA. After a window shift, tokens that were in separate windows (e.g., red, green, blue) in layer \(L\) now fall into the same window in layer \(L+1\), enabling interaction and richer context aggregation.}}{714}{figure.caption.1412}\protected@file@percent }
\newlabel{fig:chapter18_swin_shifted_windows_benefits}{{18.15}{714}{Benefits of SW-MSA. After a window shift, tokens that were in separate windows (e.g., red, green, blue) in layer \(L\) now fall into the same window in layer \(L+1\), enabling interaction and richer context aggregation}{figure.caption.1412}{}}
\@writefile{toc}{\contentsline {paragraph}{Challenges Introduced by Shifted Windows}{715}{section*.1413}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.16}{\ignorespaces Shifted windows introduce unaligned boundaries. To maintain consistent \(M \times M\) window shapes, additional padding must be applied, increasing the number of windows and total compute.}}{715}{figure.caption.1414}\protected@file@percent }
\newlabel{fig:chapter18_swin_padding_problem}{{18.16}{715}{Shifted windows introduce unaligned boundaries. To maintain consistent \(M \times M\) window shapes, additional padding must be applied, increasing the number of windows and total compute}{figure.caption.1414}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18.17}{\ignorespaces Even after shifting, objects spanning multiple windows may still not be fully captured, like the turtle in Soroush Mehraban's figure. Meanwhile, excessive windows and their corresponding zero-padding adds computational waste.}}{715}{figure.caption.1415}\protected@file@percent }
\newlabel{fig:chapter18_swin_shifted_limitations}{{18.17}{715}{Even after shifting, objects spanning multiple windows may still not be fully captured, like the turtle in Soroush Mehraban's figure. Meanwhile, excessive windows and their corresponding zero-padding adds computational waste}{figure.caption.1415}{}}
\BKM@entry{id=696,dest={73756273656374696F6E2E31382E362E35},srcline={1264}}{5C3337365C3337375C303030435C303030795C303030635C3030306C5C303030695C303030635C3030305C3034305C303030535C303030685C303030695C303030665C303030745C303030655C303030645C3030305C3034305C303030575C303030695C3030306E5C303030645C3030306F5C303030775C3030302D5C3030304D5C303030615C303030735C3030306B5C303030655C303030645C3030305C3034305C303030535C303030655C3030306C5C303030665C3030305C3034305C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030435C303030795C303030635C3030306C5C303030695C303030635C3030305C3034305C303030535C303030575C3030302D5C3030304D5C303030535C303030415C3030305C303531}
\@writefile{lof}{\contentsline {figure}{\numberline {18.18}{\ignorespaces Two consecutive Swin Transformer blocks. Each block resembles a standard Transformer encoder block but replaces global self-attention with window-based self-attention (W-MSA in the first block, SW-MSA in the second). Alternating W-MSA and SW-MSA layers enables inter-window communication and increases the receptive field, allowing more contextual information to be aggregated across blocks.}}{716}{figure.caption.1416}\protected@file@percent }
\newlabel{fig:chapter18_swin_block_pair}{{18.18}{716}{Two consecutive Swin Transformer blocks. Each block resembles a standard Transformer encoder block but replaces global self-attention with window-based self-attention (W-MSA in the first block, SW-MSA in the second). Alternating W-MSA and SW-MSA layers enables inter-window communication and increases the receptive field, allowing more contextual information to be aggregated across blocks}{figure.caption.1416}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.6.5}Cyclic Shifted Window-Masked Self Attention (Cyclic SW-MSA)}{717}{subsection.18.6.5}\protected@file@percent }
\newlabel{subsec:chapter18_cyclic_swmha}{{18.6.5}{717}{Cyclic Shifted Window-Masked Self Attention (Cyclic SW-MSA)}{subsection.18.6.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18.19}{\ignorespaces Cyclic shift in SW-MSA (adapted from Soroush Mehraban). Patches are cyclically shifted to form new overlapping windows. Masking ensures attention only occurs between spatially coherent regions. Colored segments illustrate masked-out interactions (e.g., red and yellow columns in Window 2).}}{717}{figure.caption.1417}\protected@file@percent }
\newlabel{fig:chapter18_cyclic_shift_diagram}{{18.19}{717}{Cyclic shift in SW-MSA (adapted from Soroush Mehraban). Patches are cyclically shifted to form new overlapping windows. Masking ensures attention only occurs between spatially coherent regions. Colored segments illustrate masked-out interactions (e.g., red and yellow columns in Window 2)}{figure.caption.1417}{}}
\@writefile{toc}{\contentsline {subsubsection}{Masking in SW-MSA}{717}{section*.1418}\protected@file@percent }
\newlabel{subsubsec:chapter18_masking_in_swmha}{{18.6.5}{717}{Masking in SW-MSA}{section*.1418}{}}
\@writefile{toc}{\contentsline {paragraph}{Step-by-Step Construction of the Mask}{717}{section*.1419}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Use \(-100.0\) in the Mask?}{718}{section*.1420}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Expanded Receptive Fields}{719}{section*.1421}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.20}{\ignorespaces Cyclic shifts enable tokens from different windows (same color regions) to attend to each other over successive layers. This increases receptive field without global attention.}}{719}{figure.caption.1422}\protected@file@percent }
\newlabel{fig:chapter18_cyclic_shift_receptive_field}{{18.20}{719}{Cyclic shifts enable tokens from different windows (same color regions) to attend to each other over successive layers. This increases receptive field without global attention}{figure.caption.1422}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18.21}{\ignorespaces Cyclic Shifted Window Self-Attention (SW-MSA). \textbf  {Left:} The original image is divided into four regions (A, B, C, D), each representing a contiguous spatial group of patches. \textbf  {Middle:} As in standard SW-MSA, the original window grid is shifted by \(M/2\) patches along both spatial dimensions, forming 9 new overlapping windows. Each shifted window is assigned a color-coded group ID, indicating which patches were originally grouped together in a standard (non-cyclic) shift. \textbf  {Right:} The regions A, B, C, and D are cyclically shifted by \(M/2\) to preserve the original 4-window layout. Within each window, self-attention is restricted using a masking mechanism: only patches that share the same group ID (i.e., that originated from the same window according to SW-MSA) can attend to each other. This preserves local attention structure while allowing cross-window interactions across layers (adapted from Soroush Mehraban).}}{719}{figure.caption.1423}\protected@file@percent }
\newlabel{fig:chapter18_cyclic_ws_msa}{{18.21}{719}{Cyclic Shifted Window Self-Attention (SW-MSA). \textbf {Left:} The original image is divided into four regions (A, B, C, D), each representing a contiguous spatial group of patches. \textbf {Middle:} As in standard SW-MSA, the original window grid is shifted by \(M/2\) patches along both spatial dimensions, forming 9 new overlapping windows. Each shifted window is assigned a color-coded group ID, indicating which patches were originally grouped together in a standard (non-cyclic) shift. \textbf {Right:} The regions A, B, C, and D are cyclically shifted by \(M/2\) to preserve the original 4-window layout. Within each window, self-attention is restricted using a masking mechanism: only patches that share the same group ID (i.e., that originated from the same window according to SW-MSA) can attend to each other. This preserves local attention structure while allowing cross-window interactions across layers (adapted from Soroush Mehraban)}{figure.caption.1423}{}}
\BKM@entry{id=697,dest={73756273656374696F6E2E31382E362E36},srcline={1375}}{5C3337365C3337375C303030505C303030615C303030745C303030635C303030685C3030305C3034305C3030304D5C303030655C303030725C303030675C303030695C3030306E5C303030675C3030305C3034305C303030695C3030306E5C3030305C3034305C303030535C303030775C303030695C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.6.6}Patch Merging in Swin Transformers}{720}{subsection.18.6.6}\protected@file@percent }
\newlabel{subsec:chapter18_patch_merging}{{18.6.6}{720}{Patch Merging in Swin Transformers}{subsection.18.6.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18.22}{\ignorespaces Patch merging in Swin Transformer. Four adjacent \(C\)-dimensional patch embeddings are concatenated and projected to a single \(2C\)-dimensional embedding, reducing spatial resolution while enriching feature representation. Adapted from Soroush Mehraban.}}{720}{figure.caption.1425}\protected@file@percent }
\newlabel{fig:chapter18_patch_merging}{{18.22}{720}{Patch merging in Swin Transformer. Four adjacent \(C\)-dimensional patch embeddings are concatenated and projected to a single \(2C\)-dimensional embedding, reducing spatial resolution while enriching feature representation. Adapted from Soroush Mehraban}{figure.caption.1425}{}}
\BKM@entry{id=698,dest={73756273656374696F6E2E31382E362E37},srcline={1419}}{5C3337365C3337375C303030505C3030306F5C303030735C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030455C3030306E5C303030635C3030306F5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030695C3030306E5C3030305C3034305C303030535C303030775C303030695C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.6.7}Positional Encoding in Swin Transformers}{721}{subsection.18.6.7}\protected@file@percent }
\newlabel{enrichment:swin_positional_bias}{{18.6.7}{721}{Positional Encoding in Swin Transformers}{subsection.18.6.7}{}}
\@writefile{toc}{\contentsline {paragraph}{Relative Position Bias in Swin Transformers}{721}{section*.1428}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Hierarchical Windows and Relative Offsets}{721}{section*.1429}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Relative Position Bias for Hierarchical Transformers?}{722}{section*.1430}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Implementation Detail}{722}{section*.1431}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Practical Benefits}{722}{section*.1432}\protected@file@percent }
\BKM@entry{id=699,dest={73756273656374696F6E2E31382E362E38},srcline={1465}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030535C303030775C303030695C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030565C303030615C303030725C303030695C303030615C3030306E5C303030745C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.6.8}Conclusion: The Swin Transformer Architecture and Variants}{723}{subsection.18.6.8}\protected@file@percent }
\newlabel{subsec:chapter18_swin_conclusion}{{18.6.8}{723}{Conclusion: The Swin Transformer Architecture and Variants}{subsection.18.6.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18.23}{\ignorespaces Swin Transformer backbone architecture. The model hierarchically downsamples feature maps while increasing channel dimensions across four stages.}}{723}{figure.caption.1434}\protected@file@percent }
\newlabel{fig:chapter18_swin_architecture}{{18.23}{723}{Swin Transformer backbone architecture. The model hierarchically downsamples feature maps while increasing channel dimensions across four stages}{figure.caption.1434}{}}
\BKM@entry{id=700,dest={73656374696F6E2E31382E37},srcline={1532}}{5C3337365C3337375C303030455C303030785C303030745C303030655C3030306E5C303030735C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030535C303030755C303030635C303030635C303030655C303030735C303030735C3030306F5C303030725C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C303030535C303030775C303030695C3030306E}
\BKM@entry{id=701,dest={73756273656374696F6E2E31382E372E31},srcline={1538}}{5C3337365C3337375C303030535C303030775C303030695C3030306E5C3030305C3034305C303030455C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030535C303030775C303030695C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C3030305C3034305C303030565C30303032}
\abx@aux@cite{0}{liu2022_swinv2}
\abx@aux@segm{0}{0}{liu2022_swinv2}
\@writefile{lof}{\contentsline {figure}{\numberline {18.24}{\ignorespaces Speed vs.\ accuracy comparison on ImageNet (ms/image on V100 vs.\ top-1 accuracy). Swin outperforms comparable models across the trade-off curve, achieving higher accuracy with faster inference.}}{724}{figure.caption.1437}\protected@file@percent }
\newlabel{fig:chapter18_swin_speed_vs_accuracy}{{18.24}{724}{Speed vs.\ accuracy comparison on ImageNet (ms/image on V100 vs.\ top-1 accuracy). Swin outperforms comparable models across the trade-off curve, achieving higher accuracy with faster inference}{figure.caption.1437}{}}
\@writefile{toc}{\contentsline {section}{\numberline {18.7}Extensions and Successors to Swin}{724}{section.18.7}\protected@file@percent }
\newlabel{sec:chapter18_swin_extensions}{{18.7}{724}{Extensions and Successors to Swin}{section.18.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.7.1}Swin Evolution: Swin Transformer V2}{724}{subsection.18.7.1}\protected@file@percent }
\newlabel{sec:chapter18_swin_v2}{{18.7.1}{724}{Swin Evolution: Swin Transformer V2}{subsection.18.7.1}{}}
\abx@aux@backref{610}{liu2022_swinv2}{0}{724}{724}
\@writefile{toc}{\contentsline {paragraph}{1) Scaled Cosine Attention}{725}{section*.1438}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2) Log-Spaced Continuous Position Bias (Log-CPB)}{725}{section*.1439}\protected@file@percent }
\abx@aux@cite{0}{liu2022_swinv2}
\abx@aux@segm{0}{0}{liu2022_swinv2}
\abx@aux@cite{0}{liu2022_swinv2}
\abx@aux@segm{0}{0}{liu2022_swinv2}
\@writefile{toc}{\contentsline {paragraph}{3) Residual Post-Norm}{726}{section*.1440}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.25}{\ignorespaces \textbf  {Swin V2 Architecture.} Building on the core Swin design, V2 adds scaled cosine attention, log-spaced continuous position bias, and residual post-norm. These modifications allow deeper, wider models and higher input resolutions while preserving the hierarchical, window-based approach. Source: \blx@tocontentsinit {0}\cite {liu2022_swinv2}.}}{726}{figure.caption.1441}\protected@file@percent }
\abx@aux@backref{612}{liu2022_swinv2}{0}{726}{726}
\newlabel{fig:chapter18_swin_v2_arch}{{18.25}{726}{\textbf {Swin V2 Architecture.} Building on the core Swin design, V2 adds scaled cosine attention, log-spaced continuous position bias, and residual post-norm. These modifications allow deeper, wider models and higher input resolutions while preserving the hierarchical, window-based approach. Source: \cite {liu2022_swinv2}}{figure.caption.1441}{}}
\@writefile{toc}{\contentsline {paragraph}{Implications and Results}{726}{section*.1442}\protected@file@percent }
\BKM@entry{id=702,dest={73756273656374696F6E2E31382E372E32},srcline={1616}}{5C3337365C3337375C3030304D5C303030755C3030306C5C303030745C303030695C303030735C303030635C303030615C3030306C5C303030655C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C3030305C3034305C3030305C3035305C3030304D5C303030565C303030695C303030545C3030305C303531}
\abx@aux@cite{0}{fan2021_mvit}
\abx@aux@segm{0}{0}{fan2021_mvit}
\abx@aux@cite{0}{fan2021_mvit}
\abx@aux@segm{0}{0}{fan2021_mvit}
\abx@aux@cite{0}{fan2021_mvit}
\abx@aux@segm{0}{0}{fan2021_mvit}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.7.2}Multiscale Vision Transformer (MViT)}{727}{subsection.18.7.2}\protected@file@percent }
\newlabel{sec:mvit_overview}{{18.7.2}{727}{Multiscale Vision Transformer (MViT)}{subsection.18.7.2}{}}
\abx@aux@backref{613}{fan2021_mvit}{0}{727}{727}
\@writefile{toc}{\contentsline {paragraph}{1.\ Pooling Attention (MHPA)}{727}{section*.1443}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.26}{\ignorespaces \textbf  {Multi-Head Pooling Attention (MHPA)} in MViT. Queries, keys, and values are projected and optionally pooled to reduce resolution before self-attention. This improves efficiency while allowing hierarchical feature representation across stages. Adapted from \blx@tocontentsinit {0}\cite {fan2021_mvit}.}}{727}{figure.caption.1444}\protected@file@percent }
\abx@aux@backref{615}{fan2021_mvit}{0}{727}{727}
\newlabel{fig:chapter18_mvit_mhpa}{{18.26}{727}{\textbf {Multi-Head Pooling Attention (MHPA)} in MViT. Queries, keys, and values are projected and optionally pooled to reduce resolution before self-attention. This improves efficiency while allowing hierarchical feature representation across stages. Adapted from \cite {fan2021_mvit}}{figure.caption.1444}{}}
\@writefile{toc}{\contentsline {paragraph}{How Does Pooling Work?}{728}{section*.1445}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Multiscale Hierarchy via Pooling}{728}{section*.1446}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2.\ Hierarchical Token Downsampling}{728}{section*.1447}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3.\ Global Attention vs.\ Local Windows}{728}{section*.1448}\protected@file@percent }
\BKM@entry{id=703,dest={73756273656374696F6E2E31382E372E33},srcline={1693}}{5C3337365C3337375C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030655C303030645C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C303030735C303030635C303030615C3030306C5C303030655C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C303030735C3030303A5C3030305C3034305C3030304D5C303030565C303030695C303030545C303030765C30303032}
\abx@aux@cite{0}{li2021_improved_mvit}
\abx@aux@segm{0}{0}{li2021_improved_mvit}
\@writefile{toc}{\contentsline {paragraph}{Originally Designed for Video, Effective for Images}{729}{section*.1449}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Empirical Strengths}{729}{section*.1450}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {18.7.3}Improved Multiscale Vision Transformers: MViTv2}{729}{subsection.18.7.3}\protected@file@percent }
\newlabel{sec:mvitv2}{{18.7.3}{729}{Improved Multiscale Vision Transformers: MViTv2}{subsection.18.7.3}{}}
\abx@aux@backref{616}{li2021_improved_mvit}{0}{729}{729}
\@writefile{toc}{\contentsline {subsubsection}{Decomposed Relative Positional Embeddings}{729}{section*.1451}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Motivation}{729}{section*.1452}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Decomposed Formulation}{729}{section*.1453}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Integration into Attention}{730}{section*.1454}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Residual Pooling Connections}{730}{section*.1455}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Problem}{730}{section*.1456}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Solution.}{730}{section*.1457}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Empirical Impact}{730}{section*.1458}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Performance Benefits}{730}{section*.1459}\protected@file@percent }
\abx@aux@cite{0}{tolstikhin2021_mlpmixer}
\abx@aux@segm{0}{0}{tolstikhin2021_mlpmixer}
\@writefile{toc}{\contentsline {subsubsection}{Summary}{731}{section*.1460}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Looking Ahead}{731}{section*.1461}\protected@file@percent }
\abx@aux@backref{617}{tolstikhin2021_mlpmixer}{0}{731}{731}
\BKM@entry{id=704,dest={73656374696F6E2E31382E38},srcline={1813}}{5C3337365C3337375C3030304D5C3030304C5C303030505C3030302D5C3030304D5C303030695C303030785C303030655C303030725C3030303A5C3030305C3034305C303030415C3030306C5C3030306C5C3030302D5C3030304D5C3030304C5C303030505C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C30303065}
\abx@aux@cite{0}{tolstikhin2021_mlpmixer}
\abx@aux@segm{0}{0}{tolstikhin2021_mlpmixer}
\BKM@entry{id=705,dest={73756273656374696F6E2E31382E382E31},srcline={1822}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C3030304D5C3030304C5C303030505C3030302D5C3030304D5C303030695C303030785C303030655C303030725C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C30303065}
\@writefile{toc}{\contentsline {section}{\numberline {18.8}MLP-Mixer: All-MLP Vision Architecture}{732}{section.18.8}\protected@file@percent }
\newlabel{sec:chapter18_mlpmixer}{{18.8}{732}{MLP-Mixer: All-MLP Vision Architecture}{section.18.8}{}}
\abx@aux@backref{618}{tolstikhin2021_mlpmixer}{0}{732}{732}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.8.1}The MLP-Mixer Architecture}{732}{subsection.18.8.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.27}{\ignorespaces MLP-Mixer processes the image as a sequence of \( N \) patches (tokens), each with \( C \) channels. It alternates between channel-mixing and token-mixing MLPs. Despite lacking both attention and convolutions, it retains spatial and semantic structure.}}{732}{figure.caption.1462}\protected@file@percent }
\newlabel{fig:chapter18_mlpmixer_architecture}{{18.27}{732}{MLP-Mixer processes the image as a sequence of \( N \) patches (tokens), each with \( C \) channels. It alternates between channel-mixing and token-mixing MLPs. Despite lacking both attention and convolutions, it retains spatial and semantic structure}{figure.caption.1462}{}}
\@writefile{toc}{\contentsline {paragraph}{Token-Mixing and Channel-Mixing Blocks}{732}{section*.1463}\protected@file@percent }
\BKM@entry{id=706,dest={73756273656374696F6E2E31382E382E32},srcline={1861}}{5C3337365C3337375C303030525C303030655C303030735C303030755C3030306C5C303030745C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\abx@aux@cite{0}{touvron2021_resmlp}
\abx@aux@segm{0}{0}{touvron2021_resmlp}
\abx@aux@cite{0}{liu2021_pay_attention_to_mlps}
\abx@aux@segm{0}{0}{liu2021_pay_attention_to_mlps}
\abx@aux@cite{0}{yu2022_s2mlp}
\abx@aux@segm{0}{0}{yu2022_s2mlp}
\abx@aux@cite{0}{chen2022_cyclemlp}
\abx@aux@segm{0}{0}{chen2022_cyclemlp}
\BKM@entry{id=707,dest={73756273656374696F6E2E31382E382E33},srcline={1879}}{5C3337365C3337375C3030304C5C3030306F5C3030306F5C3030306B5C303030695C3030306E5C303030675C3030305C3034305C303030415C303030685C303030655C303030615C303030645C3030303A5C3030305C3034305C303030415C303030705C303030705C3030306C5C303030795C303030695C3030306E5C303030675C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {paragraph}{CNN Equivalence}{733}{section*.1464}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {18.8.2}Results and Limitations}{733}{subsection.18.8.2}\protected@file@percent }
\abx@aux@backref{619}{touvron2021_resmlp}{0}{733}{733}
\abx@aux@backref{620}{liu2021_pay_attention_to_mlps}{0}{733}{733}
\abx@aux@backref{621}{yu2022_s2mlp}{0}{733}{733}
\abx@aux@backref{622}{chen2022_cyclemlp}{0}{733}{733}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.8.3}Looking Ahead: Applying Transformers to Object Detection}{733}{subsection.18.8.3}\protected@file@percent }
\BKM@entry{id=708,dest={73656374696F6E2E31382E39},srcline={1885}}{5C3337365C3337375C303030445C303030655C303030745C303030655C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C3030305C3034305C3030305C3035305C303030445C303030655C303030545C303030525C3030305C303531}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\@writefile{toc}{\contentsline {section}{\numberline {18.9}Detection Transformer (DeTR)}{734}{section.18.9}\protected@file@percent }
\newlabel{sec:chapter18_detr_intro}{{18.9}{734}{Detection Transformer (DeTR)}{section.18.9}{}}
\abx@aux@backref{623}{carion2020_detr}{0}{734}{734}
\@writefile{lof}{\contentsline {figure}{\numberline {18.28}{\ignorespaces Overview of the DeTR architecture. An image is passed through a CNN backbone (e.g., ResNet-50) to produce a feature map. These features are flattened and fed into a transformer encoder. A decoder attends to learned object queries and outputs a fixed number of predictions, each corresponding to a potential object. Adapted from \blx@tocontentsinit {0}\cite {carion2020_detr}.}}{734}{figure.caption.1465}\protected@file@percent }
\abx@aux@backref{625}{carion2020_detr}{0}{734}{734}
\newlabel{fig:chapter18_detr_architecture}{{18.28}{734}{Overview of the DeTR architecture. An image is passed through a CNN backbone (e.g., ResNet-50) to produce a feature map. These features are flattened and fed into a transformer encoder. A decoder attends to learned object queries and outputs a fixed number of predictions, each corresponding to a potential object. Adapted from \cite {carion2020_detr}}{figure.caption.1465}{}}
\@writefile{toc}{\contentsline {paragraph}{Architecture Overview}{734}{section*.1466}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Transformers for Detection?}{734}{section*.1467}\protected@file@percent }
\BKM@entry{id=709,dest={73756273656374696F6E2E31382E392E31},srcline={1919}}{5C3337365C3337375C3030304D5C303030615C303030745C303030635C303030685C303030695C3030306E5C303030675C3030305C3034305C303030505C303030725C303030655C303030645C303030695C303030635C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030475C303030725C3030306F5C303030755C3030306E5C303030645C3030305C3034305C303030545C303030725C303030755C303030745C303030685C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304E5C3030306F5C3030302D5C3030304F5C303030625C3030306A5C303030655C303030635C303030745C3030305C3034305C303030505C303030615C303030645C303030645C303030695C3030306E5C30303067}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.9.1}Matching Predictions and Ground Truth with No-Object Padding}{735}{subsection.18.9.1}\protected@file@percent }
\newlabel{subsec:chapter18_detr_matching}{{18.9.1}{735}{Matching Predictions and Ground Truth with No-Object Padding}{subsection.18.9.1}{}}
\abx@aux@backref{626}{carion2020_detr}{0}{735}{735}
\@writefile{toc}{\contentsline {paragraph}{Challenge:}{735}{section*.1468}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Solution: No-Object Padding}{735}{section*.1469}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.29}{\ignorespaces  \textbf  {Prediction–Ground Truth Matching in DeTR.} DETR always outputs a fixed number \(N\) of predictions per image. To supervise all predictions uniformly, the ground-truth set is padded with “no-object” entries so its size matches \(N\). The Hungarian algorithm computes an optimal one-to-one matching between predictions and padded targets. Most predictions are matched to background entries, regularizing the model to produce confident “no-object” classifications for irrelevant tokens. }}{735}{figure.caption.1470}\protected@file@percent }
\newlabel{fig:chapter18_detr_predictions_vs_paddedgt}{{18.29}{735}{\textbf {Prediction–Ground Truth Matching in DeTR.} DETR always outputs a fixed number \(N\) of predictions per image. To supervise all predictions uniformly, the ground-truth set is padded with “no-object” entries so its size matches \(N\). The Hungarian algorithm computes an optimal one-to-one matching between predictions and padded targets. Most predictions are matched to background entries, regularizing the model to produce confident “no-object” classifications for irrelevant tokens}{figure.caption.1470}{}}
\@writefile{toc}{\contentsline {paragraph}{Hungarian Matching:}{735}{section*.1471}\protected@file@percent }
\BKM@entry{id=710,dest={73756273656374696F6E2E31382E392E32},srcline={1990}}{5C3337365C3337375C303030485C303030755C3030306E5C303030675C303030615C303030725C303030695C303030615C3030306E5C3030305C3034305C3030304D5C303030615C303030745C303030635C303030685C303030695C3030306E5C303030675C3030305C3034305C3030304C5C3030306F5C303030735C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030425C3030306F5C303030755C3030306E5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030425C3030306F5C303030785C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {paragraph}{Implementation Snippet:}{736}{section*.1472}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why This Matters:}{736}{section*.1473}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {18.9.2}Hungarian Matching Loss and Bounding Box Optimization}{736}{subsection.18.9.2}\protected@file@percent }
\newlabel{subsec:chapter18_detr_loss}{{18.9.2}{736}{Hungarian Matching Loss and Bounding Box Optimization}{subsection.18.9.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Step 1: Optimal Bipartite Matching}{736}{section*.1474}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 2: Matching Cost Definition}{737}{section*.1475}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 3: Final Loss Computation}{737}{section*.1476}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Bounding Box Loss: Smooth L1 and GIoU Components}{737}{section*.1477}\protected@file@percent }
\newlabel{par:detr_bounding_box_loss_components}{{18.9.2}{737}{Bounding Box Loss: Smooth L1 and GIoU Components}{section*.1477}{}}
\@writefile{toc}{\contentsline {subparagraph}{1. Smooth L1 Loss (Huber Variant)}{737}{subparagraph*.1478}\protected@file@percent }
\abx@aux@cite{0}{rezatofighi2019_giou}
\abx@aux@segm{0}{0}{rezatofighi2019_giou}
\@writefile{toc}{\contentsline {subparagraph}{2. Generalized IoU (GIoU) Loss}{738}{subparagraph*.1479}\protected@file@percent }
\abx@aux@backref{627}{rezatofighi2019_giou}{0}{738}{738}
\@writefile{lof}{\contentsline {figure}{\numberline {18.30}{\ignorespaces  \textbf  {Illustration of GIoU behavior.} Although both examples have IoU = 0, the left prediction is spatially closer to the ground truth box than the right. GIoU correctly assigns a higher similarity to the left, allowing for useful gradients even when IoU = 0. Credit: Jinsol Kim. }}{738}{figure.caption.1480}\protected@file@percent }
\newlabel{fig:chapter18_giou_illustration}{{18.30}{738}{\textbf {Illustration of GIoU behavior.} Although both examples have IoU = 0, the left prediction is spatially closer to the ground truth box than the right. GIoU correctly assigns a higher similarity to the left, allowing for useful gradients even when IoU = 0. Credit: Jinsol Kim}{figure.caption.1480}{}}
\BKM@entry{id=711,dest={73756273656374696F6E2E31382E392E33},srcline={2138}}{5C3337365C3337375C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C3030304F5C303030765C303030655C303030725C303030765C303030695C303030655C303030775C3030303A5C3030305C3034305C303030435C3030304E5C3030304E5C3030305C3034305C303030425C303030615C303030635C3030306B5C303030625C3030306F5C3030306E5C303030655C3030305C3034305C3030302B5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C3030305C3034305C303030445C303030655C303030635C3030306F5C303030645C303030655C30303072}
\@writefile{toc}{\contentsline {subparagraph}{3. Combining Smooth L1 and GIoU}{739}{subparagraph*.1481}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{739}{section*.1482}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {18.9.3}Architecture Overview: CNN Backbone + Transformer Decoder}{739}{subsection.18.9.3}\protected@file@percent }
\newlabel{subsec:chapter18_detr_architecture}{{18.9.3}{739}{Architecture Overview: CNN Backbone + Transformer Decoder}{subsection.18.9.3}{}}
\@writefile{toc}{\contentsline {paragraph}{1.\ CNN Backbone}{739}{section*.1483}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2.\ Transformer Encoder}{739}{section*.1484}\protected@file@percent }
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\@writefile{lof}{\contentsline {figure}{\numberline {18.31}{\ignorespaces Overall DeTR architecture. A CNN backbone extracts image features that are fed into a transformer encoder. The decoder receives \(N\) learned object queries to generate predictions.}}{740}{figure.caption.1485}\protected@file@percent }
\newlabel{fig:chapter18_detr_overall_arch}{{18.31}{740}{Overall DeTR architecture. A CNN backbone extracts image features that are fed into a transformer encoder. The decoder receives \(N\) learned object queries to generate predictions}{figure.caption.1485}{}}
\@writefile{toc}{\contentsline {paragraph}{3.\ Learned Object Queries and Transformer Decoder}{740}{section*.1486}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.32}{\ignorespaces Transformer architecture in DETR. The encoder aggregates image features. The decoder uses learned object queries to generate one output per prediction slot. Adapted from \blx@tocontentsinit {0}\cite {carion2020_detr}.}}{740}{figure.caption.1487}\protected@file@percent }
\abx@aux@backref{629}{carion2020_detr}{0}{740}{740}
\newlabel{fig:chapter18_detr_transformer_arch}{{18.32}{740}{Transformer architecture in DETR. The encoder aggregates image features. The decoder uses learned object queries to generate one output per prediction slot. Adapted from \cite {carion2020_detr}}{figure.caption.1487}{}}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\abx@aux@backref{630}{vaswani2017_attention}{0}{741}{741}
\@writefile{toc}{\contentsline {paragraph}{4.\ Interpreting Object Queries}{741}{section*.1488}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.33}{\ignorespaces Specialization of object queries across COCO images. Each prediction slot learns to attend to specific regions and box sizes. Color represents box scale and orientation. Source: \blx@tocontentsinit {0}\cite {carion2020_detr}.}}{741}{figure.caption.1489}\protected@file@percent }
\abx@aux@backref{632}{carion2020_detr}{0}{741}{741}
\newlabel{fig:chapter18_detr_box_query_specialization}{{18.33}{741}{Specialization of object queries across COCO images. Each prediction slot learns to attend to specific regions and box sizes. Color represents box scale and orientation. Source: \cite {carion2020_detr}}{figure.caption.1489}{}}
\@writefile{toc}{\contentsline {paragraph}{5.\ Why Attention is a Natural Fit}{741}{section*.1490}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.34}{\ignorespaces \textbf  {Attention as Bounding Box Proxy.} Entries in the attention matrix may reflect spatial relationships between image regions—suggesting how attention can implicitly capture bounding box-like structures. This interpretation, proposed by \href  {https://www.youtube.com/watch?v=T35ba_VXkMY}{Yannic Kilcher}.}}{741}{figure.caption.1491}\protected@file@percent }
\newlabel{fig:chapter18_detr_attention_matrix}{{18.34}{741}{\textbf {Attention as Bounding Box Proxy.} Entries in the attention matrix may reflect spatial relationships between image regions—suggesting how attention can implicitly capture bounding box-like structures. This interpretation, proposed by \href {https://www.youtube.com/watch?v=T35ba_VXkMY}{Yannic Kilcher}}{figure.caption.1491}{}}
\BKM@entry{id=712,dest={73756273656374696F6E2E31382E392E34},srcline={2220}}{5C3337365C3337375C303030445C303030655C303030545C303030525C3030305C3034305C303030525C303030655C303030735C303030755C3030306C5C303030745C303030735C3030302C5C3030305C3034305C303030495C3030306D5C303030705C303030615C303030635C303030745C3030302C5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030465C3030306F5C3030306C5C3030306C5C3030306F5C303030775C3030302D5C303030555C303030705C3030305C3034305C303030575C3030306F5C303030725C3030306B}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\abx@aux@cite{0}{carion2020_detr}
\abx@aux@segm{0}{0}{carion2020_detr}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.9.4}DeTR Results, Impact, and Follow-Up Work}{742}{subsection.18.9.4}\protected@file@percent }
\newlabel{subsec:chapter18_detr_results}{{18.9.4}{742}{DeTR Results, Impact, and Follow-Up Work}{subsection.18.9.4}{}}
\abx@aux@backref{633}{carion2020_detr}{0}{742}{742}
\@writefile{toc}{\contentsline {paragraph}{From Detection to Segmentation}{742}{section*.1492}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.35}{\ignorespaces \textbf  {Object-wise mask prediction in DeTR.} Binary masks are predicted independently for each object query. These are later merged using a pixel-wise argmax operation, enabling detailed instance-level segmentation. Adapted from \blx@tocontentsinit {0}\cite {carion2020_detr}, Figure 8.}}{742}{figure.caption.1493}\protected@file@percent }
\abx@aux@backref{635}{carion2020_detr}{0}{742}{742}
\newlabel{fig:chapter18_detr_segmentation_masks}{{18.35}{742}{\textbf {Object-wise mask prediction in DeTR.} Binary masks are predicted independently for each object query. These are later merged using a pixel-wise argmax operation, enabling detailed instance-level segmentation. Adapted from \cite {carion2020_detr}, Figure 8}{figure.caption.1493}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18.36}{\ignorespaces \textbf  {Panoptic segmentation with DeTR-R101.} DETR can segment both “things” (object instances) and “stuff” (amorphous background regions) in a unified manner. The consistency and alignment of masks show that DETR learns strong spatial and semantic priors. Adapted from \blx@tocontentsinit {0}\cite {carion2020_detr}.}}{742}{figure.caption.1494}\protected@file@percent }
\abx@aux@backref{637}{carion2020_detr}{0}{742}{742}
\newlabel{fig:chapter18_detr_panoptic}{{18.36}{742}{\textbf {Panoptic segmentation with DeTR-R101.} DETR can segment both “things” (object instances) and “stuff” (amorphous background regions) in a unified manner. The consistency and alignment of masks show that DETR learns strong spatial and semantic priors. Adapted from \cite {carion2020_detr}}{figure.caption.1494}{}}
\abx@aux@cite{0}{liu2022_dab_detr}
\abx@aux@segm{0}{0}{liu2022_dab_detr}
\abx@aux@cite{0}{li2022_dn_detr}
\abx@aux@segm{0}{0}{li2022_dn_detr}
\abx@aux@cite{0}{zhu2023_re_detr}
\abx@aux@segm{0}{0}{zhu2023_re_detr}
\abx@aux@cite{0}{sun2023_nms_strikes_back}
\abx@aux@segm{0}{0}{sun2023_nms_strikes_back}
\@writefile{toc}{\contentsline {paragraph}{Real-World Usage: HuggingFace Implementation}{743}{section*.1495}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Follow-Up Works and Extensions}{743}{section*.1496}\protected@file@percent }
\abx@aux@backref{638}{liu2022_dab_detr}{0}{743}{743}
\abx@aux@backref{639}{li2022_dn_detr}{0}{743}{743}
\abx@aux@backref{640}{zhu2023_re_detr}{0}{743}{743}
\abx@aux@backref{641}{sun2023_nms_strikes_back}{0}{743}{743}
\@writefile{toc}{\contentsline {paragraph}{Broader Impact}{743}{section*.1497}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{743}{section*.1498}\protected@file@percent }
\BKM@entry{id=713,dest={636861707465722E3139},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030315C303030395C3030303A5C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C303030765C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030735C3030305C3034305C30303049}
\BKM@entry{id=714,dest={73656374696F6E2E31392E31},srcline={13}}{5C3337365C3337375C303030535C303030755C303030705C303030655C303030725C303030765C303030695C303030735C303030655C303030645C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C303030555C3030306E5C303030735C303030755C303030705C303030655C303030725C303030765C303030695C303030735C303030655C303030645C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\BKM@entry{id=715,dest={73756273656374696F6E2E31392E312E31},srcline={16}}{5C3337365C3337375C303030535C303030755C303030705C303030655C303030725C303030765C303030695C303030735C303030655C303030645C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\BKM@entry{id=716,dest={73756273656374696F6E2E31392E312E32},srcline={28}}{5C3337365C3337375C303030555C3030306E5C303030735C303030755C303030705C303030655C303030725C303030765C303030695C303030735C303030655C303030645C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {chapter}{\numberline {19}Lecture 19: Generative Models I}{744}{chapter.19}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@18}}
\ttl@writefile{ptc}{\ttl@starttoc{default@19}}
\pgfsyspdfmark {pgfid101}{0}{52099153}
\pgfsyspdfmark {pgfid100}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {19.1}Supervised vs.\ Unsupervised Learning}{744}{section.19.1}\protected@file@percent }
\newlabel{sec:chapter19_supervised_vs_unsupervised}{{19.1}{744}{Supervised vs.\ Unsupervised Learning}{section.19.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.1.1}Supervised Learning}{744}{subsection.19.1.1}\protected@file@percent }
\newlabel{subsec:chapter19_supervised_learning}{{19.1.1}{744}{Supervised Learning}{subsection.19.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.1.2}Unsupervised Learning}{744}{subsection.19.1.2}\protected@file@percent }
\newlabel{subsec:chapter19_unsupervised_learning}{{19.1.2}{744}{Unsupervised Learning}{subsection.19.1.2}{}}
\BKM@entry{id=717,dest={73656374696F6E2E31392E32},srcline={62}}{5C3337365C3337375C303030445C303030695C303030735C303030635C303030725C303030695C3030306D5C303030695C3030306E5C303030615C303030745C303030695C303030765C303030655C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C303030765C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\BKM@entry{id=718,dest={73756273656374696F6E2E31392E322E31},srcline={74}}{5C3337365C3337375C303030445C303030695C303030735C303030635C303030725C303030695C3030306D5C303030695C3030306E5C303030615C303030745C303030695C303030765C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {19.1}{\ignorespaces \textbf  {Left}: Supervised learning example (image captioning). \textbf  {Right}: Unsupervised learning example (clustering).}}{745}{figure.caption.1499}\protected@file@percent }
\newlabel{fig:chapter19_supervised_unsupervised_examples}{{19.1}{745}{\textbf {Left}: Supervised learning example (image captioning). \textbf {Right}: Unsupervised learning example (clustering)}{figure.caption.1499}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19.2}{\ignorespaces Autoencoder learns a latent representation \(z\) of input \(x\) and reconstructs it as \(\hat  {x}\), minimizing reconstruction loss \(||x - \hat  {x}||^2\).}}{745}{figure.caption.1500}\protected@file@percent }
\newlabel{fig:chapter19_autoencoder}{{19.2}{745}{Autoencoder learns a latent representation \(z\) of input \(x\) and reconstructs it as \(\hat {x}\), minimizing reconstruction loss \(||x - \hat {x}||^2\)}{figure.caption.1500}{}}
\@writefile{toc}{\contentsline {section}{\numberline {19.2}Discriminative vs.\ Generative Models}{745}{section.19.2}\protected@file@percent }
\newlabel{sec:chapter19_discriminative_vs_generative}{{19.2}{745}{Discriminative vs.\ Generative Models}{section.19.2}{}}
\BKM@entry{id=719,dest={73756273656374696F6E2E31392E322E32},srcline={87}}{5C3337365C3337375C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C303030765C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.2.1}Discriminative Models}{746}{subsection.19.2.1}\protected@file@percent }
\newlabel{subsec:chapter19_discriminative_models}{{19.2.1}{746}{Discriminative Models}{subsection.19.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19.3}{\ignorespaces Discriminative models normalize over possible labels. Even when given an invalid input (e.g., a monkey), they must output probabilities over non-fitting labels (e.g., cat, dog).}}{746}{figure.caption.1501}\protected@file@percent }
\newlabel{fig:chapter19_discriminative_example}{{19.3}{746}{Discriminative models normalize over possible labels. Even when given an invalid input (e.g., a monkey), they must output probabilities over non-fitting labels (e.g., cat, dog)}{figure.caption.1501}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.2.2}Generative Models}{746}{subsection.19.2.2}\protected@file@percent }
\newlabel{subsec:chapter19_generative_models}{{19.2.2}{746}{Generative Models}{subsection.19.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19.4}{\ignorespaces Generative models assign likelihood to every possible image. In this example, realistic animal images are given high density, while unrealistic art is rejected.}}{746}{figure.caption.1502}\protected@file@percent }
\newlabel{fig:chapter19_generative_distribution}{{19.4}{746}{Generative models assign likelihood to every possible image. In this example, realistic animal images are given high density, while unrealistic art is rejected}{figure.caption.1502}{}}
\BKM@entry{id=720,dest={73756273656374696F6E2E31392E322E33},srcline={106}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030645C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C303030765C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\BKM@entry{id=721,dest={73756273656374696F6E2E31392E322E34},srcline={119}}{5C3337365C3337375C3030304D5C3030306F5C303030645C303030655C3030306C5C3030305C3034305C303030525C303030655C3030306C5C303030615C303030745C303030695C3030306F5C3030306E5C303030735C303030685C303030695C303030705C303030735C3030305C3034305C303030765C303030695C303030615C3030305C3034305C303030425C303030615C303030795C303030655C303030735C303030275C3030305C3034305C303030525C303030755C3030306C5C30303065}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.2.3}Conditional Generative Models}{747}{subsection.19.2.3}\protected@file@percent }
\newlabel{subsec:chapter19_conditional_generative_models}{{19.2.3}{747}{Conditional Generative Models}{subsection.19.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19.5}{\ignorespaces Conditional generative models can reject inputs (e.g., art images) by assigning low likelihoods for all learned labels.}}{747}{figure.caption.1503}\protected@file@percent }
\newlabel{fig:chapter19_conditional_generative_example}{{19.5}{747}{Conditional generative models can reject inputs (e.g., art images) by assigning low likelihoods for all learned labels}{figure.caption.1503}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.2.4}Model Relationships via Bayes' Rule}{747}{subsection.19.2.4}\protected@file@percent }
\newlabel{subsec:chapter19_model_relationships}{{19.2.4}{747}{Model Relationships via Bayes' Rule}{subsection.19.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19.6}{\ignorespaces Bayes’ rule provides a bridge between discriminative, generative, and conditional generative models.}}{747}{figure.caption.1504}\protected@file@percent }
\newlabel{fig:chapter19_bayes_connection}{{19.6}{747}{Bayes’ rule provides a bridge between discriminative, generative, and conditional generative models}{figure.caption.1504}{}}
\BKM@entry{id=722,dest={73756273656374696F6E2E31392E322E35},srcline={138}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030305C3034305C3030306F5C303030665C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C303030765C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C3030305C3034305C303030545C303030615C303030785C3030306F5C3030306E5C3030306F5C3030306D5C30303079}
\abx@aux@cite{0}{ho2020_ddpm}
\abx@aux@segm{0}{0}{ho2020_ddpm}
\abx@aux@cite{0}{nichol2021_improvedddpm}
\abx@aux@segm{0}{0}{nichol2021_improvedddpm}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.2.5}Summary of Generative Model Taxonomy}{748}{subsection.19.2.5}\protected@file@percent }
\newlabel{subsec:chapter19_taxonomy}{{19.2.5}{748}{Summary of Generative Model Taxonomy}{subsection.19.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19.7}{\ignorespaces Taxonomy of generative models. Explicit density models can be tractable or approximate; implicit models include GANs and diffusion-based methods.}}{748}{figure.caption.1505}\protected@file@percent }
\newlabel{fig:chapter19_generative_taxonomy}{{19.7}{748}{Taxonomy of generative models. Explicit density models can be tractable or approximate; implicit models include GANs and diffusion-based methods}{figure.caption.1505}{}}
\abx@aux@backref{642}{ho2020_ddpm}{0}{748}{748}
\abx@aux@backref{643}{nichol2021_improvedddpm}{0}{748}{748}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\BKM@entry{id=723,dest={73656374696F6E2E31392E33},srcline={173}}{5C3337365C3337375C303030415C303030755C303030745C3030306F5C303030725C303030655C303030675C303030725C303030655C303030735C303030735C303030695C303030765C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030455C303030785C303030705C3030306C5C303030695C303030635C303030695C303030745C3030305C3034305C303030445C303030655C3030306E5C303030735C303030695C303030745C303030795C3030305C3034305C303030455C303030735C303030745C303030695C3030306D5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=724,dest={73756273656374696F6E2E31392E332E31},srcline={179}}{5C3337365C3337375C3030304D5C303030615C303030785C303030695C3030306D5C303030755C3030306D5C3030305C3034305C3030304C5C303030695C3030306B5C303030655C3030306C5C303030695C303030685C3030306F5C3030306F5C303030645C3030305C3034305C303030455C303030735C303030745C303030695C3030306D5C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=725,dest={73756273656374696F6E2E31392E332E32},srcline={200}}{5C3337365C3337375C303030415C303030755C303030745C3030306F5C303030725C303030655C303030675C303030725C303030655C303030735C303030735C303030695C303030765C303030655C3030305C3034305C303030465C303030615C303030635C303030745C3030306F5C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@backref{644}{lipman2022_flowmatching}{0}{749}{749}
\@writefile{toc}{\contentsline {section}{\numberline {19.3}Autoregressive Models and Explicit Density Estimation}{749}{section.19.3}\protected@file@percent }
\newlabel{sec:chapter19_autoregressive_models}{{19.3}{749}{Autoregressive Models and Explicit Density Estimation}{section.19.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.3.1}Maximum Likelihood Estimation}{749}{subsection.19.3.1}\protected@file@percent }
\newlabel{subsec:chapter19_mle}{{19.3.1}{749}{Maximum Likelihood Estimation}{subsection.19.3.1}{}}
\BKM@entry{id=726,dest={73756273656374696F6E2E31392E332E33},srcline={212}}{5C3337365C3337375C303030525C303030655C303030635C303030755C303030725C303030725C303030655C3030306E5C303030745C3030305C3034305C303030505C303030695C303030785C303030655C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030303A5C3030305C3034305C3030304F5C303030765C303030655C303030725C303030765C303030695C303030655C303030775C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{oord2016_pixernn}
\abx@aux@segm{0}{0}{oord2016_pixernn}
\abx@aux@cite{0}{oord2016_pixernn}
\abx@aux@segm{0}{0}{oord2016_pixernn}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.3.2}Autoregressive Factorization}{750}{subsection.19.3.2}\protected@file@percent }
\newlabel{subsec:chapter19_autoregressive_factorization}{{19.3.2}{750}{Autoregressive Factorization}{subsection.19.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.3.3}Recurrent Pixel Networks: Overview and Motivation}{750}{subsection.19.3.3}\protected@file@percent }
\newlabel{subsec:chapter19_pixelrnn_extended}{{19.3.3}{750}{Recurrent Pixel Networks: Overview and Motivation}{subsection.19.3.3}{}}
\abx@aux@backref{645}{oord2016_pixernn}{0}{750}{750}
\@writefile{toc}{\contentsline {paragraph}{Autoregressive Architectures in PixelRNN}{750}{section*.1506}\protected@file@percent }
\newlabel{sec:pixelrnn_variants}{{19.3.3}{750}{Autoregressive Architectures in PixelRNN}{section*.1506}{}}
\abx@aux@backref{646}{oord2016_pixernn}{0}{750}{750}
\@writefile{lof}{\contentsline {figure}{\numberline {19.8}{\ignorespaces High-level overview of recurrent pixel generation models. All variants follow the same masked-conv $\rightarrow $ recurrent/core $\rightarrow $ output-conv structure. The middle block can be a PixelCNN, Row LSTM, Diagonal BiLSTM, or Multi-Scale recurrent unit. Figure adapted from \blx@tocontentsinit {0}\cite {lebanoff2018_pixelrnn}.}}{750}{figure.caption.1507}\protected@file@percent }
\abx@aux@backref{648}{lebanoff2018_pixelrnn}{0}{750}{750}
\newlabel{fig:chapter19_pixelrnn_variants}{{19.8}{750}{High-level overview of recurrent pixel generation models. All variants follow the same masked-conv $\rightarrow $ recurrent/core $\rightarrow $ output-conv structure. The middle block can be a PixelCNN, Row LSTM, Diagonal BiLSTM, or Multi-Scale recurrent unit. Figure adapted from \cite {lebanoff2018_pixelrnn}}{figure.caption.1507}{}}
\abx@aux@cite{0}{oord2016_pixernn}
\abx@aux@segm{0}{0}{oord2016_pixernn}
\@writefile{toc}{\contentsline {subsubsection}{PixelCNN}{751}{section*.1508}\protected@file@percent }
\newlabel{chapter19_subsubsec:pixelcnn}{{19.3.3}{751}{PixelCNN}{section*.1508}{}}
\abx@aux@backref{649}{oord2016_pixernn}{0}{751}{751}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\@writefile{toc}{\contentsline {paragraph}{Image Generation as Sequential Prediction}{752}{section*.1509}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Autoregressive Generation Process}{752}{section*.1510}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19.9}{\ignorespaces PixelCNN generation begins with a blank canvas. For pixel \((r,c)\), the red channel is predicted based on all previous pixels (above and to the left) using a masked convolutional network. Figure adapted from \blx@tocontentsinit {0}\cite {lebanoff2018_pixelrnn}.}}{752}{figure.caption.1511}\protected@file@percent }
\abx@aux@backref{651}{lebanoff2018_pixelrnn}{0}{752}{752}
\newlabel{fig:chapter19_pixelcnn_red}{{19.9}{752}{PixelCNN generation begins with a blank canvas. For pixel \((r,c)\), the red channel is predicted based on all previous pixels (above and to the left) using a masked convolutional network. Figure adapted from \cite {lebanoff2018_pixelrnn}}{figure.caption.1511}{}}
\@writefile{toc}{\contentsline {paragraph}{Masked Convolution for Feature Extraction}{753}{section*.1512}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Red Channel: Feature Processing and Softmax}{754}{section*.1513}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Green Channel: Conditioning on Red}{754}{section*.1514}\protected@file@percent }
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\@writefile{lof}{\contentsline {figure}{\numberline {19.10}{\ignorespaces  Generation of the green channel at pixel \((r,c)\) incorporates the already sampled red value at that location. This is achieved by updating the image tensor with the red value and reapplying the PixelCNN forward pass — now using \textbf  {Mask B} in the initial convolution. Figure adapted from \blx@tocontentsinit {0}\cite {lebanoff2018_pixelrnn}. }}{755}{figure.caption.1515}\protected@file@percent }
\abx@aux@backref{653}{lebanoff2018_pixelrnn}{0}{755}{755}
\newlabel{fig:chapter19_pixelcnn_green}{{19.10}{755}{Generation of the green channel at pixel \((r,c)\) incorporates the already sampled red value at that location. This is achieved by updating the image tensor with the red value and reapplying the PixelCNN forward pass — now using \textbf {Mask B} in the initial convolution. Figure adapted from \cite {lebanoff2018_pixelrnn}}{figure.caption.1515}{}}
\@writefile{toc}{\contentsline {paragraph}{Blue Channel: Conditioning on Red and Green}{755}{section*.1516}\protected@file@percent }
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\@writefile{toc}{\contentsline {paragraph}{Moving to the Next Pixel}{756}{section*.1517}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19.11}{\ignorespaces After completing pixel \((r,c)\), generation proceeds to \((r,c+1)\), where red is again predicted first. Previously generated pixels and channels act as context. Figure adapted from \blx@tocontentsinit {0}\cite {lebanoff2018_pixelrnn}.}}{756}{figure.caption.1518}\protected@file@percent }
\abx@aux@backref{655}{lebanoff2018_pixelrnn}{0}{756}{756}
\newlabel{fig:chapter19_pixelcnn_next_pixel}{{19.11}{756}{After completing pixel \((r,c)\), generation proceeds to \((r,c+1)\), where red is again predicted first. Previously generated pixels and channels act as context. Figure adapted from \cite {lebanoff2018_pixelrnn}}{figure.caption.1518}{}}
\@writefile{toc}{\contentsline {paragraph}{Training PixelCNNs Efficiently}{756}{section*.1519}\protected@file@percent }
\abx@aux@cite{0}{pinaya2021_pixelcnn_blindspot}
\abx@aux@segm{0}{0}{pinaya2021_pixelcnn_blindspot}
\@writefile{toc}{\contentsline {paragraph}{Why Move Beyond PixelCNN? Blind Spots, Receptive Fields, and Inference Latency}{757}{section*.1520}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{1. Receptive Field Growth is Local and Incremental}{757}{subparagraph*.1521}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{2. Blind Spots: Missing Valid Context Pixels}{757}{subparagraph*.1522}\protected@file@percent }
\abx@aux@backref{656}{pinaya2021_pixelcnn_blindspot}{0}{757}{757}
\@writefile{toc}{\contentsline {subparagraph}{3. Inference Time: Slow Sequential Generation}{757}{subparagraph*.1523}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Motivation for Recurrent Alternatives}{758}{subparagraph*.1524}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Row LSTM}{759}{section*.1525}\protected@file@percent }
\newlabel{chapter19_subsubsec:rowLSTM}{{19.3.3}{759}{Row LSTM}{section*.1525}{}}
\@writefile{toc}{\contentsline {paragraph}{From Convolution Stacks to Convolutional Recurrence}{759}{section*.1526}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{What is a Convolutional LSTM?}{759}{section*.1527}\protected@file@percent }
\abx@aux@cite{0}{oord2016_pixernn}
\abx@aux@segm{0}{0}{oord2016_pixernn}
\abx@aux@cite{0}{oord2016_pixernn}
\abx@aux@segm{0}{0}{oord2016_pixernn}
\@writefile{toc}{\contentsline {paragraph}{Triangular Receptive Field}{760}{section*.1528}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19.12}{\ignorespaces Receptive fields in autoregressive image models. \textbf  {Left:} PixelCNN has a local receptive field due to its stack of masked convolutions, resulting in blind spots—e.g., diagonally adjacent pixels that are not accessible. \textbf  {Middle:} Row LSTM expands vertical context using a recurrent connection from the row above combined with a masked input convolution, but forms a \emph  {triangular} receptive field: pixels in the same row (e.g., $(r,c{-}1)$) are not incorporated into $(r,c)$. \textbf  {Right:} Diagonal BiLSTM processes pixels along diagonals (constant $r + c$), with bidirectional recurrence. This provides more complete spatial coverage by allowing information to flow across both horizontal and vertical directions simultaneously. Figure adapted from \blx@tocontentsinit {0}\cite {oord2016_pixernn}.}}{760}{figure.caption.1529}\protected@file@percent }
\abx@aux@backref{658}{oord2016_pixernn}{0}{760}{760}
\newlabel{fig:chapter19_receptive_field_comparison}{{19.12}{760}{Receptive fields in autoregressive image models. \textbf {Left:} PixelCNN has a local receptive field due to its stack of masked convolutions, resulting in blind spots—e.g., diagonally adjacent pixels that are not accessible. \textbf {Middle:} Row LSTM expands vertical context using a recurrent connection from the row above combined with a masked input convolution, but forms a \emph {triangular} receptive field: pixels in the same row (e.g., $(r,c{-}1)$) are not incorporated into $(r,c)$. \textbf {Right:} Diagonal BiLSTM processes pixels along diagonals (constant $r + c$), with bidirectional recurrence. This provides more complete spatial coverage by allowing information to flow across both horizontal and vertical directions simultaneously. Figure adapted from \cite {oord2016_pixernn}}{figure.caption.1529}{}}
\@writefile{toc}{\contentsline {paragraph}{Looking Ahead}{760}{section*.1530}\protected@file@percent }
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\@writefile{toc}{\contentsline {subsubsection}{Diagonal BiLSTM}{761}{section*.1531}\protected@file@percent }
\newlabel{chapter19_subsubsec:diagonal_bilstm}{{19.3.3}{761}{Diagonal BiLSTM}{section*.1531}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19.13}{\ignorespaces Pixel generation proceeds along diagonals \(r + c = \text  {const}\), allowing concurrent computation across a diagonal. Here shown the left-to-right direction only. Adapted from \blx@tocontentsinit {0}\cite {lebanoff2018_pixelrnn}.}}{761}{figure.caption.1532}\protected@file@percent }
\abx@aux@backref{660}{lebanoff2018_pixelrnn}{0}{761}{761}
\newlabel{fig:chapter19_diagonal_generation}{{19.13}{761}{Pixel generation proceeds along diagonals \(r + c = \text {const}\), allowing concurrent computation across a diagonal. Here shown the left-to-right direction only. Adapted from \cite {lebanoff2018_pixelrnn}}{figure.caption.1532}{}}
\@writefile{toc}{\contentsline {paragraph}{Skewing the Input for Diagonal Convolutions}{761}{section*.1533}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19.14}{\ignorespaces Skewing the input aligns diagonal pixels vertically, allowing efficient convolution across diagonals. Adapted from \blx@tocontentsinit {0}\cite {lebanoff2018_pixelrnn}.}}{761}{figure.caption.1534}\protected@file@percent }
\abx@aux@backref{662}{lebanoff2018_pixelrnn}{0}{761}{761}
\newlabel{fig:chapter19_skewing}{{19.14}{761}{Skewing the input aligns diagonal pixels vertically, allowing efficient convolution across diagonals. Adapted from \cite {lebanoff2018_pixelrnn}}{figure.caption.1534}{}}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\@writefile{toc}{\contentsline {paragraph}{Causal Correction for Bidirectionality}{762}{section*.1535}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19.15}{\ignorespaces Causal correction for bidirectional skewing. To prevent future access, one additional row is used for left-to-right propagation. Adapted from \blx@tocontentsinit {0}\cite {lebanoff2018_pixelrnn}.}}{762}{figure.caption.1536}\protected@file@percent }
\abx@aux@backref{664}{lebanoff2018_pixelrnn}{0}{762}{762}
\newlabel{fig:chapter19_ltr_skew}{{19.15}{762}{Causal correction for bidirectional skewing. To prevent future access, one additional row is used for left-to-right propagation. Adapted from \cite {lebanoff2018_pixelrnn}}{figure.caption.1536}{}}
\@writefile{toc}{\contentsline {paragraph}{Convolutional LSTM Logic}{762}{section*.1537}\protected@file@percent }
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\abx@aux@cite{0}{he2016_resnet}
\abx@aux@segm{0}{0}{he2016_resnet}
\@writefile{lof}{\contentsline {figure}{\numberline {19.16}{\ignorespaces Each diagonal’s hidden state combines features from both directions. The final output merges both flows after causal correction. Adapted from \blx@tocontentsinit {0}\cite {lebanoff2018_pixelrnn}.}}{763}{figure.caption.1538}\protected@file@percent }
\abx@aux@backref{666}{lebanoff2018_pixelrnn}{0}{763}{763}
\newlabel{fig:chapter19_bilstm_state_merge}{{19.16}{763}{Each diagonal’s hidden state combines features from both directions. The final output merges both flows after causal correction. Adapted from \cite {lebanoff2018_pixelrnn}}{figure.caption.1538}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Diagonal BiLSTM is the Most Expressive Variant}{763}{section*.1539}\protected@file@percent }
\abx@aux@cite{0}{oord2016_pixernn}
\abx@aux@segm{0}{0}{oord2016_pixernn}
\abx@aux@cite{0}{oord2016_pixernn}
\abx@aux@segm{0}{0}{oord2016_pixernn}
\@writefile{toc}{\contentsline {paragraph}{Residual Connections in PixelRNNs}{764}{section*.1540}\protected@file@percent }
\abx@aux@backref{667}{he2016_resnet}{0}{764}{764}
\@writefile{lof}{\contentsline {figure}{\numberline {19.17}{\ignorespaces Residual blocks for a PixelCNN (left) and a PixelRNN variant (right). In PixelRNN, the output of the LSTM layer is projected back to the input dimension (2h) using a \(1 \times 1\) convolution before being added to the input map. Figure adapted from \blx@tocontentsinit {0}\cite {oord2016_pixernn}.}}{764}{figure.caption.1541}\protected@file@percent }
\abx@aux@backref{669}{oord2016_pixernn}{0}{764}{764}
\newlabel{fig:chapter19_residual_connections}{{19.17}{764}{Residual blocks for a PixelCNN (left) and a PixelRNN variant (right). In PixelRNN, the output of the LSTM layer is projected back to the input dimension (2h) using a \(1 \times 1\) convolution before being added to the input map. Figure adapted from \cite {oord2016_pixernn}}{figure.caption.1541}{}}
\@writefile{toc}{\contentsline {paragraph}{Looking Ahead}{764}{section*.1542}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Multi-Scale PixelRNN}{765}{section*.1543}\protected@file@percent }
\newlabel{chapter19_subsubsec:multiscale_pixelrnn}{{19.3.3}{765}{Multi-Scale PixelRNN}{section*.1543}{}}
\@writefile{toc}{\contentsline {paragraph}{Two-Stage Architecture}{765}{section*.1544}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Conditioning via Upsampling and Biasing}{765}{section*.1545}\protected@file@percent }
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\@writefile{lof}{\contentsline {figure}{\numberline {19.18}{\ignorespaces Multi-Scale PixelRNN architecture. A small \(s \times s\) image is first generated by an unconditional PixelRNN. It is then upsampled and used to condition a second, full-resolution PixelRNN that generates an \(n \times n\) image. Figure adapted from \blx@tocontentsinit {0}\cite {lebanoff2018_pixelrnn}.}}{766}{figure.caption.1546}\protected@file@percent }
\abx@aux@backref{671}{lebanoff2018_pixelrnn}{0}{766}{766}
\newlabel{fig:chapter19_multiscale_pixelrnn}{{19.18}{766}{Multi-Scale PixelRNN architecture. A small \(s \times s\) image is first generated by an unconditional PixelRNN. It is then upsampled and used to condition a second, full-resolution PixelRNN that generates an \(n \times n\) image. Figure adapted from \cite {lebanoff2018_pixelrnn}}{figure.caption.1546}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Multi-Scale Helps}{766}{section*.1547}\protected@file@percent }
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\abx@aux@cite{0}{lebanoff2018_pixelrnn}
\abx@aux@segm{0}{0}{lebanoff2018_pixelrnn}
\@writefile{toc}{\contentsline {paragraph}{Trade-Offs and Usage}{767}{section*.1548}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Results and Qualitative Samples}{767}{section*.1549}\protected@file@percent }
\newlabel{chapter19_subsubsec:pixelrnn_results}{{19.3.3}{767}{Results and Qualitative Samples}{section*.1549}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19.19}{\ignorespaces Samples from PixelRNN models. Left: generated CIFAR-10 images. Right: samples from a model trained on downsampled ImageNet (\(32 \times 32\)). While not photorealistic, and making total sense, the images exhibit semantic structure such as edges, object boundaries, and coherent color regions. Figure adapted from \blx@tocontentsinit {0}\cite {lebanoff2018_pixelrnn}.}}{767}{figure.caption.1550}\protected@file@percent }
\abx@aux@backref{673}{lebanoff2018_pixelrnn}{0}{767}{767}
\newlabel{fig:chapter19_pixelrnn_results}{{19.19}{767}{Samples from PixelRNN models. Left: generated CIFAR-10 images. Right: samples from a model trained on downsampled ImageNet (\(32 \times 32\)). While not photorealistic, and making total sense, the images exhibit semantic structure such as edges, object boundaries, and coherent color regions. Figure adapted from \cite {lebanoff2018_pixelrnn}}{figure.caption.1550}{}}
\abx@aux@cite{0}{oord2016_pixernn}
\abx@aux@segm{0}{0}{oord2016_pixernn}
\abx@aux@cite{0}{salimans2017_pixelcnnpp}
\abx@aux@segm{0}{0}{salimans2017_pixelcnnpp}
\abx@aux@cite{0}{chen2020_imagegpt}
\abx@aux@segm{0}{0}{chen2020_imagegpt}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 19.3.3.1: Beyond PixelRNN: Advanced Autoregressive Variants}{768}{section*.1551}\protected@file@percent }
\newlabel{chapter19_enr:beyond_pixelrnn}{{19.3.3.1}{768}{\color {ocre}Enrichment \thesubsubsection : Beyond PixelRNN: Advanced Autoregressive Variants}{section*.1551}{}}
\@writefile{toc}{\contentsline {paragraph}{Gated PixelCNN}{768}{section*.1552}\protected@file@percent }
\abx@aux@backref{674}{oord2016_pixernn}{0}{768}{768}
\@writefile{toc}{\contentsline {paragraph}{PixelCNN++}{768}{section*.1553}\protected@file@percent }
\abx@aux@backref{675}{salimans2017_pixelcnnpp}{0}{768}{768}
\@writefile{toc}{\contentsline {paragraph}{ImageGPT}{768}{section*.1554}\protected@file@percent }
\abx@aux@backref{676}{chen2020_imagegpt}{0}{768}{768}
\BKM@entry{id=727,dest={73656374696F6E2E31392E34},srcline={972}}{5C3337365C3337375C303030565C303030615C303030725C303030695C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030415C303030755C303030745C3030306F5C303030655C3030306E5C303030635C3030306F5C303030645C303030655C303030725C303030735C3030305C3034305C3030305C3035305C303030565C303030415C303030455C303030735C3030305C303531}
\@writefile{toc}{\contentsline {paragraph}{Looking Ahead: From Autoregressive Models to VAEs}{769}{section*.1555}\protected@file@percent }
\BKM@entry{id=728,dest={73756273656374696F6E2E31392E342E31},srcline={983}}{5C3337365C3337375C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C3030305C3034305C3030305C3035305C3030304E5C3030306F5C3030306E5C3030302D5C303030565C303030615C303030725C303030695C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3035315C3030305C3034305C303030415C303030755C303030745C3030306F5C303030655C3030306E5C303030635C3030306F5C303030645C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {19.4}Variational Autoencoders (VAEs)}{770}{section.19.4}\protected@file@percent }
\newlabel{chapter19:vae_intro}{{19.4}{770}{Variational Autoencoders (VAEs)}{section.19.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.4.1}Regular (Non-Variational) Autoencoders}{770}{subsection.19.4.1}\protected@file@percent }
\newlabel{chapter19_subsec:regular_autoencoders}{{19.4.1}{770}{Regular (Non-Variational) Autoencoders}{subsection.19.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19.20}{\ignorespaces Autoencoder training: the model learns to reconstruct the input image by minimizing an \(\ell _2\) loss between the original image \( \mathbf  {x} \) and its reconstruction \( \hat  {\mathbf  {x}} \). This process requires no labels.}}{770}{figure.caption.1556}\protected@file@percent }
\newlabel{fig:chapter19_autoencoder_l2loss}{{19.20}{770}{Autoencoder training: the model learns to reconstruct the input image by minimizing an \(\ell _2\) loss between the original image \( \mathbf {x} \) and its reconstruction \( \hat {\mathbf {x}} \). This process requires no labels}{figure.caption.1556}{}}
\@writefile{toc}{\contentsline {paragraph}{Usage in Transfer Learning}{770}{section*.1557}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19.21}{\ignorespaces Autoencoders can be used as a pretraining mechanism. After unsupervised training, the encoder is repurposed for a downstream supervised task.}}{771}{figure.caption.1558}\protected@file@percent }
\newlabel{fig:chapter19_autoencoder_pretrain}{{19.21}{771}{Autoencoders can be used as a pretraining mechanism. After unsupervised training, the encoder is repurposed for a downstream supervised task}{figure.caption.1558}{}}
\@writefile{toc}{\contentsline {paragraph}{Architecture Patterns}{771}{section*.1559}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Limitations of Vanilla Autoencoders}{771}{section*.1560}\protected@file@percent }
\BKM@entry{id=729,dest={73756273656374696F6E2E31392E342E32},srcline={1038}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030695C3030306E5C303030675C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030565C303030415C30303045}
\@writefile{toc}{\contentsline {subsection}{\numberline {19.4.2}Introducing the VAE}{772}{subsection.19.4.2}\protected@file@percent }
\newlabel{chapter19_subsubsec:intro_vae}{{19.4.2}{772}{Introducing the VAE}{subsection.19.4.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Core Goals}{772}{section*.1561}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why a Latent Variable Model?}{772}{section*.1562}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19.22}{\ignorespaces Sampling from a trained VAE: draw latent code \( \mathbf  {z} \sim p(\mathbf  {z}) \), then decode it to produce a sample \( \mathbf  {x} \sim p_\theta (\mathbf  {x} \mid \mathbf  {z}) \).}}{773}{figure.caption.1563}\protected@file@percent }
\newlabel{fig:chapter19_vae_sampling}{{19.22}{773}{Sampling from a trained VAE: draw latent code \( \mathbf {z} \sim p(\mathbf {z}) \), then decode it to produce a sample \( \mathbf {x} \sim p_\theta (\mathbf {x} \mid \mathbf {z}) \)}{figure.caption.1563}{}}
\@writefile{toc}{\contentsline {paragraph}{Probabilistic Decoder}{773}{section*.1564}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19.23}{\ignorespaces The decoder is probabilistic: it outputs per-pixel means and variances, which define a diagonal Gaussian distribution over the image space conditioned on \( \mathbf  {z} \).}}{773}{figure.caption.1565}\protected@file@percent }
\newlabel{fig:chapter19_decoder_probabilistic}{{19.23}{773}{The decoder is probabilistic: it outputs per-pixel means and variances, which define a diagonal Gaussian distribution over the image space conditioned on \( \mathbf {z} \)}{figure.caption.1565}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Not a Full Covariance Matrix?}{774}{section*.1566}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Diagonal Assumption and Trade-Offs}{774}{section*.1567}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Marginal Likelihood: What We Want to Optimize}{774}{section*.1568}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Training VAEs and Developing the ELBO}{775}{section*.1569}\protected@file@percent }
\newlabel{chapter19_subsubsec:elbo_vae}{{19.4.2}{775}{Training VAEs and Developing the ELBO}{section*.1569}{}}
\@writefile{toc}{\contentsline {paragraph}{The Role of Bayes' Rule}{775}{section*.1570}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why This Matters}{775}{section*.1571}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Switching Objectives: Approximating the Posterior}{775}{section*.1572}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Rewriting the Log Likelihood}{775}{section*.1573}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Interpreting the ELBO}{777}{section*.1574}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19.24}{\ignorespaces VAE training: jointly optimize the encoder \( q_\phi (\mathbf  {z} \mid \mathbf  {x}) \) and decoder \( p_\theta (\mathbf  {x} \mid \mathbf  {z}) \) by maximizing the ELBO.}}{777}{figure.caption.1575}\protected@file@percent }
\newlabel{fig:chapter19_elbo_training}{{19.24}{777}{VAE training: jointly optimize the encoder \( q_\phi (\mathbf {z} \mid \mathbf {x}) \) and decoder \( p_\theta (\mathbf {x} \mid \mathbf {z}) \) by maximizing the ELBO}{figure.caption.1575}{}}
\BKM@entry{id=730,dest={636861707465722E3230},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030325C303030305C3030303A5C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C303030765C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030735C3030305C3034305C303030495C30303049}
\BKM@entry{id=731,dest={73656374696F6E2E32302E31},srcline={10}}{5C3337365C3337375C303030565C303030415C303030455C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030445C303030615C303030745C303030615C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=732,dest={73756273656374696F6E2E32302E312E31},srcline={16}}{5C3337365C3337375C303030455C3030306E5C303030635C3030306F5C303030645C303030655C303030725C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030445C303030655C303030635C3030306F5C303030645C303030655C303030725C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C303030655C3030303A5C3030305C3034305C3030304D5C3030304E5C303030495C303030535C303030545C3030305C3034305C303030455C303030785C303030615C3030306D5C303030705C3030306C5C30303065}
\BKM@entry{id=733,dest={73756273656374696F6E2E32302E312E32},srcline={28}}{5C3337365C3337375C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030505C303030695C303030705C303030655C3030306C5C303030695C3030306E5C303030655C3030303A5C3030305C3034305C303030535C303030745C303030655C303030705C3030302D5C303030625C303030795C3030302D5C303030535C303030745C303030655C30303070}
\@writefile{toc}{\contentsline {chapter}{\numberline {20}Lecture 20: Generative Models II}{778}{chapter.20}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@19}}
\ttl@writefile{ptc}{\ttl@starttoc{default@20}}
\pgfsyspdfmark {pgfid106}{0}{52099153}
\pgfsyspdfmark {pgfid105}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {20.1}VAE Training and Data Generation}{778}{section.20.1}\protected@file@percent }
\newlabel{chapter20_subsec:vae_training}{{20.1}{778}{VAE Training and Data Generation}{section.20.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.1.1}Encoder and Decoder Architecture: MNIST Example}{778}{subsection.20.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.1}{\ignorespaces Example architecture: The encoder maps input \( \mathbf  {x} \) to \( \boldsymbol  {\mu }_{z|x} \) and \( \boldsymbol  {\sigma }_{z|x} \). The decoder maps a sampled \( \mathbf  {z} \) to \( \boldsymbol  {\mu }_{x|z} \) and \( \boldsymbol  {\sigma }_{x|z} \), defining a distribution over reconstructed pixels.}}{778}{figure.caption.1576}\protected@file@percent }
\newlabel{fig:chapter20_mnist_architecture}{{20.1}{778}{Example architecture: The encoder maps input \( \mathbf {x} \) to \( \boldsymbol {\mu }_{z|x} \) and \( \boldsymbol {\sigma }_{z|x} \). The decoder maps a sampled \( \mathbf {z} \) to \( \boldsymbol {\mu }_{x|z} \) and \( \boldsymbol {\sigma }_{x|z} \), defining a distribution over reconstructed pixels}{figure.caption.1576}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.1.2}Training Pipeline: Step-by-Step}{779}{subsection.20.1.2}\protected@file@percent }
\newlabel{chapter20_subsubsec:training_stages}{{20.1.2}{779}{Training Pipeline: Step-by-Step}{subsection.20.1.2}{}}
\@writefile{toc}{\contentsline {paragraph}{The ELBO Objective}{779}{section*.1577}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.2}{\ignorespaces Full VAE training pipeline: compute the KL term from the encoder output; sample \( \mathbf  {z} \); decode into \( \mathbf  {x} \); and evaluate the reconstruction log-likelihood.}}{781}{figure.caption.1578}\protected@file@percent }
\newlabel{fig:chapter20_vae_training_pipeline}{{20.2}{781}{Full VAE training pipeline: compute the KL term from the encoder output; sample \( \mathbf {z} \); decode into \( \mathbf {x} \); and evaluate the reconstruction log-likelihood}{figure.caption.1578}{}}
\@writefile{toc}{\contentsline {subsubsection}{Why a Diagonal Gaussian Prior?}{781}{section*.1579}\protected@file@percent }
\BKM@entry{id=734,dest={73756273656374696F6E2E32302E312E33},srcline={161}}{5C3337365C3337375C303030485C3030306F5C303030775C3030305C3034305C303030435C303030615C3030306E5C3030305C3034305C303030575C303030655C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030655C3030305C3034305C303030445C303030615C303030745C303030615C3030305C3034305C303030555C303030735C303030695C3030306E5C303030675C3030305C3034305C303030565C303030415C303030455C303030735C3030303F}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.1.3}How Can We Generate Data Using VAEs?}{782}{subsection.20.1.3}\protected@file@percent }
\newlabel{chapter20_subsec:vae_sampling}{{20.1.3}{782}{How Can We Generate Data Using VAEs?}{subsection.20.1.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Sampling Procedure}{782}{section*.1580}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.3}{\ignorespaces Data generation process in a trained VAE. A latent code \( \mathbf  {z} \sim p(\mathbf  {z}) \) is passed through the decoder to generate a new image \( \hat  {\mathbf  {x}} \).}}{782}{figure.caption.1581}\protected@file@percent }
\newlabel{fig:chapter20_vae_generation}{{20.3}{782}{Data generation process in a trained VAE. A latent code \( \mathbf {z} \sim p(\mathbf {z}) \) is passed through the decoder to generate a new image \( \hat {\mathbf {x}} \)}{figure.caption.1581}{}}
\BKM@entry{id=735,dest={73656374696F6E2E32302E32},srcline={199}}{5C3337365C3337375C303030525C303030655C303030735C303030755C3030306C5C303030745C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030415C303030705C303030705C3030306C5C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030565C303030415C303030455C30303073}
\BKM@entry{id=736,dest={73756273656374696F6E2E32302E322E31},srcline={205}}{5C3337365C3337375C303030515C303030755C303030615C3030306C5C303030695C303030745C303030615C303030745C303030695C303030765C303030655C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030525C303030655C303030735C303030755C3030306C5C303030745C30303073}
\BKM@entry{id=737,dest={73756273656374696F6E2E32302E322E32},srcline={222}}{5C3337365C3337375C3030304C5C303030615C303030745C303030655C3030306E5C303030745C3030305C3034305C303030535C303030705C303030615C303030635C303030655C3030305C3034305C303030545C303030725C303030615C303030765C303030655C303030725C303030735C303030615C3030306C5C30303073}
\abx@aux@cite{0}{kingma2014_autoencoding}
\abx@aux@segm{0}{0}{kingma2014_autoencoding}
\abx@aux@cite{0}{kingma2014_autoencoding}
\abx@aux@segm{0}{0}{kingma2014_autoencoding}
\abx@aux@cite{0}{kingma2014_autoencoding}
\abx@aux@segm{0}{0}{kingma2014_autoencoding}
\@writefile{toc}{\contentsline {section}{\numberline {20.2}Results and Applications of VAEs}{783}{section.20.2}\protected@file@percent }
\newlabel{chapter20_subsec:vae_results}{{20.2}{783}{Results and Applications of VAEs}{section.20.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.2.1}Qualitative Generation Results}{783}{subsection.20.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.4}{\ignorespaces VAE-generated images on CIFAR-10 (left) and LFW faces (right). Generated samples resemble the training distribution but may lack fine detail.}}{783}{figure.caption.1582}\protected@file@percent }
\newlabel{fig:chapter20_vae_generations}{{20.4}{783}{VAE-generated images on CIFAR-10 (left) and LFW faces (right). Generated samples resemble the training distribution but may lack fine detail}{figure.caption.1582}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.2.2}Latent Space Traversals}{783}{subsection.20.2.2}\protected@file@percent }
\abx@aux@backref{677}{kingma2014_autoencoding}{0}{783}{783}
\@writefile{lof}{\contentsline {figure}{\numberline {20.5}{\ignorespaces Latent space traversal in a 2D subspace of a trained MNIST VAE. Each cell is decoded from a different \( \mathbf  {z} \). Figure from \blx@tocontentsinit {0}\cite {kingma2014_autoencoding}.}}{784}{figure.caption.1583}\protected@file@percent }
\abx@aux@backref{679}{kingma2014_autoencoding}{0}{784}{784}
\newlabel{fig:chapter20_vae_latent_traversal}{{20.5}{784}{Latent space traversal in a 2D subspace of a trained MNIST VAE. Each cell is decoded from a different \( \mathbf {z} \). Figure from \cite {kingma2014_autoencoding}}{figure.caption.1583}{}}
\@writefile{toc}{\contentsline {paragraph}{Editing with VAEs via Latent Traversals}{784}{section*.1584}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.6}{\ignorespaces Image editing using a VAE. After encoding into latent space, modifying \( \mathbf  {z} \) allows semantic transformations.}}{784}{figure.caption.1585}\protected@file@percent }
\newlabel{fig:chapter20_vae_editing}{{20.6}{784}{Image editing using a VAE. After encoding into latent space, modifying \( \mathbf {z} \) allows semantic transformations}{figure.caption.1585}{}}
\abx@aux@cite{0}{kingma2014_autoencoding}
\abx@aux@segm{0}{0}{kingma2014_autoencoding}
\abx@aux@cite{0}{kingma2014_autoencoding}
\abx@aux@segm{0}{0}{kingma2014_autoencoding}
\abx@aux@cite{0}{kingma2014_autoencoding}
\abx@aux@segm{0}{0}{kingma2014_autoencoding}
\abx@aux@cite{0}{kulkarni2015_dc_ign}
\abx@aux@segm{0}{0}{kulkarni2015_dc_ign}
\abx@aux@cite{0}{kulkarni2015_dc_ign}
\abx@aux@segm{0}{0}{kulkarni2015_dc_ign}
\abx@aux@cite{0}{kulkarni2015_dc_ign}
\abx@aux@segm{0}{0}{kulkarni2015_dc_ign}
\abx@aux@backref{680}{kingma2014_autoencoding}{0}{785}{785}
\@writefile{lof}{\contentsline {figure}{\numberline {20.7}{\ignorespaces Semantic editing with VAEs: adjusting individual latent variables changes facial attributes such as expression and pose. Adapted from \blx@tocontentsinit {0}\cite {kingma2014_autoencoding}.}}{785}{figure.caption.1586}\protected@file@percent }
\abx@aux@backref{682}{kingma2014_autoencoding}{0}{785}{785}
\newlabel{fig:chapter20_vae_face_editing}{{20.7}{785}{Semantic editing with VAEs: adjusting individual latent variables changes facial attributes such as expression and pose. Adapted from \cite {kingma2014_autoencoding}}{figure.caption.1586}{}}
\abx@aux@backref{683}{kulkarni2015_dc_ign}{0}{785}{785}
\@writefile{lof}{\contentsline {figure}{\numberline {20.8}{\ignorespaces Editing in the latent space of a VAE trained on 3D faces. Modifying latent dimensions changes pose and lighting. Adapted from \blx@tocontentsinit {0}\cite {kulkarni2015_dc_ign}.}}{785}{figure.caption.1587}\protected@file@percent }
\abx@aux@backref{685}{kulkarni2015_dc_ign}{0}{785}{785}
\newlabel{fig:chapter20_vae_graphics}{{20.8}{785}{Editing in the latent space of a VAE trained on 3D faces. Modifying latent dimensions changes pose and lighting. Adapted from \cite {kulkarni2015_dc_ign}}{figure.caption.1587}{}}
\BKM@entry{id=738,dest={73656374696F6E2E32302E33},srcline={313}}{5C3337365C3337375C303030535C303030755C3030306D5C3030306D5C303030615C303030725C303030795C3030305C3034305C3030305C3034365C3030305C3034305C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C303030735C3030303A5C3030305C3034305C303030565C303030615C303030725C303030695C303030615C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030415C303030755C303030745C3030306F5C303030655C3030306E5C303030635C3030306F5C303030645C303030655C303030725C30303073}
\@writefile{toc}{\contentsline {paragraph}{Takeaway}{786}{section*.1588}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {20.3}Summary \& Examples: Variational Autoencoders}{786}{section.20.3}\protected@file@percent }
\newlabel{chapter20_subsec:summary_vae}{{20.3}{786}{Summary \& Examples: Variational Autoencoders}{section.20.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Pros:}{786}{section*.1589}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Cons:}{786}{section*.1590}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Active Research Directions:}{786}{section*.1591}\protected@file@percent }
\BKM@entry{id=739,dest={73756273656374696F6E2E32302E332E31},srcline={354}}{5C3337365C3337375C303030565C303030515C3030302D5C303030565C303030415C303030455C3030302D5C303030325C3030303A5C3030305C3034305C303030435C3030306F5C3030306D5C303030625C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030565C303030415C303030455C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030415C303030755C303030745C3030306F5C303030725C303030655C303030675C303030725C303030655C303030735C303030735C303030695C303030765C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\abx@aux@cite{0}{razavi2019_vqvae2}
\abx@aux@segm{0}{0}{razavi2019_vqvae2}
\@writefile{lof}{\contentsline {figure}{\numberline {20.9}{\ignorespaces Comparison of autoregressive models (e.g., PixelCNNs) and variational models (e.g., VAEs), motivating hybrid methods to combine their respective strengths.}}{787}{figure.caption.1592}\protected@file@percent }
\newlabel{fig:chapter20_comparison_autoregressive_variational}{{20.9}{787}{Comparison of autoregressive models (e.g., PixelCNNs) and variational models (e.g., VAEs), motivating hybrid methods to combine their respective strengths}{figure.caption.1592}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.3.1}VQ-VAE-2: Combining VAEs with Autoregressive Models}{787}{subsection.20.3.1}\protected@file@percent }
\newlabel{subsec:chapter20_vqvae2}{{20.3.1}{787}{VQ-VAE-2: Combining VAEs with Autoregressive Models}{subsection.20.3.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation}{787}{section*.1593}\protected@file@percent }
\abx@aux@backref{686}{razavi2019_vqvae2}{0}{787}{787}
\@writefile{toc}{\contentsline {paragraph}{Architecture Overview}{787}{section*.1594}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How does autoregressive sampling begin?}{788}{section*.1595}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How does this enable generation?}{789}{section*.1596}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary Table: Dimensional Flow and Index Usage}{789}{section*.1597}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {20.1}{\ignorespaces Full data and dimensional flow in VQ-VAE-2 from raw input to final output, including intermediate stages of encoding, quantization, and reconstruction.}}{789}{table.caption.1598}\protected@file@percent }
\newlabel{tab:vqvae2_tensor_shapes}{{20.1}{789}{Full data and dimensional flow in VQ-VAE-2 from raw input to final output, including intermediate stages of encoding, quantization, and reconstruction}{table.caption.1598}{}}
\@writefile{toc}{\contentsline {paragraph}{Next: Training and Inference Flow}{789}{section*.1599}\protected@file@percent }
\abx@aux@cite{0}{oord2018_neural_discrete}
\abx@aux@segm{0}{0}{oord2018_neural_discrete}
\@writefile{lof}{\contentsline {figure}{\numberline {20.10}{\ignorespaces VQ-VAE-2 architecture: hierarchical encoding using vector quantization at two levels, followed by a decoder and autoregressive priors trained over the discrete code indices.}}{790}{figure.caption.1600}\protected@file@percent }
\newlabel{fig:chapter20_vqvae2_architecture}{{20.10}{790}{VQ-VAE-2 architecture: hierarchical encoding using vector quantization at two levels, followed by a decoder and autoregressive priors trained over the discrete code indices}{figure.caption.1600}{}}
\@writefile{toc}{\contentsline {subsubsection}{Training the VQ-VAE-2 Autoencoder}{790}{section*.1601}\protected@file@percent }
\newlabel{subsec:chapter20_vqvae2_training}{{20.3.1}{790}{Training the VQ-VAE-2 Autoencoder}{section*.1601}{}}
\@writefile{toc}{\contentsline {paragraph}{Objective Overview}{790}{section*.1602}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{1. Reconstruction Loss (\( \mathcal  {L}_{\text  {recon}} \))}{790}{section*.1603}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. Codebook Update (\( \mathcal  {L}_{\text  {codebook}} \))}{790}{section*.1604}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{(a) Gradient-Based Codebook Loss (as in the original paper)}{791}{subparagraph*.1605}\protected@file@percent }
\abx@aux@backref{687}{oord2018_neural_discrete}{0}{791}{791}
\@writefile{toc}{\contentsline {subparagraph}{(b) EMA-Based Codebook Update (Used in Practice)}{791}{subparagraph*.1606}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Summary of Update Strategies}{791}{subparagraph*.1607}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3. Commitment Loss (\( \mathcal  {L}_{\text  {commit}} \))}{792}{section*.1608}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Two Losses with Stop-Gradients Are Needed}{792}{section*.1609}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Compact Notation for Vector Quantization Loss}{792}{section*.1610}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Training Summary}{792}{section*.1611}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Training Summary with EMA Codebook Updates}{793}{section*.1612}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Training the Autoregressive Priors}{793}{section*.1613}\protected@file@percent }
\newlabel{subsec:chapter20_vqvae2_pixelcnn_priors}{{20.3.1}{793}{Training the Autoregressive Priors}{section*.1613}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation}{793}{section*.1614}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Hierarchical Modeling}{793}{section*.1615}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Overall Training Details}{794}{section*.1616}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Sampling Procedure}{794}{section*.1617}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Initialization Note}{794}{section*.1618}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Advantages of VQ-VAE-2 with Autoregressive Priors}{794}{section*.1619}\protected@file@percent }
\abx@aux@cite{0}{razavi2019_vqvae2}
\abx@aux@segm{0}{0}{razavi2019_vqvae2}
\abx@aux@backref{688}{razavi2019_vqvae2}{0}{795}{795}
\@writefile{toc}{\contentsline {paragraph}{Results \& Summary}{795}{section*.1620}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.11}{\ignorespaces Class-conditional ImageNet generations from VQ-VAE-2. Despite the use of latent variables, the model captures complex global and local features.}}{795}{figure.caption.1621}\protected@file@percent }
\newlabel{fig:chapter20_vqvae2_imagenet}{{20.11}{795}{Class-conditional ImageNet generations from VQ-VAE-2. Despite the use of latent variables, the model captures complex global and local features}{figure.caption.1621}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.12}{\ignorespaces High-quality face samples from FFHQ generated using VQ-VAE-2. The use of hierarchical latent structure supports sharp, coherent generations.}}{795}{figure.caption.1622}\protected@file@percent }
\newlabel{fig:chapter20_vqvae2_faces}{{20.12}{795}{High-quality face samples from FFHQ generated using VQ-VAE-2. The use of hierarchical latent structure supports sharp, coherent generations}{figure.caption.1622}{}}
\BKM@entry{id=740,dest={73656374696F6E2E32302E34},srcline={805}}{5C3337365C3337375C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C303030765C303030655C3030305C3034305C303030415C303030645C303030765C303030655C303030725C303030735C303030615C303030725C303030695C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030475C303030415C3030304E5C303030735C3030305C303531}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\BKM@entry{id=741,dest={73756273656374696F6E2E32302E342E31},srcline={840}}{5C3337365C3337375C303030535C303030655C303030745C303030755C303030705C3030303A5C3030305C3034305C303030495C3030306D5C303030705C3030306C5C303030695C303030635C303030695C303030745C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030765C303030695C303030615C3030305C3034305C303030415C303030645C303030765C303030655C303030725C303030735C303030615C303030725C303030695C303030615C3030306C5C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {section}{\numberline {20.4}Generative Adversarial Networks (GANs)}{796}{section.20.4}\protected@file@percent }
\newlabel{sec:chapter20_gans}{{20.4}{796}{Generative Adversarial Networks (GANs)}{section.20.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Bridging from Autoregressive Models, VAEs to GANs}{796}{section*.1623}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Enter GANs}{796}{section*.1624}\protected@file@percent }
\abx@aux@backref{689}{goodfellow2014_adversarial}{0}{796}{796}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.4.1}Setup: Implicit Generation via Adversarial Learning}{796}{subsection.20.4.1}\protected@file@percent }
\newlabel{subsec:chapter20_gan_intro}{{20.4.1}{796}{Setup: Implicit Generation via Adversarial Learning}{subsection.20.4.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Sampling from the True Distribution}{796}{section*.1625}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Discriminator as a Learned Judge}{797}{section*.1626}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Adversarial Training Dynamics}{797}{section*.1627}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.13}{\ignorespaces Generative Adversarial Networks (GANs): A generator network transforms latent noise \( \mathbf  {z} \) into samples. A discriminator tries to classify them as fake or real. The two networks are trained adversarially.}}{797}{figure.caption.1628}\protected@file@percent }
\newlabel{fig:chapter20_gan_framework}{{20.13}{797}{Generative Adversarial Networks (GANs): A generator network transforms latent noise \( \mathbf {z} \) into samples. A discriminator tries to classify them as fake or real. The two networks are trained adversarially}{figure.caption.1628}{}}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\BKM@entry{id=742,dest={73756273656374696F6E2E32302E342E32},srcline={906}}{5C3337365C3337375C303030475C303030415C3030304E5C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C303030695C303030765C30303065}
\@writefile{toc}{\contentsline {paragraph}{Core Intuition}{798}{section*.1629}\protected@file@percent }
\abx@aux@backref{690}{goodfellow2014_adversarial}{0}{798}{798}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.4.2}GAN Training Objective}{798}{subsection.20.4.2}\protected@file@percent }
\newlabel{subsec:chapter20_gan_training_objective}{{20.4.2}{798}{GAN Training Objective}{subsection.20.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.14}{\ignorespaces Adversarial training objective: the discriminator classifies between real and fake images, while the generator tries to produce fake images that fool the discriminator.}}{798}{figure.caption.1630}\protected@file@percent }
\newlabel{fig:chapter20_gan_objective}{{20.14}{798}{Adversarial training objective: the discriminator classifies between real and fake images, while the generator tries to produce fake images that fool the discriminator}{figure.caption.1630}{}}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\@writefile{toc}{\contentsline {paragraph}{Difficulties in Optimization}{799}{section*.1631}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.15}{\ignorespaces At the start of training, the generator produces poor samples. The discriminator easily identifies them, yielding vanishing gradients for the generator.}}{799}{figure.caption.1632}\protected@file@percent }
\newlabel{fig:chapter20_gan_vanishing_gradients}{{20.15}{799}{At the start of training, the generator produces poor samples. The discriminator easily identifies them, yielding vanishing gradients for the generator}{figure.caption.1632}{}}
\@writefile{toc}{\contentsline {paragraph}{Modified Generator Loss (Non-Saturating Trick)}{799}{section*.1633}\protected@file@percent }
\abx@aux@backref{691}{goodfellow2014_adversarial}{0}{799}{799}
\BKM@entry{id=743,dest={73756273656374696F6E2E32302E342E33},srcline={1014}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030475C303030415C3030304E5C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C303030695C303030765C303030655C3030305C3034305C303030495C303030735C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030615C3030306C}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\@writefile{toc}{\contentsline {paragraph}{Solution: Switch the Objective}{800}{section*.1634}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.16}{\ignorespaces Modified generator loss: maximizing \( \log D(G(\mathbf  {z})) \) yields stronger gradients early in training, when the discriminator is confident that generated samples are fake.}}{800}{figure.caption.1635}\protected@file@percent }
\newlabel{fig:chapter20_gan_nonsaturating_loss}{{20.16}{800}{Modified generator loss: maximizing \( \log D(G(\mathbf {z})) \) yields stronger gradients early in training, when the discriminator is confident that generated samples are fake}{figure.caption.1635}{}}
\@writefile{toc}{\contentsline {paragraph}{Looking Ahead: Why This Objective?}{800}{section*.1636}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {20.4.3}Why the GAN Training Objective Is Optimal}{801}{subsection.20.4.3}\protected@file@percent }
\newlabel{subsubsec:chapter20_gan_proof_optimality}{{20.4.3}{801}{Why the GAN Training Objective Is Optimal}{subsection.20.4.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Step-by-Step Derivation}{801}{section*.1637}\protected@file@percent }
\abx@aux@backref{692}{goodfellow2014_adversarial}{0}{801}{801}
\@writefile{toc}{\contentsline {paragraph}{Justification of the Mathematical Transformations}{801}{section*.1638}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Solving the Inner Maximization (Discriminator)}{801}{section*.1639}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Plugging the Optimal Discriminator into the Objective}{802}{section*.1640}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Rewriting as KL Divergences}{802}{section*.1641}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Introducing the Jensen–Shannon Divergence (JSD)}{803}{section*.1642}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Final Result: Objective Minimizes JSD}{803}{section*.1643}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{803}{section*.1644}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Important Caveats and Limitations of the Theoretical Result}{803}{section*.1645}\protected@file@percent }
\BKM@entry{id=744,dest={73656374696F6E2E32302E35},srcline={1302}}{5C3337365C3337375C303030475C303030415C3030304E5C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030505C303030725C303030615C303030635C303030745C303030695C303030635C303030655C3030303A5C3030305C3034305C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030455C303030615C303030725C3030306C5C303030795C3030305C3034305C3030304D5C303030695C3030306C5C303030655C303030735C303030745C3030306F5C3030306E5C303030655C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C3030304D5C3030306F5C303030645C303030655C303030725C3030306E5C3030305C3034305C303030415C303030645C303030765C303030615C3030306E5C303030635C303030655C30303073}
\BKM@entry{id=745,dest={73756273656374696F6E2E32302E352E31},srcline={1305}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C3030304F5C303030725C303030695C303030675C303030695C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030415C3030304E5C3030305C3034305C3030305C3035305C303030325C303030305C303030315C303030345C3030305C303531}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\BKM@entry{id=746,dest={73756273656374696F6E2E32302E352E32},srcline={1317}}{5C3337365C3337375C303030445C303030655C303030655C303030705C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030415C3030304E5C3030305C3034305C3030305C3035305C303030445C303030435C303030475C303030415C3030304E5C3030305C303531}
\abx@aux@cite{0}{radford2016_dcgan}
\abx@aux@segm{0}{0}{radford2016_dcgan}
\@writefile{toc}{\contentsline {section}{\numberline {20.5}GANs in Practice: From Early Milestones to Modern Advances}{804}{section.20.5}\protected@file@percent }
\newlabel{subsec:chapter20_gan_results}{{20.5}{804}{GANs in Practice: From Early Milestones to Modern Advances}{section.20.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.5.1}The Original GAN (2014)}{804}{subsection.20.5.1}\protected@file@percent }
\abx@aux@backref{693}{goodfellow2014_adversarial}{0}{804}{804}
\@writefile{lof}{\contentsline {figure}{\numberline {20.17}{\ignorespaces Samples from the original GAN paper~\blx@tocontentsinit {0}\cite {goodfellow2014_adversarial}. The model learns to generate MNIST digits and low-res face images.}}{804}{figure.caption.1646}\protected@file@percent }
\abx@aux@backref{695}{goodfellow2014_adversarial}{0}{804}{804}
\newlabel{fig:chapter20_gan_mnist_2014}{{20.17}{804}{Samples from the original GAN paper~\cite {goodfellow2014_adversarial}. The model learns to generate MNIST digits and low-res face images}{figure.caption.1646}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.5.2}Deep Convolutional GAN (DCGAN)}{804}{subsection.20.5.2}\protected@file@percent }
\abx@aux@backref{696}{radford2016_dcgan}{0}{804}{804}
\@writefile{toc}{\contentsline {paragraph}{Architectural Innovations and Design Principles}{804}{section*.1647}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.18}{\ignorespaces DCGAN architecture overview. The generator (up) upsamples a latent vector using transposed convolutions, while the discriminator (down) downsamples an image using strided convolutions. Key components include batch normalization, ReLU/LeakyReLU activations, and the absence of fully connected or pooling layers. Source: \href  {https://idiotdeveloper.com/what-is-deep-convolutional-generative-adversarial-networks-dcgan/}{IdiotDeveloper.com}.}}{805}{figure.caption.1648}\protected@file@percent }
\newlabel{fig:chapter20_dcgan_architecture}{{20.18}{805}{DCGAN architecture overview. The generator (up) upsamples a latent vector using transposed convolutions, while the discriminator (down) downsamples an image using strided convolutions. Key components include batch normalization, ReLU/LeakyReLU activations, and the absence of fully connected or pooling layers. Source: \href {https://idiotdeveloper.com/what-is-deep-convolutional-generative-adversarial-networks-dcgan/}{IdiotDeveloper.com}}{figure.caption.1648}{}}
\abx@aux@cite{0}{radford2016_dcgan}
\abx@aux@segm{0}{0}{radford2016_dcgan}
\abx@aux@cite{0}{radford2016_dcgan}
\abx@aux@segm{0}{0}{radford2016_dcgan}
\abx@aux@cite{0}{radford2016_dcgan}
\abx@aux@segm{0}{0}{radford2016_dcgan}
\abx@aux@cite{0}{radford2016_dcgan}
\abx@aux@segm{0}{0}{radford2016_dcgan}
\@writefile{toc}{\contentsline {paragraph}{Why it Works}{806}{section*.1649}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.19}{\ignorespaces Samples from DCGAN~\blx@tocontentsinit {0}\cite {radford2016_dcgan}, generating bedroom scenes resembling training data.}}{806}{figure.caption.1650}\protected@file@percent }
\abx@aux@backref{698}{radford2016_dcgan}{0}{806}{806}
\newlabel{fig:chapter20_dcgan_samples}{{20.19}{806}{Samples from DCGAN~\cite {radford2016_dcgan}, generating bedroom scenes resembling training data}{figure.caption.1650}{}}
\@writefile{toc}{\contentsline {paragraph}{Latent Space Interpolation}{806}{section*.1651}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.20}{\ignorespaces Latent space interpolation using DCGAN~\blx@tocontentsinit {0}\cite {radford2016_dcgan}. The generator learns to warp semantic structure, not just blend pixels.}}{806}{figure.caption.1652}\protected@file@percent }
\abx@aux@backref{700}{radford2016_dcgan}{0}{806}{806}
\newlabel{fig:chapter20_latent_interp}{{20.20}{806}{Latent space interpolation using DCGAN~\cite {radford2016_dcgan}. The generator learns to warp semantic structure, not just blend pixels}{figure.caption.1652}{}}
\abx@aux@cite{0}{radford2016_dcgan}
\abx@aux@segm{0}{0}{radford2016_dcgan}
\abx@aux@cite{0}{radford2016_dcgan}
\abx@aux@segm{0}{0}{radford2016_dcgan}
\BKM@entry{id=747,dest={73756273656374696F6E2E32302E352E33},srcline={1420}}{5C3337365C3337375C303030455C303030765C303030615C3030306C5C303030755C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C303030765C303030655C3030305C3034305C303030415C303030645C303030765C303030655C303030725C303030735C303030615C303030725C303030695C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C3030305C3035305C303030475C303030415C3030304E5C303030735C3030305C303531}
\abx@aux@cite{0}{salimans2016_improved}
\abx@aux@segm{0}{0}{salimans2016_improved}
\abx@aux@cite{0}{lucic2018_ganstudy}
\abx@aux@segm{0}{0}{lucic2018_ganstudy}
\@writefile{toc}{\contentsline {subsubsection}{Latent Vector Arithmetic}{807}{section*.1653}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.21}{\ignorespaces Attribute vector manipulation in latent space: generating “smiling man” from other distributions~\blx@tocontentsinit {0}\cite {radford2016_dcgan}.}}{807}{figure.caption.1654}\protected@file@percent }
\abx@aux@backref{702}{radford2016_dcgan}{0}{807}{807}
\newlabel{fig:chapter20_latent_arithmetic_smile}{{20.21}{807}{Attribute vector manipulation in latent space: generating “smiling man” from other distributions~\cite {radford2016_dcgan}}{figure.caption.1654}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.22}{\ignorespaces Latent vector arithmetic applied to glasses: the model captures the concept of “adding glasses” across identities.}}{807}{figure.caption.1655}\protected@file@percent }
\newlabel{fig:chapter20_latent_arithmetic_glasses}{{20.22}{807}{Latent vector arithmetic applied to glasses: the model captures the concept of “adding glasses” across identities}{figure.caption.1655}{}}
\abx@aux@cite{0}{salimans2016_improved}
\abx@aux@segm{0}{0}{salimans2016_improved}
\abx@aux@cite{0}{salimans2016_improved}
\abx@aux@segm{0}{0}{salimans2016_improved}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.5.3}Evaluating Generative Adversarial Networks (GANs)}{808}{subsection.20.5.3}\protected@file@percent }
\newlabel{chapter20_subsec:gan_evaluation}{{20.5.3}{808}{Evaluating Generative Adversarial Networks (GANs)}{subsection.20.5.3}{}}
\abx@aux@backref{703}{lucic2018_ganstudy}{0}{808}{808}
\abx@aux@backref{704}{salimans2016_improved}{0}{808}{808}
\@writefile{toc}{\contentsline {paragraph}{The Core Challenge}{808}{section*.1656}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Manual Inspection and Preference Ranking}{808}{section*.1658}\protected@file@percent }
\abx@aux@backref{705}{salimans2016_improved}{0}{808}{808}
\@writefile{toc}{\contentsline {paragraph}{Nearest Neighbor Retrieval}{808}{section*.1659}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Inception Score (IS)}{808}{section*.1661}\protected@file@percent }
\abx@aux@backref{706}{salimans2016_improved}{0}{808}{808}
\abx@aux@cite{0}{heusel2017_fid}
\abx@aux@segm{0}{0}{heusel2017_fid}
\@writefile{toc}{\contentsline {paragraph}{Fr\'echet Inception Distance (FID)}{809}{section*.1662}\protected@file@percent }
\abx@aux@backref{707}{heusel2017_fid}{0}{809}{809}
\@writefile{toc}{\contentsline {paragraph}{What Does the Formula Measure?}{809}{section*.1663}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Theoretical Background: 2-Wasserstein Distance}{809}{section*.1664}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How to Interpret FID Scores}{809}{section*.1665}\protected@file@percent }
\abx@aux@cite{0}{sajjadi2018_precision}
\abx@aux@segm{0}{0}{sajjadi2018_precision}
\abx@aux@cite{0}{binkowski2018_demystifying}
\abx@aux@segm{0}{0}{binkowski2018_demystifying}
\abx@aux@cite{0}{khrulkov2018_geometry}
\abx@aux@segm{0}{0}{khrulkov2018_geometry}
\@writefile{toc}{\contentsline {paragraph}{Why FID Is Preferred}{810}{section*.1666}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{FID Limitations}{810}{section*.1667}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{FID Summary}{810}{section*.1668}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Other Quantitative Metrics}{810}{section*.1669}\protected@file@percent }
\abx@aux@backref{708}{sajjadi2018_precision}{0}{810}{810}
\abx@aux@backref{709}{binkowski2018_demystifying}{0}{810}{810}
\abx@aux@backref{710}{khrulkov2018_geometry}{0}{810}{810}
\@writefile{toc}{\contentsline {paragraph}{Summary}{810}{section*.1671}\protected@file@percent }
\BKM@entry{id=748,dest={73756273656374696F6E2E32302E352E34},srcline={1554}}{5C3337365C3337375C303030475C303030415C3030304E5C3030305C3034305C303030455C303030785C303030705C3030306C5C3030306F5C303030735C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\BKM@entry{id=749,dest={73756273656374696F6E2E32302E352E35},srcline={1581}}{5C3337365C3337375C303030575C303030615C303030735C303030735C303030655C303030725C303030735C303030745C303030655C303030695C3030306E5C3030305C3034305C303030475C303030415C3030304E5C3030305C3034305C3030305C3035305C303030575C303030475C303030415C3030304E5C3030305C3035315C3030303A5C3030305C3034305C303030455C303030615C303030725C303030745C303030685C3030305C3034305C3030304D5C3030306F5C303030765C303030655C303030725C3034305C3033315C303030735C3030305C3034305C303030445C303030695C303030735C303030745C303030615C3030306E5C303030635C30303065}
\abx@aux@cite{0}{arjovsky2017_wgan}
\abx@aux@segm{0}{0}{arjovsky2017_wgan}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.5.4}GAN Explosion}{811}{subsection.20.5.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.23}{\ignorespaces The GAN explosion: number of GAN-related papers published per year since 2014.}}{811}{figure.caption.1672}\protected@file@percent }
\newlabel{fig:chapter20_gan_zoo}{{20.23}{811}{The GAN explosion: number of GAN-related papers published per year since 2014}{figure.caption.1672}{}}
\@writefile{toc}{\contentsline {paragraph}{Next Steps: Improving GANs}{811}{section*.1673}\protected@file@percent }
\abx@aux@backref{711}{goodfellow2014_adversarial}{0}{811}{811}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.5.5}Wasserstein GAN (WGAN): Earth Mover’s Distance}{811}{subsection.20.5.5}\protected@file@percent }
\newlabel{subsec:chapter20_wgan_principles}{{20.5.5}{811}{Wasserstein GAN (WGAN): Earth Mover’s Distance}{subsection.20.5.5}{}}
\abx@aux@backref{712}{arjovsky2017_wgan}{0}{811}{811}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\abx@aux@cite{0}{arjovsky2017_wgan}
\abx@aux@segm{0}{0}{arjovsky2017_wgan}
\@writefile{toc}{\contentsline {paragraph}{Supports and Low-Dimensional Manifolds}{812}{section*.1674}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why the JS Divergence Fails in High Dimensions}{812}{section*.1675}\protected@file@percent }
\abx@aux@backref{713}{goodfellow2014_adversarial}{0}{812}{812}
\@writefile{toc}{\contentsline {paragraph}{Why Non-Saturating GANs Still Suffer}{812}{section*.1676}\protected@file@percent }
\abx@aux@backref{714}{arjovsky2017_wgan}{0}{812}{812}
\abx@aux@cite{0}{gulrajani2017_improvedwgan}
\abx@aux@segm{0}{0}{gulrajani2017_improvedwgan}
\@writefile{toc}{\contentsline {paragraph}{The Need for a Better Distance Metric}{813}{section*.1677}\protected@file@percent }
\abx@aux@backref{715}{gulrajani2017_improvedwgan}{0}{813}{813}
\@writefile{toc}{\contentsline {paragraph}{Wasserstein-1 Distance: Transporting Mass}{813}{section*.1678}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example: Optimal Transport Plans as Joint Tables}{813}{section*.1679}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why This Matters}{814}{section*.1680}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.24}{\ignorespaces Results of WGAN and WGAN-GP on the LSUN Bedrooms dataset. Unlike standard GANs, these models successfully learn to generate a wide range of realistic images (without mode collapse), thanks to the use of the Wasserstein-1 distance and a stable training objective.}}{814}{figure.caption.1681}\protected@file@percent }
\newlabel{fig:chapter20_wgan_sample_results}{{20.24}{814}{Results of WGAN and WGAN-GP on the LSUN Bedrooms dataset. Unlike standard GANs, these models successfully learn to generate a wide range of realistic images (without mode collapse), thanks to the use of the Wasserstein-1 distance and a stable training objective}{figure.caption.1681}{}}
\@writefile{toc}{\contentsline {paragraph}{From Intractable Transport to Practical Training}{815}{section*.1682}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{What These Expectations Mean in Practice}{815}{section*.1683}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How the Training Works}{815}{section*.1684}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why This Makes Sense — Even if Samples Differ Sharply}{815}{section*.1685}\protected@file@percent }
\abx@aux@cite{0}{arjovsky2017_wgan}
\abx@aux@segm{0}{0}{arjovsky2017_wgan}
\@writefile{toc}{\contentsline {paragraph}{Summary}{816}{section*.1686}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Side-by-Side: Standard GAN vs.\ WGAN}{816}{section*.1687}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {20.2}{\ignorespaces Compact comparison of standard GAN and Wasserstein GAN (WGAN) formulations.}}{816}{table.caption.1688}\protected@file@percent }
\newlabel{tab:gan_vs_wgan_math}{{20.2}{816}{Compact comparison of standard GAN and Wasserstein GAN (WGAN) formulations}{table.caption.1688}{}}
\@writefile{toc}{\contentsline {paragraph}{What’s Missing: Enforcing the 1-Lipschitz Constraint}{816}{section*.1689}\protected@file@percent }
\abx@aux@cite{0}{arjovsky2017_wgan}
\abx@aux@segm{0}{0}{arjovsky2017_wgan}
\abx@aux@cite{0}{arjovsky2017_wgan}
\abx@aux@segm{0}{0}{arjovsky2017_wgan}
\abx@aux@cite{0}{arjovsky2017_wgan}
\abx@aux@segm{0}{0}{arjovsky2017_wgan}
\abx@aux@cite{0}{arjovsky2017_wgan}
\abx@aux@segm{0}{0}{arjovsky2017_wgan}
\abx@aux@backref{716}{arjovsky2017_wgan}{0}{817}{817}
\@writefile{toc}{\contentsline {paragraph}{Weight Clipping: A Crude Approximation}{817}{section*.1690}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Benefits of WGAN}{817}{section*.1691}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.25}{\ignorespaces  From Arjovsky et al.~\blx@tocontentsinit {0}\cite {arjovsky2017_wgan}, Figure 4. \textbf  {Top:} JS divergence estimates either increase or remain flat during training, even as samples improve. \textbf  {Bottom:} In unstable settings, the JS loss fluctuates wildly and fails to reflect sample quality. These observations highlight a core issue: \emph  {standard GAN losses are not correlated with sample fidelity}. }}{817}{figure.caption.1692}\protected@file@percent }
\abx@aux@backref{718}{arjovsky2017_wgan}{0}{817}{817}
\newlabel{fig:chapter20_wgan_js_vs_emd}{{20.25}{817}{From Arjovsky et al.~\cite {arjovsky2017_wgan}, Figure 4. \textbf {Top:} JS divergence estimates either increase or remain flat during training, even as samples improve. \textbf {Bottom:} In unstable settings, the JS loss fluctuates wildly and fails to reflect sample quality. These observations highlight a core issue: \emph {standard GAN losses are not correlated with sample fidelity}}{figure.caption.1692}{}}
\abx@aux@cite{0}{arjovsky2017_wgan}
\abx@aux@segm{0}{0}{arjovsky2017_wgan}
\@writefile{lof}{\contentsline {figure}{\numberline {20.26}{\ignorespaces  From Arjovsky et al.~\blx@tocontentsinit {0}\cite {arjovsky2017_wgan}, Figure 3. \textbf  {Top:} With both MLP and DCGAN generators, WGAN losses decrease smoothly as sample quality improves. \textbf  {Bottom:} In failed runs, both loss and visual quality stagnate. Unlike JS-based losses, the WGAN critic loss serves as a reliable proxy for training progress. }}{818}{figure.caption.1693}\protected@file@percent }
\abx@aux@backref{720}{arjovsky2017_wgan}{0}{818}{818}
\newlabel{fig:chapter20_wgan_training_curves}{{20.26}{818}{From Arjovsky et al.~\cite {arjovsky2017_wgan}, Figure 3. \textbf {Top:} With both MLP and DCGAN generators, WGAN losses decrease smoothly as sample quality improves. \textbf {Bottom:} In failed runs, both loss and visual quality stagnate. Unlike JS-based losses, the WGAN critic loss serves as a reliable proxy for training progress}{figure.caption.1693}{}}
\abx@aux@backref{721}{arjovsky2017_wgan}{0}{818}{818}
\@writefile{toc}{\contentsline {paragraph}{Limitations of Weight Clipping in Practice}{818}{section*.1694}\protected@file@percent }
\abx@aux@cite{0}{arjovsky2017_wgan}
\abx@aux@segm{0}{0}{arjovsky2017_wgan}
\abx@aux@cite{0}{gulrajani2017_improvedwgan}
\abx@aux@segm{0}{0}{gulrajani2017_improvedwgan}
\abx@aux@backref{722}{arjovsky2017_wgan}{0}{819}{819}
\abx@aux@backref{723}{gulrajani2017_improvedwgan}{0}{819}{819}
\BKM@entry{id=750,dest={73756273656374696F6E2E32302E352E36},srcline={1959}}{5C3337365C3337375C303030575C303030475C303030415C3030304E5C3030302D5C303030475C303030505C3030303A5C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030505C303030655C3030306E5C303030615C3030306C5C303030745C303030795C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030535C303030745C303030615C303030625C3030306C5C303030655C3030305C3034305C3030304C5C303030695C303030705C303030735C303030635C303030685C303030695C303030745C3030307A5C3030305C3034305C303030455C3030306E5C303030665C3030306F5C303030725C303030635C303030655C3030306D5C303030655C3030306E5C30303074}
\abx@aux@cite{0}{gulrajani2017_improvedwgan}
\abx@aux@segm{0}{0}{gulrajani2017_improvedwgan}
\abx@aux@cite{0}{villani2008_optimal}
\abx@aux@segm{0}{0}{villani2008_optimal}
\abx@aux@cite{0}{gulrajani2017_improvedwgan}
\abx@aux@segm{0}{0}{gulrajani2017_improvedwgan}
\@writefile{toc}{\contentsline {subsection}{\numberline {20.5.6}WGAN-GP: Gradient Penalty for Stable Lipschitz Enforcement}{820}{subsection.20.5.6}\protected@file@percent }
\newlabel{subsec:chapter20_wgan_gp}{{20.5.6}{820}{WGAN-GP: Gradient Penalty for Stable Lipschitz Enforcement}{subsection.20.5.6}{}}
\abx@aux@backref{724}{gulrajani2017_improvedwgan}{0}{820}{820}
\@writefile{toc}{\contentsline {paragraph}{Theoretical Motivation: Lipschitz Continuity and Gradient Norms}{820}{section*.1695}\protected@file@percent }
\abx@aux@backref{725}{villani2008_optimal}{0}{820}{820}
\@writefile{toc}{\contentsline {paragraph}{The WGAN-GP Loss Function}{820}{section*.1696}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Why Interpolated Points? Intuition and Implementation in WGAN-GP}{820}{subparagraph*.1697}\protected@file@percent }
\newlabel{subsec:wgan_gp_interpolated_points}{{20.5.6}{820}{Why Interpolated Points? Intuition and Implementation in WGAN-GP}{subparagraph*.1697}{}}
\abx@aux@backref{726}{gulrajani2017_improvedwgan}{0}{820}{820}
\@writefile{toc}{\contentsline {subparagraph}{Conceptual Motivation: \emph  {Where} Should Lipschitz Matter?}{820}{subparagraph*.1698}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Why This Avoids Over-Regularization}{821}{subparagraph*.1699}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Code Walkthrough: Penalty Computation for a Single Critic Update}{821}{subparagraph*.1700}\protected@file@percent }
\abx@aux@cite{0}{gulrajani2017_improvedwgan}
\abx@aux@segm{0}{0}{gulrajani2017_improvedwgan}
\@writefile{toc}{\contentsline {subparagraph}{Resulting Dynamics \& Why It Helps}{822}{subparagraph*.1701}\protected@file@percent }
\abx@aux@backref{727}{gulrajani2017_improvedwgan}{0}{822}{822}
\@writefile{toc}{\contentsline {subparagraph}{Interpreting the Loss Components}{822}{subparagraph*.1702}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Key Benefits of the Gradient Penalty vs.\ Weight Clipping}{822}{subparagraph*.1703}\protected@file@percent }
\abx@aux@cite{0}{gulrajani2017_improvedwgan}
\abx@aux@segm{0}{0}{gulrajani2017_improvedwgan}
\abx@aux@cite{0}{gulrajani2017_improvedwgan}
\abx@aux@segm{0}{0}{gulrajani2017_improvedwgan}
\abx@aux@cite{0}{gulrajani2017_improvedwgan}
\abx@aux@segm{0}{0}{gulrajani2017_improvedwgan}
\abx@aux@cite{0}{gulrajani2017_improvedwgan}
\abx@aux@segm{0}{0}{gulrajani2017_improvedwgan}
\@writefile{lof}{\contentsline {figure}{\numberline {20.27}{\ignorespaces From Gulrajani et al.~\blx@tocontentsinit {0}\cite {gulrajani2017_improvedwgan}. Inception scores (higher = better) for WGAN-GP vs.\ other GAN methods. WGAN-GP converges consistently, demonstrating improved stability over time.}}{823}{figure.caption.1704}\protected@file@percent }
\abx@aux@backref{729}{gulrajani2017_improvedwgan}{0}{823}{823}
\newlabel{fig:chapter20_wgan_gp_convergence}{{20.27}{823}{From Gulrajani et al.~\cite {gulrajani2017_improvedwgan}. Inception scores (higher = better) for WGAN-GP vs.\ other GAN methods. WGAN-GP converges consistently, demonstrating improved stability over time}{figure.caption.1704}{}}
\@writefile{toc}{\contentsline {paragraph}{Architectural Robustness}{823}{section*.1705}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.28}{\ignorespaces From Gulrajani et al.~\blx@tocontentsinit {0}\cite {gulrajani2017_improvedwgan}. Only WGAN-GP consistently trains all architectures with a shared set of hyperparameters. This enables broader experimentation and performance gains.}}{823}{figure.caption.1706}\protected@file@percent }
\abx@aux@backref{731}{gulrajani2017_improvedwgan}{0}{823}{823}
\newlabel{fig:chapter20_wgan_gp_archs}{{20.28}{823}{From Gulrajani et al.~\cite {gulrajani2017_improvedwgan}. Only WGAN-GP consistently trains all architectures with a shared set of hyperparameters. This enables broader experimentation and performance gains}{figure.caption.1706}{}}
\@writefile{toc}{\contentsline {paragraph}{State-of-the-Art Results on CIFAR-10}{824}{section*.1707}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{824}{section*.1708}\protected@file@percent }
\BKM@entry{id=751,dest={73656374696F6E2A2E31373039},srcline={2165}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030365C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030535C303030745C303030795C3030306C5C303030655C303030475C303030415C3030304E5C3030305C3034305C303030465C303030615C3030306D5C303030695C3030306C5C30303079}
\abx@aux@cite{0}{karras2019_stylegan}
\abx@aux@segm{0}{0}{karras2019_stylegan}
\abx@aux@cite{0}{karras2020_stylegan2}
\abx@aux@segm{0}{0}{karras2020_stylegan2}
\abx@aux@cite{0}{karras2021_stylegan3}
\abx@aux@segm{0}{0}{karras2021_stylegan3}
\abx@aux@cite{0}{karras2018_progrowing}
\abx@aux@segm{0}{0}{karras2018_progrowing}
\BKM@entry{id=752,dest={73656374696F6E2A2E31373130},srcline={2171}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030365C3030302E5C303030315C3030303A5C3030305C3034305C303030505C303030725C3030306F5C303030475C303030415C3030304E5C3030305C3034305C3030304F5C303030765C303030655C303030725C303030765C303030695C303030655C303030775C3030303A5C3030305C3034305C303030415C3030305C3034305C303030535C303030745C303030615C303030625C303030695C3030306C5C303030695C303030745C303030795C3030302D5C3030304F5C303030725C303030695C303030655C3030306E5C303030745C303030655C303030645C3030305C3034305C303030445C303030655C303030735C303030695C303030675C3030306E}
\abx@aux@cite{0}{karras2018_progrowing}
\abx@aux@segm{0}{0}{karras2018_progrowing}
\abx@aux@cite{0}{karras2018_progrowing}
\abx@aux@segm{0}{0}{karras2018_progrowing}
\@writefile{toc}{\contentsline {section}{Enrichment 20.6: The StyleGAN Family}{825}{section*.1709}\protected@file@percent }
\newlabel{enr:chapter20_stylegan}{{20.6}{825}{\color {ocre}Enrichment \thesection : The StyleGAN Family}{section*.1709}{}}
\abx@aux@backref{732}{karras2019_stylegan}{0}{825}{825}
\abx@aux@backref{733}{karras2021_stylegan3}{0}{825}{825}
\abx@aux@backref{734}{karras2020_stylegan2}{0}{825}{825}
\abx@aux@backref{735}{karras2018_progrowing}{0}{825}{825}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.6.1: ProGAN Overview: A Stability-Oriented Design}{825}{section*.1710}\protected@file@percent }
\abx@aux@backref{736}{karras2018_progrowing}{0}{825}{825}
\@writefile{toc}{\contentsline {paragraph}{Training Strategy}{825}{section*.1711}\protected@file@percent }
\abx@aux@backref{737}{karras2018_progrowing}{0}{825}{825}
\abx@aux@cite{0}{karras2018_progrowing}
\abx@aux@segm{0}{0}{karras2018_progrowing}
\abx@aux@cite{0}{wolf2019_proganblog}
\abx@aux@segm{0}{0}{wolf2019_proganblog}
\abx@aux@cite{0}{karras2018_progrowing}
\abx@aux@segm{0}{0}{karras2018_progrowing}
\abx@aux@cite{0}{wolf2019_proganblog}
\abx@aux@segm{0}{0}{wolf2019_proganblog}
\@writefile{toc}{\contentsline {paragraph}{Why This Works}{826}{section*.1712}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.29}{\ignorespaces  Progressive Growing in ProGAN: Training begins with low-resolution images (e.g., 4×4). The generator progressively grows by adding layers that increase spatial resolution (upsampling), while the discriminator grows by adding layers that decrease resolution (downsampling). Both networks expand in synchrony during training, stabilizing convergence and enabling high-resolution synthesis. Figure adapted from~\blx@tocontentsinit {0}\cite {karras2018_progrowing}, visualized clearer in~\blx@tocontentsinit {0}\cite {wolf2019_proganblog}. }}{827}{figure.caption.1713}\protected@file@percent }
\abx@aux@backref{740}{karras2018_progrowing}{0}{827}{827}
\abx@aux@backref{741}{wolf2019_proganblog}{0}{827}{827}
\newlabel{fig:chapter20_progan_growth}{{20.29}{827}{Progressive Growing in ProGAN: Training begins with low-resolution images (e.g., 4×4). The generator progressively grows by adding layers that increase spatial resolution (upsampling), while the discriminator grows by adding layers that decrease resolution (downsampling). Both networks expand in synchrony during training, stabilizing convergence and enabling high-resolution synthesis. Figure adapted from~\cite {karras2018_progrowing}, visualized clearer in~\cite {wolf2019_proganblog}}{figure.caption.1713}{}}
\BKM@entry{id=753,dest={73656374696F6E2A2E31373135},srcline={2259}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030365C3030302E5C303030325C3030303A5C3030305C3034305C303030535C303030745C303030795C3030306C5C303030655C303030475C303030415C3030304E5C3030303A5C3030305C3034305C303030535C303030745C303030795C3030306C5C303030655C3030302D5C303030425C303030615C303030735C303030655C303030645C3030305C3034305C303030535C303030795C3030306E5C303030745C303030685C303030655C303030735C303030695C303030735C3030305C3034305C303030765C303030695C303030615C3030305C3034305C3030304C5C303030615C303030745C303030655C3030306E5C303030745C3030305C3034305C3030304D5C3030306F5C303030645C303030755C3030306C5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{karras2019_stylegan}
\abx@aux@segm{0}{0}{karras2019_stylegan}
\abx@aux@cite{0}{karras2019_stylegan}
\abx@aux@segm{0}{0}{karras2019_stylegan}
\abx@aux@cite{0}{karras2019_stylegan}
\abx@aux@segm{0}{0}{karras2019_stylegan}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.6.1.1: Limitations of ProGAN: Toward Style-Based Generators}{828}{section*.1714}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.6.2: StyleGAN: Style-Based Synthesis via Latent Modulation}{828}{section*.1715}\protected@file@percent }
\newlabel{enr:chapter20_stylegan1}{{20.6.2}{828}{\color {ocre}Enrichment \thesubsection : StyleGAN: Style-Based Synthesis via Latent Modulation}{section*.1715}{}}
\abx@aux@backref{742}{karras2019_stylegan}{0}{828}{828}
\@writefile{lof}{\contentsline {figure}{\numberline {20.30}{\ignorespaces StyleGAN architecture: The latent code \( z \) is first mapped to \( w \), which then controls AdaIN layers across the generator. Stochastic noise is injected at each layer for texture variation. Image adapted from~\blx@tocontentsinit {0}\cite {karras2019_stylegan}.}}{829}{figure.caption.1716}\protected@file@percent }
\abx@aux@backref{744}{karras2019_stylegan}{0}{829}{829}
\newlabel{fig:chapter20_stylegan1_block}{{20.30}{829}{StyleGAN architecture: The latent code \( z \) is first mapped to \( w \), which then controls AdaIN layers across the generator. Stochastic noise is injected at each layer for texture variation. Image adapted from~\cite {karras2019_stylegan}}{figure.caption.1716}{}}
\@writefile{toc}{\contentsline {paragraph}{(1) Mapping Network (\(\mathcal  {Z} \to \mathcal  {W}\)):}{829}{section*.1718}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Not Just Increase the Dimensionality of \( z \)?}{829}{section*.1719}\protected@file@percent }
\abx@aux@cite{0}{karras2019_stylegan}
\abx@aux@segm{0}{0}{karras2019_stylegan}
\abx@aux@cite{0}{huang2017_adain}
\abx@aux@segm{0}{0}{huang2017_adain}
\abx@aux@backref{745}{karras2019_stylegan}{0}{830}{830}
\@writefile{toc}{\contentsline {paragraph}{(2) Modulating Each Layer via AdaIN (Block A):}{830}{section*.1720}\protected@file@percent }
\abx@aux@backref{746}{huang2017_adain}{0}{830}{830}
\abx@aux@cite{0}{huang2017_adain}
\abx@aux@segm{0}{0}{huang2017_adain}
\@writefile{toc}{\contentsline {paragraph}{(3) Fixed Learned Input (Constant Tensor):}{831}{section*.1721}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(4) Stochastic Detail Injection (Block B):}{831}{section*.1722}\protected@file@percent }
\abx@aux@backref{747}{huang2017_adain}{0}{831}{831}
\@writefile{toc}{\contentsline {paragraph}{(5) Style Mixing Regularization: Breaking Co-Adaptation Across Layers}{831}{section*.1723}\protected@file@percent }
\abx@aux@cite{0}{zhang2018_lpips}
\abx@aux@segm{0}{0}{zhang2018_lpips}
\@writefile{toc}{\contentsline {paragraph}{(6) Perceptual Path Length (PPL): Quantifying Disentanglement in Latent Space}{832}{section*.1724}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{What Is LPIPS?}{832}{section*.1725}\protected@file@percent }
\abx@aux@backref{748}{zhang2018_lpips}{0}{832}{832}
\abx@aux@cite{0}{gulrajani2017_improvedwgan}
\abx@aux@segm{0}{0}{gulrajani2017_improvedwgan}
\abx@aux@cite{0}{mescheder2018_r1regularization}
\abx@aux@segm{0}{0}{mescheder2018_r1regularization}
\@writefile{toc}{\contentsline {paragraph}{Why PPL Matters — and How It Relates to Training}{833}{section*.1726}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{(7) Loss Functions: From WGAN-GP to Non-Saturating GAN + R\textsubscript  {1}}{833}{section*.1727}\protected@file@percent }
\abx@aux@backref{749}{gulrajani2017_improvedwgan}{0}{833}{833}
\abx@aux@backref{750}{mescheder2018_r1regularization}{0}{833}{833}
\abx@aux@cite{0}{karras2019_stylegan}
\abx@aux@segm{0}{0}{karras2019_stylegan}
\abx@aux@cite{0}{karras2019_stylegan}
\abx@aux@segm{0}{0}{karras2019_stylegan}
\@writefile{toc}{\contentsline {paragraph}{Summary and Additional Contributions}{834}{section*.1728}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.31}{\ignorespaces StyleGAN results on high-resolution image synthesis. The model can generate diverse, photorealistic outputs for both \emph  {faces} and \emph  {cars} at resolutions up to \(1024 \times 1024\). These images are synthesized from latent codes using layerwise style modulation and stochastic detail injection. From Karras et al.~\blx@tocontentsinit {0}\cite {karras2019_stylegan}.}}{834}{figure.caption.1729}\protected@file@percent }
\abx@aux@backref{752}{karras2019_stylegan}{0}{834}{834}
\newlabel{fig:chapter20_stylegan_highres_faces_cars}{{20.31}{834}{StyleGAN results on high-resolution image synthesis. The model can generate diverse, photorealistic outputs for both \emph {faces} and \emph {cars} at resolutions up to \(1024 \times 1024\). These images are synthesized from latent codes using layerwise style modulation and stochastic detail injection. From Karras et al.~\cite {karras2019_stylegan}}{figure.caption.1729}{}}
\BKM@entry{id=754,dest={73656374696F6E2A2E31373331},srcline={2491}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030365C3030302E5C303030335C3030303A5C3030305C3034305C303030535C303030745C303030795C3030306C5C303030655C303030475C303030415C3030304E5C303030325C3030303A5C3030305C3034305C303030455C3030306C5C303030695C3030306D5C303030695C3030306E5C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030415C303030725C303030745C303030695C303030665C303030615C303030635C303030745C303030735C3030302C5C3030305C3034305C303030495C3030306D5C303030705C303030725C3030306F5C303030765C303030695C3030306E5C303030675C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030535C303030745C303030615C303030625C303030695C3030306C5C303030695C303030745C30303079}
\abx@aux@cite{0}{karras2020_stylegan2}
\abx@aux@segm{0}{0}{karras2020_stylegan2}
\abx@aux@cite{0}{karras2020_stylegan2}
\abx@aux@segm{0}{0}{karras2020_stylegan2}
\abx@aux@cite{0}{karras2020_stylegan2}
\abx@aux@segm{0}{0}{karras2020_stylegan2}
\abx@aux@cite{0}{karras2020_stylegan2}
\abx@aux@segm{0}{0}{karras2020_stylegan2}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.6.3: StyleGAN2: Eliminating Artifacts, Improving Training Stability}{835}{section*.1731}\protected@file@percent }
\newlabel{enr:chapter20_stylegan2}{{20.6.3}{835}{\color {ocre}Enrichment \thesubsection : StyleGAN2: Eliminating Artifacts, Improving Training Stability}{section*.1731}{}}
\abx@aux@backref{753}{karras2020_stylegan2}{0}{835}{835}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.6.3.1: Background: From StyleGAN1 to StyleGAN2}{835}{section*.1732}\protected@file@percent }
\abx@aux@backref{754}{karras2020_stylegan2}{0}{835}{835}
\@writefile{lof}{\contentsline {figure}{\numberline {20.32}{\ignorespaces Systemic artifacts in StyleGAN1 (“droplets”). Because AdaIN normalizes feature maps per channel, the generator injects localized spikes that skew normalization statistics. These spikes ultimately manifest as structured artifacts. Source: \blx@tocontentsinit {0}\cite {karras2020_stylegan2}.}}{835}{figure.caption.1733}\protected@file@percent }
\abx@aux@backref{756}{karras2020_stylegan2}{0}{835}{835}
\newlabel{fig:chapter20_stylegan1_artifacts}{{20.32}{835}{Systemic artifacts in StyleGAN1 (“droplets”). Because AdaIN normalizes feature maps per channel, the generator injects localized spikes that skew normalization statistics. These spikes ultimately manifest as structured artifacts. Source: \cite {karras2020_stylegan2}}{figure.caption.1733}{}}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.6.3.2: Weight Demodulation: A Principled Replacement for AdaIN}{836}{section*.1734}\protected@file@percent }
\abx@aux@cite{0}{karras2020_stylegan2}
\abx@aux@segm{0}{0}{karras2020_stylegan2}
\abx@aux@cite{0}{hui2020_styleganblog}
\abx@aux@segm{0}{0}{hui2020_styleganblog}
\abx@aux@cite{0}{karras2020_stylegan2}
\abx@aux@segm{0}{0}{karras2020_stylegan2}
\abx@aux@cite{0}{hui2020_styleganblog}
\abx@aux@segm{0}{0}{hui2020_styleganblog}
\@writefile{lof}{\contentsline {figure}{\numberline {20.33}{\ignorespaces In StyleGAN2, style control moves from post-convolution (AdaIN) to a \textit  {weight-centric} approach: each block uses (1) an affine transformation of the latent code, (2) weight modulation, (3) weight demodulation, and (4) a normal convolution. Adapted from \blx@tocontentsinit {0}\cite {karras2020_stylegan2}, figure by Jonathan Hui~\blx@tocontentsinit {0}\cite {hui2020_styleganblog}.}}{837}{figure.caption.1735}\protected@file@percent }
\abx@aux@backref{759}{karras2020_stylegan2}{0}{837}{837}
\abx@aux@backref{760}{hui2020_styleganblog}{0}{837}{837}
\newlabel{fig:chapter20_stylegan2_weightdemod}{{20.33}{837}{In StyleGAN2, style control moves from post-convolution (AdaIN) to a \textit {weight-centric} approach: each block uses (1) an affine transformation of the latent code, (2) weight modulation, (3) weight demodulation, and (4) a normal convolution. Adapted from \cite {karras2020_stylegan2}, figure by Jonathan Hui~\cite {hui2020_styleganblog}}{figure.caption.1735}{}}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.6.3.3: Noise Injection Relocation: Separating Style and Stochasticity}{837}{section*.1736}\protected@file@percent }
\abx@aux@cite{0}{zhang2018_lpips}
\abx@aux@segm{0}{0}{zhang2018_lpips}
\abx@aux@cite{0}{gulrajani2017_improvedwgan}
\abx@aux@segm{0}{0}{gulrajani2017_improvedwgan}
\abx@aux@cite{0}{mescheder2018_r1regularization}
\abx@aux@segm{0}{0}{mescheder2018_r1regularization}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.6.3.4: Path Length Regularization: Smoother Latent Traversals}{838}{section*.1737}\protected@file@percent }
\abx@aux@backref{761}{zhang2018_lpips}{0}{838}{838}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.6.3.5: Lazy R\textsubscript  {1} Regularization and Evolved Loss Strategy}{839}{section*.1738}\protected@file@percent }
\abx@aux@backref{762}{gulrajani2017_improvedwgan}{0}{839}{839}
\abx@aux@backref{763}{mescheder2018_r1regularization}{0}{839}{839}
\@writefile{toc}{\contentsline {paragraph}{Discriminator Loss:}{839}{section*.1739}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Generator Loss:}{839}{section*.1740}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Joint Optimization Logic:}{839}{section*.1741}\protected@file@percent }
\abx@aux@cite{0}{karras2020_stylegan2}
\abx@aux@segm{0}{0}{karras2020_stylegan2}
\abx@aux@cite{0}{karras2020_stylegan2}
\abx@aux@segm{0}{0}{karras2020_stylegan2}
\abx@aux@cite{0}{karras2020_stylegan2}
\abx@aux@segm{0}{0}{karras2020_stylegan2}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.6.3.6: No Progressive Growing}{840}{section*.1742}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.34}{\ignorespaces Phase artifact from progressive growing in StyleGAN1: teeth alignment remains fixed relative to the camera view rather than following head pose. Source: \blx@tocontentsinit {0}\cite {karras2020_stylegan2}.}}{840}{figure.caption.1743}\protected@file@percent }
\abx@aux@backref{765}{karras2020_stylegan2}{0}{840}{840}
\newlabel{fig:chapter20_stylegan2_phase_artifacts}{{20.34}{840}{Phase artifact from progressive growing in StyleGAN1: teeth alignment remains fixed relative to the camera view rather than following head pose. Source: \cite {karras2020_stylegan2}}{figure.caption.1743}{}}
\@writefile{toc}{\contentsline {paragraph}{1. Multi-Scale Skip Connections in the Generator}{840}{section*.1744}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. Residual Blocks in the Discriminator}{840}{section*.1745}\protected@file@percent }
\abx@aux@cite{0}{karras2020_stylegan2}
\abx@aux@segm{0}{0}{karras2020_stylegan2}
\abx@aux@cite{0}{karras2020_stylegan2}
\abx@aux@segm{0}{0}{karras2020_stylegan2}
\@writefile{toc}{\contentsline {paragraph}{3. Tracking Per-Resolution Contributions}{841}{section*.1746}\protected@file@percent }
\abx@aux@backref{766}{karras2020_stylegan2}{0}{841}{841}
\@writefile{lof}{\contentsline {figure}{\numberline {20.35}{\ignorespaces Resolution-wise contribution to the generator output during training. Left: a baseline network; Right: a network with doubled channels for higher resolutions. The additional capacity yields more detailed and robust high-res features. Adapted from \blx@tocontentsinit {0}\cite {karras2020_stylegan2}.}}{841}{figure.caption.1747}\protected@file@percent }
\abx@aux@backref{768}{karras2020_stylegan2}{0}{841}{841}
\newlabel{fig:chapter20_stylegan2_resolution_contribution}{{20.35}{841}{Resolution-wise contribution to the generator output during training. Left: a baseline network; Right: a network with doubled channels for higher resolutions. The additional capacity yields more detailed and robust high-res features. Adapted from \cite {karras2020_stylegan2}}{figure.caption.1747}{}}
\abx@aux@cite{0}{karras2021_stylegan3}
\abx@aux@segm{0}{0}{karras2021_stylegan3}
\abx@aux@cite{0}{karras2021_stylegan3}
\abx@aux@segm{0}{0}{karras2021_stylegan3}
\abx@aux@cite{0}{karras2021_stylegan3}
\abx@aux@segm{0}{0}{karras2021_stylegan3}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.6.3.7: StyleGAN3: Eliminating Texture Sticking}{842}{section*.1748}\protected@file@percent }
\abx@aux@backref{769}{karras2021_stylegan3}{0}{842}{842}
\@writefile{lof}{\contentsline {figure}{\numberline {20.36}{\ignorespaces \textbf  {Texture Sticking in StyleGAN2 vs. StyleGAN3.} Top: Average of jittered outputs. StyleGAN2 exhibits fixed detail artifacts, while StyleGAN3 blurs them correctly. Bottom: Pixel-strip visualization of interpolations. StyleGAN2 “locks” details to absolute positions (horizontal stripes); StyleGAN3 allows coherent texture motion. Adapted from~\blx@tocontentsinit {0}\cite {karras2021_stylegan3}.}}{842}{figure.caption.1749}\protected@file@percent }
\abx@aux@backref{771}{karras2021_stylegan3}{0}{842}{842}
\newlabel{fig:chapter20_stylegan2_vs_3}{{20.36}{842}{\textbf {Texture Sticking in StyleGAN2 vs. StyleGAN3.} Top: Average of jittered outputs. StyleGAN2 exhibits fixed detail artifacts, while StyleGAN3 blurs them correctly. Bottom: Pixel-strip visualization of interpolations. StyleGAN2 “locks” details to absolute positions (horizontal stripes); StyleGAN3 allows coherent texture motion. Adapted from~\cite {karras2021_stylegan3}}{figure.caption.1749}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Does Texture Sticking Occur?}{842}{section*.1750}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How StyleGAN3 Fixes It: Core Innovations}{842}{section*.1751}\protected@file@percent }
\abx@aux@cite{0}{alaluf2022_stylegan3editing}
\abx@aux@segm{0}{0}{alaluf2022_stylegan3editing}
\@writefile{toc}{\contentsline {paragraph}{Training Changes and Equivariance Goals}{843}{section*.1752}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Latent and Spatial Disentanglement}{843}{section*.1753}\protected@file@percent }
\abx@aux@backref{772}{alaluf2022_stylegan3editing}{0}{843}{843}
\@writefile{toc}{\contentsline {paragraph}{Impact in Practice}{844}{section*.1754}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Takeaway}{844}{section*.1755}\protected@file@percent }
\BKM@entry{id=755,dest={73656374696F6E2A2E31373536},srcline={2824}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030375C3030303A5C3030305C3034305C303030435C3030306F5C3030306E5C303030645C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030475C303030415C3030304E5C303030735C3030303A5C3030305C3034305C3030304C5C303030615C303030625C303030655C3030306C5C3030302D5C303030415C303030775C303030615C303030725C303030655C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030535C303030795C3030306E5C303030745C303030685C303030655C303030735C303030695C30303073}
\abx@aux@cite{0}{mirza2014_cgan}
\abx@aux@segm{0}{0}{mirza2014_cgan}
\BKM@entry{id=756,dest={73656374696F6E2A2E31373538},srcline={2841}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030375C3030302E5C303030315C3030303A5C3030305C3034305C303030435C3030306F5C3030306E5C303030645C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030425C303030615C303030745C303030635C303030685C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C303030435C303030425C3030304E5C3030305C303531}
\abx@aux@cite{0}{dumoulin2017_cbn}
\abx@aux@segm{0}{0}{dumoulin2017_cbn}
\@writefile{toc}{\contentsline {section}{Enrichment 20.7: Conditional GANs: Label-Aware Image Synthesis}{845}{section*.1756}\protected@file@percent }
\newlabel{enr:chapter20_conditional_gans}{{20.7}{845}{\color {ocre}Enrichment \thesection : Conditional GANs: Label-Aware Image Synthesis}{section*.1756}{}}
\abx@aux@backref{773}{mirza2014_cgan}{0}{845}{845}
\@writefile{lof}{\contentsline {figure}{\numberline {20.37}{\ignorespaces Conditional GAN setup: the class label \(y\) is injected into both the generator and discriminator, enabling generation of samples conditioned on class identity.}}{845}{figure.caption.1757}\protected@file@percent }
\newlabel{fig:chapter20_cgan_basic}{{20.37}{845}{Conditional GAN setup: the class label \(y\) is injected into both the generator and discriminator, enabling generation of samples conditioned on class identity}{figure.caption.1757}{}}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.7.1: Conditional Batch Normalization (CBN)}{845}{section*.1758}\protected@file@percent }
\newlabel{enr:chapter20_cbn}{{20.7.1}{845}{\color {ocre}Enrichment \thesubsection : Conditional Batch Normalization (CBN)}{section*.1758}{}}
\abx@aux@backref{774}{dumoulin2017_cbn}{0}{845}{845}
\@writefile{toc}{\contentsline {paragraph}{Motivation}{845}{section*.1759}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How CBN Works}{846}{section*.1760}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.38}{\ignorespaces Conditional Batch Normalization (CBN): the label \(y\) determines a class-specific affine transformation applied to normalized activations. This allows each class to modulate network features differently.}}{846}{figure.caption.1761}\protected@file@percent }
\newlabel{fig:chapter20_cbn}{{20.38}{846}{Conditional Batch Normalization (CBN): the label \(y\) determines a class-specific affine transformation applied to normalized activations. This allows each class to modulate network features differently}{figure.caption.1761}{}}
\@writefile{toc}{\contentsline {paragraph}{CBN in the Generator}{846}{section*.1762}\protected@file@percent }
\abx@aux@cite{0}{miyato2018_spectralnorm}
\abx@aux@segm{0}{0}{miyato2018_spectralnorm}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.1.1: Projection-Based Conditioning in Discriminators}{847}{section*.1763}\protected@file@percent }
\newlabel{enr:chapter20_projection_discriminator}{{20.7.1.1}{847}{\color {ocre}Enrichment \thesubsubsection : Projection-Based Conditioning in Discriminators}{section*.1763}{}}
\abx@aux@backref{775}{miyato2018_spectralnorm}{0}{847}{847}
\@writefile{toc}{\contentsline {paragraph}{Advantages of Projection-Based Conditioning:}{847}{section*.1764}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.1.2: Training Conditional GANs with CBN}{847}{section*.1765}\protected@file@percent }
\newlabel{enr:chapter20_cbn_training}{{20.7.1.2}{847}{\color {ocre}Enrichment \thesubsubsection : Training Conditional GANs with CBN}{section*.1765}{}}
\@writefile{toc}{\contentsline {paragraph}{Generator \( G(z, y) \): Label-Aware Synthesis}{847}{section*.1766}\protected@file@percent }
\abx@aux@cite{0}{miyato2018_spectralnorm}
\abx@aux@segm{0}{0}{miyato2018_spectralnorm}
\@writefile{toc}{\contentsline {paragraph}{Discriminator \( D(x, y) \): Realness and Label Consistency}{848}{section*.1767}\protected@file@percent }
\abx@aux@backref{776}{miyato2018_spectralnorm}{0}{848}{848}
\@writefile{toc}{\contentsline {paragraph}{Training Pipeline with CBN Conditioning:}{848}{section*.1768}\protected@file@percent }
\abx@aux@cite{0}{miyato2018_spectralnorm}
\abx@aux@segm{0}{0}{miyato2018_spectralnorm}
\@writefile{toc}{\contentsline {paragraph}{Log-Loss Intuition:}{849}{section*.1769}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Limitations of CBN-Only Conditioning}{849}{section*.1770}\protected@file@percent }
\abx@aux@backref{777}{miyato2018_spectralnorm}{0}{849}{849}
\BKM@entry{id=757,dest={73656374696F6E2A2E31373731},srcline={3029}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030375C3030302E5C303030325C3030303A5C3030305C3034305C303030535C303030705C303030655C303030635C303030745C303030725C303030615C3030306C5C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030535C303030745C303030615C303030625C3030306C5C303030655C3030305C3034305C303030475C303030415C3030304E5C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C30303067}
\abx@aux@cite{0}{miyato2018_spectralnorm}
\abx@aux@segm{0}{0}{miyato2018_spectralnorm}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.7.2: Spectral Normalization for Stable GAN Training}{850}{section*.1771}\protected@file@percent }
\newlabel{enr:chapter20_spectralnorm}{{20.7.2}{850}{\color {ocre}Enrichment \thesubsection : Spectral Normalization for Stable GAN Training}{section*.1771}{}}
\abx@aux@backref{778}{miyato2018_spectralnorm}{0}{850}{850}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.2.1: Spectral Normalization - Mathematical Background}{850}{section*.1772}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Eigenvalues and Eigenvectors: Invariant Directions in Linear Maps}{850}{section*.1773}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Singular Value Decomposition (SVD): Structure and Signal in Data}{852}{section*.1774}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{SVD: Structure, Meaning, and Application to Real-World Data}{853}{section*.1775}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Spectral Structure via \( X^\top X \) and \( XX^\top \)}{855}{section*.1776}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Economy (or Truncated) SVD}{855}{section*.1777}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How is SVD Computed in Practice?}{856}{section*.1778}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Spectral Norm of a Weight Matrix}{858}{section*.1779}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Fast Spectral–Norm Estimation via Power Iteration}{858}{section*.1780}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.39}{\ignorespaces Spectral Normalization constrains the Lipschitz constant by bounding the spectral norm of each layer’s weights. This ensures smoother gradient flow, preventing the discriminator from learning overly sharp decision surfaces.}}{860}{figure.caption.1781}\protected@file@percent }
\newlabel{fig:chapter20_spectralnorm}{{20.39}{860}{Spectral Normalization constrains the Lipschitz constant by bounding the spectral norm of each layer’s weights. This ensures smoother gradient flow, preventing the discriminator from learning overly sharp decision surfaces}{figure.caption.1781}{}}
\@writefile{toc}{\contentsline {paragraph}{Alternative Loss: Hinge Loss Formulation}{860}{section*.1782}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Interpretation and Benefits}{861}{section*.1783}\protected@file@percent }
\BKM@entry{id=758,dest={73656374696F6E2A2E31373834},srcline={3477}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030375C3030302E5C303030335C3030303A5C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030475C303030415C3030304E5C303030735C3030305C3034305C3030305C3035305C303030535C303030415C303030475C303030415C3030304E5C3030305C303531}
\abx@aux@cite{0}{zhang2019_sagan}
\abx@aux@segm{0}{0}{zhang2019_sagan}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.7.3: Self-Attention GANs (SAGAN)}{862}{section*.1784}\protected@file@percent }
\newlabel{enr:chapter20_sagan}{{20.7.3}{862}{\color {ocre}Enrichment \thesubsection : Self-Attention GANs (SAGAN)}{section*.1784}{}}
\abx@aux@backref{779}{zhang2019_sagan}{0}{862}{862}
\@writefile{lof}{\contentsline {figure}{\numberline {20.40}{\ignorespaces Self-Attention enables long-range spatial dependencies in GANs, yielding improved structure and realism.}}{862}{figure.caption.1785}\protected@file@percent }
\newlabel{fig:chapter20_sagan}{{20.40}{862}{Self-Attention enables long-range spatial dependencies in GANs, yielding improved structure and realism}{figure.caption.1785}{}}
\@writefile{toc}{\contentsline {paragraph}{Architecture Overview}{862}{section*.1786}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why It Helps}{862}{section*.1787}\protected@file@percent }
\abx@aux@cite{0}{miyato2018_spectralnorm}
\abx@aux@segm{0}{0}{miyato2018_spectralnorm}
\abx@aux@cite{0}{brock2019_biggan}
\abx@aux@segm{0}{0}{brock2019_biggan}
\BKM@entry{id=759,dest={73656374696F6E2A2E31373932},srcline={3542}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030375C3030302E5C303030345C3030303A5C3030305C3034305C303030425C303030695C303030675C303030475C303030415C3030304E5C303030735C3030303A5C3030305C3034305C303030535C303030635C303030615C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030555C303030705C3030305C3034305C303030475C303030415C3030304E5C30303073}
\abx@aux@cite{0}{brock2019_biggan}
\abx@aux@segm{0}{0}{brock2019_biggan}
\abx@aux@cite{0}{miyato2018_spectralnorm}
\abx@aux@segm{0}{0}{miyato2018_spectralnorm}
\@writefile{toc}{\contentsline {paragraph}{Training Details and Stabilization}{863}{section*.1788}\protected@file@percent }
\abx@aux@backref{780}{miyato2018_spectralnorm}{0}{863}{863}
\@writefile{toc}{\contentsline {paragraph}{Loss Function}{863}{section*.1789}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Quantitative Results}{863}{section*.1790}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{863}{section*.1791}\protected@file@percent }
\abx@aux@backref{781}{brock2019_biggan}{0}{863}{863}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.7.4: BigGANs: Scaling Up GANs}{863}{section*.1792}\protected@file@percent }
\newlabel{enr:chapter20_biggan}{{20.7.4}{863}{\color {ocre}Enrichment \thesubsection : BigGANs: Scaling Up GANs}{section*.1792}{}}
\abx@aux@backref{782}{brock2019_biggan}{0}{863}{863}
\@writefile{toc}{\contentsline {paragraph}{Key Innovations and Techniques}{863}{section*.1793}\protected@file@percent }
\abx@aux@backref{783}{miyato2018_spectralnorm}{0}{863}{863}
\abx@aux@cite{0}{brock2017_introspective}
\abx@aux@segm{0}{0}{brock2017_introspective}
\abx@aux@cite{0}{saxe2014_exact}
\abx@aux@segm{0}{0}{saxe2014_exact}
\abx@aux@cite{0}{zhang2019_sagan}
\abx@aux@segm{0}{0}{zhang2019_sagan}
\abx@aux@backref{784}{brock2017_introspective}{0}{864}{864}
\abx@aux@backref{785}{saxe2014_exact}{0}{864}{864}
\abx@aux@backref{786}{zhang2019_sagan}{0}{864}{864}
\@writefile{lof}{\contentsline {figure}{\numberline {20.41}{\ignorespaces BigGAN: high-fidelity, class-conditional samples across resolutions (128--512 px) on ImageNet.}}{864}{figure.caption.1794}\protected@file@percent }
\newlabel{fig:chapter20_biggan}{{20.41}{864}{BigGAN: high-fidelity, class-conditional samples across resolutions (128--512 px) on ImageNet}{figure.caption.1794}{}}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.4.1: Skip-\( z \) Connections: Hierarchical Latent Injection}{865}{section*.1795}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Mechanism:}{865}{section*.1796}\protected@file@percent }
\abx@aux@cite{0}{he2016_resnet}
\abx@aux@segm{0}{0}{he2016_resnet}
\abx@aux@cite{0}{brock2019_biggan}
\abx@aux@segm{0}{0}{brock2019_biggan}
\abx@aux@cite{0}{brock2019_biggan}
\abx@aux@segm{0}{0}{brock2019_biggan}
\@writefile{toc}{\contentsline {paragraph}{Comparison to Standard CBN:}{866}{section*.1797}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{BigGAN-deep Simplification:}{866}{section*.1798}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.4.2: Residual Architecture: Deep and Stable Generators}{866}{section*.1799}\protected@file@percent }
\abx@aux@backref{787}{he2016_resnet}{0}{866}{866}
\@writefile{toc}{\contentsline {paragraph}{Motivation and Design:}{866}{section*.1800}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{BigGAN vs. BigGAN-deep:}{866}{section*.1801}\protected@file@percent }
\abx@aux@cite{0}{brock2019_biggan}
\abx@aux@segm{0}{0}{brock2019_biggan}
\abx@aux@cite{0}{brock2019_biggan}
\abx@aux@segm{0}{0}{brock2019_biggan}
\@writefile{lof}{\contentsline {figure}{\numberline {20.42}{\ignorespaces  BigGAN architectural layout and residual blocks~\blx@tocontentsinit {0}\cite {brock2019_biggan}. (a) Generator architecture with hierarchical latent injection via skip-\( z \) connections. (b) Residual block with upsampling in the generator (ResBlock up). (c) Residual block with downsampling in the discriminator (ResBlock down). }}{867}{figure.caption.1802}\protected@file@percent }
\abx@aux@backref{789}{brock2019_biggan}{0}{867}{867}
\newlabel{fig:chapter20_biggan_architecture}{{20.42}{867}{BigGAN architectural layout and residual blocks~\cite {brock2019_biggan}. (a) Generator architecture with hierarchical latent injection via skip-\( z \) connections. (b) Residual block with upsampling in the generator (ResBlock up). (c) Residual block with downsampling in the discriminator (ResBlock down)}{figure.caption.1802}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.43}{\ignorespaces  BigGAN-deep architectural layout and residual blocks~\blx@tocontentsinit {0}\cite {brock2019_biggan}. (a) Generator structure with deeper residual hierarchies and full latent conditioning. (b) Residual block with upsampling in the generator. (c) Residual block with downsampling in the discriminator. Blocks without up/downsampling are identity-preserving and exclude pooling layers. }}{868}{figure.caption.1803}\protected@file@percent }
\abx@aux@backref{791}{brock2019_biggan}{0}{868}{868}
\newlabel{fig:chapter20_biggan_deep_architecture}{{20.43}{868}{BigGAN-deep architectural layout and residual blocks~\cite {brock2019_biggan}. (a) Generator structure with deeper residual hierarchies and full latent conditioning. (b) Residual block with upsampling in the generator. (c) Residual block with downsampling in the discriminator. Blocks without up/downsampling are identity-preserving and exclude pooling layers}{figure.caption.1803}{}}
\abx@aux@cite{0}{brock2019_biggan}
\abx@aux@segm{0}{0}{brock2019_biggan}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.4.3: Truncation Trick in BigGAN: Quality vs. Diversity}{869}{section*.1804}\protected@file@percent }
\abx@aux@backref{792}{brock2019_biggan}{0}{869}{869}
\@writefile{toc}{\contentsline {paragraph}{Truncated Normal Distributions in Latent Space}{869}{section*.1805}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Truncate?}{869}{section*.1806}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How Is \( \tau \) Chosen?}{869}{section*.1807}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Implementation in Practice}{869}{section*.1808}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Tradeoffs and Limitations}{870}{section*.1809}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{When Truncation Fails}{870}{section*.1810}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How to Make Truncation Work Reliably}{870}{section*.1811}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.4.4: Orthogonal Regularization: A Smoothness Prior for Truncated Latents}{870}{section*.1812}\protected@file@percent }
\abx@aux@cite{0}{saxe2014_exact}
\abx@aux@segm{0}{0}{saxe2014_exact}
\abx@aux@backref{793}{saxe2014_exact}{0}{871}{871}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.4.5: Exponential Moving Average (EMA) of Generator Weights}{871}{section*.1813}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.4.6: Discriminator-to-Generator Update Ratio}{872}{section*.1814}\protected@file@percent }
\abx@aux@cite{0}{donahue2019_bigbigan}
\abx@aux@segm{0}{0}{donahue2019_bigbigan}
\abx@aux@cite{0}{dhariwal2021_diffusion}
\abx@aux@segm{0}{0}{dhariwal2021_diffusion}
\abx@aux@cite{0}{lee2023_styleganT}
\abx@aux@segm{0}{0}{lee2023_styleganT}
\abx@aux@cite{0}{song2023_consistency}
\abx@aux@segm{0}{0}{song2023_consistency}
\@writefile{toc}{\contentsline {paragraph}{Results and Legacy}{873}{section*.1815}\protected@file@percent }
\abx@aux@backref{794}{donahue2019_bigbigan}{0}{873}{873}
\abx@aux@backref{795}{dhariwal2021_diffusion}{0}{873}{873}
\abx@aux@backref{796}{lee2023_styleganT}{0}{873}{873}
\abx@aux@backref{797}{song2023_consistency}{0}{873}{873}
\BKM@entry{id=760,dest={73656374696F6E2A2E31383136},srcline={3821}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030375C3030302E5C303030355C3030303A5C3030305C3034305C303030535C303030745C303030615C303030635C3030306B5C303030475C303030415C3030304E5C3030303A5C3030305C3034305C303030545C303030775C3030306F5C3030302D5C303030535C303030745C303030615C303030675C303030655C3030305C3034305C303030545C303030655C303030785C303030745C3030302D5C303030745C3030306F5C3030302D5C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030535C303030795C3030306E5C303030745C303030685C303030655C303030735C303030695C30303073}
\abx@aux@cite{0}{zhang2017_stackgan}
\abx@aux@segm{0}{0}{zhang2017_stackgan}
\abx@aux@cite{0}{reed2016_ganintcls}
\abx@aux@segm{0}{0}{reed2016_ganintcls}
\abx@aux@cite{0}{reed2016_gawnn}
\abx@aux@segm{0}{0}{reed2016_gawnn}
\abx@aux@cite{0}{zhang2017_stackgan}
\abx@aux@segm{0}{0}{zhang2017_stackgan}
\abx@aux@cite{0}{zhang2017_stackgan}
\abx@aux@segm{0}{0}{zhang2017_stackgan}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.7.5: StackGAN: Two-Stage Text-to-Image Synthesis}{874}{section*.1816}\protected@file@percent }
\newlabel{enr:chapter20_stackgan}{{20.7.5}{874}{\color {ocre}Enrichment \thesubsection : StackGAN: Two-Stage Text-to-Image Synthesis}{section*.1816}{}}
\abx@aux@backref{798}{zhang2017_stackgan}{0}{874}{874}
\abx@aux@backref{799}{reed2016_ganintcls}{0}{874}{874}
\abx@aux@backref{800}{reed2016_gawnn}{0}{874}{874}
\abx@aux@cite{0}{zhang2017_stackgan}
\abx@aux@segm{0}{0}{zhang2017_stackgan}
\abx@aux@cite{0}{zhang2017_stackgan}
\abx@aux@segm{0}{0}{zhang2017_stackgan}
\@writefile{lof}{\contentsline {figure}{\numberline {20.44}{\ignorespaces Comparison of StackGAN and a one-stage 256\(\times \)256 GAN~\blx@tocontentsinit {0}\cite {zhang2017_stackgan}. (a) Stage-I produces low-resolution sketches with basic color and shape. (b) Stage-II enhances resolution and realism. (c) One-stage GAN fails to produce plausible high-resolution outputs.}}{875}{figure.caption.1817}\protected@file@percent }
\abx@aux@backref{802}{zhang2017_stackgan}{0}{875}{875}
\newlabel{fig:chapter20_stackgan_vs_gan}{{20.44}{875}{Comparison of StackGAN and a one-stage 256\(\times \)256 GAN~\cite {zhang2017_stackgan}. (a) Stage-I produces low-resolution sketches with basic color and shape. (b) Stage-II enhances resolution and realism. (c) One-stage GAN fails to produce plausible high-resolution outputs}{figure.caption.1817}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.45}{\ignorespaces Architecture of StackGAN~\blx@tocontentsinit {0}\cite {zhang2017_stackgan}. Stage-I generator synthesizes low-resolution images from text embedding and noise. Stage-II generator refines Stage-I outputs by injecting additional detail using residual blocks and upsampling layers.}}{876}{figure.caption.1818}\protected@file@percent }
\abx@aux@backref{804}{zhang2017_stackgan}{0}{876}{876}
\newlabel{fig:chapter20_stackgan_arch}{{20.45}{876}{Architecture of StackGAN~\cite {zhang2017_stackgan}. Stage-I generator synthesizes low-resolution images from text embedding and noise. Stage-II generator refines Stage-I outputs by injecting additional detail using residual blocks and upsampling layers}{figure.caption.1818}{}}
\@writefile{toc}{\contentsline {paragraph}{From Overview to Components:}{876}{section*.1819}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.5.1: Conditioning Augmentation (CA)}{877}{section*.1820}\protected@file@percent }
\newlabel{enr:chapter20_stackgan_ca}{{20.7.5.1}{877}{\color {ocre}Enrichment \thesubsubsection : Conditioning Augmentation (CA)}{section*.1820}{}}
\@writefile{toc}{\contentsline {paragraph}{Solution: Learn a Distribution Over Conditioning Vectors}{877}{section*.1821}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Sampling via Reparameterization Trick}{877}{section*.1822}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{KL Divergence Regularization}{877}{section*.1823}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Benefits of Conditioning Augmentation}{877}{section*.1824}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary Table: Conditioning Augmentation}{878}{section*.1825}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.5.2: Stage-I Generator: Coarse Sketching from Noise and Caption}{878}{section*.1826}\protected@file@percent }
\newlabel{enr:chapter20_stackgan_stage1}{{20.7.5.2}{878}{\color {ocre}Enrichment \thesubsubsection : Stage-I Generator: Coarse Sketching from Noise and Caption}{section*.1826}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation: Why Two Stages?}{878}{section*.1827}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Architecture of Stage-I Generator}{878}{section*.1828}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Output Normalization: Why Tanh?}{879}{section*.1829}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{From Latent Tensor to Displayable Image}{879}{section*.1830}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How Channel Reduction Works in Upsampling Blocks}{879}{section*.1831}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary of Stage-I Generator}{879}{section*.1832}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.5.3: Stage-II Generator: Refinement with Residual Conditioning}{880}{section*.1833}\protected@file@percent }
\newlabel{enr:chapter20_stackgan_stage2}{{20.7.5.3}{880}{\color {ocre}Enrichment \thesubsubsection : Stage-II Generator: Refinement with Residual Conditioning}{section*.1833}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Two Stages Are Beneficial}{880}{section*.1834}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Inputs to Stage-II Generator}{880}{section*.1835}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Network Structure and Residual Design}{880}{section*.1836}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Semantic Reinforcement via Dual Conditioning}{880}{section*.1837}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Discriminator in Stage-II}{881}{section*.1838}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Overall Effect of Stage-II}{881}{section*.1839}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary of Stage-II Generator}{881}{section*.1840}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.5.4: Training Procedure and Multi-Stage Objectives}{881}{section*.1841}\protected@file@percent }
\abx@aux@cite{0}{zhang2018_stackganpp}
\abx@aux@segm{0}{0}{zhang2018_stackganpp}
\abx@aux@cite{0}{xu2018_attngan}
\abx@aux@segm{0}{0}{xu2018_attngan}
\abx@aux@cite{0}{zhu2019_dmgan}
\abx@aux@segm{0}{0}{zhu2019_dmgan}
\BKM@entry{id=761,dest={73656374696F6E2A2E31383433},srcline={4207}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030375C3030302E5C303030365C3030303A5C3030305C3034305C303030565C303030515C3030302D5C303030475C303030415C3030304E5C3030303A5C3030305C3034305C303030545C303030615C3030306D5C303030695C3030306E5C303030675C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030485C303030695C303030675C303030685C3030302D5C303030525C303030655C303030735C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030535C303030795C3030306E5C303030745C303030685C303030655C303030735C303030695C30303073}
\abx@aux@cite{0}{esser2021_vqgan}
\abx@aux@segm{0}{0}{esser2021_vqgan}
\abx@aux@cite{0}{razavi2019_vqvae2}
\abx@aux@segm{0}{0}{razavi2019_vqvae2}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.5.5: Legacy and Extensions: StackGAN++ and Beyond}{882}{section*.1842}\protected@file@percent }
\abx@aux@backref{805}{zhang2018_stackganpp}{0}{882}{882}
\abx@aux@backref{806}{xu2018_attngan}{0}{882}{882}
\abx@aux@backref{807}{zhu2019_dmgan}{0}{882}{882}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.7.6: VQ-GAN: Taming Transformers for High-Res Image Synthesis}{883}{section*.1843}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.6.1: VQ-GAN: Overview and Motivation}{883}{section*.1844}\protected@file@percent }
\newlabel{chapter20_subsubsec:vqgan_overview}{{20.7.6.1}{883}{\color {ocre}Enrichment \thesubsubsection : VQ-GAN: Overview and Motivation}{section*.1844}{}}
\abx@aux@backref{808}{esser2021_vqgan}{0}{883}{883}
\abx@aux@backref{809}{razavi2019_vqvae2}{0}{883}{883}
\abx@aux@cite{0}{esser2021_vqgan}
\abx@aux@segm{0}{0}{esser2021_vqgan}
\abx@aux@cite{0}{esser2021_vqgan}
\abx@aux@segm{0}{0}{esser2021_vqgan}
\abx@aux@cite{0}{oord2018_vqvae}
\abx@aux@segm{0}{0}{oord2018_vqvae}
\@writefile{lof}{\contentsline {figure}{\numberline {20.46}{\ignorespaces Architecture of VQ-GAN. The convolutional encoder compresses input images into discrete latent tokens using a learned codebook. The decoder reconstructs from tokens. A transformer autoregressively models the token distribution for high-resolution synthesis. Image adapted from~\blx@tocontentsinit {0}\cite {esser2021_vqgan}.}}{884}{figure.caption.1845}\protected@file@percent }
\abx@aux@backref{811}{esser2021_vqgan}{0}{884}{884}
\newlabel{fig:chapter20_vqgan_architecture}{{20.46}{884}{Architecture of VQ-GAN. The convolutional encoder compresses input images into discrete latent tokens using a learned codebook. The decoder reconstructs from tokens. A transformer autoregressively models the token distribution for high-resolution synthesis. Image adapted from~\cite {esser2021_vqgan}}{figure.caption.1845}{}}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.6.2: Training Objectives and Losses in VQ-GAN}{884}{section*.1846}\protected@file@percent }
\newlabel{chapter20_subsubsec:vqgan_losses}{{20.7.6.2}{884}{\color {ocre}Enrichment \thesubsubsection : Training Objectives and Losses in VQ-GAN}{section*.1846}{}}
\abx@aux@backref{812}{oord2018_vqvae}{0}{884}{884}
\abx@aux@cite{0}{oord2018_vqvae}
\abx@aux@segm{0}{0}{oord2018_vqvae}
\@writefile{toc}{\contentsline {paragraph}{Total Loss}{885}{section*.1847}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{1. Perceptual Reconstruction Loss \( \mathcal  {L}_{\text  {rec}} \)}{885}{section*.1848}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. Adversarial Patch Loss \( \mathcal  {L}_{\text  {GAN}} \)}{885}{section*.1849}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3. Vector Quantization Commitment and Codebook Loss \( \mathcal  {L}_{\text  {VQ}} \)}{885}{section*.1850}\protected@file@percent }
\abx@aux@backref{813}{oord2018_vqvae}{0}{885}{885}
\@writefile{toc}{\contentsline {paragraph}{Combined Optimization Strategy}{885}{section*.1851}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why This Loss Works}{885}{section*.1852}\protected@file@percent }
\abx@aux@cite{0}{oord2018_vqvae}
\abx@aux@segm{0}{0}{oord2018_vqvae}
\@writefile{toc}{\contentsline {paragraph}{Training Summary}{886}{section*.1853}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.6.3: Discrete Codebooks and Token Quantization}{886}{section*.1854}\protected@file@percent }
\newlabel{chapter20_subsubsec:vqgan_codebook}{{20.7.6.3}{886}{\color {ocre}Enrichment \thesubsubsection : Discrete Codebooks and Token Quantization}{section*.1854}{}}
\abx@aux@backref{814}{oord2018_vqvae}{0}{886}{886}
\@writefile{toc}{\contentsline {paragraph}{Latent Grid and Codebook Structure}{886}{section*.1855}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Nearest-Neighbor Quantization}{886}{section*.1856}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Gradient Flow via Stop-Gradient and Codebook Updates}{886}{section*.1857}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Codebook Capacity and Token Usage}{887}{section*.1858}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Spatial Token Grid as Transformer Input}{887}{section*.1859}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Comparison to VQ-VAE-2}{887}{section*.1860}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{887}{section*.1861}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.6.4: Autoregressive Transformer for Token Modeling}{887}{section*.1862}\protected@file@percent }
\newlabel{chapter20_subsubsec:vqgan_transformer}{{20.7.6.4}{887}{\color {ocre}Enrichment \thesubsubsection : Autoregressive Transformer for Token Modeling}{section*.1862}{}}
\@writefile{toc}{\contentsline {paragraph}{Token Sequence Construction}{887}{section*.1863}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Autoregressive Training Objective}{887}{section*.1864}\protected@file@percent }
\abx@aux@cite{0}{radford2019_language}
\abx@aux@segm{0}{0}{radford2019_language}
\@writefile{toc}{\contentsline {paragraph}{Positional Encoding and Embedding Table}{888}{section*.1865}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Sampling for Image Generation}{888}{section*.1866}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Windowed Attention for Long Sequences}{888}{section*.1867}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Comparison with Pixel-Level Modeling}{888}{section*.1868}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Transformer Variants: Decoder-Only and Encoder–Decoder}{888}{section*.1869}\protected@file@percent }
\abx@aux@backref{815}{radford2019_language}{0}{888}{888}
\abx@aux@cite{0}{raffel2020_t5}
\abx@aux@segm{0}{0}{raffel2020_t5}
\abx@aux@cite{0}{lewis2020_bart}
\abx@aux@segm{0}{0}{lewis2020_bart}
\abx@aux@backref{816}{raffel2020_t5}{0}{889}{889}
\abx@aux@backref{817}{lewis2020_bart}{0}{889}{889}
\@writefile{toc}{\contentsline {paragraph}{Training Setup}{889}{section*.1870}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{889}{section*.1871}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.6.5: Token Sampling and Grid Resolution}{889}{section*.1872}\protected@file@percent }
\newlabel{chapter20_subsubsec:vqgan_sampling}{{20.7.6.5}{889}{\color {ocre}Enrichment \thesubsubsection : Token Sampling and Grid Resolution}{section*.1872}{}}
\abx@aux@cite{0}{esser2021_vqgan}
\abx@aux@segm{0}{0}{esser2021_vqgan}
\@writefile{toc}{\contentsline {paragraph}{Autoregressive Sampling Pipeline}{890}{section*.1873}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Impact of Latent Grid Resolution}{890}{section*.1874}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Sliding Window Attention (Optional Variant)}{890}{section*.1875}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{890}{section*.1876}\protected@file@percent }
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.7.6.6: VQ-GAN: Summary and Outlook}{891}{section*.1877}\protected@file@percent }
\newlabel{chapter20_subsubsec:vqgan_summary}{{20.7.6.6}{891}{\color {ocre}Enrichment \thesubsubsection : VQ-GAN: Summary and Outlook}{section*.1877}{}}
\abx@aux@backref{818}{esser2021_vqgan}{0}{891}{891}
\@writefile{toc}{\contentsline {paragraph}{Why VQ-GAN Works}{891}{section*.1878}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Future Directions and Influence}{891}{section*.1879}\protected@file@percent }
\abx@aux@backref{819}{ramesh2021_dalle}{0}{891}{891}
\abx@aux@backref{820}{rombach2022_ldm}{0}{891}{891}
\BKM@entry{id=762,dest={73656374696F6E2A2E31383830},srcline={4528}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030385C3030303A5C3030305C3034305C303030415C303030645C303030645C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030495C3030306D5C303030705C3030306F5C303030725C303030745C303030615C3030306E5C303030745C3030305C3034305C303030475C303030415C3030304E5C3030305C3034305C303030575C3030306F5C303030725C3030306B5C30303073}
\abx@aux@cite{0}{ledig2017_srgan}
\abx@aux@segm{0}{0}{ledig2017_srgan}
\abx@aux@cite{0}{isola2017_pix2pix}
\abx@aux@segm{0}{0}{isola2017_pix2pix}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\abx@aux@cite{0}{park2019_spade}
\abx@aux@segm{0}{0}{park2019_spade}
\abx@aux@cite{0}{gupta2018_socialgan}
\abx@aux@segm{0}{0}{gupta2018_socialgan}
\abx@aux@cite{0}{kwon2023_diffusiongan}
\abx@aux@segm{0}{0}{kwon2023_diffusiongan}
\BKM@entry{id=763,dest={73656374696F6E2A2E31383831},srcline={4541}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030385C3030302E5C303030315C3030303A5C3030305C3034305C303030535C303030525C303030475C303030415C3030304E5C3030303A5C3030305C3034305C303030505C303030685C3030306F5C303030745C3030306F5C3030302D5C303030525C303030655C303030615C3030306C5C303030695C303030735C303030745C303030695C303030635C3030305C3034305C303030535C303030755C303030705C303030655C303030725C3030302D5C303030525C303030655C303030735C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{ledig2017_srgan}
\abx@aux@segm{0}{0}{ledig2017_srgan}
\@writefile{toc}{\contentsline {section}{Enrichment 20.8: Additional Important GAN Works}{892}{section*.1880}\protected@file@percent }
\abx@aux@backref{821}{ledig2017_srgan}{0}{892}{892}
\abx@aux@backref{822}{isola2017_pix2pix}{0}{892}{892}
\abx@aux@backref{823}{zhu2017_cyclegan}{0}{892}{892}
\abx@aux@backref{824}{park2019_spade}{0}{892}{892}
\abx@aux@backref{825}{gupta2018_socialgan}{0}{892}{892}
\abx@aux@backref{826}{kwon2023_diffusiongan}{0}{892}{892}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.8.1: SRGAN: Photo-Realistic Super-Resolution}{892}{section*.1881}\protected@file@percent }
\newlabel{chapter20_subsec:srgan}{{20.8.1}{892}{\color {ocre}Enrichment \thesubsection : SRGAN: Photo-Realistic Super-Resolution}{section*.1881}{}}
\abx@aux@backref{827}{ledig2017_srgan}{0}{892}{892}
\@writefile{toc}{\contentsline {paragraph}{Motivation and Limitations of Pixel-Wise Supervision}{892}{section*.1882}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Use VGG-Based Perceptual Loss?}{892}{section*.1883}\protected@file@percent }
\abx@aux@cite{0}{shi2016_espcn}
\abx@aux@segm{0}{0}{shi2016_espcn}
\abx@aux@cite{0}{shi2016_espcn}
\abx@aux@segm{0}{0}{shi2016_espcn}
\@writefile{toc}{\contentsline {paragraph}{Architecture Overview}{893}{section*.1884}\protected@file@percent }
\abx@aux@backref{828}{shi2016_espcn}{0}{893}{893}
\@writefile{toc}{\contentsline {paragraph}{Upsampling Strategy: Sub-Pixel Convolution Blocks}{893}{section*.1885}\protected@file@percent }
\abx@aux@backref{829}{shi2016_espcn}{0}{893}{893}
\abx@aux@cite{0}{ledig2017_srgan}
\abx@aux@segm{0}{0}{ledig2017_srgan}
\abx@aux@cite{0}{ledig2017_srgan}
\abx@aux@segm{0}{0}{ledig2017_srgan}
\abx@aux@cite{0}{ledig2017_srgan}
\abx@aux@segm{0}{0}{ledig2017_srgan}
\abx@aux@cite{0}{ledig2017_srgan}
\abx@aux@segm{0}{0}{ledig2017_srgan}
\@writefile{toc}{\contentsline {paragraph}{Discriminator Design}{894}{section*.1886}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.47}{\ignorespaces SRGAN architecture. Top: generator network with deep residual blocks and sub-pixel upsampling layers. Bottom: discriminator composed of convolutional blocks with increasing channel width and spatial downsampling. Figure adapted from~\blx@tocontentsinit {0}\cite {ledig2017_srgan}.}}{894}{figure.caption.1887}\protected@file@percent }
\abx@aux@backref{831}{ledig2017_srgan}{0}{894}{894}
\newlabel{fig:chapter20_srgan_architecture}{{20.47}{894}{SRGAN architecture. Top: generator network with deep residual blocks and sub-pixel upsampling layers. Bottom: discriminator composed of convolutional blocks with increasing channel width and spatial downsampling. Figure adapted from~\cite {ledig2017_srgan}}{figure.caption.1887}{}}
\abx@aux@cite{0}{shi2016_espcn}
\abx@aux@segm{0}{0}{shi2016_espcn}
\abx@aux@cite{0}{radford2016_dcgan}
\abx@aux@segm{0}{0}{radford2016_dcgan}
\BKM@entry{id=764,dest={73656374696F6E2A2E31383932},srcline={4660}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030385C3030302E5C303030325C3030303A5C3030305C3034305C303030705C303030695C303030785C303030325C303030705C303030695C303030785C3030303A5C3030305C3034305C303030505C303030615C303030695C303030725C303030655C303030645C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030302D5C303030745C3030306F5C3030302D5C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C3030306C5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030635C303030475C303030415C3030304E5C30303073}
\abx@aux@cite{0}{isola2017_pix2pix}
\abx@aux@segm{0}{0}{isola2017_pix2pix}
\@writefile{lof}{\contentsline {figure}{\numberline {20.48}{\ignorespaces Comparison of reconstruction results for 4\(\times \) super-resolution: bicubic, SRResNet (optimized for MSE), SRGAN (optimized for perceptual loss), and ground-truth. Despite lower PSNR, SRGAN achieves significantly better perceptual quality. Image adapted from~\blx@tocontentsinit {0}\cite {ledig2017_srgan}.}}{895}{figure.caption.1888}\protected@file@percent }
\abx@aux@backref{833}{ledig2017_srgan}{0}{895}{895}
\newlabel{fig:chapter20_srgan_motivation}{{20.48}{895}{Comparison of reconstruction results for 4\(\times \) super-resolution: bicubic, SRResNet (optimized for MSE), SRGAN (optimized for perceptual loss), and ground-truth. Despite lower PSNR, SRGAN achieves significantly better perceptual quality. Image adapted from~\cite {ledig2017_srgan}}{figure.caption.1888}{}}
\@writefile{toc}{\contentsline {paragraph}{Perceptual Loss Function}{895}{section*.1889}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Training Strategy}{895}{section*.1890}\protected@file@percent }
\abx@aux@backref{834}{shi2016_espcn}{0}{895}{895}
\abx@aux@backref{835}{radford2016_dcgan}{0}{895}{895}
\@writefile{toc}{\contentsline {paragraph}{Quantitative and Perceptual Results}{895}{section*.1891}\protected@file@percent }
\abx@aux@cite{0}{isola2017_pix2pix}
\abx@aux@segm{0}{0}{isola2017_pix2pix}
\abx@aux@cite{0}{isola2017_pix2pix}
\abx@aux@segm{0}{0}{isola2017_pix2pix}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.8.2: pix2pix: Paired Image-to-Image Translation with cGANs}{896}{section*.1892}\protected@file@percent }
\newlabel{enr:chapter20_pix2pix}{{20.8.2}{896}{\color {ocre}Enrichment \thesubsection : pix2pix: Paired Image-to-Image Translation with cGANs}{section*.1892}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation and Formulation}{896}{section*.1893}\protected@file@percent }
\abx@aux@backref{836}{isola2017_pix2pix}{0}{896}{896}
\@writefile{lof}{\contentsline {figure}{\numberline {20.49}{\ignorespaces pix2pix image-to-image translation results on various tasks using paired datasets: (left to right) labels to street scene, aerial photo to map, labels to facade, sketch to photo, and day to night. All results use the same underlying model. Image adapted from~\blx@tocontentsinit {0}\cite {isola2017_pix2pix}.}}{896}{figure.caption.1894}\protected@file@percent }
\abx@aux@backref{838}{isola2017_pix2pix}{0}{896}{896}
\newlabel{fig:chapter20_pix2pix_usecases}{{20.49}{896}{pix2pix image-to-image translation results on various tasks using paired datasets: (left to right) labels to street scene, aerial photo to map, labels to facade, sketch to photo, and day to night. All results use the same underlying model. Image adapted from~\cite {isola2017_pix2pix}}{figure.caption.1894}{}}
\abx@aux@cite{0}{arjovsky2017_wgan}
\abx@aux@segm{0}{0}{arjovsky2017_wgan}
\abx@aux@cite{0}{gulrajani2017_improvedwgan}
\abx@aux@segm{0}{0}{gulrajani2017_improvedwgan}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.8.2.1: Generator Architecture and L1 Loss}{897}{section*.1895}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Generator Architecture: U-Net with Skip Connections}{897}{section*.1896}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The Role of L1 Loss}{897}{section*.1897}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Not WGAN or WGAN-GP?}{897}{section*.1898}\protected@file@percent }
\abx@aux@backref{839}{arjovsky2017_wgan}{0}{897}{897}
\abx@aux@backref{840}{gulrajani2017_improvedwgan}{0}{897}{897}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\abx@aux@backref{841}{goodfellow2014_adversarial}{0}{898}{898}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.8.2.2: Discriminator Design and PatchGAN}{898}{section*.1899}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Discriminator Design and Patch-Level Realism (PatchGAN)}{898}{section*.1900}\protected@file@percent }
\newlabel{enr:chapter20_pix2pix_patchgan}{{20.8.2.2}{898}{Discriminator Design and Patch-Level Realism (PatchGAN)}{section*.1900}{}}
\abx@aux@backref{842}{goodfellow2014_adversarial}{0}{898}{898}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.8.2.3: Full Training Objective and Optimization}{899}{section*.1901}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Generator Loss: Combining Adversarial and Reconstruction Objectives}{899}{section*.1902}\protected@file@percent }
\abx@aux@cite{0}{isola2017_pix2pix}
\abx@aux@segm{0}{0}{isola2017_pix2pix}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\BKM@entry{id=765,dest={73656374696F6E2A2E31393034},srcline={4835}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030385C3030302E5C303030335C3030303A5C3030305C3034305C303030435C303030795C303030635C3030306C5C303030655C303030475C303030415C3030304E5C3030303A5C3030305C3034305C303030555C3030306E5C303030705C303030615C303030695C303030725C303030655C303030645C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030302D5C303030745C3030306F5C3030302D5C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C3030306C5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.8.2.4: Summary and Generalization Across Tasks}{900}{section*.1903}\protected@file@percent }
\abx@aux@backref{843}{isola2017_pix2pix}{0}{900}{900}
\abx@aux@backref{844}{zhu2017_cyclegan}{0}{900}{900}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.8.3: CycleGAN: Unpaired Image-to-Image Translation}{900}{section*.1904}\protected@file@percent }
\newlabel{enr:chapter20_cyclegan}{{20.8.3}{900}{\color {ocre}Enrichment \thesubsection : CycleGAN: Unpaired Image-to-Image Translation}{section*.1904}{}}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.8.3.1: Motivation: Beyond Paired Supervision in Image Translation}{900}{section*.1905}\protected@file@percent }
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\@writefile{lof}{\contentsline {figure}{\numberline {20.50}{\ignorespaces  \textbf  {Paired vs.\ Unpaired Training Data.} \emph  {Left:} Paired setting — each source image \( x_i \in X \) is matched with a corresponding target image \( y_i \in Y \), providing explicit supervision for translation. \emph  {Right:} Unpaired setting — source set \( \{x_i\}_{i=1}^N \) and target set \( \{y_j\}_{j=1}^M \) are given independently, with no direct correspondence between \( x_i \) and \( y_j \). Figure adapted from \blx@tocontentsinit {0}\cite {zhu2017_cyclegan}. }}{901}{figure.caption.1906}\protected@file@percent }
\abx@aux@backref{846}{zhu2017_cyclegan}{0}{901}{901}
\newlabel{fig:chapter20_cyclegan_paired_vs_unpaired}{{20.50}{901}{\textbf {Paired vs.\ Unpaired Training Data.} \emph {Left:} Paired setting — each source image \( x_i \in X \) is matched with a corresponding target image \( y_i \in Y \), providing explicit supervision for translation. \emph {Right:} Unpaired setting — source set \( \{x_i\}_{i=1}^N \) and target set \( \{y_j\}_{j=1}^M \) are given independently, with no direct correspondence between \( x_i \) and \( y_j \). Figure adapted from \cite {zhu2017_cyclegan}}{figure.caption.1906}{}}
\abx@aux@backref{847}{zhu2017_cyclegan}{0}{901}{901}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.8.3.2: Typical Use Cases}{901}{section*.1907}\protected@file@percent }
\abx@aux@cite{0}{cohen2018_distributionmatching}
\abx@aux@segm{0}{0}{cohen2018_distributionmatching}
\abx@aux@cite{0}{yi2019_gancyclegan_survey}
\abx@aux@segm{0}{0}{yi2019_gancyclegan_survey}
\abx@aux@cite{0}{mao2017_lsgan}
\abx@aux@segm{0}{0}{mao2017_lsgan}
\@writefile{lof}{\contentsline {figure}{\numberline {20.51}{\ignorespaces Unpaired image-to-image translation with CycleGAN. The model learns bidirectional mappings between domains without access to paired examples, enabling high-quality translation in applications. Image adapted from~\blx@tocontentsinit {0}\cite {zhu2017_cyclegan}.}}{902}{figure.caption.1908}\protected@file@percent }
\abx@aux@backref{849}{zhu2017_cyclegan}{0}{902}{902}
\newlabel{fig:chapter20_cyclegan_examples}{{20.51}{902}{Unpaired image-to-image translation with CycleGAN. The model learns bidirectional mappings between domains without access to paired examples, enabling high-quality translation in applications. Image adapted from~\cite {zhu2017_cyclegan}}{figure.caption.1908}{}}
\abx@aux@backref{850}{cohen2018_distributionmatching}{0}{902}{902}
\abx@aux@backref{851}{yi2019_gancyclegan_survey}{0}{902}{902}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.8.3.3: CycleGAN Architecture: Dual Generators and Discriminators}{902}{section*.1909}\protected@file@percent }
\newlabel{enr:chapter20_cyclegan_architecture}{{20.8.3.3}{902}{\color {ocre}Enrichment \thesubsubsection : CycleGAN Architecture: Dual Generators and Discriminators}{section*.1909}{}}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.8.3.4: CycleGAN: Loss Functions and Training Objectives}{902}{section*.1910}\protected@file@percent }
\abx@aux@backref{852}{mao2017_lsgan}{0}{903}{903}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\@writefile{lof}{\contentsline {figure}{\numberline {20.52}{\ignorespaces  \textbf  {CycleGAN architecture and cycle consistency losses.} (a) The model contains two mapping functions: \( G: X \rightarrow Y \) and \( F: Y \rightarrow X \), with corresponding adversarial discriminators \( D_Y \) and \( D_X \). Each discriminator ensures its generator's outputs are indistinguishable from real samples in its domain. (b) \emph  {Forward cycle-consistency loss:} \( x \rightarrow G(x) \rightarrow F(G(x)) \approx x \) — translating to domain \( Y \) and back should recover the original \( x \). (c) \emph  {Backward cycle-consistency loss:} \( y \rightarrow F(y) \rightarrow G(F(y)) \approx y \) — translating to domain \( X \) and back should recover the original \( y \). Figure adapted from \blx@tocontentsinit {0}\cite {zhu2017_cyclegan}. }}{904}{figure.caption.1911}\protected@file@percent }
\abx@aux@backref{854}{zhu2017_cyclegan}{0}{904}{904}
\newlabel{fig:chapter20_cyclegan_cycle_consistency}{{20.52}{904}{\textbf {CycleGAN architecture and cycle consistency losses.} (a) The model contains two mapping functions: \( G: X \rightarrow Y \) and \( F: Y \rightarrow X \), with corresponding adversarial discriminators \( D_Y \) and \( D_X \). Each discriminator ensures its generator's outputs are indistinguishable from real samples in its domain. (b) \emph {Forward cycle-consistency loss:} \( x \rightarrow G(x) \rightarrow F(G(x)) \approx x \) — translating to domain \( Y \) and back should recover the original \( x \). (c) \emph {Backward cycle-consistency loss:} \( y \rightarrow F(y) \rightarrow G(F(y)) \approx y \) — translating to domain \( X \) and back should recover the original \( y \). Figure adapted from \cite {zhu2017_cyclegan}}{figure.caption.1911}{}}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.8.3.5: Network Architecture and Practical Training Considerations}{905}{section*.1912}\protected@file@percent }
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.8.3.6: Ablation Study: Impact of Loss Components in CycleGAN}{906}{section*.1913}\protected@file@percent }
\newlabel{enr:chapter20_cyclegan_ablation}{{20.8.3.6}{906}{\color {ocre}Enrichment \thesubsubsection : Ablation Study: Impact of Loss Components in CycleGAN}{section*.1913}{}}
\abx@aux@backref{855}{zhu2017_cyclegan}{0}{906}{906}
\@writefile{toc}{\contentsline {paragraph}{Effect of Removing Loss Components}{906}{section*.1914}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Quantitative Results (from the CycleGAN Paper)}{906}{section*.1915}\protected@file@percent }
\abx@aux@backref{856}{zhu2017_cyclegan}{0}{906}{906}
\@writefile{lot}{\contentsline {table}{\numberline {20.3}{\ignorespaces Ablation study: FCN-scores for different loss variants, evaluated on Cityscapes (\emph  {labels $\rightarrow $ photo}). Results from~\blx@tocontentsinit {0}\cite {zhu2017_cyclegan}.}}{906}{table.caption.1916}\protected@file@percent }
\abx@aux@backref{858}{zhu2017_cyclegan}{0}{906}{906}
\newlabel{tab:cyclegan_ablation_label2photo}{{20.3}{906}{Ablation study: FCN-scores for different loss variants, evaluated on Cityscapes (\emph {labels $\rightarrow $ photo}). Results from~\cite {zhu2017_cyclegan}}{table.caption.1916}{}}
\@writefile{lot}{\contentsline {table}{\numberline {20.4}{\ignorespaces Ablation study: classification performance for different loss variants, evaluated on Cityscapes (\emph  {photo $\rightarrow $ labels}). Results from~\blx@tocontentsinit {0}\cite {zhu2017_cyclegan}.}}{906}{table.caption.1917}\protected@file@percent }
\abx@aux@backref{860}{zhu2017_cyclegan}{0}{906}{906}
\newlabel{tab:cyclegan_ablation_photo2label}{{20.4}{906}{Ablation study: classification performance for different loss variants, evaluated on Cityscapes (\emph {photo $\rightarrow $ labels}). Results from~\cite {zhu2017_cyclegan}}{table.caption.1917}{}}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\abx@aux@cite{0}{zhu2017_cyclegan}
\abx@aux@segm{0}{0}{zhu2017_cyclegan}
\abx@aux@cite{0}{park2019_spade}
\abx@aux@segm{0}{0}{park2019_spade}
\abx@aux@cite{0}{gupta2018_socialgan}
\abx@aux@segm{0}{0}{gupta2018_socialgan}
\abx@aux@cite{0}{clark2019_videogan}
\abx@aux@segm{0}{0}{clark2019_videogan}
\BKM@entry{id=766,dest={73656374696F6E2A2E31393232},srcline={5131}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030395C3030303A5C3030305C3034305C303030445C303030695C303030665C303030665C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030735C3030303A5C3030305C3034305C3030304D5C3030306F5C303030645C303030655C303030725C3030306E5C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C303030765C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {paragraph}{Qualitative Analysis}{907}{section*.1918}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.53}{\ignorespaces Ablation study: Visual results of different loss variants for mapping labels $\leftrightarrow $ photos on Cityscapes. From left to right: input, cycle-consistency loss alone, adversarial loss alone, GAN + forward cycle-consistency loss ($F(G(x)) \approx x$), GAN + backward cycle-consistency loss ($G(F(y)) \approx y$), CycleGAN (full method), and ground truth. Cycle alone and GAN + backward fail to produce realistic images. GAN alone and GAN + forward exhibit mode collapse, generating nearly identical outputs regardless of input. Only the full CycleGAN yields both realistic and input-consistent images. Figure adapted from~\blx@tocontentsinit {0}\cite {zhu2017_cyclegan}.}}{907}{figure.caption.1919}\protected@file@percent }
\abx@aux@backref{862}{zhu2017_cyclegan}{0}{907}{907}
\newlabel{fig:chapter20_variants_of_losses_cyclegan}{{20.53}{907}{Ablation study: Visual results of different loss variants for mapping labels $\leftrightarrow $ photos on Cityscapes. From left to right: input, cycle-consistency loss alone, adversarial loss alone, GAN + forward cycle-consistency loss ($F(G(x)) \approx x$), GAN + backward cycle-consistency loss ($G(F(y)) \approx y$), CycleGAN (full method), and ground truth. Cycle alone and GAN + backward fail to produce realistic images. GAN alone and GAN + forward exhibit mode collapse, generating nearly identical outputs regardless of input. Only the full CycleGAN yields both realistic and input-consistent images. Figure adapted from~\cite {zhu2017_cyclegan}}{figure.caption.1919}{}}
\@writefile{toc}{\contentsline {paragraph}{Summary}{907}{section*.1920}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.8.3.7: Summary and Transition to Additional Generative Approaches}{907}{section*.1921}\protected@file@percent }
\newlabel{enr:chapter20_gan_wrapup}{{20.8.3.7}{907}{\color {ocre}Enrichment \thesubsubsection : Summary and Transition to Additional Generative Approaches}{section*.1921}{}}
\abx@aux@backref{863}{park2019_spade}{0}{907}{907}
\abx@aux@backref{864}{gupta2018_socialgan}{0}{907}{907}
\abx@aux@backref{865}{clark2019_videogan}{0}{907}{907}
\abx@aux@cite{0}{song2020_ddim}
\abx@aux@segm{0}{0}{song2020_ddim}
\BKM@entry{id=767,dest={73656374696F6E2A2E31393239},srcline={5163}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030395C3030302E5C303030315C3030303A5C3030305C3034305C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030445C303030695C303030665C303030665C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\abx@aux@cite{0}{sohl2015_diffusion}
\abx@aux@segm{0}{0}{sohl2015_diffusion}
\@writefile{toc}{\contentsline {section}{Enrichment 20.9: Diffusion Models: Modern Generative Modeling}{908}{section*.1922}\protected@file@percent }
\newlabel{enr:chapter20_diffusion_modern}{{20.9}{908}{\color {ocre}Enrichment \thesection : Diffusion Models: Modern Generative Modeling}{section*.1922}{}}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.9.0.1: Motivation: Limitations of Previous Generative Models}{908}{section*.1923}\protected@file@percent }
\newlabel{enr:chapter20_diffusion_motivation}{{20.9.0.1}{908}{\color {ocre}Enrichment \thesubsubsection : Motivation: Limitations of Previous Generative Models}{section*.1923}{}}
\@writefile{toc}{\contentsline {paragraph}{Autoregressive Models (PixelCNN, PixelRNN, ...)}{908}{section*.1924}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Variational Autoencoders (VAEs)}{908}{section*.1925}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Generative Adversarial Networks (GANs)}{908}{section*.1926}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Hybrid Approaches (VQ-VAE, VQ-GAN)}{908}{section*.1927}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The Case for Diffusion Models}{908}{section*.1928}\protected@file@percent }
\abx@aux@backref{866}{song2020_ddim}{0}{908}{908}
\abx@aux@cite{0}{ho2020_ddpm}
\abx@aux@segm{0}{0}{ho2020_ddpm}
\abx@aux@cite{0}{nichol2021_improvedddpm}
\abx@aux@segm{0}{0}{nichol2021_improvedddpm}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.9.1: Introduction to Diffusion Models}{909}{section*.1929}\protected@file@percent }
\newlabel{enr:chapter20_diffusion_intro}{{20.9.1}{909}{\color {ocre}Enrichment \thesubsection : Introduction to Diffusion Models}{section*.1929}{}}
\abx@aux@backref{867}{sohl2015_diffusion}{0}{909}{909}
\@writefile{toc}{\contentsline {paragraph}{Mathematical Foundation and Dual Processes}{909}{section*.1930}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Noise Schedules: How Fast Should the Data Be Destroyed?}{909}{section*.1931}\protected@file@percent }
\abx@aux@backref{868}{ho2020_ddpm}{0}{909}{909}
\abx@aux@backref{869}{nichol2021_improvedddpm}{0}{909}{909}
\@writefile{toc}{\contentsline {paragraph}{Why Is the Process Markovian?}{910}{section*.1932}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Coupled Roles of Signal Attenuation and Noise Injection}{910}{section*.1933}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Diagonal Covariance?}{911}{section*.1934}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Closed-Form Marginals of the Forward Process}{911}{section*.1935}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Many Small Steps?}{912}{section*.1936}\protected@file@percent }
\abx@aux@cite{0}{cvpr2022_diffusion_tutorial}
\abx@aux@segm{0}{0}{cvpr2022_diffusion_tutorial}
\abx@aux@cite{0}{cvpr2022_diffusion_tutorial}
\abx@aux@segm{0}{0}{cvpr2022_diffusion_tutorial}
\@writefile{lof}{\contentsline {figure}{\numberline {20.54}{\ignorespaces Forward diffusion transforms the data distribution into Gaussian noise}}{913}{figure.caption.1937}\protected@file@percent }
\abx@aux@backref{871}{cvpr2022_diffusion_tutorial}{0}{913}{913}
\newlabel{fig:chapter20_diffused_distribution}{{20.54}{913}{Forward diffusion transforms the data distribution into Gaussian noise}{figure.caption.1937}{}}
\@writefile{toc}{\contentsline {paragraph}{Preparing for the Reverse Process}{913}{section*.1938}\protected@file@percent }
\abx@aux@cite{0}{luo2022_diffusiontutorial}
\abx@aux@segm{0}{0}{luo2022_diffusiontutorial}
\abx@aux@cite{0}{luo2022_diffusiontutorial}
\abx@aux@segm{0}{0}{luo2022_diffusiontutorial}
\newlabel{chapter20_enr:reverse_process_diffusion}{{20.9.1}{914}{Preparing for the Reverse Process}{section*.1938}{}}
\@writefile{toc}{\contentsline {paragraph}{A Tractable Alternative: Conditioning on \( \mathbf  {x}_0 \)}{914}{section*.1939}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Visual Intuition}{914}{section*.1940}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.55}{\ignorespaces \textbf  {Visual intuition for diffusion models.} An input image is progressively corrupted with Gaussian noise over multiple steps, ultimately yielding pure noise. The learned denoising process reverses this trajectory, reconstructing diverse and realistic samples. Adapted from~\blx@tocontentsinit {0}\cite {luo2022_diffusiontutorial}.}}{914}{figure.caption.1941}\protected@file@percent }
\abx@aux@backref{873}{luo2022_diffusiontutorial}{0}{914}{914}
\newlabel{fig:chapter20_diffusion_intuition}{{20.55}{914}{\textbf {Visual intuition for diffusion models.} An input image is progressively corrupted with Gaussian noise over multiple steps, ultimately yielding pure noise. The learned denoising process reverses this trajectory, reconstructing diverse and realistic samples. Adapted from~\cite {luo2022_diffusiontutorial}}{figure.caption.1941}{}}
\@writefile{toc}{\contentsline {paragraph}{Step 1: Define the joint distribution}{914}{section*.1942}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 2: Apply Gaussian conditioning}{915}{section*.1943}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 3: Simplifying the Posterior Mean and Variance}{915}{section*.1944}\protected@file@percent }
\newlabel{chapter20_simplified_posterior_ddpm}{{20.9.1}{915}{Step 3: Simplifying the Posterior Mean and Variance}{section*.1944}{}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation:}{916}{section*.1945}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Final Result}{916}{section*.1946}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why This Posterior Is Useful for Training}{916}{section*.1947}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why \( q(\mathbf  {x}_{t-1} \mid \mathbf  {x}_t, \mathbf  {x}_0) \) Is Not Used at Inference}{917}{section*.1948}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Intuition for the Denoising Process}{917}{section*.1949}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Building a Principled Loss Function}{917}{section*.1950}\protected@file@percent }
\abx@aux@cite{0}{ho2020_ddpm}
\abx@aux@segm{0}{0}{ho2020_ddpm}
\abx@aux@backref{874}{ho2020_ddpm}{0}{918}{918}
\BKM@entry{id=768,dest={73656374696F6E2A2E31393531},srcline={5604}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030395C3030302E5C303030325C3030303A5C3030305C3034305C303030445C303030655C3030306E5C3030306F5C303030695C303030735C303030695C3030306E5C303030675C3030305C3034305C303030445C303030695C303030665C303030665C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030505C303030725C3030306F5C303030625C303030615C303030625C303030695C3030306C5C303030695C303030735C303030745C303030695C303030635C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030735C3030305C3034305C3030305C3035305C303030445C303030445C303030505C3030304D5C3030305C303531}
\abx@aux@cite{0}{ho2020_ddpm}
\abx@aux@segm{0}{0}{ho2020_ddpm}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.9.2: Denoising Diffusion Probabilistic Models (DDPM)}{919}{section*.1951}\protected@file@percent }
\newlabel{enr:chapter20_ddpm}{{20.9.2}{919}{\color {ocre}Enrichment \thesubsection : Denoising Diffusion Probabilistic Models (DDPM)}{section*.1951}{}}
\abx@aux@backref{875}{ho2020_ddpm}{0}{919}{919}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.9.2.1: Summary of Core Variables in Diffusion Models}{919}{section*.1952}\protected@file@percent }
\newlabel{enr:chapter20_ddpm_variables}{{20.9.2.1}{919}{\color {ocre}Enrichment \thesubsubsection : Summary of Core Variables in Diffusion Models}{section*.1952}{}}
\@writefile{toc}{\contentsline {paragraph}{Forward Noise Schedule and Signal Retention}{919}{section*.1953}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Reverse Posterior and Posterior Parameters}{920}{section*.1954}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Learned Reverse Mean and Sampling Parameterization}{922}{section*.1955}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.9.2.2: ELBO Formulation and Loss Decomposition}{923}{section*.1956}\protected@file@percent }
\newlabel{enr:chapter20_ddpm_elbo_decomp}{{20.9.2.2}{923}{\color {ocre}Enrichment \thesubsubsection : ELBO Formulation and Loss Decomposition}{section*.1956}{}}
\@writefile{toc}{\contentsline {paragraph}{Maximum Likelihood in Latent-Variable Generative Models}{923}{section*.1957}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Introducing a Tractable Proposal Distribution}{923}{section*.1959}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why the Importance Ratio Is Well-Defined}{924}{section*.1961}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{From Integral to Expectation: Importance Sampling Identity}{924}{section*.1962}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Applying Jensen’s Inequality: A Lower Bound for Optimization}{925}{section*.1963}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Factorization of the Model and Variational Distributions}{925}{section*.1965}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Inserting a Tractable Posterior into the ELBO}{925}{section*.1966}\protected@file@percent }
\abx@aux@cite{0}{ho2020_ddpm}
\abx@aux@segm{0}{0}{ho2020_ddpm}
\@writefile{toc}{\contentsline {paragraph}{Decomposing the Log-Ratio}{926}{section*.1967}\protected@file@percent }
\abx@aux@backref{876}{ho2020_ddpm}{0}{926}{926}
\@writefile{toc}{\contentsline {paragraph}{ELBO in KL-Compatible Form}{927}{section*.1968}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Rewriting as KL Expectations}{927}{section*.1969}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Final KL-Based ELBO for Diffusion Models}{928}{section*.1970}\protected@file@percent }
\newlabel{eq:ddpm_elbo}{{20.1}{928}{Final KL-Based ELBO for Diffusion Models}{equation.20.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Interpreting the ELBO Components}{928}{section*.1971}\protected@file@percent }
\abx@aux@cite{0}{luo2022_diffusiontutorial}
\abx@aux@segm{0}{0}{luo2022_diffusiontutorial}
\abx@aux@cite{0}{luo2022_diffusiontutorial}
\abx@aux@segm{0}{0}{luo2022_diffusiontutorial}
\@writefile{toc}{\contentsline {paragraph}{Why the KL Divergence Is Tractable and Useful for Training}{929}{section*.1972}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.56}{\ignorespaces \textbf  {ELBO Decomposition in DDPMs.} Each step of the reverse process is trained to match the corresponding transition of the analytically known forward chain. The pink arrows represent the tractable posteriors \( q(\mathbf  {x}_{t-1} \mid \mathbf  {x}_t, \mathbf  {x}_0) \); the green arrows represent the learned denoisers \( p_\theta (\mathbf  {x}_{t-1} \mid \mathbf  {x}_t) \). Adapted from~\blx@tocontentsinit {0}\cite {luo2022_diffusiontutorial}.}}{929}{figure.caption.1973}\protected@file@percent }
\abx@aux@backref{878}{luo2022_diffusiontutorial}{0}{929}{929}
\newlabel{fig:chapter20_ddpm_elbo_decomp}{{20.56}{929}{\textbf {ELBO Decomposition in DDPMs.} Each step of the reverse process is trained to match the corresponding transition of the analytically known forward chain. The pink arrows represent the tractable posteriors \( q(\mathbf {x}_{t-1} \mid \mathbf {x}_t, \mathbf {x}_0) \); the green arrows represent the learned denoisers \( p_\theta (\mathbf {x}_{t-1} \mid \mathbf {x}_t) \). Adapted from~\cite {luo2022_diffusiontutorial}}{figure.caption.1973}{}}
\abx@aux@cite{0}{ho2020_ddpm}
\abx@aux@segm{0}{0}{ho2020_ddpm}
\abx@aux@cite{0}{ho2020_ddpm}
\abx@aux@segm{0}{0}{ho2020_ddpm}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.9.2.3: Noise Prediction Objective and Simplification}{930}{section*.1974}\protected@file@percent }
\newlabel{enr:chapter20_ddpm_noise_prediction}{{20.9.2.3}{930}{\color {ocre}Enrichment \thesubsubsection : Noise Prediction Objective and Simplification}{section*.1974}{}}
\@writefile{toc}{\contentsline {paragraph}{From ELBO to Mean Prediction}{930}{section*.1975}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Fixing the Variance}{930}{section*.1976}\protected@file@percent }
\abx@aux@backref{879}{ho2020_ddpm}{0}{930}{930}
\abx@aux@backref{880}{ho2020_ddpm}{0}{930}{930}
\@writefile{toc}{\contentsline {paragraph}{Rewriting the Mean via Noise Prediction}{930}{section*.1977}\protected@file@percent }
\abx@aux@cite{0}{song2021_sde}
\abx@aux@segm{0}{0}{song2021_sde}
\abx@aux@cite{0}{nichol2021_improvedddpm}
\abx@aux@segm{0}{0}{nichol2021_improvedddpm}
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\abx@aux@backref{881}{song2021_sde}{0}{932}{932}
\abx@aux@backref{882}{nichol2021_improvedddpm}{0}{932}{932}
\abx@aux@backref{883}{saharia2022_imagen}{0}{932}{932}
\abx@aux@backref{884}{rombach2022_ldm}{0}{932}{932}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.9.2.4: Training and Inference in DDPMs}{933}{section*.1978}\protected@file@percent }
\newlabel{enr:ddpm_train_sample}{{20.9.2.4}{933}{\color {ocre}Enrichment \thesubsubsection : Training and Inference in DDPMs}{section*.1978}{}}
\@writefile{toc}{\contentsline {paragraph}{Connection to the Model Distribution \boldmath \( p_\theta (x_{t-1} \mid x_t) \).}{934}{section*.1979}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Interpreting the Update.}{934}{section*.1980}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Stochasticity and Sample Diversity.}{934}{section*.1981}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Final Step Refinement.}{934}{section*.1982}\protected@file@percent }
\abx@aux@cite{0}{ronneberger2015_unet}
\abx@aux@segm{0}{0}{ronneberger2015_unet}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.9.2.5: Architecture, Datasets, and Implementation Details}{935}{section*.1983}\protected@file@percent }
\newlabel{enr:chapter20_ddpm_architecture}{{20.9.2.5}{935}{\color {ocre}Enrichment \thesubsubsection : Architecture, Datasets, and Implementation Details}{section*.1983}{}}
\@writefile{toc}{\contentsline {paragraph}{Backbone Architecture: Why U-Net Fits Denoising in Diffusion Models}{935}{section*.1984}\protected@file@percent }
\newlabel{par:chapter20_ddpm_unet_architecture}{{20.9.2.5}{935}{Backbone Architecture: Why U-Net Fits Denoising in Diffusion Models}{section*.1984}{}}
\abx@aux@backref{885}{ronneberger2015_unet}{0}{935}{935}
\@writefile{toc}{\contentsline {subparagraph}{Why an Encoder–Decoder?}{935}{subparagraph*.1985}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Multiscale Hierarchy and Architectural Intuition}{935}{subparagraph*.1986}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Walkthrough: Layer-by-Layer Data Flow}{936}{subparagraph*.1987}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Why U-Net Matches the Diffusion Objective}{936}{subparagraph*.1988}\protected@file@percent }
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\@writefile{toc}{\contentsline {paragraph}{Resolution and Depth Scaling}{937}{section*.1989}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Time Embedding via Sinusoidal Positional Encoding}{937}{section*.1990}\protected@file@percent }
\abx@aux@backref{886}{vaswani2017_attention}{0}{937}{937}
\@writefile{toc}{\contentsline {paragraph}{How the Time Embedding is Used}{937}{section*.1991}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Not Simpler Alternatives?}{937}{section*.1992}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Model Scale and Dataset Diversity}{939}{section*.1993}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{939}{section*.1994}\protected@file@percent }
\abx@aux@cite{0}{ho2020_ddpm}
\abx@aux@segm{0}{0}{ho2020_ddpm}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.9.2.6: Empirical Evaluation and Latent-Space Behavior}{940}{section*.1995}\protected@file@percent }
\newlabel{enr:ddpm_experiments}{{20.9.2.6}{940}{\color {ocre}Enrichment \thesubsubsection : Empirical Evaluation and Latent-Space Behavior}{section*.1995}{}}
\@writefile{toc}{\contentsline {paragraph}{Noise Prediction Yields Stable Training and Best Sample Quality}{940}{section*.1996}\protected@file@percent }
\abx@aux@backref{887}{ho2020_ddpm}{0}{940}{940}
\@writefile{toc}{\contentsline {paragraph}{Image Interpolation in Latent Space}{940}{section*.1997}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.57}{\ignorespaces Interpolation between two CelebA-HQ images \( x_0 \) and \( x_0' \) using latent space diffusion embeddings.}}{940}{figure.caption.1998}\protected@file@percent }
\newlabel{fig:chapter20_ddpm_latent_interp}{{20.57}{940}{Interpolation between two CelebA-HQ images \( x_0 \) and \( x_0' \) using latent space diffusion embeddings}{figure.caption.1998}{}}
\@writefile{toc}{\contentsline {paragraph}{Coarse-to-Fine Interpolation and Structural Completion}{941}{section*.1999}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.58}{\ignorespaces Interpolations between two CelebA-HQ images performed after different numbers of forward diffusion steps. Small \( t \) preserves structure; large \( t \) results in novel completions.}}{941}{figure.caption.2000}\protected@file@percent }
\newlabel{fig:chapter_20_ddpm_coarse_fine_interp}{{20.58}{941}{Interpolations between two CelebA-HQ images performed after different numbers of forward diffusion steps. Small \( t \) preserves structure; large \( t \) results in novel completions}{figure.caption.2000}{}}
\@writefile{toc}{\contentsline {paragraph}{Progressive Lossy Compression via Reverse Denoising}{942}{section*.2001}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.59}{\ignorespaces Samples \( x_0 \sim p_\theta (x_0 \mid x_t) \) from the same \( x_t \), with varying \( t \). As \( t \) decreases, more high-frequency detail is recovered.}}{942}{figure.caption.2002}\protected@file@percent }
\newlabel{fig:chapter20_ddpm_feature_recovery}{{20.59}{942}{Samples \( x_0 \sim p_\theta (x_0 \mid x_t) \) from the same \( x_t \), with varying \( t \). As \( t \) decreases, more high-frequency detail is recovered}{figure.caption.2002}{}}
\BKM@entry{id=769,dest={73656374696F6E2A2E32303033},srcline={6727}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030395C3030302E5C303030335C3030303A5C3030305C3034305C303030445C303030655C3030306E5C3030306F5C303030695C303030735C303030695C3030306E5C303030675C3030305C3034305C303030445C303030695C303030665C303030665C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030495C3030306D5C303030705C3030306C5C303030695C303030635C303030695C303030745C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030735C3030305C3034305C3030305C3035305C303030445C303030445C303030495C3030304D5C3030305C303531}
\abx@aux@cite{0}{song2020_ddim}
\abx@aux@segm{0}{0}{song2020_ddim}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.9.3: Denoising Diffusion Implicit Models (DDIM)}{943}{section*.2003}\protected@file@percent }
\newlabel{enr:chapter20_ddim}{{20.9.3}{943}{\color {ocre}Enrichment \thesubsection : Denoising Diffusion Implicit Models (DDIM)}{section*.2003}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation}{943}{section*.2004}\protected@file@percent }
\abx@aux@backref{888}{song2020_ddim}{0}{943}{943}
\@writefile{toc}{\contentsline {paragraph}{From DDPM Sampling to DDIM Inversion}{943}{section*.2005}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{1. From Forward Diffusion to Inversion}{943}{section*.2006}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{2. Reverse Step to Arbitrary \( s < t \)}{944}{section*.2007}\protected@file@percent }
\abx@aux@cite{0}{song2020_ddim}
\abx@aux@segm{0}{0}{song2020_ddim}
\abx@aux@cite{0}{song2020_ddim}
\abx@aux@segm{0}{0}{song2020_ddim}
\@writefile{lof}{\contentsline {figure}{\numberline {20.60}{\ignorespaces Comparison of reverse trajectories. DDIM reduces the number of steps by using a deterministic mapping with shared noise.}}{945}{figure.caption.2008}\protected@file@percent }
\newlabel{fig:ddim_vs_ddpm_trajectory}{{20.60}{945}{Comparison of reverse trajectories. DDIM reduces the number of steps by using a deterministic mapping with shared noise}{figure.caption.2008}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.61}{\ignorespaces  \textbf  {Graphical comparison of DDPM and DDIM inference models.} \emph  {Top:} In DDPM, the generative process is a Markov chain: each reverse step \( x_{t-1} \) depends only on the previous \( x_t \). \emph  {Bottom:} DDIM defines a non-Markovian process, where each \( x_s \) can be computed directly from \( x_t \) using the predicted noise \( \varepsilon _\theta (x_t, t) \), enabling accelerated, deterministic inference. \vspace  {0.3em}   \textit  {Adapted from~\blx@tocontentsinit {0}\cite {song2020_ddim}.} }}{945}{figure.caption.2009}\protected@file@percent }
\abx@aux@backref{890}{song2020_ddim}{0}{945}{945}
\newlabel{fig:chapter20_ddpm_vs_ddim}{{20.61}{945}{\textbf {Graphical comparison of DDPM and DDIM inference models.} \emph {Top:} In DDPM, the generative process is a Markov chain: each reverse step \( x_{t-1} \) depends only on the previous \( x_t \). \emph {Bottom:} DDIM defines a non-Markovian process, where each \( x_s \) can be computed directly from \( x_t \) using the predicted noise \( \varepsilon _\theta (x_t, t) \), enabling accelerated, deterministic inference. \vspace {0.3em} \\ \textit {Adapted from~\cite {song2020_ddim}.}}{figure.caption.2009}{}}
\@writefile{toc}{\contentsline {paragraph}{3.\ Why the “single–noise” picture is still correct}{946}{section*.2010}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{4. Optional Stochastic Extension}{947}{section*.2011}\protected@file@percent }
\abx@aux@cite{0}{song2020_ddim}
\abx@aux@segm{0}{0}{song2020_ddim}
\abx@aux@cite{0}{song2020_ddim}
\abx@aux@segm{0}{0}{song2020_ddim}
\abx@aux@backref{891}{song2020_ddim}{0}{948}{948}
\@writefile{toc}{\contentsline {paragraph}{5. Advantages of DDIM Sampling}{948}{section*.2012}\protected@file@percent }
\abx@aux@backref{892}{song2020_ddim}{0}{948}{948}
\BKM@entry{id=770,dest={73656374696F6E2A2E32303133},srcline={7029}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030395C3030302E5C303030345C3030303A5C3030305C3034305C303030475C303030755C303030695C303030645C303030615C3030306E5C303030635C303030655C3030305C3034305C303030545C303030655C303030635C303030685C3030306E5C303030695C303030715C303030755C303030655C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030445C303030695C303030665C303030665C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\abx@aux@cite{0}{dhariwal2021_beats}
\abx@aux@segm{0}{0}{dhariwal2021_beats}
\abx@aux@cite{0}{dhariwal2021_beats}
\abx@aux@segm{0}{0}{dhariwal2021_beats}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.9.4: Guidance Techniques in Diffusion Models}{949}{section*.2013}\protected@file@percent }
\newlabel{enr:chapter20_diffusion_guidance}{{20.9.4}{949}{\color {ocre}Enrichment \thesubsection : Guidance Techniques in Diffusion Models}{section*.2013}{}}
\abx@aux@backref{893}{dhariwal2021_beats}{0}{949}{949}
\abx@aux@backref{894}{dhariwal2021_beats}{0}{949}{949}
\abx@aux@cite{0}{ho2022_classifierfree}
\abx@aux@segm{0}{0}{ho2022_classifierfree}
\abx@aux@cite{0}{perez2017_film}
\abx@aux@segm{0}{0}{perez2017_film}
\newlabel{subsubsec:chapter20_classifier_free_guidance}{{20.9.4}{951}{Classifier-Free Guidance}{section*.2015}{}}
\abx@aux@backref{895}{ho2022_classifierfree}{0}{951}{951}
\@writefile{toc}{\contentsline {paragraph}{Training Procedure}{951}{section*.2016}\protected@file@percent }
\abx@aux@backref{896}{perez2017_film}{0}{951}{951}
\@writefile{toc}{\contentsline {paragraph}{Sampling with Classifier-Free Guidance}{952}{section*.2017}\protected@file@percent }
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\abx@aux@backref{897}{rombach2022_ldm}{0}{953}{953}
\abx@aux@backref{898}{radford2021_clip}{0}{953}{953}
\@writefile{toc}{\contentsline {paragraph}{Why Classifier-Free Guidance Works: A Score-Based and Intuitive View}{953}{section*.2018}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Interpretation}{954}{section*.2019}\protected@file@percent }
\abx@aux@cite{0}{zhihu2023_classifierfreeguidance}
\abx@aux@segm{0}{0}{zhihu2023_classifierfreeguidance}
\abx@aux@cite{0}{zhihu2023_classifierfreeguidance}
\abx@aux@segm{0}{0}{zhihu2023_classifierfreeguidance}
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\abx@aux@cite{0}{ramesh2022_dalle2}
\abx@aux@segm{0}{0}{ramesh2022_dalle2}
\@writefile{toc}{\contentsline {paragraph}{Typical Settings}{955}{section*.2020}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.62}{\ignorespaces  \textbf  {Effect of Guidance Scale in Classifier-Free Guidance (Stable Diffusion v1.5).} Each column shows images generated from the same prompt using different guidance scales. As the scale increases from left to right (values: $-15 \sim 1$, $3$, $7.5$, $10$, $15$, $30$), the outputs transition from weakly conditioned or incoherent samples to more strongly aligned and vivid ones. However, overly high values (e.g., $30$) may introduce distortions or oversaturation. Guidance scales $7.5/10$ typically produce the most realistic and semantically faithful results. \textit  {Adapted from~\blx@tocontentsinit {0}\cite {zhihu2023_classifierfreeguidance}.} }}{955}{figure.caption.2021}\protected@file@percent }
\abx@aux@backref{900}{zhihu2023_classifierfreeguidance}{0}{955}{955}
\newlabel{fig:chapter20_guidance_scale}{{20.62}{955}{\textbf {Effect of Guidance Scale in Classifier-Free Guidance (Stable Diffusion v1.5).} Each column shows images generated from the same prompt using different guidance scales. As the scale increases from left to right (values: $-15 \sim 1$, $3$, $7.5$, $10$, $15$, $30$), the outputs transition from weakly conditioned or incoherent samples to more strongly aligned and vivid ones. However, overly high values (e.g., $30$) may introduce distortions or oversaturation. Guidance scales $7.5/10$ typically produce the most realistic and semantically faithful results. \textit {Adapted from~\cite {zhihu2023_classifierfreeguidance}.}}{figure.caption.2021}{}}
\@writefile{toc}{\contentsline {paragraph}{Advantages}{955}{section*.2022}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Adoption in Large-Scale Models}{955}{section*.2023}\protected@file@percent }
\abx@aux@backref{901}{saharia2022_imagen}{0}{955}{955}
\abx@aux@backref{902}{rombach2022_ldm}{0}{955}{955}
\abx@aux@backref{903}{ramesh2022_dalle2}{0}{955}{955}
\BKM@entry{id=771,dest={73656374696F6E2A2E32303234},srcline={7370}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030395C3030302E5C303030355C3030303A5C3030305C3034305C303030435C303030615C303030735C303030635C303030615C303030645C303030655C303030645C3030305C3034305C303030445C303030695C303030665C303030665C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\abx@aux@cite{0}{ho2021_cascaded}
\abx@aux@segm{0}{0}{ho2021_cascaded}
\abx@aux@cite{0}{ho2021_cascaded}
\abx@aux@segm{0}{0}{ho2021_cascaded}
\abx@aux@cite{0}{ho2021_cascaded}
\abx@aux@segm{0}{0}{ho2021_cascaded}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.9.5: Cascaded Diffusion Models}{956}{section*.2024}\protected@file@percent }
\newlabel{enr:chapter20_cascaded_diffusion}{{20.9.5}{956}{\color {ocre}Enrichment \thesubsection : Cascaded Diffusion Models}{section*.2024}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation and Overview}{956}{section*.2025}\protected@file@percent }
\abx@aux@backref{904}{ho2021_cascaded}{0}{956}{956}
\@writefile{lof}{\contentsline {figure}{\numberline {20.63}{\ignorespaces  \textbf  {Overview of a Cascaded Diffusion Pipeline.} The first model generates a low-resolution sample from noise (left). Subsequent models condition on this sample (upsampled) to generate higher-resolution versions. At each stage, the model receives \( x_t \) (the noisy image), the class label \( y \), and a low-resolution guidance image. This modular design enables each model to specialize at a given scale. Figure adapted from~\blx@tocontentsinit {0}\cite {ho2021_cascaded}. }}{956}{figure.caption.2026}\protected@file@percent }
\abx@aux@backref{906}{ho2021_cascaded}{0}{956}{956}
\newlabel{fig:chapter20_cascaded_diffusion_example}{{20.63}{956}{\textbf {Overview of a Cascaded Diffusion Pipeline.} The first model generates a low-resolution sample from noise (left). Subsequent models condition on this sample (upsampled) to generate higher-resolution versions. At each stage, the model receives \( x_t \) (the noisy image), the class label \( y \), and a low-resolution guidance image. This modular design enables each model to specialize at a given scale. Figure adapted from~\cite {ho2021_cascaded}}{figure.caption.2026}{}}
\abx@aux@cite{0}{ho2021_cascaded}
\abx@aux@segm{0}{0}{ho2021_cascaded}
\abx@aux@cite{0}{ho2021_cascaded}
\abx@aux@segm{0}{0}{ho2021_cascaded}
\@writefile{toc}{\contentsline {paragraph}{Architecture: U-Net Design for Cascaded Diffusion Models}{957}{section*.2027}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.64}{\ignorespaces  \textbf  {U-Net architecture used in CDMs.} The first model receives a noisy image \( x_t \sim q(x_t \mid x_0) \) and class label \( y \). Subsequent models (super-resolution stages) additionally take a lower-resolution guide image \( z \), which is the upsampled output of the previous stage. All inputs are processed through downsampling and upsampling blocks with skip connections. Timestep \( t \) and label \( y \) are embedded and injected into each block (not shown). Figure adapted from~\blx@tocontentsinit {0}\cite {ho2021_cascaded}. }}{957}{figure.caption.2028}\protected@file@percent }
\abx@aux@backref{908}{ho2021_cascaded}{0}{957}{957}
\newlabel{fig:chapter20_unet_architecture}{{20.64}{957}{\textbf {U-Net architecture used in CDMs.} The first model receives a noisy image \( x_t \sim q(x_t \mid x_0) \) and class label \( y \). Subsequent models (super-resolution stages) additionally take a lower-resolution guide image \( z \), which is the upsampled output of the previous stage. All inputs are processed through downsampling and upsampling blocks with skip connections. Timestep \( t \) and label \( y \) are embedded and injected into each block (not shown). Figure adapted from~\cite {ho2021_cascaded}}{figure.caption.2028}{}}
\abx@aux@cite{0}{ho2021_cascaded}
\abx@aux@segm{0}{0}{ho2021_cascaded}
\@writefile{toc}{\contentsline {paragraph}{Empirical Performance of CDMs}{959}{section*.2029}\protected@file@percent }
\abx@aux@backref{909}{ho2021_cascaded}{0}{959}{959}
\BKM@entry{id=772,dest={73656374696F6E2A2E32303330},srcline={7532}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030395C3030302E5C303030365C3030303A5C3030305C3034305C303030505C303030725C3030306F5C303030675C303030725C303030655C303030735C303030735C303030695C303030765C303030655C3030305C3034305C303030445C303030695C303030735C303030745C303030695C3030306C5C3030306C5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030465C303030615C303030735C303030745C3030305C3034305C303030535C303030615C3030306D5C303030705C3030306C5C303030695C3030306E5C30303067}
\abx@aux@cite{0}{salimans2022_progressive}
\abx@aux@segm{0}{0}{salimans2022_progressive}
\abx@aux@cite{0}{salimans2022_progressive}
\abx@aux@segm{0}{0}{salimans2022_progressive}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.9.6: Progressive Distillation for Fast Sampling}{960}{section*.2030}\protected@file@percent }
\newlabel{enr:chapter20_progressive_distillation}{{20.9.6}{960}{\color {ocre}Enrichment \thesubsection : Progressive Distillation for Fast Sampling}{section*.2030}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation}{960}{section*.2031}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.65}{\ignorespaces  \textbf  {Progressive Distillation Process.} Each iteration compresses the original sampling schedule into fewer steps. A 4-step DDIM sampler \( f(z; \eta ) \) is distilled into a 1-step student \( f(z; \theta ) \) that mimics its behavior. Distillation can be viewed as amortizing ODE integration across fewer steps. Figure adapted from~\blx@tocontentsinit {0}\cite {salimans2022_progressive}. }}{960}{figure.caption.2032}\protected@file@percent }
\abx@aux@backref{911}{salimans2022_progressive}{0}{960}{960}
\newlabel{fig:chapter20_progressive_distillation_scheme}{{20.65}{960}{\textbf {Progressive Distillation Process.} Each iteration compresses the original sampling schedule into fewer steps. A 4-step DDIM sampler \( f(z; \eta ) \) is distilled into a 1-step student \( f(z; \theta ) \) that mimics its behavior. Distillation can be viewed as amortizing ODE integration across fewer steps. Figure adapted from~\cite {salimans2022_progressive}}{figure.caption.2032}{}}
\@writefile{toc}{\contentsline {paragraph}{Pseudocode: Progressive Distillation Loop}{961}{section*.2033}\protected@file@percent }
\abx@aux@cite{0}{song2020_ddim}
\abx@aux@segm{0}{0}{song2020_ddim}
\abx@aux@cite{0}{nichol2021_improvedddpm}
\abx@aux@segm{0}{0}{nichol2021_improvedddpm}
\@writefile{toc}{\contentsline {paragraph}{Prerequisites Required to Understand The Progressive Distillation Loop}{962}{section*.2034}\protected@file@percent }
\abx@aux@backref{912}{song2020_ddim}{0}{962}{962}
\abx@aux@backref{913}{nichol2021_improvedddpm}{0}{962}{962}
\abx@aux@cite{0}{salimans2022_progressive}
\abx@aux@segm{0}{0}{salimans2022_progressive}
\abx@aux@cite{0}{song2020_ddim}
\abx@aux@segm{0}{0}{song2020_ddim}
\@writefile{toc}{\contentsline {paragraph}{What Is SNR and Why Use It?}{963}{section*.2035}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Cosine Schedule and Angular Construction}{963}{section*.2036}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Teacher Trajectory Construction via Two DDIM Steps}{963}{section*.2037}\protected@file@percent }
\abx@aux@backref{914}{salimans2022_progressive}{0}{963}{963}
\abx@aux@backref{915}{song2020_ddim}{0}{963}{963}
\abx@aux@cite{0}{ho2020_ddpm}
\abx@aux@segm{0}{0}{ho2020_ddpm}
\abx@aux@backref{916}{ho2020_ddpm}{0}{965}{965}
\@writefile{toc}{\contentsline {paragraph}{Empirical Results and Sample Quality}{967}{section*.2038}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.66}{\ignorespaces  \textbf  {Sample quality vs. number of steps for distilled vs. baseline samplers.} Shown are FID scores across 4 benchmark settings: unconditional CIFAR-10, class-conditional ImageNet \( 64 \times 64 \), LSUN Bedrooms \( 128 \times 128 \), and LSUN Churches \( 128 \times 128 \). Distilled samplers match or outperform DDIM and stochastic samplers with far fewer steps. }}{967}{figure.caption.2039}\protected@file@percent }
\newlabel{fig:chapter20_sampling_fid_comparison}{{20.66}{967}{\textbf {Sample quality vs. number of steps for distilled vs. baseline samplers.} Shown are FID scores across 4 benchmark settings: unconditional CIFAR-10, class-conditional ImageNet \( 64 \times 64 \), LSUN Bedrooms \( 128 \times 128 \), and LSUN Churches \( 128 \times 128 \). Distilled samplers match or outperform DDIM and stochastic samplers with far fewer steps}{figure.caption.2039}{}}
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{968}{section*.2040}\protected@file@percent }
\BKM@entry{id=773,dest={73656374696F6E2A2E32303431},srcline={7948}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030395C3030302E5C303030375C3030303A5C3030305C3034305C303030565C303030655C3030306C5C3030306F5C303030635C303030695C303030745C303030795C3030302D5C303030535C303030705C303030615C303030635C303030655C3030305C3034305C303030535C303030615C3030306D5C303030705C3030306C5C303030695C3030306E5C303030675C3030303A5C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030445C303030655C3030306E5C3030306F5C303030695C303030735C303030695C3030306E5C303030675C3030305C3034305C303030545C303030725C303030615C3030306A5C303030655C303030635C303030745C3030306F5C303030725C303030695C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.9.7: Velocity-Space Sampling: Learning Denoising Trajectories}{969}{section*.2041}\protected@file@percent }
\newlabel{enr:chapter20_velocity_space}{{20.9.7}{969}{\color {ocre}Enrichment \thesubsection : Velocity-Space Sampling: Learning Denoising Trajectories}{section*.2041}{}}
\BKM@entry{id=774,dest={73656374696F6E2A2E32303432},srcline={8014}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030305C3030303A5C3030305C3034305C303030465C3030306C5C3030306F5C303030775C3030305C3034305C3030304D5C303030615C303030745C303030635C303030685C303030695C3030306E5C303030675C3030303A5C3030305C3034305C303030425C303030655C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030445C303030695C303030665C303030665C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030555C303030735C303030695C3030306E5C303030675C3030305C3034305C303030465C3030306C5C3030306F5C303030775C30303073}
\abx@aux@cite{0}{perko2013_differential}
\abx@aux@segm{0}{0}{perko2013_differential}
\@writefile{toc}{\contentsline {section}{Enrichment 20.10: Flow Matching: Beating Diffusion Using Flows}{971}{section*.2042}\protected@file@percent }
\newlabel{enr:chapter20_flow_matching}{{20.10}{971}{\color {ocre}Enrichment \thesection : Flow Matching: Beating Diffusion Using Flows}{section*.2042}{}}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\abx@aux@cite{0}{lipman2024_flowmatchingguidecode}
\abx@aux@segm{0}{0}{lipman2024_flowmatchingguidecode}
\abx@aux@cite{0}{kilcher2022_flowmatchingyt}
\abx@aux@segm{0}{0}{kilcher2022_flowmatchingyt}
\abx@aux@cite{0}{vantai2022_flowmatchingyt}
\abx@aux@segm{0}{0}{vantai2022_flowmatchingyt}
\abx@aux@cite{0}{gat2024_discreteflowmatching}
\abx@aux@segm{0}{0}{gat2024_discreteflowmatching}
\abx@aux@cite{0}{chen2023_riemannianfm}
\abx@aux@segm{0}{0}{chen2023_riemannianfm}
\abx@aux@cite{0}{holderrieth2024_gm}
\abx@aux@segm{0}{0}{holderrieth2024_gm}
\abx@aux@backref{917}{perko2013_differential}{0}{972}{972}
\abx@aux@backref{918}{lipman2022_flowmatching}{0}{972}{972}
\BKM@entry{id=775,dest={73656374696F6E2A2E32303434},srcline={8104}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030305C3030302E5C303030315C3030303A5C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C303030765C303030655C3030305C3034305C303030465C3030306C5C3030306F5C303030775C303030735C3030303A5C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030625C303030795C3030305C3034305C303030545C303030725C303030615C3030306A5C303030655C303030635C303030745C3030306F5C303030725C303030795C3030305C3034305C303030495C3030306E5C303030745C303030655C303030675C303030725C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{vantai2023_trainingflows}
\abx@aux@segm{0}{0}{vantai2023_trainingflows}
\abx@aux@cite{0}{vantai2023_trainingflows}
\abx@aux@segm{0}{0}{vantai2023_trainingflows}
\@writefile{toc}{\contentsline {paragraph}{Further Reading}{973}{section*.2043}\protected@file@percent }
\abx@aux@backref{919}{lipman2022_flowmatching}{0}{973}{973}
\abx@aux@backref{920}{lipman2024_flowmatchingguidecode}{0}{973}{973}
\abx@aux@backref{921}{kilcher2022_flowmatchingyt}{0}{973}{973}
\abx@aux@backref{922}{vantai2022_flowmatchingyt}{0}{973}{973}
\abx@aux@backref{923}{gat2024_discreteflowmatching}{0}{973}{973}
\abx@aux@backref{924}{chen2023_riemannianfm}{0}{973}{973}
\abx@aux@backref{925}{holderrieth2024_gm}{0}{973}{973}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.10.1: Generative Flows: Learning by Trajectory Integration}{973}{section*.2044}\protected@file@percent }
\newlabel{enr:chapter20_flow_trajectory_integration}{{20.10.1}{973}{\color {ocre}Enrichment \thesubsection : Generative Flows: Learning by Trajectory Integration}{section*.2044}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation: From Mapping to Likelihood.}{973}{section*.2045}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{From KL to Log-Likelihood}{973}{section*.2046}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How Does \( p_1 \) Arise from a Flow?}{973}{section*.2047}\protected@file@percent }
\abx@aux@cite{0}{rudin1976_real}
\abx@aux@segm{0}{0}{rudin1976_real}
\@writefile{lof}{\contentsline {figure}{\numberline {20.67}{\ignorespaces \textbf  {Training Generative Flows to Match Data Distributions.} Generative flow models define a transformation \( \psi _t \) that maps samples from a tractable base distribution \( p_0 \) (e.g., standard Gaussian) to a more complex target distribution \( p_1 \), with \( x_1 = \psi _1(x_0) \). The goal is to learn \( \psi _1 \) such that the model distribution \( p_1 \) matches the data distribution \( q \). During training, we observe data samples \( x_1 \sim q(x) \), invert the flow to recover latent variables \( x_0 = \psi _1^{-1}(x_1) \), and evaluate likelihoods using the change-of-variables formula. This general framework enables exact maximum likelihood estimation for flows that are smooth, invertible, and volume-tracking. Later, we extend this idea by modeling \( \psi _t \) as the solution to an ODE parameterized by a velocity field \( v_t(x) \), leading to continuous normalizing flows (CNFs) and flow matching. Adapted from~\blx@tocontentsinit {0}\cite {vantai2023_trainingflows}.}}{974}{figure.caption.2048}\protected@file@percent }
\abx@aux@backref{927}{vantai2023_trainingflows}{0}{974}{974}
\newlabel{fig:chapter20_training_flows}{{20.67}{974}{\textbf {Training Generative Flows to Match Data Distributions.} Generative flow models define a transformation \( \psi _t \) that maps samples from a tractable base distribution \( p_0 \) (e.g., standard Gaussian) to a more complex target distribution \( p_1 \), with \( x_1 = \psi _1(x_0) \). The goal is to learn \( \psi _1 \) such that the model distribution \( p_1 \) matches the data distribution \( q \). During training, we observe data samples \( x_1 \sim q(x) \), invert the flow to recover latent variables \( x_0 = \psi _1^{-1}(x_1) \), and evaluate likelihoods using the change-of-variables formula. This general framework enables exact maximum likelihood estimation for flows that are smooth, invertible, and volume-tracking. Later, we extend this idea by modeling \( \psi _t \) as the solution to an ODE parameterized by a velocity field \( v_t(x) \), leading to continuous normalizing flows (CNFs) and flow matching. Adapted from~\cite {vantai2023_trainingflows}}{figure.caption.2048}{}}
\abx@aux@backref{928}{rudin1976_real}{0}{974}{974}
\abx@aux@cite{0}{dinh2017_realnvp}
\abx@aux@segm{0}{0}{dinh2017_realnvp}
\abx@aux@cite{0}{kingma2018_glow}
\abx@aux@segm{0}{0}{kingma2018_glow}
\abx@aux@cite{0}{grathwohl2019_ffjord}
\abx@aux@segm{0}{0}{grathwohl2019_ffjord}
\abx@aux@cite{0}{chen2019_neuralode}
\abx@aux@segm{0}{0}{chen2019_neuralode}
\abx@aux@cite{0}{grathwohl2019_ffjord}
\abx@aux@segm{0}{0}{grathwohl2019_ffjord}
\abx@aux@cite{0}{song2021_sde}
\abx@aux@segm{0}{0}{song2021_sde}
\abx@aux@backref{929}{dinh2017_realnvp}{0}{975}{975}
\abx@aux@backref{930}{kingma2018_glow}{0}{975}{975}
\abx@aux@backref{931}{grathwohl2019_ffjord}{0}{975}{975}
\abx@aux@backref{932}{chen2019_neuralode}{0}{975}{975}
\abx@aux@backref{933}{grathwohl2019_ffjord}{0}{975}{975}
\abx@aux@backref{934}{song2021_sde}{0}{975}{975}
\@writefile{toc}{\contentsline {paragraph}{The Role of the Continuity Equation}{975}{section*.2049}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Flux: Constructing \( p_t(x) v_t(x) \)}{975}{section*.2050}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Divergence: Understanding \( \nabla \cdot (p_t v_t) \)}{976}{section*.2051}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Putting the Continuity Equation in Plain English}{976}{section*.2052}\protected@file@percent }
\abx@aux@cite{0}{chen2019_neuralode}
\abx@aux@segm{0}{0}{chen2019_neuralode}
\abx@aux@cite{0}{grathwohl2019_ffjord}
\abx@aux@segm{0}{0}{grathwohl2019_ffjord}
\abx@aux@cite{0}{song2021_sde}
\abx@aux@segm{0}{0}{song2021_sde}
\@writefile{toc}{\contentsline {paragraph}{Broader Implications for Continuous-Time Generative Models}{977}{section*.2053}\protected@file@percent }
\abx@aux@backref{935}{chen2019_neuralode}{0}{977}{977}
\abx@aux@backref{936}{grathwohl2019_ffjord}{0}{977}{977}
\abx@aux@backref{937}{song2021_sde}{0}{977}{977}
\@writefile{toc}{\contentsline {paragraph}{Interpretation}{978}{section*.2055}\protected@file@percent }
\abx@aux@cite{0}{grathwohl2019_ffjord}
\abx@aux@segm{0}{0}{grathwohl2019_ffjord}
\abx@aux@cite{0}{chen2019_neuralode}
\abx@aux@segm{0}{0}{chen2019_neuralode}
\@writefile{toc}{\contentsline {paragraph}{Why Pure CNF–Likelihood Training Is Not Scalable?}{979}{section*.2056}\protected@file@percent }
\abx@aux@backref{938}{grathwohl2019_ffjord}{0}{979}{979}
\abx@aux@backref{939}{chen2019_neuralode}{0}{979}{979}
\abx@aux@cite{0}{grathwohl2019_ffjord}
\abx@aux@segm{0}{0}{grathwohl2019_ffjord}
\abx@aux@cite{0}{song2021_sde}
\abx@aux@segm{0}{0}{song2021_sde}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\abx@aux@backref{940}{grathwohl2019_ffjord}{0}{980}{980}
\abx@aux@backref{941}{song2021_sde}{0}{980}{980}
\@writefile{toc}{\contentsline {paragraph}{Flow Matching: A New Approach}{980}{section*.2057}\protected@file@percent }
\abx@aux@backref{942}{lipman2022_flowmatching}{0}{980}{980}
\BKM@entry{id=776,dest={73656374696F6E2A2E32303538},srcline={8495}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030305C3030302E5C303030325C3030303A5C3030305C3034305C303030445C303030655C303030765C303030655C3030306C5C3030306F5C303030705C3030306D5C303030655C3030306E5C303030745C3030305C3034305C3030306F5C303030665C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030465C3030306C5C3030306F5C303030775C3030305C3034305C3030304D5C303030615C303030745C303030635C303030685C303030695C3030306E5C303030675C3030305C3034305C3030304F5C303030625C3030306A5C303030655C303030635C303030745C303030695C303030765C30303065}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.10.2: Development of the Flow Matching Objective}{981}{section*.2058}\protected@file@percent }
\newlabel{enr:chapter20_flow_matching_objective}{{20.10.2}{981}{\color {ocre}Enrichment \thesubsection : Development of the Flow Matching Objective}{section*.2058}{}}
\@writefile{toc}{\contentsline {paragraph}{From Density Path to Vector Field}{981}{section*.2059}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The Naive Flow Matching Objective}{981}{section*.2060}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why the Naive Objective Is Intractable}{982}{section*.2062}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{A Local Solution via Conditional Paths}{982}{section*.2063}\protected@file@percent }
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\@writefile{toc}{\contentsline {paragraph}{Recovering the Marginal Vector Field}{983}{section*.2064}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why This Identity Is Valid}{983}{section*.2065}\protected@file@percent }
\abx@aux@backref{943}{lipman2022_flowmatching}{0}{983}{983}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\abx@aux@backref{944}{lipman2022_flowmatching}{0}{984}{984}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\@writefile{toc}{\contentsline {paragraph}{From Validity to Practicality: The Need for a Tractable Objective}{985}{section*.2066}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Conditional Flow Matching (CFM): A Sample-Based Reformulation}{985}{section*.2067}\protected@file@percent }
\abx@aux@backref{945}{lipman2022_flowmatching}{0}{985}{985}
\BKM@entry{id=777,dest={73656374696F6E2A2E32303639},srcline={8769}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030305C3030302E5C303030335C3030303A5C3030305C3034305C303030435C3030306F5C3030306E5C303030645C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030505C303030725C3030306F5C303030625C303030615C303030625C303030695C3030306C5C303030695C303030745C303030795C3030305C3034305C303030505C303030615C303030745C303030685C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030565C303030655C303030635C303030745C3030306F5C303030725C3030305C3034305C303030465C303030695C303030655C3030306C5C303030645C30303073}
\abx@aux@backref{946}{lipman2022_flowmatching}{0}{986}{986}
\@writefile{toc}{\contentsline {paragraph}{Why This Is Powerful}{986}{section*.2068}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.10.3: Conditional Probability Paths and Vector Fields}{986}{section*.2069}\protected@file@percent }
\newlabel{enr:chapter20_conditional_paths}{{20.10.3}{986}{\color {ocre}Enrichment \thesubsection : Conditional Probability Paths and Vector Fields}{section*.2069}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation}{986}{section*.2070}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Canonical Gaussian Conditional Paths}{986}{section*.2071}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Deriving the Velocity Field from the Continuity Equation}{987}{section*.2072}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{General Gaussian Conditional Paths and Affine Flow Maps}{987}{section*.2073}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The Canonical Affine Flow and Induced Velocity Field}{987}{section*.2074}\protected@file@percent }
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\@writefile{toc}{\contentsline {paragraph}{The Conditional Flow Matching Loss}{988}{section*.2076}\protected@file@percent }
\abx@aux@backref{947}{lipman2022_flowmatching}{0}{988}{988}
\@writefile{toc}{\contentsline {paragraph}{From Theory to Practice: Training with Conditional Flow Matching}{989}{section*.2078}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Implementation Notes}{989}{section*.2080}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{990}{section*.2081}\protected@file@percent }
\BKM@entry{id=778,dest={73656374696F6E2A2E32303832},srcline={8994}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030305C3030302E5C303030345C3030303A5C3030305C3034305C303030435C303030685C3030306F5C3030306F5C303030735C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306E5C303030645C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030505C303030615C303030745C303030685C303030735C3030305C3034305C3030302D5C3030305C3034305C303030445C303030695C303030665C303030665C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030765C303030735C3030305C3034305C3030304F5C30303054}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.10.4: Choosing Conditional Paths - Diffusion vs OT}{991}{section*.2082}\protected@file@percent }
\newlabel{enr:chapter20_diffusion_ot_cfm}{{20.10.4}{991}{\color {ocre}Enrichment \thesubsection : Choosing Conditional Paths - Diffusion vs OT}{section*.2082}{}}
\@writefile{toc}{\contentsline {subsubsection}{Choosing Conditional Paths – Diffusion vs OT}{991}{section*.2083}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Variance Exploding (VE) Conditional Paths}{991}{section*.2084}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Variance Preserving (VP) Conditional Paths}{991}{section*.2085}\protected@file@percent }
\abx@aux@cite{0}{mccann1997_convexity}
\abx@aux@segm{0}{0}{mccann1997_convexity}
\@writefile{toc}{\contentsline {paragraph}{Limitations of Diffusion-Based Conditional Paths}{992}{section*.2086}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Optimal Transport Conditional Probability Paths}{992}{section*.2087}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{What Is Optimal Transport?}{992}{section*.2088}\protected@file@percent }
\abx@aux@backref{948}{mccann1997_convexity}{0}{992}{992}
\@writefile{toc}{\contentsline {paragraph}{Affine OT Flow Between Gaussians}{993}{section*.2089}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The OT Vector Field}{993}{section*.2090}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The Corresponding Flow Map and CFM Loss}{993}{section*.2091}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Vector Field Geometry: Diffusion vs. Optimal Transport}{993}{section*.2092}\protected@file@percent }
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\@writefile{lof}{\contentsline {figure}{\numberline {20.68}{\ignorespaces  \textbf  {Local vector fields for diffusion (left) and OT (right) conditional paths.} Each plot visualizes how the conditional velocity field \( u_t(x \mid x_1) \) evolves over time and space. In diffusion-based flows (left), the velocity direction is state-dependent and becomes increasingly steep as \( t \to 1 \), leading to curved sample trajectories and large vector magnitudes near the end. This causes the norm \( \|u_t(x)\|_2 \) to spike, resulting in high-magnitude regions shown in blue near the target. In contrast, OT-based flows (right) define a fixed affine direction from noise to data, inducing straight-line trajectories with time-constant acceleration. Here, the velocity norm is uniform or gently varying, yielding mostly red or yellow shades across the field. \textcolor {gray}{Color denotes velocity magnitude: \textbf  {blue} = high, \textbf  {red} = low.} \emph  {Adapted from Figure~2 in~\blx@tocontentsinit {0}\cite {lipman2022_flowmatching}}. }}{994}{figure.caption.2093}\protected@file@percent }
\abx@aux@backref{950}{lipman2022_flowmatching}{0}{994}{994}
\newlabel{fig:chapter20_diffusion_vs_ot}{{20.68}{994}{\textbf {Local vector fields for diffusion (left) and OT (right) conditional paths.} Each plot visualizes how the conditional velocity field \( u_t(x \mid x_1) \) evolves over time and space. In diffusion-based flows (left), the velocity direction is state-dependent and becomes increasingly steep as \( t \to 1 \), leading to curved sample trajectories and large vector magnitudes near the end. This causes the norm \( \|u_t(x)\|_2 \) to spike, resulting in high-magnitude regions shown in blue near the target. In contrast, OT-based flows (right) define a fixed affine direction from noise to data, inducing straight-line trajectories with time-constant acceleration. Here, the velocity norm is uniform or gently varying, yielding mostly red or yellow shades across the field. \textcolor {gray}{Color denotes velocity magnitude: \textbf {blue} = high, \textbf {red} = low.} \emph {Adapted from Figure~2 in~\cite {lipman2022_flowmatching}}}{figure.caption.2093}{}}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\@writefile{toc}{\contentsline {paragraph}{Why Optimal Transport Defines a Superior Learning Signal}{995}{section*.2094}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.69}{\ignorespaces  \textbf  {Macroscopic sampling trajectories under diffusion and OT vector fields.} \emph  {Left:} Diffusion-based paths tend to overshoot and curve as they approach the target \( x_1 \), requiring corrective backtracking and tighter numerical integration tolerances. These nonlinear trajectories are induced by state- and time-dependent velocity fields. \emph  {Right:} Optimal Transport trajectories follow straight-line segments from \( x_0 \) to \( x_1 \) with constant direction and time-scaled speed. This linearity enables efficient sampling using only \( \sim \!10\!-\!30 \) integration steps. \emph  {Adapted from Figure~3 in~\blx@tocontentsinit {0}\cite {lipman2022_flowmatching}}. }}{995}{figure.caption.2095}\protected@file@percent }
\abx@aux@backref{952}{lipman2022_flowmatching}{0}{995}{995}
\newlabel{fig:chapter20_diffusion_vs_ot_paths}{{20.69}{995}{\textbf {Macroscopic sampling trajectories under diffusion and OT vector fields.} \emph {Left:} Diffusion-based paths tend to overshoot and curve as they approach the target \( x_1 \), requiring corrective backtracking and tighter numerical integration tolerances. These nonlinear trajectories are induced by state- and time-dependent velocity fields. \emph {Right:} Optimal Transport trajectories follow straight-line segments from \( x_0 \) to \( x_1 \) with constant direction and time-scaled speed. This linearity enables efficient sampling using only \( \sim \!10\!-\!30 \) integration steps. \emph {Adapted from Figure~3 in~\cite {lipman2022_flowmatching}}}{figure.caption.2095}{}}
\@writefile{toc}{\contentsline {paragraph}{OT-based Conditional Flow Matching Inference}{996}{section*.2096}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Takeaway}{996}{section*.2097}\protected@file@percent }
\BKM@entry{id=779,dest={73656374696F6E2A2E32303938},srcline={9284}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030305C3030302E5C303030355C3030303A5C3030305C3034305C303030495C3030306D5C303030705C3030306C5C303030655C3030306D5C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030302C5C3030305C3034305C303030455C303030785C303030705C303030655C303030725C303030695C3030306D5C303030655C3030306E5C303030745C303030735C3030302C5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030525C303030655C3030306C5C303030615C303030745C303030655C303030645C3030305C3034305C303030575C3030306F5C303030725C3030306B}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\abx@aux@cite{0}{ho2020_ddpm}
\abx@aux@segm{0}{0}{ho2020_ddpm}
\abx@aux@cite{0}{song2021_sde}
\abx@aux@segm{0}{0}{song2021_sde}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.10.5: Implementation, Experiments, and Related Work}{997}{section*.2098}\protected@file@percent }
\newlabel{enr:chapter20_cfm_implementation_experiments}{{20.10.5}{997}{\color {ocre}Enrichment \thesubsection : Implementation, Experiments, and Related Work}{section*.2098}{}}
\@writefile{toc}{\contentsline {paragraph}{Implementation Details}{997}{section*.2099}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Empirical Results: OT vs.\ Diffusion}{997}{section*.2100}\protected@file@percent }
\abx@aux@backref{953}{lipman2022_flowmatching}{0}{997}{997}
\@writefile{lof}{\contentsline {figure}{\numberline {20.70}{\ignorespaces  \textbf  {Effect of training objective on CNF trajectories.} \emph  {Left:} Trajectories of CNFs trained on 2D checkerboard data. OT-based flows introduce structure earlier, while diffusion-based ones lag and show less spatial coherence. \emph  {Right:} Midpoint-solver based sampling is much faster and more stable with OT. Adapted from Figure~4 in~\blx@tocontentsinit {0}\cite {lipman2022_flowmatching}. }}{997}{figure.caption.2101}\protected@file@percent }
\abx@aux@backref{955}{lipman2022_flowmatching}{0}{997}{997}
\newlabel{fig:chapter20_cnf_trajectories}{{20.70}{997}{\textbf {Effect of training objective on CNF trajectories.} \emph {Left:} Trajectories of CNFs trained on 2D checkerboard data. OT-based flows introduce structure earlier, while diffusion-based ones lag and show less spatial coherence. \emph {Right:} Midpoint-solver based sampling is much faster and more stable with OT. Adapted from Figure~4 in~\cite {lipman2022_flowmatching}}{figure.caption.2101}{}}
\@writefile{toc}{\contentsline {paragraph}{Quantitative Benchmarks}{997}{section*.2102}\protected@file@percent }
\abx@aux@cite{0}{hoang2018_mgan}
\abx@aux@segm{0}{0}{hoang2018_mgan}
\abx@aux@cite{0}{lin2018_pacgan}
\abx@aux@segm{0}{0}{lin2018_pacgan}
\abx@aux@cite{0}{sage2018_logogan}
\abx@aux@segm{0}{0}{sage2018_logogan}
\abx@aux@cite{0}{lucic2019_selfgan}
\abx@aux@segm{0}{0}{lucic2019_selfgan}
\abx@aux@cite{0}{lucic2019_selfgan}
\abx@aux@segm{0}{0}{lucic2019_selfgan}
\abx@aux@cite{0}{armandpour2021_pgmg}
\abx@aux@segm{0}{0}{armandpour2021_pgmg}
\abx@aux@cite{0}{vincent2011_dsm}
\abx@aux@segm{0}{0}{vincent2011_dsm}
\abx@aux@cite{0}{song2021_sde}
\abx@aux@segm{0}{0}{song2021_sde}
\abx@aux@cite{0}{chen2019_neuralode}
\abx@aux@segm{0}{0}{chen2019_neuralode}
\abx@aux@cite{0}{grathwohl2019_ffjord}
\abx@aux@segm{0}{0}{grathwohl2019_ffjord}
\abx@aux@cite{0}{tong2020_otflow}
\abx@aux@segm{0}{0}{tong2020_otflow}
\abx@aux@cite{0}{xu2018_swf}
\abx@aux@segm{0}{0}{xu2018_swf}
\@writefile{lot}{\contentsline {table}{\numberline {20.5}{\ignorespaces Likelihood (NLL), sample quality (FID), and evaluation cost (NFE). Lower is better. Adapted from Table~1 in~\blx@tocontentsinit {0}\cite {lipman2022_flowmatching}.}}{998}{table.caption.2103}\protected@file@percent }
\abx@aux@backref{957}{lipman2022_flowmatching}{0}{998}{998}
\newlabel{tab:chapter20_flowmatching_results}{{20.5}{998}{Likelihood (NLL), sample quality (FID), and evaluation cost (NFE). Lower is better. Adapted from Table~1 in~\cite {lipman2022_flowmatching}}{table.caption.2103}{}}
\abx@aux@backref{958}{ho2020_ddpm}{0}{998}{998}
\abx@aux@backref{959}{song2021_sde}{0}{998}{998}
\@writefile{toc}{\contentsline {paragraph}{Additional Comparisons}{998}{section*.2104}\protected@file@percent }
\abx@aux@backref{960}{hoang2018_mgan}{0}{998}{998}
\abx@aux@backref{961}{lin2018_pacgan}{0}{998}{998}
\abx@aux@backref{962}{sage2018_logogan}{0}{998}{998}
\abx@aux@backref{963}{lucic2019_selfgan}{0}{998}{998}
\abx@aux@backref{964}{lucic2019_selfgan}{0}{998}{998}
\abx@aux@backref{965}{armandpour2021_pgmg}{0}{998}{998}
\@writefile{toc}{\contentsline {paragraph}{Related Work and Positioning}{998}{section*.2105}\protected@file@percent }
\abx@aux@backref{966}{vincent2011_dsm}{0}{998}{998}
\abx@aux@backref{967}{song2021_sde}{0}{998}{998}
\abx@aux@backref{968}{chen2019_neuralode}{0}{998}{998}
\abx@aux@backref{969}{grathwohl2019_ffjord}{0}{998}{998}
\abx@aux@backref{970}{tong2020_otflow}{0}{998}{998}
\abx@aux@backref{971}{xu2018_swf}{0}{998}{998}
\abx@aux@cite{0}{gat2024_discreteflowmatching}
\abx@aux@segm{0}{0}{gat2024_discreteflowmatching}
\abx@aux@cite{0}{chen2023_riemannianfm}
\abx@aux@segm{0}{0}{chen2023_riemannianfm}
\abx@aux@cite{0}{pooladian2023_msfm}
\abx@aux@segm{0}{0}{pooladian2023_msfm}
\abx@aux@cite{0}{kornilov2024_ofm}
\abx@aux@segm{0}{0}{kornilov2024_ofm}
\abx@aux@cite{0}{yang2024_consistencyfm}
\abx@aux@segm{0}{0}{yang2024_consistencyfm}
\abx@aux@cite{0}{nguyen2023_boss}
\abx@aux@segm{0}{0}{nguyen2023_boss}
\abx@aux@backref{972}{gat2024_discreteflowmatching}{0}{999}{999}
\abx@aux@backref{973}{chen2023_riemannianfm}{0}{999}{999}
\abx@aux@backref{974}{pooladian2023_msfm}{0}{999}{999}
\abx@aux@backref{975}{kornilov2024_ofm}{0}{999}{999}
\abx@aux@backref{976}{yang2024_consistencyfm}{0}{999}{999}
\abx@aux@backref{977}{nguyen2023_boss}{0}{999}{999}
\@writefile{toc}{\contentsline {paragraph}{Outlook}{999}{section*.2106}\protected@file@percent }
\BKM@entry{id=780,dest={73656374696F6E2A2E32313037},srcline={9414}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030315C3030303A5C3030305C3034305C303030415C303030645C303030645C303030695C303030745C303030695C3030306F5C3030306E5C303030615C3030306C5C3030305C3034305C303030505C303030695C3030306F5C3030306E5C303030655C303030655C303030725C303030695C3030306E5C303030675C3030305C3034305C303030575C3030306F5C303030725C3030306B5C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C303030765C303030655C3030305C3034305C303030415C30303049}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\abx@aux@cite{0}{raffel2020_t5}
\abx@aux@segm{0}{0}{raffel2020_t5}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\BKM@entry{id=781,dest={73656374696F6E2A2E32313038},srcline={9426}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030315C3030302E5C303030315C3030303A5C3030305C3034305C303030475C3030304C5C303030495C303030445C303030455C3030303A5C3030305C3034305C303030545C303030655C303030785C303030745C3030302D5C303030475C303030755C303030695C303030645C303030655C303030645C3030305C3034305C303030445C303030695C303030665C303030665C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030655C303030725C3030302D5C303030465C303030725C303030655C303030655C3030305C3034305C303030475C303030755C303030695C303030645C303030615C3030306E5C303030635C30303065}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\@writefile{toc}{\contentsline {section}{Enrichment 20.11: Additional Pioneering Works in Generative AI}{1000}{section*.2107}\protected@file@percent }
\newlabel{enr:chapter20_generative_milestones}{{20.11}{1000}{\color {ocre}Enrichment \thesection : Additional Pioneering Works in Generative AI}{section*.2107}{}}
\abx@aux@backref{978}{radford2021_clip}{0}{1000}{1000}
\abx@aux@backref{979}{raffel2020_t5}{0}{1000}{1000}
\abx@aux@backref{980}{nichol2022_glide}{0}{1000}{1000}
\abx@aux@backref{981}{ramesh2021_dalle}{0}{1000}{1000}
\abx@aux@backref{982}{rombach2022_ldm}{0}{1000}{1000}
\abx@aux@backref{983}{ruiz2023_dreambooth}{0}{1000}{1000}
\abx@aux@backref{984}{zhang2023_controlnet}{0}{1000}{1000}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.11.1: GLIDE: Text-Guided Diffusion with Classifier-Free Guidance}{1000}{section*.2108}\protected@file@percent }
\newlabel{enr:chapter20_glide}{{20.11.1}{1000}{\color {ocre}Enrichment \thesubsection : GLIDE: Text-Guided Diffusion with Classifier-Free Guidance}{section*.2108}{}}
\abx@aux@backref{985}{nichol2022_glide}{0}{1000}{1000}
\abx@aux@backref{986}{ramesh2021_dalle}{0}{1000}{1000}
\@writefile{toc}{\contentsline {paragraph}{Model Architecture and Conditioning Mechanism}{1000}{section*.2109}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.71}{\ignorespaces Selected samples from GLIDE using classifier-free guidance~\blx@tocontentsinit {0}\cite {nichol2022_glide}. Prompts include complex compositions and stylistic renderings. The model accurately generates unseen concepts like “a crayon drawing of a space elevator” and interprets spatial relationships such as “a red cube on top of a blue cube,” including plausible shadows and 3D structure.}}{1001}{figure.caption.2110}\protected@file@percent }
\abx@aux@backref{988}{nichol2022_glide}{0}{1001}{1001}
\newlabel{fig:chapter20_glide_examples}{{20.71}{1001}{Selected samples from GLIDE using classifier-free guidance~\cite {nichol2022_glide}. Prompts include complex compositions and stylistic renderings. The model accurately generates unseen concepts like “a crayon drawing of a space elevator” and interprets spatial relationships such as “a red cube on top of a blue cube,” including plausible shadows and 3D structure}{figure.caption.2110}{}}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\abx@aux@backref{989}{radford2021_clip}{0}{1002}{1002}
\abx@aux@backref{990}{nichol2022_glide}{0}{1002}{1002}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\abx@aux@cite{0}{ho2021_cascaded}
\abx@aux@segm{0}{0}{ho2021_cascaded}
\abx@aux@backref{991}{nichol2022_glide}{0}{1003}{1003}
\abx@aux@backref{992}{ho2021_cascaded}{0}{1003}{1003}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\abx@aux@cite{0}{ho2021_cascaded}
\abx@aux@segm{0}{0}{ho2021_cascaded}
\@writefile{toc}{\contentsline {paragraph}{Super-Resolution Modules in \textsc  {GLIDE}}{1004}{section*.2111}\protected@file@percent }
\abx@aux@backref{993}{nichol2022_glide}{0}{1004}{1004}
\@writefile{toc}{\contentsline {paragraph}{Relationship to Cascaded Diffusion Models (CDMs)}{1004}{section*.2112}\protected@file@percent }
\abx@aux@backref{994}{nichol2022_glide}{0}{1004}{1004}
\abx@aux@backref{995}{ho2021_cascaded}{0}{1004}{1004}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\abx@aux@cite{0}{dhariwal2021_beats}
\abx@aux@segm{0}{0}{dhariwal2021_beats}
\abx@aux@cite{0}{vaswani2017_attention}
\abx@aux@segm{0}{0}{vaswani2017_attention}
\@writefile{toc}{\contentsline {paragraph}{Full Generation Pipeline of \textsc  {GLIDE}}{1005}{section*.2113}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ADM U-Net Architecture in \textsc  {GLIDE}}{1005}{section*.2114}\protected@file@percent }
\newlabel{enr:chapter20_adm_unet}{{20.11.1}{1005}{ADM U-Net Architecture in \textsc {GLIDE}}{section*.2114}{}}
\abx@aux@backref{996}{nichol2022_glide}{0}{1005}{1005}
\abx@aux@backref{997}{dhariwal2021_beats}{0}{1005}{1005}
\abx@aux@backref{998}{vaswani2017_attention}{0}{1005}{1005}
\@writefile{toc}{\contentsline {paragraph}{Summary of the GLIDE System}{1006}{section*.2115}\protected@file@percent }
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\@writefile{toc}{\contentsline {paragraph}{Text-Guided Editing and Inpainting Capabilities}{1007}{section*.2116}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.72}{\ignorespaces Text-conditional inpainting with GLIDE~\blx@tocontentsinit {0}\cite {nichol2022_glide}. The masked region (green) is filled based on a new prompt. The model seamlessly aligns with the lighting, texture, and composition of the original image.}}{1007}{figure.caption.2117}\protected@file@percent }
\abx@aux@backref{1000}{nichol2022_glide}{0}{1007}{1007}
\newlabel{fig:chapter20_glide_inpainting}{{20.72}{1007}{Text-conditional inpainting with GLIDE~\cite {nichol2022_glide}. The masked region (green) is filled based on a new prompt. The model seamlessly aligns with the lighting, texture, and composition of the original image}{figure.caption.2117}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.73}{\ignorespaces Iterative scene construction with GLIDE. A base image is progressively edited via masked regions and updated prompts (e.g., adding a coffee table, a vase, or shifting the wall upward).}}{1008}{figure.caption.2118}\protected@file@percent }
\newlabel{fig:chapter20_iterative_scene}{{20.73}{1008}{Iterative scene construction with GLIDE. A base image is progressively edited via masked regions and updated prompts (e.g., adding a coffee table, a vase, or shifting the wall upward)}{figure.caption.2118}{}}
\abx@aux@cite{0}{meng2022_sde}
\abx@aux@segm{0}{0}{meng2022_sde}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\@writefile{toc}{\contentsline {paragraph}{Sketch-Based Conditional Editing with SDEdit}{1009}{section*.2119}\protected@file@percent }
\abx@aux@backref{1001}{meng2022_sde}{0}{1009}{1009}
\@writefile{lof}{\contentsline {figure}{\numberline {20.74}{\ignorespaces Sketch-guided editing with GLIDE, using text-conditional SDEdit~\blx@tocontentsinit {0}\cite {nichol2022_glide}. The user sketches a hat and provides the prompt “a corgi wearing a purple hat and a red tie”. The model transforms the sketch into a plausible image aligned with both visual and linguistic guidance.}}{1009}{figure.caption.2120}\protected@file@percent }
\abx@aux@backref{1003}{nichol2022_glide}{0}{1009}{1009}
\newlabel{fig:chapter20_glide_sketch_edit}{{20.74}{1009}{Sketch-guided editing with GLIDE, using text-conditional SDEdit~\cite {nichol2022_glide}. The user sketches a hat and provides the prompt “a corgi wearing a purple hat and a red tie”. The model transforms the sketch into a plausible image aligned with both visual and linguistic guidance}{figure.caption.2120}{}}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\abx@aux@cite{0}{ho2022_classifierfree}
\abx@aux@segm{0}{0}{ho2022_classifierfree}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\@writefile{toc}{\contentsline {paragraph}{Classifier-Free Guidance vs.\ CLIP Guidance}{1010}{section*.2121}\protected@file@percent }
\newlabel{subsubsec:chapter20_cfg_vs_clip}{{20.11.1}{1010}{Classifier-Free Guidance vs.\ CLIP Guidance}{section*.2121}{}}
\abx@aux@backref{1004}{radford2021_clip}{0}{1010}{1010}
\abx@aux@backref{1005}{ho2022_classifierfree}{0}{1010}{1010}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\abx@aux@cite{0}{yu2022_parti}
\abx@aux@segm{0}{0}{yu2022_parti}
\@writefile{lof}{\contentsline {figure}{\numberline {20.75}{\ignorespaces Trade-off between diversity and fidelity in GLIDE~\blx@tocontentsinit {0}\cite {nichol2022_glide}. Classifier-free guidance (CFG) achieves sharper, more realistic images while preserving more variation than CLIP-based guidance.}}{1011}{figure.caption.2122}\protected@file@percent }
\abx@aux@backref{1007}{nichol2022_glide}{0}{1011}{1011}
\newlabel{fig:chapter20_glide_diversity_fidelity}{{20.75}{1011}{Trade-off between diversity and fidelity in GLIDE~\cite {nichol2022_glide}. Classifier-free guidance (CFG) achieves sharper, more realistic images while preserving more variation than CLIP-based guidance}{figure.caption.2122}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.76}{\ignorespaces Elo scores for guidance methods in GLIDE~\blx@tocontentsinit {0}\cite {nichol2022_glide}. CFG outperforms CLIP guidance across both photorealism and semantic alignment.}}{1011}{figure.caption.2123}\protected@file@percent }
\abx@aux@backref{1009}{nichol2022_glide}{0}{1011}{1011}
\newlabel{fig:chapter20_glide_elo}{{20.76}{1011}{Elo scores for guidance methods in GLIDE~\cite {nichol2022_glide}. CFG outperforms CLIP guidance across both photorealism and semantic alignment}{figure.caption.2123}{}}
\abx@aux@backref{1010}{rombach2022_ldm}{0}{1011}{1011}
\abx@aux@backref{1011}{saharia2022_imagen}{0}{1011}{1011}
\abx@aux@backref{1012}{yu2022_parti}{0}{1011}{1011}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\abx@aux@cite{0}{ho2022_classifierfree}
\abx@aux@segm{0}{0}{ho2022_classifierfree}
\@writefile{toc}{\contentsline {paragraph}{Failure Cases and Architectural Limitations}{1012}{section*.2124}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.77}{\ignorespaces Failure examples from \textsc  {GLIDE}~\blx@tocontentsinit {0}\cite {nichol2022_glide}. The model exhibits spatial inconsistencies, compositional errors, or semantic drift.}}{1012}{figure.caption.2125}\protected@file@percent }
\abx@aux@backref{1014}{nichol2022_glide}{0}{1012}{1012}
\newlabel{fig:chapter20_glide_failure_cases}{{20.77}{1012}{Failure examples from \textsc {GLIDE}~\cite {nichol2022_glide}. The model exhibits spatial inconsistencies, compositional errors, or semantic drift}{figure.caption.2125}{}}
\abx@aux@backref{1015}{nichol2022_glide}{0}{1012}{1012}
\abx@aux@backref{1016}{ho2022_classifierfree}{0}{1012}{1012}
\abx@aux@cite{0}{ramesh2022_dalle2}
\abx@aux@segm{0}{0}{ramesh2022_dalle2}
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\BKM@entry{id=782,dest={73656374696F6E2A2E32313236},srcline={9865}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030315C3030302E5C303030325C3030303A5C3030305C3034305C303030445C303030415C3030304C5C3030304C5C3030305C3236375C303030455C3030305C3034305C303030315C3030303A5C3030305C3034305C303030445C303030695C303030735C303030635C303030725C303030655C303030745C303030655C3030305C3034305C303030545C3030306F5C3030306B5C303030655C3030306E5C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030545C303030655C303030785C303030745C3030302D5C303030745C3030306F5C3030302D5C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\abx@aux@cite{0}{brown2020_language}
\abx@aux@segm{0}{0}{brown2020_language}
\abx@aux@backref{1017}{ramesh2022_dalle2}{0}{1013}{1013}
\abx@aux@backref{1018}{ramesh2021_dalle}{0}{1013}{1013}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.11.2: DALL·E 1: Discrete Tokens for Text-to-Image Generation}{1013}{section*.2126}\protected@file@percent }
\newlabel{enr:chapter20_dalle1}{{20.11.2}{1013}{\color {ocre}Enrichment \thesubsection : DALL·E 1: Discrete Tokens for Text-to-Image Generation}{section*.2126}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation: Turning Images into Token Sequences for GPT-Style Modelling}{1013}{section*.2127}\protected@file@percent }
\newlabel{sec:chapter20_dalle1_motivation}{{20.11.2}{1013}{Motivation: Turning Images into Token Sequences for GPT-Style Modelling}{section*.2127}{}}
\abx@aux@backref{1019}{ramesh2021_dalle}{0}{1013}{1013}
\abx@aux@backref{1020}{brown2020_language}{0}{1013}{1013}
\abx@aux@cite{0}{esser2021_vqgan}
\abx@aux@segm{0}{0}{esser2021_vqgan}
\abx@aux@cite{0}{he2022_mae}
\abx@aux@segm{0}{0}{he2022_mae}
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\abx@aux@backref{1021}{esser2021_vqgan}{0}{1014}{1014}
\abx@aux@backref{1022}{he2022_mae}{0}{1014}{1014}
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\abx@aux@cite{0}{jang2017_gumbel}
\abx@aux@segm{0}{0}{jang2017_gumbel}
\@writefile{lof}{\contentsline {figure}{\numberline {20.78}{\ignorespaces Examples from \textsc  {DALL$\cdot $E}~\blx@tocontentsinit {0}\cite {ramesh2021_dalle}. The model demonstrates the ability to combine distinct concepts (e.g., “an illustration of a baby hedgehog in a christmas sweater walking a dog”), anthropomorphize animals, render textual descriptions into stylized lettering, and even perform basic image-to-image translation. These outputs illustrate DALL$\cdot $E's capacity for visual reasoning and compositional generalization.}}{1015}{figure.caption.2128}\protected@file@percent }
\abx@aux@backref{1024}{ramesh2021_dalle}{0}{1015}{1015}
\newlabel{fig:chapter20_dalle1_examples}{{20.78}{1015}{Examples from \textsc {DALL$\cdot $E}~\cite {ramesh2021_dalle}. The model demonstrates the ability to combine distinct concepts (e.g., “an illustration of a baby hedgehog in a christmas sweater walking a dog”), anthropomorphize animals, render textual descriptions into stylized lettering, and even perform basic image-to-image translation. These outputs illustrate DALL$\cdot $E's capacity for visual reasoning and compositional generalization}{figure.caption.2128}{}}
\@writefile{toc}{\contentsline {paragraph}{How VQ-VAE Enables Discrete Tokenization}{1015}{section*.2129}\protected@file@percent }
\abx@aux@backref{1025}{ramesh2021_dalle}{0}{1015}{1015}
\abx@aux@backref{1026}{ramesh2021_dalle}{0}{1015}{1015}
\abx@aux@backref{1027}{jang2017_gumbel}{0}{1015}{1015}
\abx@aux@cite{0}{zhang2018_lpips}
\abx@aux@segm{0}{0}{zhang2018_lpips}
\abx@aux@backref{1028}{zhang2018_lpips}{0}{1016}{1016}
\abx@aux@cite{0}{jang2017_gumbel}
\abx@aux@segm{0}{0}{jang2017_gumbel}
\abx@aux@backref{1029}{jang2017_gumbel}{0}{1017}{1017}
\@writefile{lof}{\contentsline {figure}{\numberline {20.79}{\ignorespaces  \textbf  {Training the VQ-VAE in DALL$\cdot $E~1.} The encoder outputs logits \( \boldsymbol  {\ell }_{i,j} \in \mathbb  {R}^K \), which are converted into relaxed categorical distributions \( p_{i,j}(k) \) via Gumbel-softmax. These define convex combinations over codebook vectors \( \vec  {e}_k \), yielding continuous latent vectors \( \vec  {z}_{i,j} \). The decoder reconstructs the image from the full grid \( \{ \vec  {z}_{i,j} \} \). The ELBO loss drives both reconstruction and codebook utilization. At inference, the encoder performs hard argmax token selection for compatibility with transformer-based generation.  \textit  {(Figure created by the author using DALL$\cdot $E-generated visual elements.)} }}{1018}{figure.caption.2130}\protected@file@percent }
\newlabel{fig:chapter20_dalle1_vqvae_training}{{20.79}{1018}{\textbf {Training the VQ-VAE in DALL$\cdot $E~1.} The encoder outputs logits \( \boldsymbol {\ell }_{i,j} \in \mathbb {R}^K \), which are converted into relaxed categorical distributions \( p_{i,j}(k) \) via Gumbel-softmax. These define convex combinations over codebook vectors \( \vec {e}_k \), yielding continuous latent vectors \( \vec {z}_{i,j} \). The decoder reconstructs the image from the full grid \( \{ \vec {z}_{i,j} \} \). The ELBO loss drives both reconstruction and codebook utilization. At inference, the encoder performs hard argmax token selection for compatibility with transformer-based generation.\\ \textit {(Figure created by the author using DALL$\cdot $E-generated visual elements.)}}{figure.caption.2130}{}}
\abx@aux@cite{0}{razavi2019_vqvae2}
\abx@aux@segm{0}{0}{razavi2019_vqvae2}
\@writefile{lof}{\contentsline {figure}{\numberline {20.80}{\ignorespaces  \textbf  {Inference pipeline in DALL$\cdot $E~1.} At inference time, the system receives a raw text prompt, which is first tokenized into a sequence of subword units using Byte Pair Encoding (BPE). This token sequence is fed into a \emph  {decoder-only transformer}, which autoregressively predicts a sequence of 1024 discrete image tokens, each representing the index of a visual codebook vector. The output sequence is reshaped into a \( 32 \times 32 \) spatial grid and passed to the \textbf  {frozen VQ-VAE decoder}, which translates these symbolic tokens into a high-resolution \( 256 \times 256 \) RGB image. This modular architecture cleanly separates text understanding, symbolic image generation, and pixel-level rendering.  \textit  {(Figure created by the author to illustrate the DALL$\cdot $E~1 inference process.)} }}{1020}{figure.caption.2131}\protected@file@percent }
\newlabel{fig:chapter20_dalle1_inference_pipeline}{{20.80}{1020}{\textbf {Inference pipeline in DALL$\cdot $E~1.} At inference time, the system receives a raw text prompt, which is first tokenized into a sequence of subword units using Byte Pair Encoding (BPE). This token sequence is fed into a \emph {decoder-only transformer}, which autoregressively predicts a sequence of 1024 discrete image tokens, each representing the index of a visual codebook vector. The output sequence is reshaped into a \( 32 \times 32 \) spatial grid and passed to the \textbf {frozen VQ-VAE decoder}, which translates these symbolic tokens into a high-resolution \( 256 \times 256 \) RGB image. This modular architecture cleanly separates text understanding, symbolic image generation, and pixel-level rendering.\\ \textit {(Figure created by the author to illustrate the DALL$\cdot $E~1 inference process.)}}{figure.caption.2131}{}}
\@writefile{toc}{\contentsline {paragraph}{Clarifying Terminology: dVAE vs. VQ-VAE}{1020}{section*.2132}\protected@file@percent }
\abx@aux@backref{1030}{razavi2019_vqvae2}{0}{1020}{1020}
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\@writefile{toc}{\contentsline {paragraph}{Training Datasets and Sample Generation Pipeline}{1021}{section*.2133}\protected@file@percent }
\abx@aux@backref{1031}{ramesh2021_dalle}{0}{1021}{1021}
\abx@aux@backref{1032}{radford2021_clip}{0}{1021}{1021}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\abx@aux@backref{1033}{radford2021_clip}{0}{1022}{1022}
\@writefile{lof}{\contentsline {figure}{\numberline {20.81}{\ignorespaces  \textbf  {Effect of Sample Pool Size on Reranked Outputs.} Adapted from~\blx@tocontentsinit {0}\cite {ramesh2021_dalle}, this figure illustrates how increasing the number of sampled candidates \( N \) improves the top-ranked image quality. The prompt is “a group of urinals is near the trees.” Each image is generated independently using temperature-based decoding and scored by CLIP for alignment with the caption. At small \( N \), none of the candidates are coherent. As \( N \) increases, the diversity improves the chance that CLIP surfaces a relevant and visually accurate result. This demonstrates the power—but also the computational cost—of large-scale sampling combined with contrastive reranking. }}{1022}{figure.caption.2134}\protected@file@percent }
\abx@aux@backref{1035}{ramesh2021_dalle}{0}{1022}{1022}
\newlabel{fig:chapter20_dalle1_contrastive_reranking}{{20.81}{1022}{\textbf {Effect of Sample Pool Size on Reranked Outputs.} Adapted from~\cite {ramesh2021_dalle}, this figure illustrates how increasing the number of sampled candidates \( N \) improves the top-ranked image quality. The prompt is “a group of urinals is near the trees.” Each image is generated independently using temperature-based decoding and scored by CLIP for alignment with the caption. At small \( N \), none of the candidates are coherent. As \( N \) increases, the diversity improves the chance that CLIP surfaces a relevant and visually accurate result. This demonstrates the power—but also the computational cost—of large-scale sampling combined with contrastive reranking}{figure.caption.2134}{}}
\abx@aux@cite{0}{tao2022_dfgan}
\abx@aux@segm{0}{0}{tao2022_dfgan}
\abx@aux@cite{0}{tao2022_dfgan}
\abx@aux@segm{0}{0}{tao2022_dfgan}
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\abx@aux@cite{0}{tao2022_dfgan}
\abx@aux@segm{0}{0}{tao2022_dfgan}
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\abx@aux@cite{0}{wah2011_cub}
\abx@aux@segm{0}{0}{wah2011_cub}
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\@writefile{toc}{\contentsline {paragraph}{Experimental Results and Motivation for DALL$\cdot $E~2}{1023}{section*.2135}\protected@file@percent }
\abx@aux@backref{1036}{tao2022_dfgan}{0}{1023}{1023}
\@writefile{lof}{\contentsline {figure}{\numberline {20.82}{\ignorespaces  \textbf  {Human evaluation on MS-COCO.} Compared to DF-GAN~\blx@tocontentsinit {0}\cite {tao2022_dfgan}, DALL$\cdot $E~1’s samples were chosen as more realistic and better aligned with the input caption in 90\% and 93.3\% of evaluations, respectively. Voting was performed by five independent human raters. Adapted from~\blx@tocontentsinit {0}\cite {ramesh2021_dalle}. }}{1023}{figure.caption.2136}\protected@file@percent }
\abx@aux@backref{1039}{tao2022_dfgan}{0}{1023}{1023}
\abx@aux@backref{1040}{ramesh2021_dalle}{0}{1023}{1023}
\newlabel{fig:chapter20_dalle1_human_comparison}{{20.82}{1023}{\textbf {Human evaluation on MS-COCO.} Compared to DF-GAN~\cite {tao2022_dfgan}, DALL$\cdot $E~1’s samples were chosen as more realistic and better aligned with the input caption in 90\% and 93.3\% of evaluations, respectively. Voting was performed by five independent human raters. Adapted from~\cite {ramesh2021_dalle}}{figure.caption.2136}{}}
\abx@aux@backref{1041}{wah2011_cub}{0}{1023}{1023}
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\@writefile{lof}{\contentsline {figure}{\numberline {20.83}{\ignorespaces  \textbf  {FID and IS on MS-COCO and CUB.} On MS-COCO, DALL$\cdot $E~1 matches or outperforms prior models depending on blur level, suggesting good high-level coherence. On CUB, its lack of fine-grained knowledge leads to significantly worse FID scores, highlighting domain transfer limitations. Adapted from~\blx@tocontentsinit {0}\cite {ramesh2021_dalle}. }}{1024}{figure.caption.2137}\protected@file@percent }
\abx@aux@backref{1043}{ramesh2021_dalle}{0}{1024}{1024}
\newlabel{fig:chapter20_dalle1_fid_and_is}{{20.83}{1024}{\textbf {FID and IS on MS-COCO and CUB.} On MS-COCO, DALL$\cdot $E~1 matches or outperforms prior models depending on blur level, suggesting good high-level coherence. On CUB, its lack of fine-grained knowledge leads to significantly worse FID scores, highlighting domain transfer limitations. Adapted from~\cite {ramesh2021_dalle}}{figure.caption.2137}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.84}{\ignorespaces  \textbf  {Zero-shot samples from DALL$\cdot $E~1 on the CUB dataset.} While capturing bird-like features, the generations struggle with consistent anatomy or species-level details, reflecting DALL$\cdot $E’s limited resolution and domain-specific expressivity. Adapted from~\blx@tocontentsinit {0}\cite {ramesh2021_dalle}. }}{1024}{figure.caption.2138}\protected@file@percent }
\abx@aux@backref{1045}{ramesh2021_dalle}{0}{1024}{1024}
\newlabel{fig:chapter20_dalle1_zero_shot_cub}{{20.84}{1024}{\textbf {Zero-shot samples from DALL$\cdot $E~1 on the CUB dataset.} While capturing bird-like features, the generations struggle with consistent anatomy or species-level details, reflecting DALL$\cdot $E’s limited resolution and domain-specific expressivity. Adapted from~\cite {ramesh2021_dalle}}{figure.caption.2138}{}}
\abx@aux@backref{1046}{radford2021_clip}{0}{1025}{1025}
\BKM@entry{id=783,dest={73656374696F6E2A2E32313339},srcline={10258}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030315C3030302E5C303030335C3030303A5C3030305C3034305C303030445C303030415C3030304C5C3030304C5C303030455C3030305C3034305C303030325C3030303A5C3030305C3034305C303030445C303030695C303030665C303030665C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030505C303030725C303030695C3030306F5C303030725C303030735C3030305C3034305C3030306F5C303030765C303030655C303030725C3030305C3034305C303030435C3030304C5C303030495C303030505C3030305C3034305C303030455C3030306D5C303030625C303030655C303030645C303030645C303030695C3030306E5C303030675C30303073}
\abx@aux@cite{0}{ramesh2022_dalle2}
\abx@aux@segm{0}{0}{ramesh2022_dalle2}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.11.3: DALL$\cdot $E~2: Diffusion Priors over CLIP Embeddings}{1026}{section*.2139}\protected@file@percent }
\newlabel{enr:chapter20_dalle2}{{20.11.3}{1026}{\color {ocre}Enrichment \thesubsection : DALL$\cdot $E~2: Diffusion Priors over CLIP Embeddings}{section*.2139}{}}
\@writefile{toc}{\contentsline {paragraph}{System Overview and Architectural Shift}{1026}{section*.2140}\protected@file@percent }
\abx@aux@backref{1047}{ramesh2022_dalle2}{0}{1026}{1026}
\abx@aux@backref{1048}{radford2021_clip}{0}{1026}{1026}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\@writefile{lof}{\contentsline {figure}{\numberline {20.85}{\ignorespaces  \textbf  {DALL$\cdot $E~2 Architecture Overview.} The figure is divided into two conceptual stages. \textbf  {Top (above the dotted line):} CLIP pretraining. Images and text captions are mapped into a shared latent space via contrastive learning, producing paired embeddings \( z_i \in \mathbb  {R}^d \) (image) and \( z_t \in \mathbb  {R}^d \) (text). This CLIP model is pretrained independently and remains \emph  {frozen} throughout DALL$\cdot $E~2 training. \textbf  {Bottom (below the dotted line):} DALL$\cdot $E~2 generation pipeline. The frozen text embedding \( z_t \) is passed to a diffusion prior that samples a compatible image embedding \( z_i \), aligned with both the text and the CLIP image manifold. This embedding then conditions a cascade of diffusion decoders, which generate a high-resolution image \( x \in \mathbb  {R}^{H \times W \times 3} \). Both the prior and decoder are trained end-to-end using CLIP-based supervision. }}{1027}{figure.caption.2141}\protected@file@percent }
\newlabel{fig:chapter20_dalle2_architecture}{{20.85}{1027}{\textbf {DALL$\cdot $E~2 Architecture Overview.} The figure is divided into two conceptual stages. \textbf {Top (above the dotted line):} CLIP pretraining. Images and text captions are mapped into a shared latent space via contrastive learning, producing paired embeddings \( z_i \in \mathbb {R}^d \) (image) and \( z_t \in \mathbb {R}^d \) (text). This CLIP model is pretrained independently and remains \emph {frozen} throughout DALL$\cdot $E~2 training. \textbf {Bottom (below the dotted line):} DALL$\cdot $E~2 generation pipeline. The frozen text embedding \( z_t \) is passed to a diffusion prior that samples a compatible image embedding \( z_i \), aligned with both the text and the CLIP image manifold. This embedding then conditions a cascade of diffusion decoders, which generate a high-resolution image \( x \in \mathbb {R}^{H \times W \times 3} \). Both the prior and decoder are trained end-to-end using CLIP-based supervision}{figure.caption.2141}{}}
\@writefile{toc}{\contentsline {paragraph}{Diffusion Prior: Bridging Text and Image Embeddings}{1027}{section*.2142}\protected@file@percent }
\abx@aux@backref{1049}{radford2021_clip}{0}{1027}{1027}
\abx@aux@cite{0}{ho2020_ddpm}
\abx@aux@segm{0}{0}{ho2020_ddpm}
\abx@aux@cite{0}{nichol2021_improvedddpm}
\abx@aux@segm{0}{0}{nichol2021_improvedddpm}
\@writefile{toc}{\contentsline {subparagraph}{Training Objective}{1028}{subparagraph*.2143}\protected@file@percent }
\abx@aux@backref{1050}{ho2020_ddpm}{0}{1028}{1028}
\abx@aux@backref{1051}{nichol2021_improvedddpm}{0}{1028}{1028}
\abx@aux@cite{0}{nichol2021_improvedddpm}
\abx@aux@segm{0}{0}{nichol2021_improvedddpm}
\abx@aux@cite{0}{ho2020_ddpm}
\abx@aux@segm{0}{0}{ho2020_ddpm}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\abx@aux@backref{1052}{nichol2021_improvedddpm}{0}{1029}{1029}
\@writefile{toc}{\contentsline {subparagraph}{Model Architecture}{1029}{subparagraph*.2144}\protected@file@percent }
\abx@aux@backref{1053}{ho2020_ddpm}{0}{1029}{1029}
\abx@aux@backref{1054}{nichol2022_glide}{0}{1029}{1029}
\abx@aux@cite{0}{dhariwal2021_beats}
\abx@aux@segm{0}{0}{dhariwal2021_beats}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\abx@aux@cite{0}{ho2020_ddpm}
\abx@aux@segm{0}{0}{ho2020_ddpm}
\abx@aux@cite{0}{lu2022_dpm_solver}
\abx@aux@segm{0}{0}{lu2022_dpm_solver}
\abx@aux@backref{1055}{dhariwal2021_beats}{0}{1030}{1030}
\abx@aux@backref{1056}{nichol2022_glide}{0}{1030}{1030}
\abx@aux@backref{1057}{ho2020_ddpm}{0}{1030}{1030}
\abx@aux@backref{1058}{lu2022_dpm_solver}{0}{1030}{1030}
\abx@aux@cite{0}{salimans2022_progressive}
\abx@aux@segm{0}{0}{salimans2022_progressive}
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\abx@aux@cite{0}{lipman2022_flowmatching}
\abx@aux@segm{0}{0}{lipman2022_flowmatching}
\abx@aux@backref{1059}{salimans2022_progressive}{0}{1031}{1031}
\abx@aux@backref{1060}{saharia2022_imagen}{0}{1031}{1031}
\abx@aux@backref{1061}{lipman2022_flowmatching}{0}{1031}{1031}
\@writefile{lof}{\contentsline {figure}{\numberline {20.86}{\ignorespaces  \textbf  {DALL$\cdot $E~2 text-to-image examples.} These 1024\(\times \)1024 samples, generated by a production-scale version of the model, demonstrate high fidelity and strong semantic alignment. The use of CLIP-based priors and diffusion decoders enables complex compositional reasoning and stylistic control, outperforming discrete-token models. }}{1031}{figure.caption.2145}\protected@file@percent }
\newlabel{fig:chapter20_dalle2_examples}{{20.86}{1031}{\textbf {DALL$\cdot $E~2 text-to-image examples.} These 1024\(\times \)1024 samples, generated by a production-scale version of the model, demonstrate high fidelity and strong semantic alignment. The use of CLIP-based priors and diffusion decoders enables complex compositional reasoning and stylistic control, outperforming discrete-token models}{figure.caption.2145}{}}
\abx@aux@cite{0}{ronneberger2015_unet}
\abx@aux@segm{0}{0}{ronneberger2015_unet}
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\@writefile{toc}{\contentsline {paragraph}{Diffusion-Based Decoder}{1032}{section*.2146}\protected@file@percent }
\abx@aux@backref{1062}{ronneberger2015_unet}{0}{1032}{1032}
\abx@aux@backref{1063}{saharia2022_imagen}{0}{1033}{1033}
\abx@aux@backref{1064}{rombach2022_ldm}{0}{1033}{1033}
\@writefile{toc}{\contentsline {paragraph}{Semantic Interpolation and Reconstruction in CLIP Latents}{1033}{section*.2147}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.87}{\ignorespaces \textbf  {Reconstructions from truncated CLIP embeddings.} Each row reconstructs an image from a version of its CLIP embedding projected into a subset of PCA components. As more dimensions are retained, visual fidelity improves. Rightmost column shows the original image.}}{1033}{figure.caption.2148}\protected@file@percent }
\newlabel{fig:chapter20_dalle2_reconstruction_clip_latents}{{20.87}{1033}{\textbf {Reconstructions from truncated CLIP embeddings.} Each row reconstructs an image from a version of its CLIP embedding projected into a subset of PCA components. As more dimensions are retained, visual fidelity improves. Rightmost column shows the original image}{figure.caption.2148}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.88}{\ignorespaces \textbf  {Semantic variations from CLIP embeddings.} Multiple outputs from the decoder using the same image embedding with different noise seeds. Style and fine-grained details vary while core semantic features (e.g., clock, strokes, color gradients) are preserved.}}{1034}{figure.caption.2149}\protected@file@percent }
\newlabel{fig:chapter20_dalle2_variations_by_clip_encoding}{{20.88}{1034}{\textbf {Semantic variations from CLIP embeddings.} Multiple outputs from the decoder using the same image embedding with different noise seeds. Style and fine-grained details vary while core semantic features (e.g., clock, strokes, color gradients) are preserved}{figure.caption.2149}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.89}{\ignorespaces \textbf  {Interpolation between CLIP image embeddings.} Interpolated vectors in the CLIP embedding space generate images that blend structural and stylistic aspects from two inputs. Each row fixes the decoder noise seed.}}{1035}{figure.caption.2150}\protected@file@percent }
\newlabel{fig:chapter20_dalle2_interpolation_between_codes}{{20.89}{1035}{\textbf {Interpolation between CLIP image embeddings.} Interpolated vectors in the CLIP embedding space generate images that blend structural and stylistic aspects from two inputs. Each row fixes the decoder noise seed}{figure.caption.2150}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.90}{\ignorespaces \textbf  {Text-based image editing via CLIP latent arithmetic.} Rows show gradual edits by interpolating between a reference image embedding and a direction defined by CLIP text embeddings. DDIM inversion ensures a faithful reconstruction of the source.}}{1036}{figure.caption.2151}\protected@file@percent }
\newlabel{fig:chapter20_dalle2_clip_image_embeddings_interpolation}{{20.90}{1036}{\textbf {Text-based image editing via CLIP latent arithmetic.} Rows show gradual edits by interpolating between a reference image embedding and a direction defined by CLIP text embeddings. DDIM inversion ensures a faithful reconstruction of the source}{figure.caption.2151}{}}
\abx@aux@cite{0}{ramesh2022_dalle2}
\abx@aux@segm{0}{0}{ramesh2022_dalle2}
\abx@aux@cite{0}{ramesh2022_dalle2}
\abx@aux@segm{0}{0}{ramesh2022_dalle2}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\abx@aux@cite{0}{goh2021_multimodal}
\abx@aux@segm{0}{0}{goh2021_multimodal}
\abx@aux@cite{0}{zhou2022_prompt}
\abx@aux@segm{0}{0}{zhou2022_prompt}
\@writefile{toc}{\contentsline {paragraph}{Robustness and Generalization of the Decoder}{1037}{section*.2152}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.91}{\ignorespaces \textbf  {Typographic attacks and decoder robustness.} Despite misleading visual tokens (e.g., text overlays), the decoder can still produce correct samples (e.g., apples) when conditioned on misleading CLIP embeddings. This suggests a degree of semantic resilience inherited from the latent space, though susceptibility to adversarial perturbations remains a concern. Figure adapted from \blx@tocontentsinit {0}\cite {ramesh2022_dalle2}.}}{1037}{figure.caption.2153}\protected@file@percent }
\abx@aux@backref{1066}{ramesh2022_dalle2}{0}{1037}{1037}
\newlabel{fig:chapter20_dalle2_typographic_attacks}{{20.91}{1037}{\textbf {Typographic attacks and decoder robustness.} Despite misleading visual tokens (e.g., text overlays), the decoder can still produce correct samples (e.g., apples) when conditioned on misleading CLIP embeddings. This suggests a degree of semantic resilience inherited from the latent space, though susceptibility to adversarial perturbations remains a concern. Figure adapted from \cite {ramesh2022_dalle2}}{figure.caption.2153}{}}
\abx@aux@backref{1067}{radford2021_clip}{0}{1037}{1037}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\abx@aux@cite{0}{schuhmann2021_laion}
\abx@aux@segm{0}{0}{schuhmann2021_laion}
\abx@aux@cite{0}{goh2021_multimodal}
\abx@aux@segm{0}{0}{goh2021_multimodal}
\abx@aux@cite{0}{zhou2022_prompt}
\abx@aux@segm{0}{0}{zhou2022_prompt}
\abx@aux@backref{1068}{goh2021_multimodal}{0}{1038}{1038}
\abx@aux@backref{1069}{zhou2022_prompt}{0}{1038}{1038}
\@writefile{toc}{\contentsline {paragraph}{Dataset Construction and Semantic Pretraining}{1038}{section*.2154}\protected@file@percent }
\abx@aux@backref{1070}{radford2021_clip}{0}{1038}{1038}
\abx@aux@backref{1071}{schuhmann2021_laion}{0}{1038}{1038}
\abx@aux@backref{1072}{goh2021_multimodal}{0}{1038}{1038}
\abx@aux@backref{1073}{zhou2022_prompt}{0}{1038}{1038}
\abx@aux@cite{0}{nichol2022_glide}
\abx@aux@segm{0}{0}{nichol2022_glide}
\abx@aux@cite{0}{ramesh2022_dalle2}
\abx@aux@segm{0}{0}{ramesh2022_dalle2}
\abx@aux@cite{0}{ramesh2022_dalle2}
\abx@aux@segm{0}{0}{ramesh2022_dalle2}
\@writefile{toc}{\contentsline {paragraph}{Image Quality and Diversity: Qualitative and Quantitative Results}{1039}{section*.2155}\protected@file@percent }
\abx@aux@backref{1074}{nichol2022_glide}{0}{1039}{1039}
\@writefile{lof}{\contentsline {figure}{\numberline {20.92}{\ignorespaces  \textbf  {Zero-shot generation on MS-COCO prompts.} DALL$\cdot $E~2 generates high-fidelity images that surpass prior models in semantic alignment and detail preservation, despite no supervised training on the target distribution. Figure adapted from \blx@tocontentsinit {0}\cite {ramesh2022_dalle2}. }}{1039}{figure.caption.2156}\protected@file@percent }
\abx@aux@backref{1076}{ramesh2022_dalle2}{0}{1039}{1039}
\newlabel{fig:chapter20_dalle2_random_samples}{{20.92}{1039}{\textbf {Zero-shot generation on MS-COCO prompts.} DALL$\cdot $E~2 generates high-fidelity images that surpass prior models in semantic alignment and detail preservation, despite no supervised training on the target distribution. Figure adapted from \cite {ramesh2022_dalle2}}{figure.caption.2156}{}}
\abx@aux@cite{0}{ramesh2022_dalle2}
\abx@aux@segm{0}{0}{ramesh2022_dalle2}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\abx@aux@cite{0}{ramesh2022_dalle2}
\abx@aux@segm{0}{0}{ramesh2022_dalle2}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\@writefile{toc}{\contentsline {paragraph}{Design Limitations and Architectural Tradeoffs}{1040}{section*.2157}\protected@file@percent }
\abx@aux@backref{1077}{ramesh2022_dalle2}{0}{1040}{1040}
\abx@aux@backref{1078}{radford2021_clip}{0}{1040}{1040}
\@writefile{toc}{\contentsline {paragraph}{Stepping Towards Latent Diffusion Models}{1040}{section*.2158}\protected@file@percent }
\abx@aux@backref{1079}{ramesh2022_dalle2}{0}{1040}{1040}
\abx@aux@backref{1080}{rombach2022_ldm}{0}{1040}{1040}
\BKM@entry{id=784,dest={73656374696F6E2A2E32313539},srcline={10696}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030315C3030302E5C303030345C3030303A5C3030305C3034305C3030304C5C303030615C303030745C303030655C3030306E5C303030745C3030305C3034305C303030445C303030695C303030665C303030665C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030735C3030305C3034305C3030305C3035305C3030304C5C303030445C3030304D5C303030735C3030305C303531}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.11.4: Latent Diffusion Models (LDMs)}{1042}{section*.2159}\protected@file@percent }
\newlabel{enr:chapter20_ldm}{{20.11.4}{1042}{\color {ocre}Enrichment \thesubsection : Latent Diffusion Models (LDMs)}{section*.2159}{}}
\@writefile{toc}{\contentsline {paragraph}{Overview and Conceptual Shift}{1042}{section*.2160}\protected@file@percent }
\abx@aux@backref{1081}{rombach2022_ldm}{0}{1042}{1042}
\@writefile{toc}{\contentsline {paragraph}{Autoencoder Architecture and Training Objective}{1042}{section*.2161}\protected@file@percent }
\abx@aux@backref{1082}{rombach2022_ldm}{0}{1042}{1042}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\@writefile{lof}{\contentsline {figure}{\numberline {20.93}{\ignorespaces  \textbf  {Latent Diffusion Model architecture overview}~\blx@tocontentsinit {0}\cite {rombach2022_ldm}. LDMs operate in a learned latent space \( \mathcal  {Z} \), obtained via a pretrained autoencoder. Conditioning (e.g., on text) is supported either via concatenation or through cross-attention layers within the denoising U-Net. \emph  {Figure adapted from the original paper (Fig.~3).} }}{1043}{figure.caption.2162}\protected@file@percent }
\abx@aux@backref{1084}{rombach2022_ldm}{0}{1043}{1043}
\newlabel{fig:chapter20_ldm_architecture}{{20.93}{1043}{\textbf {Latent Diffusion Model architecture overview}~\cite {rombach2022_ldm}. LDMs operate in a learned latent space \( \mathcal {Z} \), obtained via a pretrained autoencoder. Conditioning (e.g., on text) is supported either via concatenation or through cross-attention layers within the denoising U-Net. \emph {Figure adapted from the original paper (Fig.~3).}}{figure.caption.2162}{}}
\@writefile{toc}{\contentsline {paragraph}{Autoencoder Architecture and Latent Normalization}{1044}{section*.2163}\protected@file@percent }
\abx@aux@backref{1085}{rombach2022_ldm}{0}{1044}{1044}
\@writefile{toc}{\contentsline {subparagraph}{Encoder and Decoder Design}{1044}{subparagraph*.2164}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Latent Normalization for Diffusion Compatibility}{1044}{subparagraph*.2165}\protected@file@percent }
\abx@aux@cite{0}{ho2020_ddpm}
\abx@aux@segm{0}{0}{ho2020_ddpm}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\@writefile{toc}{\contentsline {paragraph}{Denoising Diffusion in Latent Space}{1045}{section*.2166}\protected@file@percent }
\abx@aux@backref{1086}{ho2020_ddpm}{0}{1045}{1045}
\@writefile{toc}{\contentsline {subparagraph}{Architecture of the Denoising U-Net}{1045}{subparagraph*.2167}\protected@file@percent }
\abx@aux@backref{1087}{rombach2022_ldm}{0}{1045}{1045}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\abx@aux@cite{0}{ramesh2022_dalle2}
\abx@aux@segm{0}{0}{ramesh2022_dalle2}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.11.4.1: Decoder Fidelity Without Explicit Text Conditioning}{1047}{section*.2168}\protected@file@percent }
\newlabel{enr:chapter20_ldm_decoder_limitations}{{20.11.4.1}{1047}{\color {ocre}Enrichment \thesubsubsection : Decoder Fidelity Without Explicit Text Conditioning}{section*.2168}{}}
\abx@aux@backref{1088}{rombach2022_ldm}{0}{1047}{1047}
\@writefile{toc}{\contentsline {paragraph}{Why It Still Works}{1047}{section*.2169}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Trade-offs and Alternatives}{1047}{section*.2170}\protected@file@percent }
\abx@aux@backref{1089}{ramesh2022_dalle2}{0}{1047}{1047}
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{1047}{section*.2171}\protected@file@percent }
\abx@aux@cite{0}{ho2022_classifierfree}
\abx@aux@segm{0}{0}{ho2022_classifierfree}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\@writefile{toc}{\contentsline {paragraph}{Classifier-Free Guidance (CFG)}{1048}{section*.2172}\protected@file@percent }
\abx@aux@backref{1090}{ho2022_classifierfree}{0}{1048}{1048}
\@writefile{toc}{\contentsline {paragraph}{Empirical Results and Ablations}{1048}{section*.2173}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.94}{\ignorespaces  \textbf  {Text-guided object removal using an LDM inpainting model}~\blx@tocontentsinit {0}\cite {rombach2022_ldm}. The model receives a binary mask and a natural language prompt and fills in plausible structure matching the surrounding scene. \emph  {Figure adapted from the original paper (Fig.~11).} }}{1048}{figure.caption.2174}\protected@file@percent }
\abx@aux@backref{1092}{rombach2022_ldm}{0}{1048}{1048}
\newlabel{fig:chapter20_ldm_object_removal}{{20.94}{1048}{\textbf {Text-guided object removal using an LDM inpainting model}~\cite {rombach2022_ldm}. The model receives a binary mask and a natural language prompt and fills in plausible structure matching the surrounding scene. \emph {Figure adapted from the original paper (Fig.~11).}}{figure.caption.2174}{}}
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\@writefile{toc}{\contentsline {paragraph}{Limitations and Transition to Newer Works Like \emph  {Imagen}}{1049}{section*.2175}\protected@file@percent }
\abx@aux@backref{1093}{saharia2022_imagen}{0}{1049}{1049}
\BKM@entry{id=785,dest={73656374696F6E2A2E32313736},srcline={11010}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030315C3030302E5C303030355C3030303A5C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030306E5C3030303A5C3030305C3034305C303030535C303030635C303030615C3030306C5C303030695C3030306E5C303030675C3030305C3034305C3030304C5C303030615C3030306E5C303030675C303030755C303030615C303030675C303030655C3030305C3034305C303030465C303030695C303030645C303030655C3030306C5C303030695C303030745C303030795C3030305C3034305C303030695C3030306E5C3030305C3034305C303030545C303030655C303030785C303030745C303030325C303030495C3030306D5C303030675C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\abx@aux@cite{0}{raffel2020_t5}
\abx@aux@segm{0}{0}{raffel2020_t5}
\abx@aux@cite{0}{yu2022_parti}
\abx@aux@segm{0}{0}{yu2022_parti}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.11.5: Imagen: Scaling Language Fidelity in Text2Img Models}{1050}{section*.2176}\protected@file@percent }
\newlabel{subsec:chapter20_imagen}{{20.11.5}{1050}{\color {ocre}Enrichment \thesubsection : Imagen: Scaling Language Fidelity in Text2Img Models}{section*.2176}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation and Context}{1050}{section*.2177}\protected@file@percent }
\abx@aux@backref{1094}{rombach2022_ldm}{0}{1050}{1050}
\abx@aux@backref{1095}{saharia2022_imagen}{0}{1050}{1050}
\abx@aux@backref{1096}{raffel2020_t5}{0}{1050}{1050}
\abx@aux@backref{1097}{yu2022_parti}{0}{1050}{1050}
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\@writefile{toc}{\contentsline {subsubsection}{Cascaded Diffusion Pipeline}{1051}{section*.2178}\protected@file@percent }
\newlabel{subsubsec:chapter20_imagen_cascade}{{20.11.5}{1051}{Cascaded Diffusion Pipeline}{section*.2178}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.95}{\ignorespaces  \textbf  {Visualization of the Imagen architecture}~\blx@tocontentsinit {0}\cite {saharia2022_imagen}. A frozen T5-XXL encoder processes the input prompt into a fixed text embedding. A base diffusion model generates a \( 64 \times 64 \) image, which is then upsampled to \( 1024 \times 1024 \) in two SR stages. Each model is trained independently. \emph  {Figure adapted from the original paper}. }}{1051}{figure.caption.2179}\protected@file@percent }
\abx@aux@backref{1099}{saharia2022_imagen}{0}{1051}{1051}
\newlabel{fig:chapter20_imagen_architecture}{{20.95}{1051}{\textbf {Visualization of the Imagen architecture}~\cite {saharia2022_imagen}. A frozen T5-XXL encoder processes the input prompt into a fixed text embedding. A base diffusion model generates a \( 64 \times 64 \) image, which is then upsampled to \( 1024 \times 1024 \) in two SR stages. Each model is trained independently. \emph {Figure adapted from the original paper}}{figure.caption.2179}{}}
\@writefile{toc}{\contentsline {subsubsection}{Classifier-Free Guidance and Dynamic Thresholding}{1052}{section*.2180}\protected@file@percent }
\newlabel{subsubsec:chapter20_imagen_cfg}{{20.11.5}{1052}{Classifier-Free Guidance and Dynamic Thresholding}{section*.2180}{}}
\@writefile{toc}{\contentsline {paragraph}{Problem: Oversaturation from Large Guidance}{1052}{section*.2181}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Naïve Solution: Static Thresholding}{1052}{section*.2182}\protected@file@percent }
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\@writefile{toc}{\contentsline {paragraph}{Dynamic Thresholding: an Adaptive Alternative to Static Clipping}{1053}{section*.2183}\protected@file@percent }
\abx@aux@backref{1100}{saharia2022_imagen}{0}{1053}{1053}
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\@writefile{lof}{\contentsline {figure}{\numberline {20.96}{\ignorespaces  \textbf  {Comparison of thresholding strategies under high CFG weights}~\blx@tocontentsinit {0}\cite {saharia2022_imagen}. Static clipping (middle) removes extreme values but can oversaturate or flatten images. Dynamic thresholding (bottom) scales predictions adaptively, preserving more detail while preventing distortions. \emph  {Figure adapted from the original paper}. }}{1054}{figure.caption.2184}\protected@file@percent }
\abx@aux@backref{1102}{saharia2022_imagen}{0}{1054}{1054}
\newlabel{fig:chapter20_imagen_thresholding}{{20.96}{1054}{\textbf {Comparison of thresholding strategies under high CFG weights}~\cite {saharia2022_imagen}. Static clipping (middle) removes extreme values but can oversaturate or flatten images. Dynamic thresholding (bottom) scales predictions adaptively, preserving more detail while preventing distortions. \emph {Figure adapted from the original paper}}{figure.caption.2184}{}}
\@writefile{toc}{\contentsline {subsubsection}{Experimental Findings and DrawBench Evaluation}{1054}{section*.2185}\protected@file@percent }
\newlabel{subsubsec:chapter20_imagen_findings}{{20.11.5}{1054}{Experimental Findings and DrawBench Evaluation}{section*.2185}{}}
\@writefile{toc}{\contentsline {paragraph}{Scaling the Text Encoder}{1054}{section*.2186}\protected@file@percent }
\abx@aux@backref{1103}{saharia2022_imagen}{0}{1054}{1054}
\@writefile{lof}{\contentsline {figure}{\numberline {20.97}{\ignorespaces  \textbf  {Imagen ablation results}~\blx@tocontentsinit {0}\cite {saharia2022_imagen}. Scaling the text encoder improves image-text alignment (left) and perceptual quality (right) more effectively than scaling the diffusion model. Classifier-free guidance values are swept along the Pareto curves. \emph  {Adapted from the original paper}. }}{1054}{figure.caption.2187}\protected@file@percent }
\abx@aux@backref{1105}{saharia2022_imagen}{0}{1054}{1054}
\newlabel{fig:chapter20_imagen_findings}{{20.97}{1054}{\textbf {Imagen ablation results}~\cite {saharia2022_imagen}. Scaling the text encoder improves image-text alignment (left) and perceptual quality (right) more effectively than scaling the diffusion model. Classifier-free guidance values are swept along the Pareto curves. \emph {Adapted from the original paper}}{figure.caption.2187}{}}
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\@writefile{toc}{\contentsline {paragraph}{DrawBench: A Diverse Prompt Evaluation Suite}{1055}{section*.2188}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.98}{\ignorespaces  \textbf  {Human preference results on DrawBench}~\blx@tocontentsinit {0}\cite {saharia2022_imagen}. Imagen outperforms prior models—including DALL$\cdot $E~2, GLIDE, and Latent Diffusion—in both text-image alignment and visual fidelity across 200 prompts. \emph  {Figure adapted from the original paper}. }}{1055}{figure.caption.2189}\protected@file@percent }
\abx@aux@backref{1107}{saharia2022_imagen}{0}{1055}{1055}
\newlabel{fig:chapter20_imagen_drawbench_comparison}{{20.98}{1055}{\textbf {Human preference results on DrawBench}~\cite {saharia2022_imagen}. Imagen outperforms prior models—including DALL$\cdot $E~2, GLIDE, and Latent Diffusion—in both text-image alignment and visual fidelity across 200 prompts. \emph {Figure adapted from the original paper}}{figure.caption.2189}{}}
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\abx@aux@cite{0}{saharia2022_imagen}
\abx@aux@segm{0}{0}{saharia2022_imagen}
\abx@aux@cite{0}{ramesh2022_dalle2}
\abx@aux@segm{0}{0}{ramesh2022_dalle2}
\@writefile{toc}{\contentsline {paragraph}{Qualitative Samples}{1056}{section*.2190}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.99}{\ignorespaces  \textbf  {Photorealistic samples from Imagen}~\blx@tocontentsinit {0}\cite {saharia2022_imagen}. The model handles fine-grained semantics (e.g., ``a dragon fruit wearing a karate belt in the snow'') and imaginative compositions (e.g., ``a cute corgi lives in a house made of sushi'') with high fidelity. \emph  {Figure adapted from the original paper}. }}{1056}{figure.caption.2191}\protected@file@percent }
\abx@aux@backref{1109}{saharia2022_imagen}{0}{1056}{1056}
\newlabel{fig:chapter20_imagen_examples}{{20.99}{1056}{\textbf {Photorealistic samples from Imagen}~\cite {saharia2022_imagen}. The model handles fine-grained semantics (e.g., ``a dragon fruit wearing a karate belt in the snow'') and imaginative compositions (e.g., ``a cute corgi lives in a house made of sushi'') with high fidelity. \emph {Figure adapted from the original paper}}{figure.caption.2191}{}}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.11.5.1: Toward Fine-Grained Control and Editable Generation}{1056}{section*.2192}\protected@file@percent }
\newlabel{enr:chapter20_editable_generation}{{20.11.5.1}{1056}{\color {ocre}Enrichment \thesubsubsection : Toward Fine-Grained Control and Editable Generation}{section*.2192}{}}
\@writefile{toc}{\contentsline {paragraph}{From Fidelity to Controllability}{1056}{section*.2193}\protected@file@percent }
\abx@aux@backref{1110}{saharia2022_imagen}{0}{1056}{1056}
\abx@aux@backref{1111}{ramesh2022_dalle2}{0}{1056}{1056}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\abx@aux@cite{0}{zhou2024_transfusion}
\abx@aux@segm{0}{0}{zhou2024_transfusion}
\@writefile{toc}{\contentsline {paragraph}{Why Prompt-Aware Attention Control Is Needed}{1057}{section*.2194}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Key Approaches and Innovations}{1057}{section*.2195}\protected@file@percent }
\abx@aux@backref{1112}{hertz2022_prompt2prompt}{0}{1057}{1057}
\abx@aux@backref{1113}{ruiz2023_dreambooth}{0}{1057}{1057}
\abx@aux@backref{1114}{zhang2023_controlnet}{0}{1057}{1057}
\abx@aux@backref{1115}{ye2023_ipadapter}{0}{1057}{1057}
\abx@aux@backref{1116}{zhou2024_transfusion}{0}{1057}{1057}
\BKM@entry{id=786,dest={73656374696F6E2A2E32313936},srcline={11296}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030315C3030302E5C303030365C3030303A5C3030305C3034305C303030505C303030725C3030306F5C3030306D5C303030705C303030745C3030302D5C303030745C3030306F5C3030302D5C303030505C303030725C3030306F5C3030306D5C303030705C303030745C3030305C3034305C3030305C3035305C303030505C303030325C303030505C3030305C3035315C3030303A5C3030305C3034305C303030435C303030725C3030306F5C303030735C303030735C3030302D5C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030455C303030645C303030695C303030745C303030695C3030306E5C303030675C3030305C3034305C303030695C3030306E5C3030305C3034305C303030445C3030304D5C30303073}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.11.6: Prompt-to-Prompt (P2P): Cross-Attention Editing in DMs}{1058}{section*.2196}\protected@file@percent }
\newlabel{enr:chapter20_prompt_to_prompt}{{20.11.6}{1058}{\color {ocre}Enrichment \thesubsection : Prompt-to-Prompt (P2P): Cross-Attention Editing in DMs}{section*.2196}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation and Core Insight}{1058}{section*.2197}\protected@file@percent }
\abx@aux@backref{1117}{hertz2022_prompt2prompt}{0}{1058}{1058}
\@writefile{lof}{\contentsline {figure}{\numberline {20.100}{\ignorespaces  \textbf  {Prompt-to-Prompt editing capabilities}~\blx@tocontentsinit {0}\cite {hertz2022_prompt2prompt}. The method enables fine-grained modifications by editing text prompts and guiding the diffusion process via attention control. Examples include adjective reweighting (top-left), object replacement (top-right), style editing (bottom-left), and progressive prompt refinement (bottom-right). }}{1058}{figure.caption.2198}\protected@file@percent }
\abx@aux@backref{1119}{hertz2022_prompt2prompt}{0}{1058}{1058}
\newlabel{fig:chapter20_p2p_examples}{{20.100}{1058}{\textbf {Prompt-to-Prompt editing capabilities}~\cite {hertz2022_prompt2prompt}. The method enables fine-grained modifications by editing text prompts and guiding the diffusion process via attention control. Examples include adjective reweighting (top-left), object replacement (top-right), style editing (bottom-left), and progressive prompt refinement (bottom-right)}{figure.caption.2198}{}}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\@writefile{toc}{\contentsline {subparagraph}{Cross-Attention as the Mechanism for Prompt Influence}{1059}{subparagraph*.2199}\protected@file@percent }
\newlabel{subsec:chapter20_p2p_cross_attention}{{20.11.6}{1059}{Cross-Attention as the Mechanism for Prompt Influence}{subparagraph*.2199}{}}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\@writefile{lof}{\contentsline {figure}{\numberline {20.101}{\ignorespaces  \textbf  {Cross-attention maps in text-to-image diffusion models}~\blx@tocontentsinit {0}\cite {hertz2022_prompt2prompt}. \emph  {Top row:} average cross-attention maps for each word in the prompt that generated the image shown on the left, aggregated across timesteps and layers. These maps visualize the typical spatial influence of each token throughout the diffusion process. \emph  {Bottom rows:} temporal attention maps at selected denoising steps, focusing on the tokens ``bear'' and ``bird''. Early in the denoising process, attention maps are diffuse and spatially ambiguous, while later steps exhibit sharper, more localized influence, revealing how semantic concepts gradually consolidate into precise spatial regions. This temporal evolution illustrates the emergence of spatial grounding in cross-attention and underpins the feasibility of attention-based control mechanisms like Prompt-to-Prompt. }}{1060}{figure.caption.2200}\protected@file@percent }
\abx@aux@backref{1121}{hertz2022_prompt2prompt}{0}{1060}{1060}
\newlabel{fig:chapter20_p2p_avg_attention_maps}{{20.101}{1060}{\textbf {Cross-attention maps in text-to-image diffusion models}~\cite {hertz2022_prompt2prompt}. \emph {Top row:} average cross-attention maps for each word in the prompt that generated the image shown on the left, aggregated across timesteps and layers. These maps visualize the typical spatial influence of each token throughout the diffusion process. \emph {Bottom rows:} temporal attention maps at selected denoising steps, focusing on the tokens ``bear'' and ``bird''. Early in the denoising process, attention maps are diffuse and spatially ambiguous, while later steps exhibit sharper, more localized influence, revealing how semantic concepts gradually consolidate into precise spatial regions. This temporal evolution illustrates the emergence of spatial grounding in cross-attention and underpins the feasibility of attention-based control mechanisms like Prompt-to-Prompt}{figure.caption.2200}{}}
\@writefile{toc}{\contentsline {subparagraph}{Editing by Cross-Attention Injection}{1060}{subparagraph*.2201}\protected@file@percent }
\newlabel{subsec:chapter20_p2p_injection}{{20.11.6}{1060}{Editing by Cross-Attention Injection}{subparagraph*.2201}{}}
\abx@aux@backref{1122}{hertz2022_prompt2prompt}{0}{1060}{1060}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\@writefile{lof}{\contentsline {figure}{\numberline {20.102}{\ignorespaces  \textbf  {Prompt-to-Prompt method overview}~\blx@tocontentsinit {0}\cite {hertz2022_prompt2prompt}. Top: input prompt is embedded and fused with image features through cross-attention layers that produce one attention map per word. Bottom: for editing, Prompt-to-Prompt injects cross-attention maps \( M_t \) from the original prompt into the generation process of the edited prompt. This enables semantic manipulations such as word replacement, addition, or style transfer, while preserving spatial layout and object coherence. }}{1064}{figure.caption.2202}\protected@file@percent }
\abx@aux@backref{1124}{hertz2022_prompt2prompt}{0}{1064}{1064}
\newlabel{fig:chapter20_p2p_high_level_method}{{20.102}{1064}{\textbf {Prompt-to-Prompt method overview}~\cite {hertz2022_prompt2prompt}. Top: input prompt is embedded and fused with image features through cross-attention layers that produce one attention map per word. Bottom: for editing, Prompt-to-Prompt injects cross-attention maps \( M_t \) from the original prompt into the generation process of the edited prompt. This enables semantic manipulations such as word replacement, addition, or style transfer, while preserving spatial layout and object coherence}{figure.caption.2202}{}}
\@writefile{toc}{\contentsline {subparagraph}{Use Case: Content Modifications via Prompt Edits}{1064}{subparagraph*.2203}\protected@file@percent }
\newlabel{subsec:chapter20_p2p_content_modification}{{20.11.6}{1064}{Use Case: Content Modifications via Prompt Edits}{subparagraph*.2203}{}}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\@writefile{lof}{\contentsline {figure}{\numberline {20.103}{\ignorespaces  \textbf  {Content modification through attention injection}~\blx@tocontentsinit {0}\cite {hertz2022_prompt2prompt}. An original image generated from the prompt “lemon cake” is edited by modifying the object type in the prompt. Top row: Prompt-to-Prompt preserves attention maps for shared words, yielding structurally consistent variations. Bottom row: Only the random seed is reused, resulting in less coherent object geometry and structure. }}{1065}{figure.caption.2204}\protected@file@percent }
\abx@aux@backref{1126}{hertz2022_prompt2prompt}{0}{1065}{1065}
\newlabel{fig:chapter20_p2p_content_modification}{{20.103}{1065}{\textbf {Content modification through attention injection}~\cite {hertz2022_prompt2prompt}. An original image generated from the prompt “lemon cake” is edited by modifying the object type in the prompt. Top row: Prompt-to-Prompt preserves attention maps for shared words, yielding structurally consistent variations. Bottom row: Only the random seed is reused, resulting in less coherent object geometry and structure}{figure.caption.2204}{}}
\@writefile{toc}{\contentsline {subparagraph}{Use Case: Object Preservation Across Scene Changes}{1065}{subparagraph*.2205}\protected@file@percent }
\newlabel{subsec:chapter20_p2p_object_preservation}{{20.11.6}{1065}{Use Case: Object Preservation Across Scene Changes}{subparagraph*.2205}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.104}{\ignorespaces  \textbf  {Preserving object structure through selective attention injection}~\blx@tocontentsinit {0}\cite {hertz2022_prompt2prompt}. The attention maps for the token “butterfly” are injected from the original image (top-left) into edited prompts. While the background and surrounding context change, the butterfly’s appearance and spatial configuration remain consistent, highlighting Prompt-to-Prompt’s ability to localize and preserve selected visual elements. }}{1066}{figure.caption.2206}\protected@file@percent }
\abx@aux@backref{1128}{hertz2022_prompt2prompt}{0}{1066}{1066}
\newlabel{fig:chapter20_p2p_object_preservation}{{20.104}{1066}{\textbf {Preserving object structure through selective attention injection}~\cite {hertz2022_prompt2prompt}. The attention maps for the token “butterfly” are injected from the original image (top-left) into edited prompts. While the background and surrounding context change, the butterfly’s appearance and spatial configuration remain consistent, highlighting Prompt-to-Prompt’s ability to localize and preserve selected visual elements}{figure.caption.2206}{}}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\@writefile{toc}{\contentsline {subparagraph}{Use Case: Controlled Blending via Partial Attention Injection}{1067}{subparagraph*.2207}\protected@file@percent }
\newlabel{subsec:chapter20_p2p_partial_injection}{{20.11.6}{1067}{Use Case: Controlled Blending via Partial Attention Injection}{subparagraph*.2207}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.105}{\ignorespaces  \textbf  {Blending source and target semantics through partial attention injection}~\blx@tocontentsinit {0}\cite {hertz2022_prompt2prompt}. Each example begins with an original image and prompt (top row). The prompt is edited by replacing one token (e.g., ``car'' \(\rightarrow \) ``bicycle''). In the rows below, cross-attention maps from the original prompt are injected into the edited generation for a growing portion of the denoising process—from 0\% (left) to 100\% (right). Low injection favors the edited prompt but may distort layout; high injection preserves the original structure but inhibits visual change. Intermediate levels yield blended results. }}{1067}{figure.caption.2208}\protected@file@percent }
\abx@aux@backref{1130}{hertz2022_prompt2prompt}{0}{1067}{1067}
\newlabel{fig:chapter20_p2p_attention_injection_examples}{{20.105}{1067}{\textbf {Blending source and target semantics through partial attention injection}~\cite {hertz2022_prompt2prompt}. Each example begins with an original image and prompt (top row). The prompt is edited by replacing one token (e.g., ``car'' \(\rightarrow \) ``bicycle''). In the rows below, cross-attention maps from the original prompt are injected into the edited generation for a growing portion of the denoising process—from 0\% (left) to 100\% (right). Low injection favors the edited prompt but may distort layout; high injection preserves the original structure but inhibits visual change. Intermediate levels yield blended results}{figure.caption.2208}{}}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\@writefile{toc}{\contentsline {subparagraph}{Use Case: Emphasizing and De-emphasizing Concepts}{1068}{subparagraph*.2209}\protected@file@percent }
\newlabel{subsec:chapter20_p2p_emphasis}{{20.11.6}{1068}{Use Case: Emphasizing and De-emphasizing Concepts}{subparagraph*.2209}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.106}{\ignorespaces  \textbf  {Controlling emphasis via cross-attention scaling}~\blx@tocontentsinit {0}\cite {hertz2022_prompt2prompt}. Top: Reducing cross-attention for selected words (e.g., “blossom”) softens their visual presence. Bottom: Increasing attention weight (e.g., for “snowy” or “fluffy”) amplifies the visual attributes tied to that token. }}{1068}{figure.caption.2210}\protected@file@percent }
\abx@aux@backref{1132}{hertz2022_prompt2prompt}{0}{1068}{1068}
\newlabel{fig:chapter20_p2p_emphasis}{{20.106}{1068}{\textbf {Controlling emphasis via cross-attention scaling}~\cite {hertz2022_prompt2prompt}. Top: Reducing cross-attention for selected words (e.g., “blossom”) softens their visual presence. Bottom: Increasing attention weight (e.g., for “snowy” or “fluffy”) amplifies the visual attributes tied to that token}{figure.caption.2210}{}}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\@writefile{toc}{\contentsline {subparagraph}{Use Case: Text-Guided Stylization while Preserving Layout}{1069}{subparagraph*.2211}\protected@file@percent }
\newlabel{subsec:chapter20_p2p_stylization}{{20.11.6}{1069}{Use Case: Text-Guided Stylization while Preserving Layout}{subparagraph*.2211}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.107}{\ignorespaces  \textbf  {Prompt-based image stylization with structural consistency}~\blx@tocontentsinit {0}\cite {hertz2022_prompt2prompt}. Top: converting a sketch or drawing into realistic photographs under various stylistic prompts (e.g., ``a relaxing photo'', ``a dramatic photo''). Bottom: transforming a real photo into stylized renderings using art-related descriptors (e.g., ``charcoal sketch'', ``impressionist painting'', ``neo-classical style''). In all cases, Prompt-to-Prompt preserves spatial layout by injecting source attention maps while allowing the new style tokens to influence appearance. }}{1069}{figure.caption.2212}\protected@file@percent }
\abx@aux@backref{1134}{hertz2022_prompt2prompt}{0}{1069}{1069}
\newlabel{fig:chapter20_p2p_stylization}{{20.107}{1069}{\textbf {Prompt-based image stylization with structural consistency}~\cite {hertz2022_prompt2prompt}. Top: converting a sketch or drawing into realistic photographs under various stylistic prompts (e.g., ``a relaxing photo'', ``a dramatic photo''). Bottom: transforming a real photo into stylized renderings using art-related descriptors (e.g., ``charcoal sketch'', ``impressionist painting'', ``neo-classical style''). In all cases, Prompt-to-Prompt preserves spatial layout by injecting source attention maps while allowing the new style tokens to influence appearance}{figure.caption.2212}{}}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\@writefile{toc}{\contentsline {subparagraph}{Use Case: Editing Real Images via Inversion and Prompt-to-Prompt}{1070}{subparagraph*.2213}\protected@file@percent }
\newlabel{subsec:chapter20_p2p_real_image_editing}{{20.11.6}{1070}{Use Case: Editing Real Images via Inversion and Prompt-to-Prompt}{subparagraph*.2213}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.108}{\ignorespaces  \textbf  {Prompt-based editing of real images}. Left: Real photos are inverted into latent noise vectors using DDIM inversion. Right: Edited versions are generated using Prompt-to-Prompt by modifying the prompt and injecting attention maps as needed. Figure adapted from \blx@tocontentsinit {0}\cite {hertz2022_prompt2prompt}. }}{1070}{figure.caption.2214}\protected@file@percent }
\abx@aux@backref{1136}{hertz2022_prompt2prompt}{0}{1070}{1070}
\newlabel{fig:chapter20_p2p_real_image_editing}{{20.108}{1070}{\textbf {Prompt-based editing of real images}. Left: Real photos are inverted into latent noise vectors using DDIM inversion. Right: Edited versions are generated using Prompt-to-Prompt by modifying the prompt and injecting attention maps as needed. Figure adapted from \cite {hertz2022_prompt2prompt}}{figure.caption.2214}{}}
\@writefile{toc}{\contentsline {subparagraph}{Limitations and Transition to Personalized Editing}{1070}{subparagraph*.2215}\protected@file@percent }
\newlabel{subsec:chapter20_p2p_limitations}{{20.11.6}{1070}{Limitations and Transition to Personalized Editing}{subparagraph*.2215}{}}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\abx@aux@backref{1137}{ruiz2023_dreambooth}{0}{1071}{1071}
\BKM@entry{id=787,dest={73656374696F6E2A2E32323136},srcline={11775}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030315C3030302E5C303030375C3030303A5C3030305C3034305C303030445C303030725C303030655C303030615C3030306D5C303030425C3030306F5C3030306F5C303030745C303030685C3030303A5C3030305C3034305C303030505C303030655C303030725C303030735C3030306F5C3030306E5C303030615C3030306C5C303030695C3030307A5C303030655C303030645C3030305C3034305C303030545C303030655C303030785C303030745C3030302D5C303030745C3030306F5C3030302D5C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.11.7: DreamBooth: Personalized Text-to-Image Generation}{1072}{section*.2216}\protected@file@percent }
\newlabel{enr:chapter20_dreambooth}{{20.11.7}{1072}{\color {ocre}Enrichment \thesubsection : DreamBooth: Personalized Text-to-Image Generation}{section*.2216}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation and Core Insight}{1072}{section*.2217}\protected@file@percent }
\abx@aux@backref{1138}{ruiz2023_dreambooth}{0}{1072}{1072}
\@writefile{lof}{\contentsline {figure}{\numberline {20.109}{\ignorespaces  \textbf  {DreamBooth enables subject-driven generation}. With only 3–5 images of a subject (left), DreamBooth fine-tunes a diffusion model to produce diverse outputs (right) via prompts like “a \texttt  {sks} dog in the Acropolis”. The results demonstrate consistent identity preservation across varying contexts, lighting, and articulation. Figure adapted from \blx@tocontentsinit {0}\cite {ruiz2023_dreambooth}. }}{1072}{figure.caption.2218}\protected@file@percent }
\abx@aux@backref{1140}{ruiz2023_dreambooth}{0}{1072}{1072}
\newlabel{fig:chapter20_dreambooth_example}{{20.109}{1072}{\textbf {DreamBooth enables subject-driven generation}. With only 3–5 images of a subject (left), DreamBooth fine-tunes a diffusion model to produce diverse outputs (right) via prompts like “a \texttt {sks} dog in the Acropolis”. The results demonstrate consistent identity preservation across varying contexts, lighting, and articulation. Figure adapted from \cite {ruiz2023_dreambooth}}{figure.caption.2218}{}}
\@writefile{toc}{\contentsline {subparagraph}{Model Setup and Identifier Creation}{1072}{subparagraph*.2219}\protected@file@percent }
\newlabel{subsec:chapter20_dreambooth_setup}{{20.11.7}{1072}{Model Setup and Identifier Creation}{subparagraph*.2219}{}}
\abx@aux@backref{1141}{ruiz2023_dreambooth}{0}{1072}{1072}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\@writefile{lof}{\contentsline {figure}{\numberline {20.110}{\ignorespaces \textbf  {DreamBooth finetuning process}. Given a few images of a subject (e.g., a specific dog), the model is trained on prompts like \texttt  {"a [V] dog"} to tie the unique token \texttt  {[V]} to the subject’s identity. Simultaneously, prompts like \texttt  {"a dog"} are used with unrelated samples from the same class to enforce intra-class diversity via prior-preservation loss. Figure adapted from \blx@tocontentsinit {0}\cite {ruiz2023_dreambooth}.}}{1074}{figure.caption.2220}\protected@file@percent }
\abx@aux@backref{1143}{ruiz2023_dreambooth}{0}{1074}{1074}
\newlabel{fig:chapter20_dreambooth_finetuning}{{20.110}{1074}{\textbf {DreamBooth finetuning process}. Given a few images of a subject (e.g., a specific dog), the model is trained on prompts like \texttt {"a [V] dog"} to tie the unique token \texttt {[V]} to the subject’s identity. Simultaneously, prompts like \texttt {"a dog"} are used with unrelated samples from the same class to enforce intra-class diversity via prior-preservation loss. Figure adapted from \cite {ruiz2023_dreambooth}}{figure.caption.2220}{}}
\newlabel{subsubsec:chapter20_dreambooth_token_selection}{{20.11.7}{1074}{Identifier Token Selection Strategy}{section*.2221}{}}
\newlabel{subsubsec:chapter20_dreambooth_tokenizer_overview}{{20.11.7}{1074}{Tokenizer Overview and Motivation}{section*.2222}{}}
\newlabel{subsubsec:chapter20_dreambooth_rare_token_selection}{{20.11.7}{1075}{Rare Token Selection for Subject Identity Binding}{section*.2223}{}}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\@writefile{toc}{\contentsline {subparagraph}{Training Objective and Prior Preservation}{1077}{subparagraph*.2224}\protected@file@percent }
\newlabel{subsec:chapter20_dreambooth_training}{{20.11.7}{1077}{Training Objective and Prior Preservation}{subparagraph*.2224}{}}
\@writefile{toc}{\contentsline {paragraph}{Main Loss: Denoising Objective}{1077}{section*.2225}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Preventing Overfitting: Prior Preservation Loss}{1077}{section*.2226}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.111}{\ignorespaces  \textbf  {Encouraging diversity with prior-preservation loss}. Without regularization (left), the model overfits to the subject’s training images, replicating pose and context. With prior preservation (right), the model generalizes across poses and settings while maintaining subject identity. Figure adapted from \blx@tocontentsinit {0}\cite {ruiz2023_dreambooth}. }}{1078}{figure.caption.2227}\protected@file@percent }
\abx@aux@backref{1145}{ruiz2023_dreambooth}{0}{1078}{1078}
\newlabel{fig:chapter20_dreambooth_prior_preservation}{{20.111}{1078}{\textbf {Encouraging diversity with prior-preservation loss}. Without regularization (left), the model overfits to the subject’s training images, replicating pose and context. With prior preservation (right), the model generalizes across poses and settings while maintaining subject identity. Figure adapted from \cite {ruiz2023_dreambooth}}{figure.caption.2227}{}}
\@writefile{toc}{\contentsline {paragraph}{Effect and Interpretation}{1078}{section*.2228}\protected@file@percent }
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\@writefile{toc}{\contentsline {subparagraph}{Subject-Driven Generation in New Contexts}{1079}{subparagraph*.2229}\protected@file@percent }
\newlabel{subsec:chapter20_dreambooth_subject_context}{{20.11.7}{1079}{Subject-Driven Generation in New Contexts}{subparagraph*.2229}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.112}{\ignorespaces  \textbf  {Recontextualization and Identity Preservation} — adapted from the DreamBooth paper~\blx@tocontentsinit {0}\cite {ruiz2023_dreambooth}. The model generates visually consistent outputs of two distinct subjects—a personalized teapot and a backpack—placed in novel contexts. For the teapot, DreamBooth adapts to prompts like “floating in milk", “transparent with milk inside", or “pouring tea", preserving identity and even enabling material transformations (e.g., transparency). For the backpack, it generates varied scenes such as “in Boston”, “at the Grand Canyon", while maintaining structural and stylistic fidelity. These generations illustrate how DreamBooth supports compositional control beyond the training distribution. }}{1079}{figure.caption.2230}\protected@file@percent }
\abx@aux@backref{1147}{ruiz2023_dreambooth}{0}{1079}{1079}
\newlabel{fig:chapter20_dreambooth_recontextualization}{{20.112}{1079}{\textbf {Recontextualization and Identity Preservation} — adapted from the DreamBooth paper~\cite {ruiz2023_dreambooth}. The model generates visually consistent outputs of two distinct subjects—a personalized teapot and a backpack—placed in novel contexts. For the teapot, DreamBooth adapts to prompts like “floating in milk", “transparent with milk inside", or “pouring tea", preserving identity and even enabling material transformations (e.g., transparency). For the backpack, it generates varied scenes such as “in Boston”, “at the Grand Canyon", while maintaining structural and stylistic fidelity. These generations illustrate how DreamBooth supports compositional control beyond the training distribution}{figure.caption.2230}{}}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\@writefile{lof}{\contentsline {figure}{\numberline {20.113}{\ignorespaces  \textbf  {Expression manipulation} — adapted from the DreamBooth paper~\blx@tocontentsinit {0}\cite {ruiz2023_dreambooth}. DreamBooth enables semantic edits to a personalized dog subject, synthesizing novel expressions that were absent from the input images. Notably, subject-defining features—such as the asymmetric white streak on the dog’s face—are consistently preserved. }}{1080}{figure.caption.2231}\protected@file@percent }
\abx@aux@backref{1149}{ruiz2023_dreambooth}{0}{1080}{1080}
\newlabel{fig:chapter20_dreambooth_expression_manipulation}{{20.113}{1080}{\textbf {Expression manipulation} — adapted from the DreamBooth paper~\cite {ruiz2023_dreambooth}. DreamBooth enables semantic edits to a personalized dog subject, synthesizing novel expressions that were absent from the input images. Notably, subject-defining features—such as the asymmetric white streak on the dog’s face—are consistently preserved}{figure.caption.2231}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.114}{\ignorespaces  \textbf  {Outfitting with accessories} — adapted from the DreamBooth paper~\blx@tocontentsinit {0}\cite {ruiz2023_dreambooth}. Given prompts like \texttt  {“a sks dog wearing a police/chef/witch outfit”}, the model synthesizes identity-consistent variations that exhibit plausible deformations and realistic interaction between the subject and the accessories—despite such scenes never being seen during training. }}{1080}{figure.caption.2232}\protected@file@percent }
\abx@aux@backref{1151}{ruiz2023_dreambooth}{0}{1080}{1080}
\newlabel{fig:chapter20_dreambooth_outfitting}{{20.114}{1080}{\textbf {Outfitting with accessories} — adapted from the DreamBooth paper~\cite {ruiz2023_dreambooth}. Given prompts like \texttt {“a sks dog wearing a police/chef/witch outfit”}, the model synthesizes identity-consistent variations that exhibit plausible deformations and realistic interaction between the subject and the accessories—despite such scenes never being seen during training}{figure.caption.2232}{}}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\@writefile{lof}{\contentsline {figure}{\numberline {20.115}{\ignorespaces  \textbf  {Novel view synthesis and stylization} — adapted from the DreamBooth paper~\blx@tocontentsinit {0}\cite {ruiz2023_dreambooth}. DreamBooth generalizes beyond training views to generate novel camera angles, stylized renditions (e.g., Van Gogh painting of the \texttt  {sks} dog), and compositional variants that preserve the core identity of the subject across diverse conditions. }}{1081}{figure.caption.2233}\protected@file@percent }
\abx@aux@backref{1153}{ruiz2023_dreambooth}{0}{1081}{1081}
\newlabel{fig:chapter20_dreambooth_novel_synth}{{20.115}{1081}{\textbf {Novel view synthesis and stylization} — adapted from the DreamBooth paper~\cite {ruiz2023_dreambooth}. DreamBooth generalizes beyond training views to generate novel camera angles, stylized renditions (e.g., Van Gogh painting of the \texttt {sks} dog), and compositional variants that preserve the core identity of the subject across diverse conditions}{figure.caption.2233}{}}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\@writefile{lof}{\contentsline {figure}{\numberline {20.116}{\ignorespaces  \textbf  {Failure cases} — adapted from the DreamBooth paper~\blx@tocontentsinit {0}\cite {ruiz2023_dreambooth}. (a) \emph  {Unseen context errors:} The model fails to render subject-consistent outputs in unfamiliar environments (e.g., synthesizing a backpack on the moon or inside the International Space Station). (b) \emph  {Context-appearance entanglement:} Visual details from training backgrounds (e.g., the Bolivian salt flats or a blue fabric backdrop) unintentionally bind to the subject, leaking into generations. (c) \emph  {Overfitting:} The model recreates poses and scenes from the original images it was trained on, reducing its capacity for diverse generalization. }}{1082}{figure.caption.2234}\protected@file@percent }
\abx@aux@backref{1155}{ruiz2023_dreambooth}{0}{1082}{1082}
\newlabel{fig:chapter20_dreambooth_failures}{{20.116}{1082}{\textbf {Failure cases} — adapted from the DreamBooth paper~\cite {ruiz2023_dreambooth}. (a) \emph {Unseen context errors:} The model fails to render subject-consistent outputs in unfamiliar environments (e.g., synthesizing a backpack on the moon or inside the International Space Station). (b) \emph {Context-appearance entanglement:} Visual details from training backgrounds (e.g., the Bolivian salt flats or a blue fabric backdrop) unintentionally bind to the subject, leaking into generations. (c) \emph {Overfitting:} The model recreates poses and scenes from the original images it was trained on, reducing its capacity for diverse generalization}{figure.caption.2234}{}}
\abx@aux@backref{1156}{hertz2022_prompt2prompt}{0}{1082}{1082}
\BKM@entry{id=788,dest={73656374696F6E2A2E32323335},srcline={12110}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030315C3030302E5C303030385C3030303A5C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030725C3030306F5C3030306C5C3030304E5C303030655C303030745C3030305C3034305C3034305C3032335C3030305C3034305C303030535C303030745C303030725C303030755C303030635C303030745C303030755C303030725C303030655C303030645C3030305C3034305C303030435C3030306F5C3030306E5C303030645C303030695C303030745C303030695C3030306F5C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030445C303030695C303030665C303030665C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\abx@aux@cite{0}{ruiz2023_dreambooth}
\abx@aux@segm{0}{0}{ruiz2023_dreambooth}
\abx@aux@cite{0}{hertz2022_prompt2prompt}
\abx@aux@segm{0}{0}{hertz2022_prompt2prompt}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.11.8: ControlNet – Structured Conditioning for Diffusion Models}{1083}{section*.2235}\protected@file@percent }
\newlabel{enr:chapter20_controlnet}{{20.11.8}{1083}{\color {ocre}Enrichment \thesubsection : ControlNet – Structured Conditioning for Diffusion Models}{section*.2235}{}}
\@writefile{toc}{\contentsline {subparagraph}{Motivation and Background}{1083}{subparagraph*.2236}\protected@file@percent }
\newlabel{subsec:chapter20_controlnet_motivation}{{20.11.8}{1083}{Motivation and Background}{subparagraph*.2236}{}}
\abx@aux@backref{1157}{ruiz2023_dreambooth}{0}{1083}{1083}
\abx@aux@backref{1158}{hertz2022_prompt2prompt}{0}{1083}{1083}
\abx@aux@backref{1159}{zhang2023_controlnet}{0}{1083}{1083}
\@writefile{lof}{\contentsline {figure}{\numberline {20.117}{\ignorespaces  \textbf  {Controllable generation using ControlNet} — adapted from the ControlNet paper~\blx@tocontentsinit {0}\cite {zhang2023_controlnet}. Users supply structured visual conditions, such as edge maps (top row) or pose keypoints (bottom row), alongside prompts to guide image synthesis. While the default prompt is ``a high-quality, detailed, and professional image'', additional text (e.g., ``chef in a kitchen'') can further refine semantic content. ControlNet enables precise alignment of the generation with both prompt and visual conditions. }}{1083}{figure.caption.2237}\protected@file@percent }
\abx@aux@backref{1161}{zhang2023_controlnet}{0}{1083}{1083}
\newlabel{fig:chapter20_controlnet_examples}{{20.117}{1083}{\textbf {Controllable generation using ControlNet} — adapted from the ControlNet paper~\cite {zhang2023_controlnet}. Users supply structured visual conditions, such as edge maps (top row) or pose keypoints (bottom row), alongside prompts to guide image synthesis. While the default prompt is ``a high-quality, detailed, and professional image'', additional text (e.g., ``chef in a kitchen'') can further refine semantic content. ControlNet enables precise alignment of the generation with both prompt and visual conditions}{figure.caption.2237}{}}
\abx@aux@cite{0}{schuhmann2022_laion5b}
\abx@aux@segm{0}{0}{schuhmann2022_laion5b}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\abx@aux@cite{0}{li2017_lwf}
\abx@aux@segm{0}{0}{li2017_lwf}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\@writefile{toc}{\contentsline {subparagraph}{Block Injection and Architectural Motivation}{1084}{subparagraph*.2238}\protected@file@percent }
\newlabel{subsec:chapter20_controlnet_blocks}{{20.11.8}{1084}{Block Injection and Architectural Motivation}{subparagraph*.2238}{}}
\abx@aux@backref{1162}{schuhmann2022_laion5b}{0}{1084}{1084}
\abx@aux@backref{1163}{rombach2022_ldm}{0}{1084}{1084}
\abx@aux@backref{1164}{li2017_lwf}{0}{1084}{1084}
\abx@aux@backref{1165}{zhang2023_controlnet}{0}{1084}{1084}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.11.8.1: ControlNet Architecture}{1084}{section*.2239}\protected@file@percent }
\newlabel{enr:chapter20_controlnet_architecture}{{20.11.8.1}{1084}{\color {ocre}Enrichment \thesubsubsection : ControlNet Architecture}{section*.2239}{}}
\@writefile{toc}{\contentsline {paragraph}{Injecting Spatial Conditioning into Frozen Networks}{1084}{section*.2240}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ControlNet Architectural Design}{1085}{section*.2241}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Motivation for Additive Injection: Why Not Inject \( c \) Directly?}{1085}{section*.2242}\protected@file@percent }
\newlabel{sec:chapter20_why_additive}{{20.11.8.1}{1085}{Motivation for Additive Injection: Why Not Inject \( c \) Directly?}{section*.2242}{}}
\@writefile{toc}{\contentsline {paragraph}{Component Breakdown}{1085}{section*.2243}\protected@file@percent }
\newlabel{sec:chapter20_component_breakdown}{{20.11.8.1}{1085}{Component Breakdown}{section*.2243}{}}
\@writefile{toc}{\contentsline {paragraph}{How Can the Output Change If the U-Net Is Frozen? And Why Is Denoising Still Valid?}{1086}{section*.2244}\protected@file@percent }
\newlabel{sec:chapter20_frozen_output_change}{{20.11.8.1}{1086}{How Can the Output Change If the U-Net Is Frozen? And Why Is Denoising Still Valid?}{section*.2244}{}}
\abx@aux@cite{0}{esser2021_vqgan}
\abx@aux@segm{0}{0}{esser2021_vqgan}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\@writefile{toc}{\contentsline {paragraph}{Training Objective}{1087}{section*.2245}\protected@file@percent }
\newlabel{sec:chapter20_training_objective}{{20.11.8.1}{1087}{Training Objective}{section*.2245}{}}
\abx@aux@backref{1166}{esser2021_vqgan}{0}{1087}{1087}
\abx@aux@backref{1167}{rombach2022_ldm}{0}{1087}{1087}
\abx@aux@backref{1168}{radford2021_clip}{0}{1087}{1087}
\abx@aux@backref{1169}{zhang2023_controlnet}{0}{1087}{1087}
\newlabel{eq:controlnet_loss}{{20.19}{1087}{Training Objective}{equation.20.19}{}}
\@writefile{toc}{\contentsline {paragraph}{Why ControlNet Preserves Denoising Capability}{1087}{section*.2246}\protected@file@percent }
\newlabel{sec:chapter20_denoising_validity}{{20.11.8.1}{1087}{Why ControlNet Preserves Denoising Capability}{section*.2246}{}}
\abx@aux@backref{1170}{rombach2022_ldm}{0}{1087}{1087}
\abx@aux@backref{1171}{zhang2023_controlnet}{0}{1087}{1087}
\abx@aux@backref{1172}{zhang2023_controlnet}{0}{1087}{1087}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\@writefile{lof}{\contentsline {figure}{\numberline {20.118}{\ignorespaces \textbf  {ControlNet block-level augmentation} — adapted from~\blx@tocontentsinit {0}\cite {zhang2023_controlnet}. (a) Standard U-Net block with frozen weights. (b) Trainable residual path processes condition inputs and injects them via zero-initialized \( 1 \times 1 \) convolutions.}}{1088}{figure.caption.2247}\protected@file@percent }
\abx@aux@backref{1174}{zhang2023_controlnet}{0}{1088}{1088}
\newlabel{fig:chapter20_controlnet_block_detail}{{20.118}{1088}{\textbf {ControlNet block-level augmentation} — adapted from~\cite {zhang2023_controlnet}. (a) Standard U-Net block with frozen weights. (b) Trainable residual path processes condition inputs and injects them via zero-initialized \( 1 \times 1 \) convolutions}{figure.caption.2247}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.119}{\ignorespaces \textbf  {ControlNet-enhanced architecture} — adapted from~\blx@tocontentsinit {0}\cite {zhang2023_controlnet}. Residual branches (blue) process spatial control inputs and merge into the frozen U-Net backbone (gray) via zero-conv paths (white).}}{1089}{figure.caption.2248}\protected@file@percent }
\abx@aux@backref{1176}{zhang2023_controlnet}{0}{1089}{1089}
\newlabel{fig:chapter20_controlnet_architecture_detail}{{20.119}{1089}{\textbf {ControlNet-enhanced architecture} — adapted from~\cite {zhang2023_controlnet}. Residual branches (blue) process spatial control inputs and merge into the frozen U-Net backbone (gray) via zero-conv paths (white)}{figure.caption.2248}{}}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 20.11.8.2: Training Behavior and Sudden Convergence}{1090}{section*.2249}\protected@file@percent }
\newlabel{enr:chapter20_controlnet_training}{{20.11.8.2}{1090}{\color {ocre}Enrichment \thesubsubsection : Training Behavior and Sudden Convergence}{section*.2249}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.120}{\ignorespaces  \textbf  {Sudden convergence in ControlNet training} — adapted from the ControlNet paper~\blx@tocontentsinit {0}\cite {zhang2023_controlnet}. Top: condition input (a sketch of an apple). Middle: model output at intermediate steps. Bottom: final image after convergence. Around step 6,133, the model rapidly begins aligning with the condition. Prior to this, the base model produces realistic but unaligned samples. }}{1090}{figure.caption.2250}\protected@file@percent }
\abx@aux@backref{1178}{zhang2023_controlnet}{0}{1090}{1090}
\newlabel{fig:chapter20_controlnet_sudden_convergence}{{20.120}{1090}{\textbf {Sudden convergence in ControlNet training} — adapted from the ControlNet paper~\cite {zhang2023_controlnet}. Top: condition input (a sketch of an apple). Middle: model output at intermediate steps. Bottom: final image after convergence. Around step 6,133, the model rapidly begins aligning with the condition. Prior to this, the base model produces realistic but unaligned samples}{figure.caption.2250}{}}
\abx@aux@cite{0}{ho2022_classifierfree}
\abx@aux@segm{0}{0}{ho2022_classifierfree}
\@writefile{toc}{\contentsline {subparagraph}{Classifier-Free Guidance and Resolution-Aware Weighting}{1091}{subparagraph*.2251}\protected@file@percent }
\newlabel{subsec:chapter20_controlnet_cfg}{{20.11.8.2}{1091}{Classifier-Free Guidance and Resolution-Aware Weighting}{subparagraph*.2251}{}}
\abx@aux@backref{1179}{ho2022_classifierfree}{0}{1091}{1091}
\@writefile{toc}{\contentsline {subparagraph}{Resolution-Aware Weighting (CFG-RW)}{1091}{subparagraph*.2252}\protected@file@percent }
\newlabel{subsec:chapter20_controlnet_cfg_rw}{{20.11.8.2}{1091}{Resolution-Aware Weighting (CFG-RW)}{subparagraph*.2252}{}}
\@writefile{toc}{\contentsline {paragraph}{Why resolution matters}{1091}{section*.2253}\protected@file@percent }
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\@writefile{toc}{\contentsline {paragraph}{Why It Works}{1092}{section*.2254}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Training Intuition With CFG-RW}{1092}{section*.2255}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.121}{\ignorespaces  \textbf  {Impact of Classifier-Free Guidance and Resolution Weighting} — adapted from the ControlNet paper~\blx@tocontentsinit {0}\cite {zhang2023_controlnet}. Left: Generation without CFG shows weak alignment to the input. Middle: Applying CFG improves semantic consistency. Right: CFG with resolution weighting (CFG-RW) enhances both prompt fidelity and image quality. }}{1092}{figure.caption.2256}\protected@file@percent }
\abx@aux@backref{1181}{zhang2023_controlnet}{0}{1092}{1092}
\newlabel{fig:chapter20_controlnet_cfg_effect}{{20.121}{1092}{\textbf {Impact of Classifier-Free Guidance and Resolution Weighting} — adapted from the ControlNet paper~\cite {zhang2023_controlnet}. Left: Generation without CFG shows weak alignment to the input. Middle: Applying CFG improves semantic consistency. Right: CFG with resolution weighting (CFG-RW) enhances both prompt fidelity and image quality}{figure.caption.2256}{}}
\@writefile{toc}{\contentsline {subparagraph}{Limitations of ControlNet and the Need for Semantic Conditioning}{1093}{subparagraph*.2257}\protected@file@percent }
\newlabel{subsec:chapter20_controlnet_limitations}{{20.11.8.2}{1093}{Limitations of ControlNet and the Need for Semantic Conditioning}{subparagraph*.2257}{}}
\@writefile{toc}{\contentsline {paragraph}{Preprocessing Dependency}{1093}{section*.2258}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Lack of Semantic Awareness}{1093}{section*.2259}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Limited Compositionality and Scalability}{1093}{section*.2260}\protected@file@percent }
\BKM@entry{id=789,dest={73656374696F6E2A2E32323631},srcline={12479}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030315C3030302E5C303030395C3030303A5C3030305C3034305C303030495C303030505C3030302D5C303030415C303030645C303030615C303030705C303030745C303030655C303030725C3030305C3034305C3034305C3032345C3030305C3034305C303030535C303030655C3030306D5C303030615C3030306E5C303030745C303030695C303030635C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030505C303030725C3030306F5C3030306D5C303030705C303030745C303030695C3030306E5C303030675C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030445C3030304D5C30303073}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.11.9: IP-Adapter — Semantic Image Prompting for DMs}{1094}{section*.2261}\protected@file@percent }
\newlabel{enr:chapter20_ipadapter}{{20.11.9}{1094}{\color {ocre}Enrichment \thesubsection : IP-Adapter — Semantic Image Prompting for DMs}{section*.2261}{}}
\@writefile{toc}{\contentsline {subparagraph}{Motivation and Background}{1094}{subparagraph*.2262}\protected@file@percent }
\newlabel{subsec:chapter20_ipadapter_motivation}{{20.11.9}{1094}{Motivation and Background}{subparagraph*.2262}{}}
\@writefile{toc}{\contentsline {subparagraph}{Introducing IP-Adapter: A Lightweight and Compatible Solution}{1094}{subparagraph*.2263}\protected@file@percent }
\newlabel{subsec:chapter20_ipadapter_intro}{{20.11.9}{1094}{Introducing IP-Adapter: A Lightweight and Compatible Solution}{subparagraph*.2263}{}}
\abx@aux@backref{1182}{ye2023_ipadapter}{0}{1094}{1094}
\@writefile{toc}{\contentsline {paragraph}{Why IP-Adapter Works Without Compromising the Base Model}{1095}{section*.2264}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{1. Image Guidance via Decoupled Cross-Attention in U-Net Blocks}{1095}{subparagraph*.2265}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{2. The Base U-Net Remains Fully Frozen}{1095}{subparagraph*.2266}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{3. Safe Integration via Additive Fusion}{1095}{subparagraph*.2267}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{4. Denoising Logic is Preserved by Construction}{1095}{subparagraph*.2268}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{5. \(\lambda \) Offers Explicit, Safe, Inference-Time Control}{1095}{subparagraph*.2269}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{6. Summary: Why This Architecture is Effective and Non-Destructive}{1096}{subparagraph*.2270}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ControlNet vs. IP-Adapter: Structural vs. Semantic Conditioning}{1096}{section*.2271}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{ControlNet: Explicit Structural Conditioning}{1096}{subparagraph*.2272}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{ControlNet \& Raw Images}{1096}{subparagraph*.2273}\protected@file@percent }
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\@writefile{lof}{\contentsline {figure}{\numberline {20.122}{\ignorespaces  \textbf  {Applications of IP-Adapter with pretrained text-to-image diffusion models.} The central image in each example serves as the image prompt. \textbf  {Right Column:} Showcases image variation, multimodal generation, and inpainting guided by the image prompt. \textbf  {Left Column:} Displays controllable generation achieved by combining the image prompt with additional structural conditions. Adapted from~\blx@tocontentsinit {0}\cite {ye2023_ipadapter}. }}{1099}{figure.caption.2274}\protected@file@percent }
\abx@aux@backref{1184}{ye2023_ipadapter}{0}{1099}{1099}
\newlabel{fig:chapter20_ipadapter_demonstration}{{20.122}{1099}{\textbf {Applications of IP-Adapter with pretrained text-to-image diffusion models.} The central image in each example serves as the image prompt. \textbf {Right Column:} Showcases image variation, multimodal generation, and inpainting guided by the image prompt. \textbf {Left Column:} Displays controllable generation achieved by combining the image prompt with additional structural conditions. Adapted from~\cite {ye2023_ipadapter}}{figure.caption.2274}{}}
\@writefile{toc}{\contentsline {paragraph}{Key Architectural Components and Detailed Integration}{1099}{section*.2275}\protected@file@percent }
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\@writefile{lof}{\contentsline {figure}{\numberline {20.123}{\ignorespaces  \textbf  {IP-Adapter Architecture with Decoupled Cross-Attention.} A reference image is encoded into a global feature vector, projected into visual tokens via \( \phi \), and used to form parallel attention pathways at each U-Net cross-attention site. These visual branches operate alongside frozen text-conditioned paths, and their outputs are fused via addition. Adapted from~\blx@tocontentsinit {0}\cite {ye2023_ipadapter}. }}{1101}{figure.caption.2276}\protected@file@percent }
\abx@aux@backref{1186}{ye2023_ipadapter}{0}{1101}{1101}
\newlabel{fig:chapter20_ipadapter_architecture}{{20.123}{1101}{\textbf {IP-Adapter Architecture with Decoupled Cross-Attention.} A reference image is encoded into a global feature vector, projected into visual tokens via \( \phi \), and used to form parallel attention pathways at each U-Net cross-attention site. These visual branches operate alongside frozen text-conditioned paths, and their outputs are fused via addition. Adapted from~\cite {ye2023_ipadapter}}{figure.caption.2276}{}}
\@writefile{toc}{\contentsline {subparagraph}{Versatility and Generalization without Fine-Tuning}{1101}{subparagraph*.2277}\protected@file@percent }
\newlabel{subsec:chapter20_ipadapter_generalization}{{20.11.9}{1101}{Versatility and Generalization without Fine-Tuning}{subparagraph*.2277}{}}
\abx@aux@backref{1187}{zhang2023_controlnet}{0}{1101}{1101}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\@writefile{lof}{\contentsline {figure}{\numberline {20.124}{\ignorespaces  \textbf  {Multimodal Conditioning with IP-Adapter and ControlNet.} Adapted from~\blx@tocontentsinit {0}\cite {ye2023_ipadapter}, this figure showcases identity-preserving generation under explicit structural guidance. Each row pairs a visual prompt (left) with a structured control map (right), such as edge maps or pose skeletons, processed by ControlNet (first two rows)/T2I-Adapter (last row). The trained IP-Adapter injects visual semantics via decoupled cross-attention, while ControlNet/T2I-Adapter enforces the geometric layout. No fine-tuning of the adapter is required for such multimodal compositional synthesis, demonstrating its generalization across tasks and conditioning modalities. }}{1102}{figure.caption.2278}\protected@file@percent }
\abx@aux@backref{1189}{ye2023_ipadapter}{0}{1102}{1102}
\newlabel{fig:chapter20_ipadapter_variety}{{20.124}{1102}{\textbf {Multimodal Conditioning with IP-Adapter and ControlNet.} Adapted from~\cite {ye2023_ipadapter}, this figure showcases identity-preserving generation under explicit structural guidance. Each row pairs a visual prompt (left) with a structured control map (right), such as edge maps or pose skeletons, processed by ControlNet (first two rows)/T2I-Adapter (last row). The trained IP-Adapter injects visual semantics via decoupled cross-attention, while ControlNet/T2I-Adapter enforces the geometric layout. No fine-tuning of the adapter is required for such multimodal compositional synthesis, demonstrating its generalization across tasks and conditioning modalities}{figure.caption.2278}{}}
\abx@aux@cite{0}{ramesh2022_dalle2}
\abx@aux@segm{0}{0}{ramesh2022_dalle2}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\abx@aux@cite{0}{xu2024_versatile}
\abx@aux@segm{0}{0}{xu2024_versatile}
\abx@aux@cite{0}{sd2022_variations}
\abx@aux@segm{0}{0}{sd2022_variations}
\abx@aux@cite{0}{sd2022_unclip}
\abx@aux@segm{0}{0}{sd2022_unclip}
\abx@aux@cite{0}{mou2023_t2iadapter}
\abx@aux@segm{0}{0}{mou2023_t2iadapter}
\abx@aux@cite{0}{zhao2023_unicontrolnet}
\abx@aux@segm{0}{0}{zhao2023_unicontrolnet}
\abx@aux@cite{0}{xu2023_promptfreediffusion}
\abx@aux@segm{0}{0}{xu2023_promptfreediffusion}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\abx@aux@cite{0}{xu2023_promptfreediffusion}
\abx@aux@segm{0}{0}{xu2023_promptfreediffusion}
\abx@aux@cite{0}{mou2023_t2iadapter}
\abx@aux@segm{0}{0}{mou2023_t2iadapter}
\abx@aux@cite{0}{zhao2023_unicontrolnet}
\abx@aux@segm{0}{0}{zhao2023_unicontrolnet}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\abx@aux@cite{0}{xu2023_promptfreediffusion}
\abx@aux@segm{0}{0}{xu2023_promptfreediffusion}
\abx@aux@cite{0}{mou2023_t2iadapter}
\abx@aux@segm{0}{0}{mou2023_t2iadapter}
\abx@aux@cite{0}{zhao2023_unicontrolnet}
\abx@aux@segm{0}{0}{zhao2023_unicontrolnet}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\@writefile{toc}{\contentsline {paragraph}{Comparative Evaluation Across Structural Control Tasks}{1103}{section*.2279}\protected@file@percent }
\abx@aux@backref{1190}{ramesh2022_dalle2}{0}{1103}{1103}
\abx@aux@backref{1191}{rombach2022_ldm}{0}{1103}{1103}
\abx@aux@backref{1192}{xu2024_versatile}{0}{1103}{1103}
\abx@aux@backref{1193}{sd2022_variations}{0}{1103}{1103}
\abx@aux@backref{1194}{sd2022_unclip}{0}{1103}{1103}
\abx@aux@backref{1195}{mou2023_t2iadapter}{0}{1103}{1103}
\abx@aux@backref{1196}{zhao2023_unicontrolnet}{0}{1103}{1103}
\abx@aux@backref{1197}{xu2023_promptfreediffusion}{0}{1103}{1103}
\abx@aux@backref{1198}{zhang2023_controlnet}{0}{1103}{1103}
\@writefile{lof}{\contentsline {figure}{\numberline {20.125}{\ignorespaces  \textbf  {Comparison of IP-Adapter with Other Structural Conditioning Methods.} Adapted from~\blx@tocontentsinit {0}\cite {ye2023_ipadapter}, this figure compares IP-Adapter against competing approaches across diverse control tasks. Baselines include SeeCoder~\blx@tocontentsinit {0}\cite {xu2023_promptfreediffusion}, T2I-Adapter (Style)~\blx@tocontentsinit {0}\cite {mou2023_t2iadapter}, Uni-ControlNet~\blx@tocontentsinit {0}\cite {zhao2023_unicontrolnet}, ControlNet-Shuffle and ControlNet-Reference~\blx@tocontentsinit {0}\cite {zhang2023_controlnet}. IP-Adapter demonstrates high-quality synthesis across edge, sketch, and pose conditioning, despite using a fixed image encoder and shared attention module across all tasks. Notably, it requires no task-specific fine-tuning—unlike some of the alternatives shown—highlighting its efficiency and generalization. }}{1103}{figure.caption.2280}\protected@file@percent }
\abx@aux@backref{1204}{ye2023_ipadapter}{0}{1103}{1103}
\abx@aux@backref{1205}{xu2023_promptfreediffusion}{0}{1103}{1103}
\abx@aux@backref{1206}{mou2023_t2iadapter}{0}{1103}{1103}
\abx@aux@backref{1207}{zhao2023_unicontrolnet}{0}{1103}{1103}
\abx@aux@backref{1208}{zhang2023_controlnet}{0}{1103}{1103}
\newlabel{fig:chapter20_ipadapter_comparison_to_others}{{20.125}{1103}{\textbf {Comparison of IP-Adapter with Other Structural Conditioning Methods.} Adapted from~\cite {ye2023_ipadapter}, this figure compares IP-Adapter against competing approaches across diverse control tasks. Baselines include SeeCoder~\cite {xu2023_promptfreediffusion}, T2I-Adapter (Style)~\cite {mou2023_t2iadapter}, Uni-ControlNet~\cite {zhao2023_unicontrolnet}, ControlNet-Shuffle and ControlNet-Reference~\cite {zhang2023_controlnet}. IP-Adapter demonstrates high-quality synthesis across edge, sketch, and pose conditioning, despite using a fixed image encoder and shared attention module across all tasks. Notably, it requires no task-specific fine-tuning—unlike some of the alternatives shown—highlighting its efficiency and generalization}{figure.caption.2280}{}}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\abx@aux@cite{0}{meng2022_sde}
\abx@aux@segm{0}{0}{meng2022_sde}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\@writefile{toc}{\contentsline {paragraph}{Image-to-Image Translation, Inpainting, and Multimodal Prompting}{1104}{section*.2281}\protected@file@percent }
\newlabel{par:chapter20_ipadapter_img2img}{{20.11.9}{1104}{Image-to-Image Translation, Inpainting, and Multimodal Prompting}{section*.2281}{}}
\abx@aux@backref{1209}{ye2023_ipadapter}{0}{1104}{1104}
\abx@aux@backref{1210}{meng2022_sde}{0}{1104}{1104}
\@writefile{lof}{\contentsline {figure}{\numberline {20.126}{\ignorespaces  \textbf  {Image-to-Image Translation and Inpainting with IP-Adapter.} Adapted from~\blx@tocontentsinit {0}\cite {ye2023_ipadapter}, this figure illustrates IP-Adapter's ability to preserve semantic fidelity (e.g., style, identity) while enabling controllable edits. In these examples, the structure is inferred directly from the source image or masked regions, demonstrating IP-Adapter’s capability in settings \emph  {without} explicit structural control modules like ControlNet. However, IP-Adapter remains fully compatible with such modules when needed for more complex conditioning. }}{1104}{figure.caption.2282}\protected@file@percent }
\abx@aux@backref{1212}{ye2023_ipadapter}{0}{1104}{1104}
\newlabel{fig:chapter20_ipadapter_img2img_inpainting}{{20.126}{1104}{\textbf {Image-to-Image Translation and Inpainting with IP-Adapter.} Adapted from~\cite {ye2023_ipadapter}, this figure illustrates IP-Adapter's ability to preserve semantic fidelity (e.g., style, identity) while enabling controllable edits. In these examples, the structure is inferred directly from the source image or masked regions, demonstrating IP-Adapter’s capability in settings \emph {without} explicit structural control modules like ControlNet. However, IP-Adapter remains fully compatible with such modules when needed for more complex conditioning}{figure.caption.2282}{}}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\@writefile{lof}{\contentsline {figure}{\numberline {20.127}{\ignorespaces  \textbf  {Multimodal Generation with IP-Adapter (Image + Text).} Adapted from~\blx@tocontentsinit {0}\cite {ye2023_ipadapter}, this figure illustrates how IP-Adapter enables expressive generation by combining image and text prompts. The top row shows an image of a horse used as the visual prompt. Subsequent generations introduce text prompts like ``wearing a top hat'' or ``a red horse'' to modify attributes without altering the base identity. Further examples show compositional edits: a red car’s scene is changed to ``in snowy winter'', or its appearance is modified to ``a green car'' using simple text. The adapter enables these edits while preserving fidelity to the original image prompt—without fine-tuning. }}{1105}{figure.caption.2283}\protected@file@percent }
\abx@aux@backref{1214}{ye2023_ipadapter}{0}{1105}{1105}
\newlabel{fig:chapter20_ipadapter_multimodal}{{20.127}{1105}{\textbf {Multimodal Generation with IP-Adapter (Image + Text).} Adapted from~\cite {ye2023_ipadapter}, this figure illustrates how IP-Adapter enables expressive generation by combining image and text prompts. The top row shows an image of a horse used as the visual prompt. Subsequent generations introduce text prompts like ``wearing a top hat'' or ``a red horse'' to modify attributes without altering the base identity. Further examples show compositional edits: a red car’s scene is changed to ``in snowy winter'', or its appearance is modified to ``a green car'' using simple text. The adapter enables these edits while preserving fidelity to the original image prompt—without fine-tuning}{figure.caption.2283}{}}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\@writefile{lof}{\contentsline {figure}{\numberline {20.128}{\ignorespaces  \textbf  {Comparison with other multimodal prompting methods} — adapted from the IP-Adapter paper~\blx@tocontentsinit {0}\cite {ye2023_ipadapter}. IP-Adapter outperforms BLIP-Diffusion, Uni-ControlNet, and other baselines in compositional generation with image + text prompts, demonstrating strong identity preservation and prompt compliance. }}{1106}{figure.caption.2284}\protected@file@percent }
\abx@aux@backref{1216}{ye2023_ipadapter}{0}{1106}{1106}
\newlabel{fig:chapter20_ipadapter_multimodal_comparison}{{20.128}{1106}{\textbf {Comparison with other multimodal prompting methods} — adapted from the IP-Adapter paper~\cite {ye2023_ipadapter}. IP-Adapter outperforms BLIP-Diffusion, Uni-ControlNet, and other baselines in compositional generation with image + text prompts, demonstrating strong identity preservation and prompt compliance}{figure.caption.2284}{}}
\@writefile{toc}{\contentsline {paragraph}{Ablation: Validating Architectural Design}{1106}{section*.2285}\protected@file@percent }
\newlabel{par:chapter20_ipadapter_ablation}{{20.11.9}{1106}{Ablation: Validating Architectural Design}{section*.2285}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.129}{\ignorespaces  \textbf  {Comparison with a simple adapter lacking decoupled cross-attention} — adapted from the IP-Adapter paper~\blx@tocontentsinit {0}\cite {ye2023_ipadapter}. While the simple adapter fails to preserve fine-grained appearance and identity attributes, IP-Adapter produces accurate and semantically aligned generations by decoupling image attention from textual conditioning. }}{1107}{figure.caption.2286}\protected@file@percent }
\abx@aux@backref{1218}{ye2023_ipadapter}{0}{1107}{1107}
\newlabel{fig:chapter20_ipadapter_simple_adapter_comparison}{{20.129}{1107}{\textbf {Comparison with a simple adapter lacking decoupled cross-attention} — adapted from the IP-Adapter paper~\cite {ye2023_ipadapter}. While the simple adapter fails to preserve fine-grained appearance and identity attributes, IP-Adapter produces accurate and semantically aligned generations by decoupling image attention from textual conditioning}{figure.caption.2286}{}}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\@writefile{lof}{\contentsline {figure}{\numberline {20.130}{\ignorespaces  \textbf  {Effect of Fine-Grained Image Tokens on Generation.} Adapted from~\blx@tocontentsinit {0}\cite {ye2023_ipadapter}, this figure compares IP-Adapter using global visual tokens (mid row) versus fine-grained visual tokens (last row). While the fine-grained variant improves alignment with local texture and background details, it can reduce variation across samples due to stronger conditioning. The global-token version provides more generative flexibility while maintaining high semantic fidelity. }}{1108}{figure.caption.2287}\protected@file@percent }
\abx@aux@backref{1220}{ye2023_ipadapter}{0}{1108}{1108}
\newlabel{fig:chapter20_ipadapter_fine_grained}{{20.130}{1108}{\textbf {Effect of Fine-Grained Image Tokens on Generation.} Adapted from~\cite {ye2023_ipadapter}, this figure compares IP-Adapter using global visual tokens (mid row) versus fine-grained visual tokens (last row). While the fine-grained variant improves alignment with local texture and background details, it can reduce variation across samples due to stronger conditioning. The global-token version provides more generative flexibility while maintaining high semantic fidelity}{figure.caption.2287}{}}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\abx@aux@cite{0}{zhu2023_transfusion}
\abx@aux@segm{0}{0}{zhu2023_transfusion}
\@writefile{toc}{\contentsline {paragraph}{Looking Forward}{1109}{section*.2288}\protected@file@percent }
\abx@aux@backref{1221}{ye2023_ipadapter}{0}{1109}{1109}
\abx@aux@backref{1222}{zhu2023_transfusion}{0}{1109}{1109}
\BKM@entry{id=790,dest={73656374696F6E2A2E32323839},srcline={12916}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030315C3030302E5C303030315C303030305C3030303A5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C303030755C303030735C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030555C3030306E5C303030695C303030665C303030695C303030655C303030645C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C3030306D5C3030306F5C303030645C303030615C3030306C5C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\abx@aux@cite{0}{lu2023_chameleon}
\abx@aux@segm{0}{0}{lu2023_chameleon}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\abx@aux@cite{0}{zhu2023_transfusion}
\abx@aux@segm{0}{0}{zhu2023_transfusion}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.11.10: Transfusion: Unified Multimodal Generation}{1110}{section*.2289}\protected@file@percent }
\newlabel{enr:chapter20_transfusion}{{20.11.10}{1110}{\color {ocre}Enrichment \thesubsection : Transfusion: Unified Multimodal Generation}{section*.2289}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation and Overview}{1110}{section*.2290}\protected@file@percent }
\newlabel{par:chapter20_transfusion_overview}{{20.11.10}{1110}{Motivation and Overview}{section*.2290}{}}
\abx@aux@backref{1223}{ramesh2021_dalle}{0}{1110}{1110}
\abx@aux@backref{1224}{lu2023_chameleon}{0}{1110}{1110}
\abx@aux@backref{1225}{ye2023_ipadapter}{0}{1110}{1110}
\abx@aux@backref{1226}{zhang2023_controlnet}{0}{1110}{1110}
\abx@aux@backref{1227}{zhu2023_transfusion}{0}{1110}{1110}
\abx@aux@cite{0}{zhang2023_controlnet}
\abx@aux@segm{0}{0}{zhang2023_controlnet}
\abx@aux@cite{0}{ye2023_ipadapter}
\abx@aux@segm{0}{0}{ye2023_ipadapter}
\abx@aux@cite{0}{zhu2023_transfusion}
\abx@aux@segm{0}{0}{zhu2023_transfusion}
\abx@aux@cite{0}{zhu2023_transfusion}
\abx@aux@segm{0}{0}{zhu2023_transfusion}
\abx@aux@backref{1228}{zhang2023_controlnet}{0}{1111}{1111}
\abx@aux@backref{1229}{ye2023_ipadapter}{0}{1111}{1111}
\@writefile{lof}{\contentsline {figure}{\numberline {20.131}{\ignorespaces  \textbf  {High-level architecture of Transfusion} — adapted from the Transfusion paper~\blx@tocontentsinit {0}\cite {zhu2023_transfusion}. A single transformer handles interleaved sequences of text tokens and continuous image patch embeddings. During training, text tokens are supervised using a next-token prediction loss, while image tokens are optimized with a denoising diffusion loss. Modality delimiters like \texttt  {<BOI>} and \texttt  {<EOI>} enable the model to seamlessly reason across modalities. }}{1111}{figure.caption.2291}\protected@file@percent }
\abx@aux@backref{1231}{zhu2023_transfusion}{0}{1111}{1111}
\newlabel{fig:chapter20_transfusion_high_level}{{20.131}{1111}{\textbf {High-level architecture of Transfusion} — adapted from the Transfusion paper~\cite {zhu2023_transfusion}. A single transformer handles interleaved sequences of text tokens and continuous image patch embeddings. During training, text tokens are supervised using a next-token prediction loss, while image tokens are optimized with a denoising diffusion loss. Modality delimiters like \texttt {<BOI>} and \texttt {<EOI>} enable the model to seamlessly reason across modalities}{figure.caption.2291}{}}
\abx@aux@cite{0}{kingma2014_autoencoding}
\abx@aux@segm{0}{0}{kingma2014_autoencoding}
\@writefile{toc}{\contentsline {paragraph}{Architecture and Training Pipeline of Transfusion}{1112}{section*.2292}\protected@file@percent }
\newlabel{par:chapter20_transfusion_architecture_training}{{20.11.10}{1112}{Architecture and Training Pipeline of Transfusion}{section*.2292}{}}
\@writefile{toc}{\contentsline {subparagraph}{Part 1: Image Tokenization Pipeline}{1112}{subparagraph*.2293}\protected@file@percent }
\newlabel{subpar:chapter20_transfusion_tokenization_pipeline}{{20.11.10}{1112}{Part 1: Image Tokenization Pipeline}{subparagraph*.2293}{}}
\abx@aux@backref{1232}{kingma2014_autoencoding}{0}{1112}{1112}
\abx@aux@cite{0}{zhu2023_transfusion}
\abx@aux@segm{0}{0}{zhu2023_transfusion}
\abx@aux@cite{0}{zhu2023_transfusion}
\abx@aux@segm{0}{0}{zhu2023_transfusion}
\@writefile{lof}{\contentsline {figure}{\numberline {20.132}{\ignorespaces  \textbf  {Image tokenization in Transfusion} — adapted from the Transfusion paper~\blx@tocontentsinit {0}\cite {zhu2023_transfusion}. A pretrained VAE encodes each image into a spatial latent map, which is then converted into patch tokens using either a shallow linear projection or a few downsampling blocks of a small U-Net. These patches are inserted into the transformer sequence between special boundary tokens \texttt  {<BOI>} and \texttt  {<EOI>}, enabling the model to process image and text jointly in a unified token stream. }}{1113}{figure.caption.2294}\protected@file@percent }
\abx@aux@backref{1234}{zhu2023_transfusion}{0}{1113}{1113}
\newlabel{fig:chapter20_transfusion_image_conversion}{{20.132}{1113}{\textbf {Image tokenization in Transfusion} — adapted from the Transfusion paper~\cite {zhu2023_transfusion}. A pretrained VAE encodes each image into a spatial latent map, which is then converted into patch tokens using either a shallow linear projection or a few downsampling blocks of a small U-Net. These patches are inserted into the transformer sequence between special boundary tokens \texttt {<BOI>} and \texttt {<EOI>}, enabling the model to process image and text jointly in a unified token stream}{figure.caption.2294}{}}
\@writefile{toc}{\contentsline {subparagraph}{Part 2: Text Tokenization Pipeline}{1113}{subparagraph*.2295}\protected@file@percent }
\newlabel{subpar:chapter20_transfusion_text_tokenization}{{20.11.10}{1113}{Part 2: Text Tokenization Pipeline}{subparagraph*.2295}{}}
\abx@aux@cite{0}{zhu2023_transfusion}
\abx@aux@segm{0}{0}{zhu2023_transfusion}
\abx@aux@cite{0}{zhu2023_transfusion}
\abx@aux@segm{0}{0}{zhu2023_transfusion}
\@writefile{toc}{\contentsline {subparagraph}{Part 3: Multimodal Sequence Construction}{1114}{subparagraph*.2296}\protected@file@percent }
\newlabel{subpar:chapter20_transfusion_multimodal_sequence}{{20.11.10}{1114}{Part 3: Multimodal Sequence Construction}{subparagraph*.2296}{}}
\@writefile{toc}{\contentsline {subparagraph}{Part 4: Transformer Processing with Hybrid Attention}{1114}{subparagraph*.2297}\protected@file@percent }
\newlabel{subpar:chapter20_transfusion_attention_mechanism}{{20.11.10}{1114}{Part 4: Transformer Processing with Hybrid Attention}{subparagraph*.2297}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.133}{\ignorespaces  \textbf  {Hybrid attention with intra-image bidirectional conditioning} — adapted from the Transfusion paper~\blx@tocontentsinit {0}\cite {zhu2023_transfusion}. While the overall sequence obeys a causal attention mask (for autoregressive generation), Transfusion relaxes this constraint within image segments. Patches from the same image can attend to each other bidirectionally, allowing the model to better capture local visual dependencies without violating the causal structure needed for autoregressive inference. }}{1114}{figure.caption.2298}\protected@file@percent }
\abx@aux@backref{1236}{zhu2023_transfusion}{0}{1114}{1114}
\newlabel{fig:chapter20_transfusion_patch_conditioning}{{20.133}{1114}{\textbf {Hybrid attention with intra-image bidirectional conditioning} — adapted from the Transfusion paper~\cite {zhu2023_transfusion}. While the overall sequence obeys a causal attention mask (for autoregressive generation), Transfusion relaxes this constraint within image segments. Patches from the same image can attend to each other bidirectionally, allowing the model to better capture local visual dependencies without violating the causal structure needed for autoregressive inference}{figure.caption.2298}{}}
\@writefile{toc}{\contentsline {subparagraph}{Part 5: Training Objectives and Loss Functions}{1115}{subparagraph*.2299}\protected@file@percent }
\newlabel{subpar:chapter20_transfusion_loss_functions}{{20.11.10}{1115}{Part 5: Training Objectives and Loss Functions}{subparagraph*.2299}{}}
\@writefile{toc}{\contentsline {subparagraph}{Part 6: Key Advantages of the Training Design}{1116}{subparagraph*.2300}\protected@file@percent }
\abx@aux@cite{0}{zhu2023_transfusion}
\abx@aux@segm{0}{0}{zhu2023_transfusion}
\abx@aux@cite{0}{zhu2023_transfusion}
\abx@aux@segm{0}{0}{zhu2023_transfusion}
\@writefile{toc}{\contentsline {paragraph}{Empirical Results and Qualitative Examples}{1117}{section*.2301}\protected@file@percent }
\newlabel{par:chapter20_transfusion_results}{{20.11.10}{1117}{Empirical Results and Qualitative Examples}{section*.2301}{}}
\@writefile{toc}{\contentsline {subparagraph}{Showcase: High-Quality Multi-Modal Generation}{1117}{subparagraph*.2302}\protected@file@percent }
\newlabel{subpar:chapter20_transfusion_generation}{{20.11.10}{1117}{Showcase: High-Quality Multi-Modal Generation}{subparagraph*.2302}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.134}{\ignorespaces  \textbf  {Examples generated by Transfusion} — adapted from ~\blx@tocontentsinit {0}\cite {zhu2023_transfusion}. Each image was generated by a 7B-parameter model trained from scratch on 2T multimodal tokens. Prompts range from artistic to scene-specific, such as “A chromeplated cat sculpture placed on a Persian rug” and “A wall in a royal castle. There are two paintings on the wall. The one on the left a detailed oil painting of the royal raccoon king. The one on the right a detailed oil painting of the royal raccoon queen”. These results highlight Transfusion’s ability to interpret rich, compositional text and produce visually grounded responses. }}{1117}{figure.caption.2303}\protected@file@percent }
\abx@aux@backref{1238}{zhu2023_transfusion}{0}{1117}{1117}
\newlabel{fig:chapter20_transfusion_examples}{{20.134}{1117}{\textbf {Examples generated by Transfusion} — adapted from ~\cite {zhu2023_transfusion}. Each image was generated by a 7B-parameter model trained from scratch on 2T multimodal tokens. Prompts range from artistic to scene-specific, such as “A chromeplated cat sculpture placed on a Persian rug” and “A wall in a royal castle. There are two paintings on the wall. The one on the left a detailed oil painting of the royal raccoon king. The one on the right a detailed oil painting of the royal raccoon queen”. These results highlight Transfusion’s ability to interpret rich, compositional text and produce visually grounded responses}{figure.caption.2303}{}}
\abx@aux@cite{0}{zhu2023_transfusion}
\abx@aux@segm{0}{0}{zhu2023_transfusion}
\abx@aux@cite{0}{zhu2023_transfusion}
\abx@aux@segm{0}{0}{zhu2023_transfusion}
\@writefile{toc}{\contentsline {subparagraph}{Zero-Shot Image Editing via Fine-Tuning}{1118}{subparagraph*.2304}\protected@file@percent }
\newlabel{subpar:chapter20_transfusion_editing}{{20.11.10}{1118}{Zero-Shot Image Editing via Fine-Tuning}{subparagraph*.2304}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20.135}{\ignorespaces  \textbf  {Image editing examples with Transfusion} — adapted from ~\blx@tocontentsinit {0}\cite {zhu2023_transfusion}. After fine-tuning on just 8k paired text–edit examples, the model performs successful localized edits such as object removal, replacement, and attribute modification. Notably, global image coherence and realism are preserved despite minimal fine-tuning and no explicit editing modules. }}{1118}{figure.caption.2305}\protected@file@percent }
\abx@aux@backref{1240}{zhu2023_transfusion}{0}{1118}{1118}
\newlabel{fig:chapter20_transfusion_image_editing}{{20.135}{1118}{\textbf {Image editing examples with Transfusion} — adapted from ~\cite {zhu2023_transfusion}. After fine-tuning on just 8k paired text–edit examples, the model performs successful localized edits such as object removal, replacement, and attribute modification. Notably, global image coherence and realism are preserved despite minimal fine-tuning and no explicit editing modules}{figure.caption.2305}{}}
\abx@aux@cite{0}{zhou2024_transfusion}
\abx@aux@segm{0}{0}{zhou2024_transfusion}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\abx@aux@cite{0}{zhou2024_transfusion}
\abx@aux@segm{0}{0}{zhou2024_transfusion}
\abx@aux@cite{0}{zhou2024_transfusion}
\abx@aux@segm{0}{0}{zhou2024_transfusion}
\abx@aux@cite{0}{zhou2024_transfusion}
\abx@aux@segm{0}{0}{zhou2024_transfusion}
\abx@aux@cite{0}{zhou2024_transfusion}
\abx@aux@segm{0}{0}{zhou2024_transfusion}
\@writefile{toc}{\contentsline {paragraph}{Ablation Studies and Experimental Insights}{1119}{section*.2306}\protected@file@percent }
\newlabel{par:chapter20_transfusion_ablations}{{20.11.10}{1119}{Ablation Studies and Experimental Insights}{section*.2306}{}}
\abx@aux@backref{1241}{zhou2024_transfusion}{0}{1119}{1119}
\@writefile{toc}{\contentsline {subparagraph}{Interpreting Evaluation Metrics}{1119}{subparagraph*.2307}\protected@file@percent }
\newlabel{par:chapter20_transfusion_metrics_overview}{{20.11.10}{1119}{Interpreting Evaluation Metrics}{subparagraph*.2307}{}}
\abx@aux@backref{1242}{radford2021_clip}{0}{1119}{1119}
\@writefile{toc}{\contentsline {subparagraph}{Attention Masking: Causal vs.\ Bidirectional}{1119}{subparagraph*.2308}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {20.6}{\ignorespaces \textbf  {Effect of attention masking} in 0.76 B \emph  {Transfusion} models (\(2\times 2\) patches). Adapted from \blx@tocontentsinit {0}\cite {zhou2024_transfusion}.}}{1119}{table.caption.2309}\protected@file@percent }
\abx@aux@backref{1244}{zhou2024_transfusion}{0}{1119}{1119}
\newlabel{tab:chapter20_transfusion_attention_masking}{{20.6}{1119}{\textbf {Effect of attention masking} in 0.76 B \emph {Transfusion} models (\(2\times 2\) patches). Adapted from \cite {zhou2024_transfusion}}{table.caption.2309}{}}
\@writefile{toc}{\contentsline {subparagraph}{Patch Size Variations}{1119}{subparagraph*.2310}\protected@file@percent }
\abx@aux@cite{0}{zhou2024_transfusion}
\abx@aux@segm{0}{0}{zhou2024_transfusion}
\abx@aux@cite{0}{zhou2024_transfusion}
\abx@aux@segm{0}{0}{zhou2024_transfusion}
\abx@aux@cite{0}{zhou2024_transfusion}
\abx@aux@segm{0}{0}{zhou2024_transfusion}
\abx@aux@cite{0}{zhou2024_transfusion}
\abx@aux@segm{0}{0}{zhou2024_transfusion}
\abx@aux@cite{0}{zhou2024_transfusion}
\abx@aux@segm{0}{0}{zhou2024_transfusion}
\abx@aux@cite{0}{zhou2024_transfusion}
\abx@aux@segm{0}{0}{zhou2024_transfusion}
\abx@aux@cite{0}{podell2023_sdxl}
\abx@aux@segm{0}{0}{podell2023_sdxl}
\abx@aux@cite{0}{deepfloyd2023_if}
\abx@aux@segm{0}{0}{deepfloyd2023_if}
\abx@aux@cite{0}{esser2024_scalingrectifiedflow}
\abx@aux@segm{0}{0}{esser2024_scalingrectifiedflow}
\abx@aux@cite{0}{zhu2024_chameleon}
\abx@aux@segm{0}{0}{zhu2024_chameleon}
\abx@aux@cite{0}{zhou2024_transfusion}
\abx@aux@segm{0}{0}{zhou2024_transfusion}
\@writefile{lot}{\contentsline {table}{\numberline {20.7}{\ignorespaces \textbf  {Effect of patch size} in 0.76 B \emph  {Transfusion} models. \textbf  {Bold}=best overall. Adapted from \blx@tocontentsinit {0}\cite {zhou2024_transfusion}.}}{1120}{table.caption.2311}\protected@file@percent }
\abx@aux@backref{1246}{zhou2024_transfusion}{0}{1120}{1120}
\newlabel{tab:chapter20_transfusion_patch_sizes}{{20.7}{1120}{\textbf {Effect of patch size} in 0.76 B \emph {Transfusion} models. \textbf {Bold}=best overall. Adapted from \cite {zhou2024_transfusion}}{table.caption.2311}{}}
\@writefile{toc}{\contentsline {subparagraph}{Encoding Architecture: Linear vs. U-Net}{1120}{subparagraph*.2312}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {20.8}{\ignorespaces \textbf  {Linear vs.\ U-Net encoders} (0.76 B and 7.0 B). Adapted from \blx@tocontentsinit {0}\cite {zhou2024_transfusion}.}}{1120}{table.caption.2313}\protected@file@percent }
\abx@aux@backref{1248}{zhou2024_transfusion}{0}{1120}{1120}
\newlabel{tab:chapter20_transfusion_unet_vs_linear}{{20.8}{1120}{\textbf {Linear vs.\ U-Net encoders} (0.76 B and 7.0 B). Adapted from \cite {zhou2024_transfusion}}{table.caption.2313}{}}
\@writefile{toc}{\contentsline {subparagraph}{Noise Scheduling in Image-to-Text Training}{1120}{subparagraph*.2314}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {20.9}{\ignorespaces \textbf  {Effect of diffusion-noise capping}. Adapted from \blx@tocontentsinit {0}\cite {zhou2024_transfusion}.}}{1120}{table.caption.2315}\protected@file@percent }
\abx@aux@backref{1250}{zhou2024_transfusion}{0}{1120}{1120}
\newlabel{tab:chapter20_transfusion_noise_schedule}{{20.9}{1120}{\textbf {Effect of diffusion-noise capping}. Adapted from \cite {zhou2024_transfusion}}{table.caption.2315}{}}
\@writefile{toc}{\contentsline {subparagraph}{Comparison to Specialized Generative Models}{1120}{subparagraph*.2316}\protected@file@percent }
\abx@aux@cite{0}{zhou2024_transfusion}
\abx@aux@segm{0}{0}{zhou2024_transfusion}
\@writefile{lot}{\contentsline {table}{\numberline {20.10}{\ignorespaces \textbf  {Comparison with prior work} on image and multimodal tasks. Adapted from \blx@tocontentsinit {0}\cite {zhou2024_transfusion}.}}{1121}{table.caption.2317}\protected@file@percent }
\abx@aux@backref{1252}{zhou2024_transfusion}{0}{1121}{1121}
\newlabel{tab:chapter20_transfusion_sota}{{20.10}{1121}{\textbf {Comparison with prior work} on image and multimodal tasks. Adapted from \cite {zhou2024_transfusion}}{table.caption.2317}{}}
\abx@aux@backref{1253}{podell2023_sdxl}{0}{1121}{1121}
\abx@aux@backref{1254}{deepfloyd2023_if}{0}{1121}{1121}
\abx@aux@backref{1255}{esser2024_scalingrectifiedflow}{0}{1121}{1121}
\abx@aux@backref{1256}{zhu2024_chameleon}{0}{1121}{1121}
\abx@aux@backref{1257}{zhou2024_transfusion}{0}{1121}{1121}
\@writefile{toc}{\contentsline {paragraph}{Summary}{1121}{section*.2318}\protected@file@percent }
\newlabel{par:chapter20_ablations_summary}{{20.11.10}{1121}{Summary}{section*.2318}{}}
\abx@aux@backref{1258}{zhou2024_transfusion}{0}{1121}{1121}
\BKM@entry{id=791,dest={73656374696F6E2A2E32333139},srcline={13390}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030315C3030302E5C303030315C303030315C3030303A5C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C3030305C3034305C303030415C303030755C303030745C3030306F5C303030725C303030655C303030675C303030725C303030655C303030735C303030735C303030695C303030765C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030695C3030306E5C303030675C3030305C3034305C3030305C3035305C303030565C303030415C303030525C3030305C303531}
\abx@aux@cite{0}{tian2024_var}
\abx@aux@segm{0}{0}{tian2024_var}
\abx@aux@cite{0}{tian2024_var}
\abx@aux@segm{0}{0}{tian2024_var}
\abx@aux@cite{0}{tian2024_var}
\abx@aux@segm{0}{0}{tian2024_var}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.11.11: Visual Autoregressive Modeling (VAR)}{1122}{section*.2319}\protected@file@percent }
\newlabel{enr:chapter20_VAR}{{20.11.11}{1122}{\color {ocre}Enrichment \thesubsection : Visual Autoregressive Modeling (VAR)}{section*.2319}{}}
\abx@aux@backref{1259}{tian2024_var}{0}{1122}{1122}
\@writefile{lof}{\contentsline {figure}{\numberline {20.136}{\ignorespaces  \textbf  {Autoregressive modeling paradigms for image generation} — adapted from~\blx@tocontentsinit {0}\cite {tian2024_var}. (a) Standard language AR modeling predicts tokens sequentially. (b) Classical image AR methods flatten a 2D grid into a raster-scan sequence. (c) \emph  {VAR} predicts multi-scale token maps hierarchically: coarse levels first, with progressively finer resolutions conditioned on earlier stages. }}{1122}{figure.caption.2320}\protected@file@percent }
\abx@aux@backref{1261}{tian2024_var}{0}{1122}{1122}
\newlabel{fig:chapter20_var_ar_paradigms}{{20.136}{1122}{\textbf {Autoregressive modeling paradigms for image generation} — adapted from~\cite {tian2024_var}. (a) Standard language AR modeling predicts tokens sequentially. (b) Classical image AR methods flatten a 2D grid into a raster-scan sequence. (c) \emph {VAR} predicts multi-scale token maps hierarchically: coarse levels first, with progressively finer resolutions conditioned on earlier stages}{figure.caption.2320}{}}
\abx@aux@cite{0}{tian2024_var}
\abx@aux@segm{0}{0}{tian2024_var}
\abx@aux@cite{0}{tian2024_var}
\abx@aux@segm{0}{0}{tian2024_var}
\abx@aux@cite{0}{tian2024_var}
\abx@aux@segm{0}{0}{tian2024_var}
\abx@aux@cite{0}{ramesh2021_dalle}
\abx@aux@segm{0}{0}{ramesh2021_dalle}
\@writefile{toc}{\contentsline {subparagraph}{Multi-Scale Architecture for Coarse-to-Fine Generation: How VAR Works}{1123}{subparagraph*.2321}\protected@file@percent }
\newlabel{subpar:chapter20_var_architecture}{{20.11.11}{1123}{Multi-Scale Architecture for Coarse-to-Fine Generation: How VAR Works}{subparagraph*.2321}{}}
\abx@aux@backref{1262}{tian2024_var}{0}{1123}{1123}
\@writefile{toc}{\contentsline {paragraph}{Overview: A Two-Stage Pipeline for Image Generation}{1123}{section*.2322}\protected@file@percent }
\newlabel{par:var_overview}{{20.11.11}{1123}{Overview: A Two-Stage Pipeline for Image Generation}{section*.2322}{}}
\abx@aux@backref{1263}{tian2024_var}{0}{1123}{1123}
\@writefile{toc}{\contentsline {paragraph}{Stage 1: Multi-Scale VQ-VAE for Hierarchical Tokenization}{1123}{section*.2323}\protected@file@percent }
\newlabel{par:var_stage1_vqvae}{{20.11.11}{1123}{Stage 1: Multi-Scale VQ-VAE for Hierarchical Tokenization}{section*.2323}{}}
\abx@aux@backref{1264}{tian2024_var}{0}{1123}{1123}
\abx@aux@backref{1265}{ramesh2021_dalle}{0}{1123}{1123}
\@writefile{toc}{\contentsline {subparagraph}{Hierarchical Token Encoding via Residual Refinement}{1124}{subparagraph*.2324}\protected@file@percent }
\newlabel{subpar:var_token_encoding}{{20.11.11}{1124}{Hierarchical Token Encoding via Residual Refinement}{subparagraph*.2324}{}}
\@writefile{toc}{\contentsline {subparagraph}{Token Decoding and Image Reconstruction}{1124}{subparagraph*.2325}\protected@file@percent }
\newlabel{subpar:var_token_decoding}{{20.11.11}{1124}{Token Decoding and Image Reconstruction}{subparagraph*.2325}{}}
\@writefile{toc}{\contentsline {subparagraph}{Training Objective for the VQ-VAE}{1125}{subparagraph*.2326}\protected@file@percent }
\newlabel{subpar:var_vqvae_loss}{{20.11.11}{1125}{Training Objective for the VQ-VAE}{subparagraph*.2326}{}}
\@writefile{toc}{\contentsline {paragraph}{Stage 2: Scale-Aware Autoregressive Transformer}{1125}{section*.2327}\protected@file@percent }
\newlabel{par:var_stage2_transformer}{{20.11.11}{1125}{Stage 2: Scale-Aware Autoregressive Transformer}{section*.2327}{}}
\@writefile{toc}{\contentsline {subparagraph}{From Tokens to Embeddings: Transformer Inputs}{1125}{subparagraph*.2328}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Why a Second Stage is Needed}{1126}{subparagraph*.2329}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Autoregressive Modeling Across Scales}{1126}{subparagraph*.2330}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Training Procedure}{1126}{subparagraph*.2331}\protected@file@percent }
\abx@aux@cite{0}{tian2024_var}
\abx@aux@segm{0}{0}{tian2024_var}
\abx@aux@cite{0}{tian2024_var}
\abx@aux@segm{0}{0}{tian2024_var}
\@writefile{toc}{\contentsline {subparagraph}{Inference and Generation}{1127}{subparagraph*.2332}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Final Decoding and Image Reconstruction}{1127}{subparagraph*.2333}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.137}{\ignorespaces  \textbf  {Two-stage VAR architecture} — based on~\blx@tocontentsinit {0}\cite {tian2024_var}. In Stage 1, a multi-scale VQ-VAE encodes the image into hierarchical token maps. In Stage 2, a transformer autoregressively predicts these maps one scale at a time. A blockwise attention mask ensures each scale \( {r}_k \) only attends to \( {s} \) and \( {r}_{<k} \). }}{1127}{figure.caption.2334}\protected@file@percent }
\abx@aux@backref{1267}{tian2024_var}{0}{1127}{1127}
\newlabel{fig:chapter20_var_pipeline}{{20.137}{1127}{\textbf {Two-stage VAR architecture} — based on~\cite {tian2024_var}. In Stage 1, a multi-scale VQ-VAE encodes the image into hierarchical token maps. In Stage 2, a transformer autoregressively predicts these maps one scale at a time. A blockwise attention mask ensures each scale \( {r}_k \) only attends to \( {s} \) and \( {r}_{<k} \)}{figure.caption.2334}{}}
\abx@aux@cite{0}{tian2024_var}
\abx@aux@segm{0}{0}{tian2024_var}
\abx@aux@cite{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@segm{0}{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@cite{0}{tian2024_var}
\abx@aux@segm{0}{0}{tian2024_var}
\abx@aux@cite{0}{tian2024_var}
\abx@aux@segm{0}{0}{tian2024_var}
\@writefile{toc}{\contentsline {paragraph}{Benefits of the VAR Design}{1128}{section*.2335}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Experimental Results: High-Quality Generation and Editing}{1128}{section*.2336}\protected@file@percent }
\newlabel{par:chapter20_var_results}{{20.11.11}{1128}{Experimental Results: High-Quality Generation and Editing}{section*.2336}{}}
\abx@aux@backref{1268}{tian2024_var}{0}{1128}{1128}
\abx@aux@backref{1269}{imagenet2009_hierarchicaldatabase}{0}{1128}{1128}
\@writefile{lof}{\contentsline {figure}{\numberline {20.138}{\ignorespaces  \textbf  {Image generation and editing with VAR} — adapted from~\blx@tocontentsinit {0}\cite {tian2024_var}. Top: Unconditional samples at \(512 \times 512\) resolution. Middle: Samples at \(256 \times 256\). Bottom: Zero-shot image editing results, where input images are modified using conditional prompts without task-specific fine-tuning. }}{1128}{figure.caption.2337}\protected@file@percent }
\abx@aux@backref{1271}{tian2024_var}{0}{1128}{1128}
\newlabel{fig:chapter20_var_examples}{{20.138}{1128}{\textbf {Image generation and editing with VAR} — adapted from~\cite {tian2024_var}. Top: Unconditional samples at \(512 \times 512\) resolution. Middle: Samples at \(256 \times 256\). Bottom: Zero-shot image editing results, where input images are modified using conditional prompts without task-specific fine-tuning}{figure.caption.2337}{}}
\abx@aux@cite{0}{tian2024_var}
\abx@aux@segm{0}{0}{tian2024_var}
\abx@aux@cite{0}{tian2024_var}
\abx@aux@segm{0}{0}{tian2024_var}
\abx@aux@cite{0}{brock2019_biggan}
\abx@aux@segm{0}{0}{brock2019_biggan}
\abx@aux@cite{0}{kang2023_gigagan}
\abx@aux@segm{0}{0}{kang2023_gigagan}
\abx@aux@cite{0}{sauer2022_styleganxl}
\abx@aux@segm{0}{0}{sauer2022_styleganxl}
\abx@aux@cite{0}{dhariwal2021_beats}
\abx@aux@segm{0}{0}{dhariwal2021_beats}
\abx@aux@cite{0}{ho2021_cascaded}
\abx@aux@segm{0}{0}{ho2021_cascaded}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\abx@aux@cite{0}{peebles2023_dit}
\abx@aux@segm{0}{0}{peebles2023_dit}
\abx@aux@cite{0}{yu2023_ldit}
\abx@aux@segm{0}{0}{yu2023_ldit}
\abx@aux@cite{0}{chang2022_maskgit}
\abx@aux@segm{0}{0}{chang2022_maskgit}
\abx@aux@cite{0}{esser2021_vqgan}
\abx@aux@segm{0}{0}{esser2021_vqgan}
\abx@aux@cite{0}{yu2022_vitvq}
\abx@aux@segm{0}{0}{yu2022_vitvq}
\abx@aux@cite{0}{lee2022_rqtransformer}
\abx@aux@segm{0}{0}{lee2022_rqtransformer}
\abx@aux@cite{0}{yu2023_ldit}
\abx@aux@segm{0}{0}{yu2023_ldit}
\@writefile{toc}{\contentsline {subparagraph}{Comparison with Other Generative Paradigms}{1129}{subparagraph*.2338}\protected@file@percent }
\newlabel{subpar:chapter20_var_comparison}{{20.11.11}{1129}{Comparison with Other Generative Paradigms}{subparagraph*.2338}{}}
\@writefile{lot}{\contentsline {table}{\numberline {20.11}{\ignorespaces  Comparison of generative model families on ImageNet \(256 \times 256\) — adapted from~\blx@tocontentsinit {0}\cite {tian2024_var}. VAR models (bottom rows) outperform all baselines in fidelity and inference speed. “\textbf  {↓}” or “\textbf  {↑}” indicate whether lower or higher is better. Wall-clock time is reported relative to VAR. }}{1129}{table.caption.2339}\protected@file@percent }
\abx@aux@backref{1273}{tian2024_var}{0}{1129}{1129}
\newlabel{tab:chapter20_var_comparison}{{20.11}{1129}{Comparison of generative model families on ImageNet \(256 \times 256\) — adapted from~\cite {tian2024_var}. VAR models (bottom rows) outperform all baselines in fidelity and inference speed. “\textbf {↓}” or “\textbf {↑}” indicate whether lower or higher is better. Wall-clock time is reported relative to VAR}{table.caption.2339}{}}
\abx@aux@backref{1274}{brock2019_biggan}{0}{1129}{1129}
\abx@aux@backref{1275}{kang2023_gigagan}{0}{1129}{1129}
\abx@aux@backref{1276}{sauer2022_styleganxl}{0}{1129}{1129}
\abx@aux@backref{1277}{dhariwal2021_beats}{0}{1129}{1129}
\abx@aux@backref{1278}{ho2021_cascaded}{0}{1129}{1129}
\abx@aux@backref{1279}{rombach2022_ldm}{0}{1129}{1129}
\abx@aux@backref{1280}{peebles2023_dit}{0}{1129}{1129}
\abx@aux@backref{1281}{yu2023_ldit}{0}{1129}{1129}
\abx@aux@backref{1282}{chang2022_maskgit}{0}{1129}{1129}
\abx@aux@backref{1283}{esser2021_vqgan}{0}{1129}{1129}
\abx@aux@backref{1284}{yu2022_vitvq}{0}{1129}{1129}
\abx@aux@backref{1285}{lee2022_rqtransformer}{0}{1129}{1129}
\abx@aux@backref{1286}{yu2023_ldit}{0}{1129}{1129}
\abx@aux@cite{0}{esser2021_vqgan}
\abx@aux@segm{0}{0}{esser2021_vqgan}
\abx@aux@cite{0}{yu2022_vitvq}
\abx@aux@segm{0}{0}{yu2022_vitvq}
\abx@aux@cite{0}{lee2022_rqtransformer}
\abx@aux@segm{0}{0}{lee2022_rqtransformer}
\newlabel{par:chapter20_var_advantages_over_vqvae}{{20.11.11}{1130}{Comparison with Other Generative Paradigms}{table.caption.2339}{}}
\abx@aux@backref{1287}{esser2021_vqgan}{0}{1130}{1130}
\abx@aux@backref{1288}{yu2022_vitvq}{0}{1130}{1130}
\abx@aux@backref{1289}{lee2022_rqtransformer}{0}{1130}{1130}
\newlabel{par:chapter20_var_avoids_blur}{{20.11.11}{1130}{Comparison with Other Generative Paradigms}{table.caption.2339}{}}
\abx@aux@cite{0}{tian2024_var}
\abx@aux@segm{0}{0}{tian2024_var}
\abx@aux@cite{0}{tian2024_var}
\abx@aux@segm{0}{0}{tian2024_var}
\abx@aux@cite{0}{tian2024_var}
\abx@aux@segm{0}{0}{tian2024_var}
\@writefile{toc}{\contentsline {paragraph}{Scaling Trends, Model Comparison, and Future Outlook}{1131}{section*.2340}\protected@file@percent }
\newlabel{par:chapter20_var_scaling_comparison_future}{{20.11.11}{1131}{Scaling Trends, Model Comparison, and Future Outlook}{section*.2340}{}}
\abx@aux@backref{1290}{tian2024_var}{0}{1131}{1131}
\@writefile{toc}{\contentsline {subparagraph}{Scaling Efficiency and Sample Quality}{1131}{subparagraph*.2341}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.139}{\ignorespaces  \textbf  {Scaling behavior of VAR} — adapted from~\blx@tocontentsinit {0}\cite {tian2024_var}. VAR outperforms diffusion models like L-DiT-3B with fewer parameters and faster inference, validating its architectural scalability. }}{1131}{figure.caption.2342}\protected@file@percent }
\abx@aux@backref{1292}{tian2024_var}{0}{1131}{1131}
\newlabel{fig:chapter20_var_scaling}{{20.139}{1131}{\textbf {Scaling behavior of VAR} — adapted from~\cite {tian2024_var}. VAR outperforms diffusion models like L-DiT-3B with fewer parameters and faster inference, validating its architectural scalability}{figure.caption.2342}{}}
\abx@aux@cite{0}{dhariwal2021_beats}
\abx@aux@segm{0}{0}{dhariwal2021_beats}
\abx@aux@cite{0}{peebles2023_dit}
\abx@aux@segm{0}{0}{peebles2023_dit}
\abx@aux@cite{0}{yu2023_ldit}
\abx@aux@segm{0}{0}{yu2023_ldit}
\abx@aux@cite{0}{sauer2022_styleganxl}
\abx@aux@segm{0}{0}{sauer2022_styleganxl}
\abx@aux@cite{0}{esser2021_vqgan}
\abx@aux@segm{0}{0}{esser2021_vqgan}
\abx@aux@cite{0}{yu2022_vitvq}
\abx@aux@segm{0}{0}{yu2022_vitvq}
\abx@aux@cite{0}{lee2022_rqtransformer}
\abx@aux@segm{0}{0}{lee2022_rqtransformer}
\abx@aux@cite{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@segm{0}{0}{imagenet2009_hierarchicaldatabase}
\abx@aux@cite{0}{tian2024_var}
\abx@aux@segm{0}{0}{tian2024_var}
\abx@aux@cite{0}{tian2024_var}
\abx@aux@segm{0}{0}{tian2024_var}
\@writefile{toc}{\contentsline {subparagraph}{Comparison to Diffusion and Autoregressive Models}{1132}{subparagraph*.2343}\protected@file@percent }
\abx@aux@backref{1293}{dhariwal2021_beats}{0}{1132}{1132}
\abx@aux@backref{1294}{peebles2023_dit}{0}{1132}{1132}
\abx@aux@backref{1295}{yu2023_ldit}{0}{1132}{1132}
\abx@aux@backref{1296}{sauer2022_styleganxl}{0}{1132}{1132}
\abx@aux@backref{1297}{esser2021_vqgan}{0}{1132}{1132}
\abx@aux@backref{1298}{yu2022_vitvq}{0}{1132}{1132}
\abx@aux@backref{1299}{lee2022_rqtransformer}{0}{1132}{1132}
\@writefile{toc}{\contentsline {paragraph}{Qualitative Scaling Effects of VAR}{1132}{section*.2344}\protected@file@percent }
\newlabel{par:chapter20_var_scaling_visual}{{20.11.11}{1132}{Qualitative Scaling Effects of VAR}{section*.2344}{}}
\abx@aux@backref{1300}{imagenet2009_hierarchicaldatabase}{0}{1132}{1132}
\@writefile{lof}{\contentsline {figure}{\numberline {20.140}{\ignorespaces  \textbf  {Visual effect of scaling model size and training compute in VAR} — based on~\blx@tocontentsinit {0}\cite {tian2024_var}. Each row corresponds to a specific ImageNet class: \emph  {flamingo, arctic wolf , macaw, Siamese cat, oscilloscope, husky, mollymawk, volcano, and catamaran}. From left to right, generations improve in clarity, structure, and texture with increasing model depth and training steps. }}{1133}{figure.caption.2345}\protected@file@percent }
\abx@aux@backref{1302}{tian2024_var}{0}{1133}{1133}
\newlabel{fig:chapter20_var_scaling_samples}{{20.140}{1133}{\textbf {Visual effect of scaling model size and training compute in VAR} — based on~\cite {tian2024_var}. Each row corresponds to a specific ImageNet class: \emph {flamingo, arctic wolf , macaw, Siamese cat, oscilloscope, husky, mollymawk, volcano, and catamaran}. From left to right, generations improve in clarity, structure, and texture with increasing model depth and training steps}{figure.caption.2345}{}}
\@writefile{toc}{\contentsline {subparagraph}{Limitations and Future Directions}{1134}{subparagraph*.2346}\protected@file@percent }
\newlabel{subpar:chapter20_var_limitations}{{20.11.11}{1134}{Limitations and Future Directions}{subparagraph*.2346}{}}
\BKM@entry{id=792,dest={73656374696F6E2A2E32333437},srcline={13928}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030305C3030302E5C303030315C303030315C3030302E5C303030315C303030325C3030303A5C3030305C3034305C303030445C303030695C303030545C3030303A5C3030305C3034305C303030445C303030695C303030665C303030665C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C30303073}
\abx@aux@cite{0}{dhariwal2021_beats}
\abx@aux@segm{0}{0}{dhariwal2021_beats}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\abx@aux@cite{0}{peebles2023_dit}
\abx@aux@segm{0}{0}{peebles2023_dit}
\abx@aux@cite{0}{peebles2023_dit}
\abx@aux@segm{0}{0}{peebles2023_dit}
\abx@aux@cite{0}{peebles2023_dit}
\abx@aux@segm{0}{0}{peebles2023_dit}
\@writefile{toc}{\contentsline {subsection}{Enrichment 20.11.12: DiT: Diffusion Transformers}{1135}{section*.2347}\protected@file@percent }
\newlabel{enr:chapter20_dit}{{20.11.12}{1135}{\color {ocre}Enrichment \thesubsection : DiT: Diffusion Transformers}{section*.2347}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation and context}{1135}{section*.2348}\protected@file@percent }
\abx@aux@backref{1303}{dhariwal2021_beats}{0}{1135}{1135}
\abx@aux@backref{1304}{rombach2022_ldm}{0}{1135}{1135}
\abx@aux@backref{1305}{peebles2023_dit}{0}{1135}{1135}
\@writefile{lof}{\contentsline {figure}{\numberline {20.141}{\ignorespaces \textbf  {Selected DiT samples on ImageNet}. Curated generations from class-conditional DiT-XL/2 at 512$\times $512 and 256$\times $256 illustrate fidelity and diversity across categories; credit: Peebles \& Xie~\blx@tocontentsinit {0}\cite {peebles2023_dit}.}}{1135}{figure.caption.2349}\protected@file@percent }
\abx@aux@backref{1307}{peebles2023_dit}{0}{1135}{1135}
\newlabel{fig:chapter20_dit_examples}{{20.141}{1135}{\textbf {Selected DiT samples on ImageNet}. Curated generations from class-conditional DiT-XL/2 at 512$\times $512 and 256$\times $256 illustrate fidelity and diversity across categories; credit: Peebles \& Xie~\cite {peebles2023_dit}}{figure.caption.2349}{}}
\@writefile{toc}{\contentsline {paragraph}{High-level overview}{1135}{section*.2350}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Why transformers? Intuition.}{1135}{subparagraph*.2351}\protected@file@percent }
\abx@aux@cite{0}{peebles2023_dit}
\abx@aux@segm{0}{0}{peebles2023_dit}
\abx@aux@cite{0}{peebles2023_dit}
\abx@aux@segm{0}{0}{peebles2023_dit}
\abx@aux@cite{0}{peebles2023_dit}
\abx@aux@segm{0}{0}{peebles2023_dit}
\@writefile{toc}{\contentsline {paragraph}{Method: architecture and components}{1136}{section*.2352}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Tokenization (patchify) of the latent.}{1136}{subparagraph*.2353}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.142}{\ignorespaces \textbf  {Input specification and patchify}. A spatial latent of shape $I{\times }I{\times }C$ becomes $T{=}(I/p)^2$ tokens of width $d$. Smaller $p$ increases sequence length and compute; credit: Peebles \& Xie~\blx@tocontentsinit {0}\cite {peebles2023_dit}.}}{1136}{figure.caption.2354}\protected@file@percent }
\abx@aux@backref{1309}{peebles2023_dit}{0}{1136}{1136}
\newlabel{fig:chapter20_dit_input}{{20.142}{1136}{\textbf {Input specification and patchify}. A spatial latent of shape $I{\times }I{\times }C$ becomes $T{=}(I/p)^2$ tokens of width $d$. Smaller $p$ increases sequence length and compute; credit: Peebles \& Xie~\cite {peebles2023_dit}}{figure.caption.2354}{}}
\abx@aux@cite{0}{peebles2023_dit}
\abx@aux@segm{0}{0}{peebles2023_dit}
\abx@aux@cite{0}{peebles2023_dit}
\abx@aux@segm{0}{0}{peebles2023_dit}
\@writefile{toc}{\contentsline {subparagraph}{High-level overview: DiT as a transformer backbone for diffusion}{1137}{subparagraph*.2355}\protected@file@percent }
\abx@aux@backref{1310}{peebles2023_dit}{0}{1137}{1137}
\@writefile{lof}{\contentsline {figure}{\numberline {20.143}{\ignorespaces \textbf  {DiT architecture at a glance.} Latent patches are embedded and passed through $N$ transformer blocks, then a per-token head maps back to the latent grid. Right: conditioning variants evaluated by \blx@tocontentsinit {0}\cite {peebles2023_dit}.}}{1137}{figure.caption.2356}\protected@file@percent }
\abx@aux@backref{1312}{peebles2023_dit}{0}{1137}{1137}
\newlabel{fig:chapter20_dit_arch}{{20.143}{1137}{\textbf {DiT architecture at a glance.} Latent patches are embedded and passed through $N$ transformer blocks, then a per-token head maps back to the latent grid. Right: conditioning variants evaluated by \cite {peebles2023_dit}}{figure.caption.2356}{}}
\@writefile{toc}{\contentsline {subparagraph}{From AdaIN to adaLN: motivation and adaptation}{1137}{subparagraph*.2357}\protected@file@percent }
\newlabel{par:chapter20_dit_adain}{{20.11.12}{1137}{From AdaIN to adaLN: motivation and adaptation}{subparagraph*.2357}{}}
\@writefile{toc}{\contentsline {subparagraph}{DiT block: adaLN and the adaLN-Zero variant}{1137}{subparagraph*.2358}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Head and parameterization}{1138}{subparagraph*.2359}\protected@file@percent }
\abx@aux@cite{0}{peebles2023_dit}
\abx@aux@segm{0}{0}{peebles2023_dit}
\abx@aux@cite{0}{peebles2023_dit}
\abx@aux@segm{0}{0}{peebles2023_dit}
\@writefile{toc}{\contentsline {subparagraph}{Conditioning and guidance}{1139}{subparagraph*.2360}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.144}{\ignorespaces \textbf  {Conditioning ablations.} On DiT-XL/2, adaLN-Zero outperforms alternatives in both speed and FID; cross-attention trades flexibility for extra compute~\blx@tocontentsinit {0}\cite {peebles2023_dit}.}}{1139}{figure.caption.2361}\protected@file@percent }
\abx@aux@backref{1314}{peebles2023_dit}{0}{1139}{1139}
\newlabel{fig:chapter20_dit_cond}{{20.144}{1139}{\textbf {Conditioning ablations.} On DiT-XL/2, adaLN-Zero outperforms alternatives in both speed and FID; cross-attention trades flexibility for extra compute~\cite {peebles2023_dit}}{figure.caption.2361}{}}
\@writefile{toc}{\contentsline {subparagraph}{Training objective and setup}{1139}{subparagraph*.2362}\protected@file@percent }
\abx@aux@cite{0}{peebles2023_dit}
\abx@aux@segm{0}{0}{peebles2023_dit}
\abx@aux@cite{0}{peebles2023_dit}
\abx@aux@segm{0}{0}{peebles2023_dit}
\abx@aux@cite{0}{peebles2023_dit}
\abx@aux@segm{0}{0}{peebles2023_dit}
\abx@aux@cite{0}{peebles2023_dit}
\abx@aux@segm{0}{0}{peebles2023_dit}
\@writefile{toc}{\contentsline {subsubsection}{Experiments and ablations}{1140}{section*.2363}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Scaling and SOTA comparisons.}{1140}{subparagraph*.2364}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.145}{\ignorespaces \textbf  {Scaling behavior and comparison to diffusion baselines}. Left: FID steadily improves with model flops over 400K iterations across S/B/L/XL. Right: DiT-XL/2 is compute-efficient and outperforms prior U-Net diffusion baselines (ADM/LDM). Bubble area indicates flops; credit: Peebles \& Xie~\blx@tocontentsinit {0}\cite {peebles2023_dit}.}}{1140}{figure.caption.2365}\protected@file@percent }
\abx@aux@backref{1316}{peebles2023_dit}{0}{1140}{1140}
\newlabel{fig:chapter20_dit_scaling_sota}{{20.145}{1140}{\textbf {Scaling behavior and comparison to diffusion baselines}. Left: FID steadily improves with model flops over 400K iterations across S/B/L/XL. Right: DiT-XL/2 is compute-efficient and outperforms prior U-Net diffusion baselines (ADM/LDM). Bubble area indicates flops; credit: Peebles \& Xie~\cite {peebles2023_dit}}{figure.caption.2365}{}}
\@writefile{toc}{\contentsline {subparagraph}{Training-time scaling trends.}{1140}{subparagraph*.2366}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.146}{\ignorespaces \textbf  {FID-50K vs.\ training steps under model/patch sweeps}. Scaling depth/width and reducing patch size (more tokens) both improve sample quality at all stages; credit: Peebles \& Xie~\blx@tocontentsinit {0}\cite {peebles2023_dit}.}}{1140}{figure.caption.2367}\protected@file@percent }
\abx@aux@backref{1318}{peebles2023_dit}{0}{1140}{1140}
\newlabel{fig:chapter20_dit_scaling_curves}{{20.146}{1140}{\textbf {FID-50K vs.\ training steps under model/patch sweeps}. Scaling depth/width and reducing patch size (more tokens) both improve sample quality at all stages; credit: Peebles \& Xie~\cite {peebles2023_dit}}{figure.caption.2367}{}}
\abx@aux@cite{0}{peebles2023_dit}
\abx@aux@segm{0}{0}{peebles2023_dit}
\abx@aux@cite{0}{peebles2023_dit}
\abx@aux@segm{0}{0}{peebles2023_dit}
\@writefile{toc}{\contentsline {subparagraph}{Qualitative scaling: more flops $\rightarrow $ better images.}{1141}{subparagraph*.2368}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.147}{\ignorespaces \textbf  {Qualitative scaling analysis}. Bigger backbones and smaller patches yield sharper textures and more coherent structure. The most convincing results appear in the bottom-right (XL with $p{=}2$); credit: Peebles \& Xie~\blx@tocontentsinit {0}\cite {peebles2023_dit}.}}{1141}{figure.caption.2369}\protected@file@percent }
\abx@aux@backref{1320}{peebles2023_dit}{0}{1141}{1141}
\newlabel{fig:chapter20_dit_visual_scaling}{{20.147}{1141}{\textbf {Qualitative scaling analysis}. Bigger backbones and smaller patches yield sharper textures and more coherent structure. The most convincing results appear in the bottom-right (XL with $p{=}2$); credit: Peebles \& Xie~\cite {peebles2023_dit}}{figure.caption.2369}{}}
\abx@aux@cite{0}{peebles2023_dit}
\abx@aux@segm{0}{0}{peebles2023_dit}
\abx@aux@cite{0}{peebles2023_dit}
\abx@aux@segm{0}{0}{peebles2023_dit}
\abx@aux@cite{0}{peebles2023_dit}
\abx@aux@segm{0}{0}{peebles2023_dit}
\abx@aux@cite{0}{peebles2023_dit}
\abx@aux@segm{0}{0}{peebles2023_dit}
\@writefile{toc}{\contentsline {subparagraph}{Gflops predict FID.}{1142}{subparagraph*.2370}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.148}{\ignorespaces \textbf  {Transformer Gflops vs.\ FID-50K}. A strong inverse correlation indicates predictable quality gains with higher compute; credit: Peebles \& Xie~\blx@tocontentsinit {0}\cite {peebles2023_dit}.}}{1142}{figure.caption.2371}\protected@file@percent }
\abx@aux@backref{1322}{peebles2023_dit}{0}{1142}{1142}
\newlabel{fig:chapter20_dit_gflops_fid}{{20.148}{1142}{\textbf {Transformer Gflops vs.\ FID-50K}. A strong inverse correlation indicates predictable quality gains with higher compute; credit: Peebles \& Xie~\cite {peebles2023_dit}}{figure.caption.2371}{}}
\@writefile{toc}{\contentsline {subparagraph}{Total training compute vs.\ FID.}{1142}{subparagraph*.2372}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.149}{\ignorespaces \textbf  {Training compute vs.\ FID}. Larger DiTs use training compute more efficiently, suggesting “train larger for shorter” can be superior to “train smaller for longer”; credit: Peebles \& Xie~\blx@tocontentsinit {0}\cite {peebles2023_dit}.}}{1142}{figure.caption.2373}\protected@file@percent }
\abx@aux@backref{1324}{peebles2023_dit}{0}{1142}{1142}
\newlabel{fig:chapter20_dit_compute_fid}{{20.149}{1142}{\textbf {Training compute vs.\ FID}. Larger DiTs use training compute more efficiently, suggesting “train larger for shorter” can be superior to “train smaller for longer”; credit: Peebles \& Xie~\cite {peebles2023_dit}}{figure.caption.2373}{}}
\abx@aux@cite{0}{peebles2023_dit}
\abx@aux@segm{0}{0}{peebles2023_dit}
\abx@aux@cite{0}{peebles2023_dit}
\abx@aux@segm{0}{0}{peebles2023_dit}
\abx@aux@cite{0}{peebles2023_dit}
\abx@aux@segm{0}{0}{peebles2023_dit}
\@writefile{toc}{\contentsline {subparagraph}{Sampling compute cannot replace model compute.}{1143}{subparagraph*.2374}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {20.150}{\ignorespaces \textbf  {Sampling compute vs.\ FID-10K}. Small models do not close the gap to large ones by sampling longer; large models constitute the lower envelope of the quality–budget frontier; credit: Peebles \& Xie~\blx@tocontentsinit {0}\cite {peebles2023_dit}.}}{1143}{figure.caption.2375}\protected@file@percent }
\abx@aux@backref{1326}{peebles2023_dit}{0}{1143}{1143}
\newlabel{fig:chapter20_dit_sampling_vs_modelcompute}{{20.150}{1143}{\textbf {Sampling compute vs.\ FID-10K}. Small models do not close the gap to large ones by sampling longer; large models constitute the lower envelope of the quality–budget frontier; credit: Peebles \& Xie~\cite {peebles2023_dit}}{figure.caption.2375}{}}
\@writefile{toc}{\contentsline {subparagraph}{Benchmark summary (ImageNet 256/512).}{1143}{subparagraph*.2376}\protected@file@percent }
\abx@aux@backref{1327}{peebles2023_dit}{0}{1143}{1143}
\@writefile{toc}{\contentsline {paragraph}{What changed vs.\ Stable Diffusion and why it matters}{1143}{section*.2377}\protected@file@percent }
\abx@aux@cite{0}{yu2023_ldit}
\abx@aux@segm{0}{0}{yu2023_ldit}
\abx@aux@cite{0}{rombach2022_ldm}
\abx@aux@segm{0}{0}{rombach2022_ldm}
\@writefile{toc}{\contentsline {paragraph}{Relation to prior and follow-ups}{1144}{section*.2378}\protected@file@percent }
\abx@aux@backref{1328}{yu2023_ldit}{0}{1144}{1144}
\abx@aux@backref{1329}{rombach2022_ldm}{0}{1144}{1144}
\@writefile{toc}{\contentsline {subsubsection}{Limitations and future work}{1144}{section*.2379}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Practical recipe}{1144}{section*.2380}\protected@file@percent }
\BKM@entry{id=793,dest={636861707465722E3231},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030325C303030315C3030303A5C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030735C3030305C3034305C3030305C3034365C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C30303073}
\BKM@entry{id=794,dest={73656374696F6E2E32312E31},srcline={13}}{5C3337365C3337375C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C3030305C3034305C303030465C303030695C3030306C5C303030745C303030655C303030725C30303073}
\BKM@entry{id=795,dest={73756273656374696F6E2E32312E312E31},srcline={16}}{5C3337365C3337375C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030465C303030695C303030725C303030735C303030745C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C3030305C3034305C303030465C303030695C3030306C5C303030745C303030655C303030725C30303073}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\abx@aux@cite{0}{he2016_resnet}
\abx@aux@segm{0}{0}{he2016_resnet}
\abx@aux@cite{0}{huang2018_densenet}
\abx@aux@segm{0}{0}{huang2018_densenet}
\@writefile{toc}{\contentsline {chapter}{\numberline {21}Lecture 21: Visualizing Models \& Generating Images}{1145}{chapter.21}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@20}}
\ttl@writefile{ptc}{\ttl@starttoc{default@21}}
\pgfsyspdfmark {pgfid119}{0}{52099153}
\pgfsyspdfmark {pgfid118}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {21.1}Visualizing Layer Filters}{1145}{section.21.1}\protected@file@percent }
\newlabel{sec:chapter21_first_layer_filters}{{21.1}{1145}{Visualizing Layer Filters}{section.21.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.1.1}Visualizing First Layer Filters}{1145}{subsection.21.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Architecture Comparison}{1145}{section*.2381}\protected@file@percent }
\abx@aux@backref{1330}{krizhevsky2012_alexnet}{0}{1145}{1145}
\abx@aux@backref{1331}{he2016_resnet}{0}{1145}{1145}
\abx@aux@backref{1332}{huang2018_densenet}{0}{1145}{1145}
\BKM@entry{id=796,dest={73756273656374696F6E2E32312E312E32},srcline={51}}{5C3337365C3337375C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030485C303030695C303030675C303030685C303030655C303030725C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C3030305C3034305C303030465C303030695C3030306C5C303030745C303030655C303030725C30303073}
\abx@aux@cite{0}{karpathy_convnetjs}
\abx@aux@segm{0}{0}{karpathy_convnetjs}
\@writefile{lof}{\contentsline {figure}{\numberline {21.1}{\ignorespaces Visualization of first-layer convolutional filters from AlexNet, ResNet-18, and DenseNet-121. Each filter is represented as a color image of shape \(3 \times K \times K\), revealing sensitivity to edges, orientations, and color gradients.}}{1146}{figure.caption.2382}\protected@file@percent }
\newlabel{fig:chapter21_first_layer_filters}{{21.1}{1146}{Visualization of first-layer convolutional filters from AlexNet, ResNet-18, and DenseNet-121. Each filter is represented as a color image of shape \(3 \times K \times K\), revealing sensitivity to edges, orientations, and color gradients}{figure.caption.2382}{}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation and Limitations}{1146}{section*.2383}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {21.1.2}Visualizing Higher Layer Filters}{1146}{subsection.21.1.2}\protected@file@percent }
\newlabel{sec:chapter21_higher_layer_filters}{{21.1.2}{1146}{Visualizing Higher Layer Filters}{subsection.21.1.2}{}}
\abx@aux@cite{0}{karpathy_convnetjs}
\abx@aux@segm{0}{0}{karpathy_convnetjs}
\abx@aux@cite{0}{karpathy_convnetjs}
\abx@aux@segm{0}{0}{karpathy_convnetjs}
\@writefile{toc}{\contentsline {paragraph}{Example: ConvNetJS Visualization}{1147}{section*.2384}\protected@file@percent }
\abx@aux@backref{1333}{karpathy_convnetjs}{0}{1147}{1147}
\@writefile{lof}{\contentsline {figure}{\numberline {21.2}{\ignorespaces Raw filter weights from the first three convolutional layers of a ConvNet trained on CIFAR-10. While the first-layer filters display interpretable patterns, deeper filters lack obvious structure, reflecting their abstraction from pixel-level semantics. Visualization source: ConvNetJS~\blx@tocontentsinit {0}\cite {karpathy_convnetjs}.}}{1147}{figure.caption.2385}\protected@file@percent }
\abx@aux@backref{1335}{karpathy_convnetjs}{0}{1147}{1147}
\newlabel{fig:chapter21_higher_layer_filters}{{21.2}{1147}{Raw filter weights from the first three convolutional layers of a ConvNet trained on CIFAR-10. While the first-layer filters display interpretable patterns, deeper filters lack obvious structure, reflecting their abstraction from pixel-level semantics. Visualization source: ConvNetJS~\cite {karpathy_convnetjs}}{figure.caption.2385}{}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation and Motivation for Indirect Methods}{1147}{section*.2386}\protected@file@percent }
\BKM@entry{id=797,dest={73656374696F6E2E32312E32},srcline={86}}{5C3337365C3337375C3030304C5C303030615C303030735C303030745C3030305C3034305C3030304C5C303030615C303030795C303030655C303030725C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C303030735C3030303A5C3030305C3034305C3030304E5C303030655C303030615C303030725C303030655C303030735C303030745C3030305C3034305C3030304E5C303030655C303030695C303030675C303030685C303030625C3030306F5C303030725C303030735C3030302C5C3030305C3034305C303030445C303030695C3030306D5C303030655C3030306E5C303030735C303030695C3030306F5C3030306E5C303030615C3030306C5C303030695C303030745C303030795C3030305C3034305C303030525C303030655C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\BKM@entry{id=798,dest={73756273656374696F6E2E32312E322E31},srcline={92}}{5C3337365C3337375C303030535C303030655C3030306D5C303030615C3030306E5C303030745C303030695C303030635C3030305C3034305C303030535C303030695C3030306D5C303030695C3030306C5C303030615C303030725C303030695C303030745C303030795C3030305C3034305C303030765C303030695C303030615C3030305C3034305C3030304E5C303030655C303030615C303030725C303030655C303030735C303030745C3030305C3034305C3030304E5C303030655C303030695C303030675C303030685C303030625C3030306F5C303030725C30303073}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\@writefile{toc}{\contentsline {section}{\numberline {21.2}Last Layer Features: Nearest Neighbors, Dimensionality Reduction}{1148}{section.21.2}\protected@file@percent }
\newlabel{sec:chapter21_fc_features}{{21.2}{1148}{Last Layer Features: Nearest Neighbors, Dimensionality Reduction}{section.21.2}{}}
\abx@aux@backref{1336}{krizhevsky2012_alexnet}{0}{1148}{1148}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.2.1}Semantic Similarity via Nearest Neighbors}{1148}{subsection.21.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.3}{\ignorespaces Comparison of nearest neighbors for a test image. \textbf  {Left:} Retrieval in raw pixel space, which is sensitive to visual noise and low-level similarity. \textbf  {Right:} Retrieval in the last layer's feature space, which captures object-level semantics such as shape, class, and pose. Figure adapted from~\blx@tocontentsinit {0}\cite {krizhevsky2012_alexnet}.}}{1148}{figure.caption.2387}\protected@file@percent }
\abx@aux@backref{1338}{krizhevsky2012_alexnet}{0}{1148}{1148}
\newlabel{fig:chapter21_nn_fc7}{{21.3}{1148}{Comparison of nearest neighbors for a test image. \textbf {Left:} Retrieval in raw pixel space, which is sensitive to visual noise and low-level similarity. \textbf {Right:} Retrieval in the last layer's feature space, which captures object-level semantics such as shape, class, and pose. Figure adapted from~\cite {krizhevsky2012_alexnet}}{figure.caption.2387}{}}
\BKM@entry{id=799,dest={73756273656374696F6E2E32312E322E32},srcline={106}}{5C3337365C3337375C303030445C303030695C3030306D5C303030655C3030306E5C303030735C303030695C3030306F5C3030306E5C303030615C3030306C5C303030695C303030745C303030795C3030305C3034305C303030525C303030655C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030455C3030306D5C303030625C303030655C303030645C303030645C303030695C3030306E5C303030675C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{maaten2008_tsne}
\abx@aux@segm{0}{0}{maaten2008_tsne}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\abx@aux@cite{0}{maaten2008_tsne}
\abx@aux@segm{0}{0}{maaten2008_tsne}
\abx@aux@cite{0}{krizhevsky2012_alexnet}
\abx@aux@segm{0}{0}{krizhevsky2012_alexnet}
\abx@aux@cite{0}{maaten2008_tsne}
\abx@aux@segm{0}{0}{maaten2008_tsne}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.2.2}Dimensionality Reduction and Embedding Visualization}{1149}{subsection.21.2.2}\protected@file@percent }
\newlabel{sec:chapter21_fc_features_dim_red}{{21.2.2}{1149}{Dimensionality Reduction and Embedding Visualization}{subsection.21.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21.4}{\ignorespaces A 2D t-SNE visualization of feature vectors extracted from the final FC layer of a CNN trained on ImageNet. Each point represents a test image, positioned such that nearby points in the plot correspond to images with similar features. This nonlinear embedding preserves local neighborhoods, revealing the semantic organization of the learned representation space. For instance, in the bottom left, flower images form a coherent cluster that transitions smoothly into butterflies, illustrating how the network encodes visual similarity. Figure adapted from~\blx@tocontentsinit {0}\cite {maaten2008_tsne, krizhevsky2012_alexnet}.}}{1149}{figure.caption.2388}\protected@file@percent }
\abx@aux@backref{1341}{krizhevsky2012_alexnet}{0}{1149}{1149}
\abx@aux@backref{1342}{maaten2008_tsne}{0}{1149}{1149}
\newlabel{fig:chapter21_tsne_projection}{{21.4}{1149}{A 2D t-SNE visualization of feature vectors extracted from the final FC layer of a CNN trained on ImageNet. Each point represents a test image, positioned such that nearby points in the plot correspond to images with similar features. This nonlinear embedding preserves local neighborhoods, revealing the semantic organization of the learned representation space. For instance, in the bottom left, flower images form a coherent cluster that transitions smoothly into butterflies, illustrating how the network encodes visual similarity. Figure adapted from~\cite {maaten2008_tsne, krizhevsky2012_alexnet}}{figure.caption.2388}{}}
\abx@aux@cite{0}{paepper2023_sdembeddingviz}
\abx@aux@segm{0}{0}{paepper2023_sdembeddingviz}
\abx@aux@cite{0}{paepper2023_sdembeddingviz}
\abx@aux@segm{0}{0}{paepper2023_sdembeddingviz}
\abx@aux@backref{1343}{maaten2008_tsne}{0}{1150}{1150}
\@writefile{lof}{\contentsline {figure}{\numberline {21.5}{\ignorespaces A t-SNE visualization of image embeddings generated by Stable Diffusion. Each point represents a high-dimensional image embedding projected into 2D space. Notably, several red apples are embedded close to tomatoes, likely due to visual similarity in shape and color (both being red and round). This kind of confusion highlights how the model organizes its internal representation space and helps diagnose classification ambiguity. Such insights can inform improvements like augmenting the training set, refining class definitions, or modifying the architecture to better separate semantically similar classes. Visualization adapted from~\blx@tocontentsinit {0}\cite {paepper2023_sdembeddingviz}.}}{1150}{figure.caption.2389}\protected@file@percent }
\abx@aux@backref{1345}{paepper2023_sdembeddingviz}{0}{1150}{1150}
\newlabel{fig:chapter21_apples_tomatoes_tsne}{{21.5}{1150}{A t-SNE visualization of image embeddings generated by Stable Diffusion. Each point represents a high-dimensional image embedding projected into 2D space. Notably, several red apples are embedded close to tomatoes, likely due to visual similarity in shape and color (both being red and round). This kind of confusion highlights how the model organizes its internal representation space and helps diagnose classification ambiguity. Such insights can inform improvements like augmenting the training set, refining class definitions, or modifying the architecture to better separate semantically similar classes. Visualization adapted from~\cite {paepper2023_sdembeddingviz}}{figure.caption.2389}{}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation and Applications}{1150}{section*.2390}\protected@file@percent }
\BKM@entry{id=800,dest={73656374696F6E2E32312E33},srcline={157}}{5C3337365C3337375C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030415C303030635C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304D5C303030615C303030785C303030695C3030306D5C303030615C3030306C5C3030306C5C303030795C3030305C3034305C303030415C303030635C303030745C303030695C303030765C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030505C303030615C303030745C303030635C303030685C303030655C30303073}
\abx@aux@cite{0}{yosinski2015_deepviz}
\abx@aux@segm{0}{0}{yosinski2015_deepviz}
\abx@aux@cite{0}{yosinski2015_deepviz}
\abx@aux@segm{0}{0}{yosinski2015_deepviz}
\@writefile{toc}{\contentsline {section}{\numberline {21.3}Visualizing Activations and Maximally Activating Patches}{1151}{section.21.3}\protected@file@percent }
\newlabel{sec:chapter21_activations_max_patches}{{21.3}{1151}{Visualizing Activations and Maximally Activating Patches}{section.21.3}{}}
\@writefile{toc}{\contentsline {paragraph}{How to Visualize Activations}{1151}{section*.2391}\protected@file@percent }
\abx@aux@cite{0}{luo2017_understanding_receptive_field}
\abx@aux@segm{0}{0}{luo2017_understanding_receptive_field}
\@writefile{lof}{\contentsline {figure}{\numberline {21.6}{\ignorespaces Example activations from the \texttt  {conv5} layer of a CNN. Each grayscale patch shows the activation map of a single filter. Brighter regions correspond to stronger activations. The predominance of dark areas arises from two key effects: (1) the use of ReLU, which sets all negative pre-activation values to zero, producing sparse feature maps; and (2) the visualization step, which rescales each map—originally containing real-valued outputs from \(-\infty \) to \(+\infty \)—into the \([0, 255]\) display range. When most values are near zero, this rescaling flattens the output, making subtle responses appear uniformly dark. Figure adapted from~\blx@tocontentsinit {0}\cite {yosinski2015_deepviz}.}}{1152}{figure.caption.2392}\protected@file@percent }
\abx@aux@backref{1347}{yosinski2015_deepviz}{0}{1152}{1152}
\newlabel{fig:chapter21_conv5_activations}{{21.6}{1152}{Example activations from the \texttt {conv5} layer of a CNN. Each grayscale patch shows the activation map of a single filter. Brighter regions correspond to stronger activations. The predominance of dark areas arises from two key effects: (1) the use of ReLU, which sets all negative pre-activation values to zero, producing sparse feature maps; and (2) the visualization step, which rescales each map—originally containing real-valued outputs from \(-\infty \) to \(+\infty \)—into the \([0, 255]\) display range. When most values are near zero, this rescaling flattens the output, making subtle responses appear uniformly dark. Figure adapted from~\cite {yosinski2015_deepviz}}{figure.caption.2392}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Do Activation Maps Reveal Spatial Information?}{1152}{section*.2393}\protected@file@percent }
\abx@aux@backref{1348}{luo2017_understanding_receptive_field}{0}{1152}{1152}
\abx@aux@cite{0}{zeiler2014_visualizing}
\abx@aux@segm{0}{0}{zeiler2014_visualizing}
\abx@aux@cite{0}{yosinski2015_deepviz}
\abx@aux@segm{0}{0}{yosinski2015_deepviz}
\abx@aux@cite{0}{bau2017_network_dissection}
\abx@aux@segm{0}{0}{bau2017_network_dissection}
\abx@aux@cite{0}{clevert2016_fast_and_accurate}
\abx@aux@segm{0}{0}{clevert2016_fast_and_accurate}
\@writefile{toc}{\contentsline {paragraph}{What Do Activations Reveal?}{1153}{section*.2394}\protected@file@percent }
\abx@aux@backref{1349}{zeiler2014_visualizing}{0}{1153}{1153}
\abx@aux@backref{1350}{bau2017_network_dissection}{0}{1153}{1153}
\abx@aux@backref{1351}{yosinski2015_deepviz}{0}{1153}{1153}
\abx@aux@backref{1352}{clevert2016_fast_and_accurate}{0}{1153}{1153}
\@writefile{toc}{\contentsline {paragraph}{What Can We Do With Activation Maps?}{1153}{section*.2395}\protected@file@percent }
\newlabel{par:chapter21_what_to_do_activations}{{21.3}{1153}{What Can We Do With Activation Maps?}{section*.2395}{}}
\BKM@entry{id=801,dest={73756273656374696F6E2E32312E332E31},srcline={240}}{5C3337365C3337375C3030304D5C303030615C303030785C303030695C3030306D5C303030615C3030306C5C3030306C5C303030795C3030305C3034305C303030415C303030635C303030745C303030695C303030765C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030505C303030615C303030745C303030635C303030685C303030655C30303073}
\abx@aux@cite{0}{springenberg2015_allconv}
\abx@aux@segm{0}{0}{springenberg2015_allconv}
\abx@aux@cite{0}{springenberg2015_allconv}
\abx@aux@segm{0}{0}{springenberg2015_allconv}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.3.1}Maximally Activating Patches}{1154}{subsection.21.3.1}\protected@file@percent }
\newlabel{subsec:chapter21_max_patches}{{21.3.1}{1154}{Maximally Activating Patches}{subsection.21.3.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Methodology}{1154}{section*.2396}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.7}{\ignorespaces Maximally activating input patches for various neurons in a CNN. Each row shows patches from different input images that produced high activations for a specific neuron. These patches often reveal consistent visual motifs—such as specific textures, faces, or object parts—suggesting that the neuron has become specialized for detecting that pattern. Figure adapted from~\blx@tocontentsinit {0}\cite {springenberg2015_allconv}.}}{1154}{figure.caption.2397}\protected@file@percent }
\abx@aux@backref{1354}{springenberg2015_allconv}{0}{1154}{1154}
\newlabel{fig:chapter21_max_patches}{{21.7}{1154}{Maximally activating input patches for various neurons in a CNN. Each row shows patches from different input images that produced high activations for a specific neuron. These patches often reveal consistent visual motifs—such as specific textures, faces, or object parts—suggesting that the neuron has become specialized for detecting that pattern. Figure adapted from~\cite {springenberg2015_allconv}}{figure.caption.2397}{}}
\@writefile{toc}{\contentsline {paragraph}{Intuition and Insights}{1155}{section*.2398}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{From “What It Sees” to “What It Uses”}{1155}{section*.2399}\protected@file@percent }
\BKM@entry{id=802,dest={73656374696F6E2E32312E34},srcline={311}}{5C3337365C3337375C303030535C303030615C3030306C5C303030695C303030655C3030306E5C303030635C303030795C3030305C3034305C303030765C303030695C303030615C3030305C3034305C3030304F5C303030635C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=803,dest={73756273656374696F6E2E32312E342E31},srcline={317}}{5C3337365C3337375C3030304F5C303030635C303030635C3030306C5C303030755C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030535C303030655C3030306E5C303030735C303030695C303030745C303030695C303030765C303030695C303030745C30303079}
\abx@aux@cite{0}{zeiler2014_visualizing}
\abx@aux@segm{0}{0}{zeiler2014_visualizing}
\abx@aux@cite{0}{zeiler2014_visualizing}
\abx@aux@segm{0}{0}{zeiler2014_visualizing}
\abx@aux@cite{0}{zeiler2014_visualizing}
\abx@aux@segm{0}{0}{zeiler2014_visualizing}
\@writefile{toc}{\contentsline {section}{\numberline {21.4}Saliency via Occlusion and Backpropagation}{1156}{section.21.4}\protected@file@percent }
\newlabel{sec:chapter21_saliency}{{21.4}{1156}{Saliency via Occlusion and Backpropagation}{section.21.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.4.1}Occlusion Sensitivity}{1156}{subsection.21.4.1}\protected@file@percent }
\newlabel{subsec:chapter21_occlusion_sensitivity}{{21.4.1}{1156}{Occlusion Sensitivity}{subsection.21.4.1}{}}
\abx@aux@backref{1355}{zeiler2014_visualizing}{0}{1156}{1156}
\@writefile{lof}{\contentsline {figure}{\numberline {21.8}{\ignorespaces Occlusion-based saliency maps~\blx@tocontentsinit {0}\cite {zeiler2014_visualizing}. Each image patch is occluded in turn, and the drop in class confidence is recorded. \textbf  {Darker regions indicate locations where occlusion most reduced the model's confidence}, corresponding to spatial regions that were most important to the prediction.}}{1156}{figure.caption.2400}\protected@file@percent }
\abx@aux@backref{1357}{zeiler2014_visualizing}{0}{1156}{1156}
\newlabel{fig:chapter21_occlusion_saliency}{{21.8}{1156}{Occlusion-based saliency maps~\cite {zeiler2014_visualizing}. Each image patch is occluded in turn, and the drop in class confidence is recorded. \textbf {Darker regions indicate locations where occlusion most reduced the model's confidence}, corresponding to spatial regions that were most important to the prediction}{figure.caption.2400}{}}
\@writefile{toc}{\contentsline {paragraph}{Methodology}{1156}{section*.2401}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{From Patch Scores to Pixel-Level Saliency}{1156}{section*.2402}\protected@file@percent }
\BKM@entry{id=804,dest={73756273656374696F6E2E32312E342E32},srcline={377}}{5C3337365C3337375C303030535C303030615C3030306C5C303030695C303030655C3030306E5C303030635C303030795C3030305C3034305C303030765C303030695C303030615C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{simonyan2014_deepinside}
\abx@aux@segm{0}{0}{simonyan2014_deepinside}
\abx@aux@cite{0}{simonyan2014_deepinside}
\abx@aux@segm{0}{0}{simonyan2014_deepinside}
\abx@aux@cite{0}{simonyan2014_deepinside}
\abx@aux@segm{0}{0}{simonyan2014_deepinside}
\@writefile{toc}{\contentsline {paragraph}{Intuition and Interpretation}{1157}{section*.2403}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 21.4.1.1: Advantages and Limitations of Occlusion Sensitivity}{1157}{section*.2404}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {21.4.2}Saliency via Gradient Backpropagation}{1157}{subsection.21.4.2}\protected@file@percent }
\newlabel{subsec:chapter21_saliency_backprop}{{21.4.2}{1157}{Saliency via Gradient Backpropagation}{subsection.21.4.2}{}}
\abx@aux@backref{1358}{simonyan2014_deepinside}{0}{1157}{1157}
\abx@aux@cite{0}{simonyan2014_deepinside}
\abx@aux@segm{0}{0}{simonyan2014_deepinside}
\abx@aux@cite{0}{simonyan2014_deepinside}
\abx@aux@segm{0}{0}{simonyan2014_deepinside}
\abx@aux@cite{0}{simonyan2014_deepinside}
\abx@aux@segm{0}{0}{simonyan2014_deepinside}
\abx@aux@cite{0}{rother2004_grabcut}
\abx@aux@segm{0}{0}{rother2004_grabcut}
\@writefile{lof}{\contentsline {figure}{\numberline {21.9}{\ignorespaces Gradient-based saliency map~\blx@tocontentsinit {0}\cite {simonyan2014_deepinside}: pixel importance is estimated by computing the gradient of the class score with respect to each input pixel. Brighter regions correspond to pixels where small changes most strongly influence the model's output for the predicted class. In this example, the saliency map highlights the dog's shape—indicating that the network's decision relies on semantically meaningful regions of the input image.}}{1158}{figure.caption.2405}\protected@file@percent }
\abx@aux@backref{1360}{simonyan2014_deepinside}{0}{1158}{1158}
\newlabel{fig:chapter21_gradient_saliency}{{21.9}{1158}{Gradient-based saliency map~\cite {simonyan2014_deepinside}: pixel importance is estimated by computing the gradient of the class score with respect to each input pixel. Brighter regions correspond to pixels where small changes most strongly influence the model's output for the predicted class. In this example, the saliency map highlights the dog's shape—indicating that the network's decision relies on semantically meaningful regions of the input image}{figure.caption.2405}{}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation and Use Cases}{1158}{section*.2406}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Towards Unsupervised Segmentation}{1158}{section*.2407}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.10}{\ignorespaces Foreground extraction via GrabCut applied to saliency maps. Although no explicit segmentation labels are used, the resulting masks capture object shapes such as birds, snakes, and insects—indicating that CNN attention aligns well with perceptually salient regions. Figure adapted from~\blx@tocontentsinit {0}\cite {simonyan2014_deepinside}.}}{1158}{figure.caption.2408}\protected@file@percent }
\abx@aux@backref{1362}{simonyan2014_deepinside}{0}{1158}{1158}
\newlabel{fig:chapter21_grabcut_saliency}{{21.10}{1158}{Foreground extraction via GrabCut applied to saliency maps. Although no explicit segmentation labels are used, the resulting masks capture object shapes such as birds, snakes, and insects—indicating that CNN attention aligns well with perceptually salient regions. Figure adapted from~\cite {simonyan2014_deepinside}}{figure.caption.2408}{}}
\BKM@entry{id=805,dest={73656374696F6E2E32312E35},srcline={417}}{5C3337365C3337375C303030475C303030755C303030695C303030645C303030655C303030645C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C303030665C3030305C3034305C303030495C3030306E5C303030745C303030655C303030725C3030306D5C303030655C303030645C303030695C303030615C303030745C303030655C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C30303073}
\BKM@entry{id=806,dest={73756273656374696F6E2E32312E352E31},srcline={423}}{5C3337365C3337375C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030655C3030305C3034305C303030495C3030306E5C303030745C303030655C303030725C3030306D5C303030655C303030645C303030695C303030615C303030745C303030655C3030305C3034305C3030304E5C303030655C303030755C303030725C3030306F5C3030306E5C30303073}
\BKM@entry{id=807,dest={73756273656374696F6E2E32312E352E32},srcline={436}}{5C3337365C3337375C303030475C303030755C303030695C303030645C303030655C303030645C3030305C3034305C303030425C303030615C303030635C3030306B5C303030705C303030725C3030306F5C303030705C303030615C303030675C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030435C3030306C5C303030655C303030615C3030306E5C303030655C303030725C3030305C3034305C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\abx@aux@cite{0}{springenberg2015_allconv}
\abx@aux@segm{0}{0}{springenberg2015_allconv}
\abx@aux@backref{1363}{simonyan2014_deepinside}{0}{1159}{1159}
\abx@aux@backref{1364}{rother2004_grabcut}{0}{1159}{1159}
\@writefile{toc}{\contentsline {section}{\numberline {21.5}Guided Backpropagation of Intermediate Features}{1159}{section.21.5}\protected@file@percent }
\newlabel{sec:chapter21_guided_backprop}{{21.5}{1159}{Guided Backpropagation of Intermediate Features}{section.21.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.5.1}Backpropagation to Visualize Intermediate Neurons}{1159}{subsection.21.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {21.5.2}Guided Backpropagation: Cleaner Gradient Visualizations}{1159}{subsection.21.5.2}\protected@file@percent }
\newlabel{subsec:chapter21_guided_backprop}{{21.5.2}{1159}{Guided Backpropagation: Cleaner Gradient Visualizations}{subsection.21.5.2}{}}
\abx@aux@backref{1365}{springenberg2015_allconv}{0}{1159}{1159}
\abx@aux@cite{0}{springenberg2015_allconv}
\abx@aux@segm{0}{0}{springenberg2015_allconv}
\abx@aux@cite{0}{springenberg2015_allconv}
\abx@aux@segm{0}{0}{springenberg2015_allconv}
\BKM@entry{id=808,dest={73756273656374696F6E2E32312E352E33},srcline={467}}{5C3337365C3337375C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030495C3030306E5C303030745C303030655C303030725C3030306D5C303030655C303030645C303030695C303030615C303030745C303030655C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030445C303030655C303030745C303030655C303030635C303030745C3030306F5C303030725C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {21.11}{\ignorespaces Comparison of gradient flow in standard backpropagation vs. guided backpropagation. The latter only allows gradients to pass through ReLU units when both the activation and incoming gradient are positive—resulting in sharper and more interpretable saliency maps. Figure adapted from~\blx@tocontentsinit {0}\cite {springenberg2015_allconv}.}}{1160}{figure.caption.2409}\protected@file@percent }
\abx@aux@backref{1367}{springenberg2015_allconv}{0}{1160}{1160}
\newlabel{fig:chapter21_guided_backprop_masking}{{21.11}{1160}{Comparison of gradient flow in standard backpropagation vs. guided backpropagation. The latter only allows gradients to pass through ReLU units when both the activation and incoming gradient are positive—resulting in sharper and more interpretable saliency maps. Figure adapted from~\cite {springenberg2015_allconv}}{figure.caption.2409}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Does This Help? Intuition and Impact}{1160}{section*.2410}\protected@file@percent }
\BKM@entry{id=809,dest={73656374696F6E2E32312E36},srcline={492}}{5C3337365C3337375C303030475C303030725C303030615C303030645C303030695C303030655C3030306E5C303030745C3030305C3034305C303030415C303030735C303030635C303030655C3030306E5C303030745C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.5.3}Visualizing Intermediate Feature Detectors}{1161}{subsection.21.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.12}{\ignorespaces Visualizing intermediate features using guided backpropagation. Each row corresponds to one neuron. \textbf  {Left:} Input patches from the dataset that maximally activated the neuron. \textbf  {Right:} Guided backpropagation visualizations showing which pixels in the patch most contributed to the activation.}}{1161}{figure.caption.2411}\protected@file@percent }
\newlabel{fig:chapter21_guided_backprop_neurons}{{21.12}{1161}{Visualizing intermediate features using guided backpropagation. Each row corresponds to one neuron. \textbf {Left:} Input patches from the dataset that maximally activated the neuron. \textbf {Right:} Guided backpropagation visualizations showing which pixels in the patch most contributed to the activation}{figure.caption.2411}{}}
\@writefile{toc}{\contentsline {paragraph}{From Saliency to Synthesis}{1161}{section*.2412}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {21.6}Gradient Ascent and Class Visualization}{1161}{section.21.6}\protected@file@percent }
\newlabel{sec:chapter21_gradient_ascent}{{21.6}{1161}{Gradient Ascent and Class Visualization}{section.21.6}{}}
\BKM@entry{id=810,dest={73756273656374696F6E2E32312E362E31},srcline={525}}{5C3337365C3337375C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C3030304D5C303030615C3030306B5C303030695C3030306E5C303030675C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C303030735C3030305C3034305C3030304C5C3030306F5C3030306F5C3030306B5C3030305C3034305C3030304E5C303030615C303030745C303030755C303030725C303030615C3030306C}
\abx@aux@cite{0}{simonyan2014_deepinside}
\abx@aux@segm{0}{0}{simonyan2014_deepinside}
\abx@aux@cite{0}{simonyan2014_deepinside}
\abx@aux@segm{0}{0}{simonyan2014_deepinside}
\abx@aux@cite{0}{simonyan2014_deepinside}
\abx@aux@segm{0}{0}{simonyan2014_deepinside}
\@writefile{toc}{\contentsline {paragraph}{Objective Function}{1162}{section*.2413}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Optimization via Gradient Ascent}{1162}{section*.2414}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.13}{\ignorespaces Gradient ascent loop: we synthesize an image \(I\) to maximize neuron output by repeatedly updating it with the gradient \(\nabla _I f(I)\).}}{1162}{figure.caption.2415}\protected@file@percent }
\newlabel{fig:chapter21_gradient_ascent_loop}{{21.13}{1162}{Gradient ascent loop: we synthesize an image \(I\) to maximize neuron output by repeatedly updating it with the gradient \(\nabla _I f(I)\)}{figure.caption.2415}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.6.1}Regularization: Making Images Look Natural}{1162}{subsection.21.6.1}\protected@file@percent }
\abx@aux@backref{1368}{simonyan2014_deepinside}{0}{1162}{1162}
\BKM@entry{id=811,dest={73756273656374696F6E2E32312E362E32},srcline={556}}{5C3337365C3337375C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030495C3030306E5C303030745C303030655C303030725C3030306D5C303030655C303030645C303030695C303030615C303030745C303030655C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {21.14}{\ignorespaces Synthetic images generated by optimizing the input to maximally activate specific output neurons (e.g., \texttt  {dumbbell}, \texttt  {dalmatian}), using simple \(\ell _2\) regularization to encourage smoothness. Distinct visual features—such as dumbbell handles or dalmatian-like black-and-white spots—emerge in the synthesized inputs, offering insight into the discriminative patterns the network associates with each class. Figure adapted from~\blx@tocontentsinit {0}\cite {simonyan2014_deepinside}.}}{1163}{figure.caption.2416}\protected@file@percent }
\abx@aux@backref{1370}{simonyan2014_deepinside}{0}{1163}{1163}
\newlabel{fig:chapter21_naive_l2_regularization}{{21.14}{1163}{Synthetic images generated by optimizing the input to maximally activate specific output neurons (e.g., \texttt {dumbbell}, \texttt {dalmatian}), using simple \(\ell _2\) regularization to encourage smoothness. Distinct visual features—such as dumbbell handles or dalmatian-like black-and-white spots—emerge in the synthesized inputs, offering insight into the discriminative patterns the network associates with each class. Figure adapted from~\cite {simonyan2014_deepinside}}{figure.caption.2416}{}}
\@writefile{toc}{\contentsline {paragraph}{Advanced Regularizers}{1163}{section*.2417}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.15}{\ignorespaces Improved results using enhanced regularizers: clear patterns emerge that resemble flamingos, cobras, pelicans, and beetles, according to their respective class synthesized image.}}{1163}{figure.caption.2418}\protected@file@percent }
\newlabel{fig:chapter21_enhanced_regularization}{{21.15}{1163}{Improved results using enhanced regularizers: clear patterns emerge that resemble flamingos, cobras, pelicans, and beetles, according to their respective class synthesized image}{figure.caption.2418}{}}
\abx@aux@cite{0}{nguyen2016_multifaceted}
\abx@aux@segm{0}{0}{nguyen2016_multifaceted}
\abx@aux@cite{0}{nguyen2016_multifaceted}
\abx@aux@segm{0}{0}{nguyen2016_multifaceted}
\abx@aux@cite{0}{nguyen2016_multifaceted}
\abx@aux@segm{0}{0}{nguyen2016_multifaceted}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.6.2}Visualizing Intermediate Features}{1164}{subsection.21.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.16}{\ignorespaces Neuron visualizations at different layers: eye-like motifs, spider-like webs in layer 5, and red/green blobs in layer 3.}}{1164}{figure.caption.2419}\protected@file@percent }
\newlabel{fig:chapter21_synthetic_intermediate_patterns}{{21.16}{1164}{Neuron visualizations at different layers: eye-like motifs, spider-like webs in layer 5, and red/green blobs in layer 3}{figure.caption.2419}{}}
\@writefile{toc}{\contentsline {subsubsection}{Multifaceted Feature Visualization via Generative Models}{1164}{section*.2420}\protected@file@percent }
\abx@aux@backref{1371}{nguyen2016_multifaceted}{0}{1164}{1164}
\abx@aux@cite{0}{nguyen2016_multifaceted}
\abx@aux@segm{0}{0}{nguyen2016_multifaceted}
\abx@aux@cite{0}{nguyen2016_multifaceted}
\abx@aux@segm{0}{0}{nguyen2016_multifaceted}
\BKM@entry{id=812,dest={73656374696F6E2E32312E37},srcline={594}}{5C3337365C3337375C303030415C303030645C303030765C303030655C303030725C303030735C303030615C303030725C303030695C303030615C3030306C5C3030305C3034305C303030455C303030785C303030615C3030306D5C303030705C3030306C5C303030655C303030735C3030303A5C3030305C3034305C303030415C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C303030445C303030695C303030765C303030655C3030305C3034305C303030695C3030306E5C303030745C3030306F5C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C3030305C3034305C303030565C303030755C3030306C5C3030306E5C303030655C303030725C303030615C303030625C303030695C3030306C5C303030695C303030745C30303079}
\@writefile{lof}{\contentsline {figure}{\numberline {21.17}{\ignorespaces Examples of realistic images synthesized using the multifaceted feature visualization approach such as 'strawberry', 'orange'. Figure adapted from~\blx@tocontentsinit {0}\cite {nguyen2016_multifaceted}.}}{1165}{figure.caption.2421}\protected@file@percent }
\abx@aux@backref{1373}{nguyen2016_multifaceted}{0}{1165}{1165}
\newlabel{fig:chapter21_multifaceted}{{21.17}{1165}{Examples of realistic images synthesized using the multifaceted feature visualization approach such as 'strawberry', 'orange'. Figure adapted from~\cite {nguyen2016_multifaceted}}{figure.caption.2421}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21.18}{\ignorespaces Examples of realistic images synthesized via a generative model to maximally activate neurons associated with classes like “toaster”, “triumphal arch”, “cellphone”, and others. The approach ensures that each synthesized image remains within the natural image manifold. Figure adapted from~\blx@tocontentsinit {0}\cite {nguyen2016_multifaceted}.}}{1165}{figure.caption.2422}\protected@file@percent }
\abx@aux@backref{1375}{nguyen2016_multifaceted}{0}{1165}{1165}
\newlabel{fig:chapter21_multifaceted_modes}{{21.18}{1165}{Examples of realistic images synthesized via a generative model to maximally activate neurons associated with classes like “toaster”, “triumphal arch”, “cellphone”, and others. The approach ensures that each synthesized image remains within the natural image manifold. Figure adapted from~\cite {nguyen2016_multifaceted}}{figure.caption.2422}{}}
\@writefile{toc}{\contentsline {paragraph}{Realism vs. Fidelity}{1165}{section*.2423}\protected@file@percent }
\BKM@entry{id=813,dest={73756273656374696F6E2E32312E372E31},srcline={600}}{5C3337365C3337375C303030465C303030755C3030306E5C303030645C303030615C3030306D5C303030655C3030306E5C303030745C303030615C3030306C5C3030305C3034305C303030415C303030745C303030745C303030615C303030635C3030306B5C3030305C3034305C3030304D5C303030655C303030635C303030685C303030615C3030306E5C303030695C303030735C3030306D5C30303073}
\@writefile{toc}{\contentsline {section}{\numberline {21.7}Adversarial Examples: A Deep Dive into Model Vulnerability}{1166}{section.21.7}\protected@file@percent }
\newlabel{subsec:chapter21_adversarial_examples}{{21.7}{1166}{Adversarial Examples: A Deep Dive into Model Vulnerability}{section.21.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.7.1}Fundamental Attack Mechanisms}{1166}{subsection.21.7.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.19}{\ignorespaces Adversarial examples: small, visually indistinguishable perturbations can cause drastic misclassifications—e.g., an elephant becomes a koala, and a schooner becomes an iPod.}}{1166}{figure.caption.2424}\protected@file@percent }
\newlabel{fig:chapter21_adversarial_perturbation}{{21.19}{1166}{Adversarial examples: small, visually indistinguishable perturbations can cause drastic misclassifications—e.g., an elephant becomes a koala, and a schooner becomes an iPod}{figure.caption.2424}{}}
\BKM@entry{id=814,dest={73756273656374696F6E2E32312E372E32},srcline={628}}{5C3337365C3337375C303030545C303030615C303030785C3030306F5C3030306E5C3030306F5C3030306D5C303030795C3030305C3034305C3030306F5C303030665C3030305C3034305C303030415C303030645C303030765C303030655C303030725C303030735C303030615C303030725C303030695C303030615C3030306C5C3030305C3034305C303030415C303030745C303030745C303030615C303030635C3030306B5C30303073}
\abx@aux@cite{0}{goodfellow2015_explaining}
\abx@aux@segm{0}{0}{goodfellow2015_explaining}
\abx@aux@cite{0}{madry2018_towards}
\abx@aux@segm{0}{0}{madry2018_towards}
\abx@aux@cite{0}{madry2018_towards}
\abx@aux@segm{0}{0}{madry2018_towards}
\abx@aux@cite{0}{carlini2017_towards}
\abx@aux@segm{0}{0}{carlini2017_towards}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.7.2}Taxonomy of Adversarial Attacks}{1167}{subsection.21.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{White-box attacks}{1167}{section*.2425}\protected@file@percent }
\abx@aux@backref{1376}{goodfellow2015_explaining}{0}{1167}{1167}
\abx@aux@backref{1377}{madry2018_towards}{0}{1167}{1167}
\abx@aux@backref{1378}{madry2018_towards}{0}{1167}{1167}
\abx@aux@cite{0}{chen2020_hopskipjump}
\abx@aux@segm{0}{0}{chen2020_hopskipjump}
\abx@aux@cite{0}{moosavi2017_universal}
\abx@aux@segm{0}{0}{moosavi2017_universal}
\abx@aux@backref{1379}{carlini2017_towards}{0}{1168}{1168}
\@writefile{toc}{\contentsline {paragraph}{Black-box attacks}{1168}{section*.2426}\protected@file@percent }
\abx@aux@backref{1380}{chen2020_hopskipjump}{0}{1168}{1168}
\abx@aux@backref{1381}{moosavi2017_universal}{0}{1168}{1168}
\BKM@entry{id=815,dest={73756273656374696F6E2E32312E372E33},srcline={710}}{5C3337365C3337375C3030304D5C303030695C3030306C5C303030655C303030735C303030745C3030306F5C3030306E5C303030655C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030525C3030306F5C303030625C303030755C303030735C303030745C3030306E5C303030655C303030735C303030735C3030305C3034305C303030455C303030765C303030615C3030306C5C303030755C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{carlini2017_towards}
\abx@aux@segm{0}{0}{carlini2017_towards}
\abx@aux@cite{0}{carlini2020_adaptive}
\abx@aux@segm{0}{0}{carlini2020_adaptive}
\abx@aux@cite{0}{carlini2017_defensive_distillation}
\abx@aux@segm{0}{0}{carlini2017_defensive_distillation}
\abx@aux@cite{0}{carlini2023_universal_llm}
\abx@aux@segm{0}{0}{carlini2023_universal_llm}
\BKM@entry{id=816,dest={73756273656374696F6E2E32312E372E34},srcline={723}}{5C3337365C3337375C303030445C303030655C303030665C303030655C3030306E5C303030735C303030655C3030305C3034305C303030545C3030306F5C3030306F5C3030306C5C303030625C3030306F5C303030785C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030495C303030745C303030735C3030305C3034305C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\abx@aux@cite{0}{madry2018_towards}
\abx@aux@segm{0}{0}{madry2018_towards}
\@writefile{lof}{\contentsline {figure}{\numberline {21.20}{\ignorespaces White-box attacks use internal gradients to craft precise perturbations. Black-box attacks rely on queries or surrogate transfer to mislead the model without internal access.}}{1169}{figure.caption.2427}\protected@file@percent }
\newlabel{fig:chapter21_attack_taxonomy}{{21.20}{1169}{White-box attacks use internal gradients to craft precise perturbations. Black-box attacks rely on queries or surrogate transfer to mislead the model without internal access}{figure.caption.2427}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.7.3}Milestones in Robustness Evaluation}{1169}{subsection.21.7.3}\protected@file@percent }
\abx@aux@backref{1382}{carlini2017_towards}{0}{1169}{1169}
\abx@aux@backref{1383}{carlini2020_adaptive}{0}{1169}{1169}
\abx@aux@backref{1384}{carlini2017_defensive_distillation}{0}{1169}{1169}
\abx@aux@backref{1385}{carlini2023_universal_llm}{0}{1169}{1169}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.7.4}Defense Toolbox and Its Limitations}{1169}{subsection.21.7.4}\protected@file@percent }
\abx@aux@backref{1386}{madry2018_towards}{0}{1169}{1169}
\BKM@entry{id=817,dest={73756273656374696F6E2E32312E372E35},srcline={736}}{5C3337365C3337375C303030525C303030655C303030615C3030306C5C3030302D5C303030575C3030306F5C303030725C3030306C5C303030645C3030305C3034305C303030525C303030655C3030306C5C303030655C303030765C303030615C3030306E5C303030635C303030655C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030505C303030655C303030725C303030735C303030695C303030735C303030745C303030655C3030306E5C303030745C3030305C3034305C303030525C303030695C303030735C3030306B5C30303073}
\BKM@entry{id=818,dest={73756273656374696F6E2E32312E372E36},srcline={747}}{5C3337365C3337375C3030304F5C303030705C303030655C3030306E5C3030305C3034305C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030545C303030685C303030655C3030306F5C303030725C303030655C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C303030435C3030306F5C3030306E5C3030306E5C303030655C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\abx@aux@cite{0}{nakkiran2020_deep_double_descent}
\abx@aux@segm{0}{0}{nakkiran2020_deep_double_descent}
\BKM@entry{id=819,dest={73656374696F6E2E32312E38},srcline={751}}{5C3337365C3337375C303030435C3030306C5C303030615C303030735C303030735C3030305C3034305C303030415C303030635C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C303030615C303030705C303030705C303030695C3030306E5C303030675C3030305C3034305C3030305C3035305C303030435C303030415C3030304D5C3030305C3035315C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030475C303030725C303030615C303030645C3030302D5C303030435C303030415C3030304D}
\abx@aux@cite{0}{zhou2016_cam}
\abx@aux@segm{0}{0}{zhou2016_cam}
\abx@aux@cite{0}{zhou2016_cam}
\abx@aux@segm{0}{0}{zhou2016_cam}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.7.5}Real-World Relevance and Persistent Risks}{1170}{subsection.21.7.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {21.7.6}Open Challenges and Theoretical Connections}{1170}{subsection.21.7.6}\protected@file@percent }
\abx@aux@backref{1387}{nakkiran2020_deep_double_descent}{0}{1170}{1170}
\@writefile{toc}{\contentsline {section}{\numberline {21.8}Class Activation Mapping (CAM) and Grad-CAM}{1170}{section.21.8}\protected@file@percent }
\newlabel{sec:chapter21_cam_gradcam}{{21.8}{1170}{Class Activation Mapping (CAM) and Grad-CAM}{section.21.8}{}}
\abx@aux@backref{1388}{zhou2016_cam}{0}{1170}{1170}
\@writefile{toc}{\contentsline {paragraph}{Mechanism of CAM}{1170}{section*.2428}\protected@file@percent }
\abx@aux@backref{1389}{zhou2016_cam}{0}{1170}{1170}
\@writefile{lof}{\contentsline {figure}{\numberline {21.21}{\ignorespaces CAM pipeline: from feature maps to class-specific weighted sums, resulting in localization maps.}}{1171}{figure.caption.2429}\protected@file@percent }
\newlabel{fig:chapter21_cam_process}{{21.21}{1171}{CAM pipeline: from feature maps to class-specific weighted sums, resulting in localization maps}{figure.caption.2429}{}}
\BKM@entry{id=820,dest={73756273656374696F6E2E32312E382E31},srcline={804}}{5C3337365C3337375C303030475C303030655C3030306E5C303030655C303030725C303030615C3030306C5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030765C303030695C303030615C3030305C3034305C303030475C303030725C303030615C303030645C3030302D5C303030435C303030415C3030304D}
\abx@aux@cite{0}{selvaraju2017_gradcam}
\abx@aux@segm{0}{0}{selvaraju2017_gradcam}
\@writefile{lof}{\contentsline {figure}{\numberline {21.22}{\ignorespaces Examples of CAM heatmaps for the classes \emph  {dome} and \emph  {barbell}. While effective, CAM is limited to the last conv layer.}}{1172}{figure.caption.2430}\protected@file@percent }
\newlabel{fig:chapter21_cam_examples}{{21.22}{1172}{Examples of CAM heatmaps for the classes \emph {dome} and \emph {barbell}. While effective, CAM is limited to the last conv layer}{figure.caption.2430}{}}
\@writefile{toc}{\contentsline {paragraph}{Limitations of CAM}{1172}{section*.2431}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {21.8.1}Generalization via Grad-CAM}{1172}{subsection.21.8.1}\protected@file@percent }
\abx@aux@backref{1390}{selvaraju2017_gradcam}{0}{1172}{1172}
\@writefile{lof}{\contentsline {figure}{\numberline {21.23}{\ignorespaces Grad-CAM architecture: gradients are backpropagated to a target conv layer to produce class-discriminative maps.}}{1173}{figure.caption.2432}\protected@file@percent }
\newlabel{fig:chapter21_gradcam_process}{{21.23}{1173}{Grad-CAM architecture: gradients are backpropagated to a target conv layer to produce class-discriminative maps}{figure.caption.2432}{}}
\abx@aux@cite{0}{springenberg2015_allconv}
\abx@aux@segm{0}{0}{springenberg2015_allconv}
\abx@aux@cite{0}{selvaraju2017_gradcam}
\abx@aux@segm{0}{0}{selvaraju2017_gradcam}
\abx@aux@cite{0}{zeiler2014_visualizing}
\abx@aux@segm{0}{0}{zeiler2014_visualizing}
\abx@aux@cite{0}{springenberg2015_allconv}
\abx@aux@segm{0}{0}{springenberg2015_allconv}
\abx@aux@cite{0}{selvaraju2017_gradcam}
\abx@aux@segm{0}{0}{selvaraju2017_gradcam}
\abx@aux@cite{0}{zeiler2014_visualizing}
\abx@aux@segm{0}{0}{zeiler2014_visualizing}
\abx@aux@cite{0}{selvaraju2017_gradcam}
\abx@aux@segm{0}{0}{selvaraju2017_gradcam}
\abx@aux@cite{0}{vinyals2015_showtell}
\abx@aux@segm{0}{0}{vinyals2015_showtell}
\abx@aux@cite{0}{johnson2015_densecap}
\abx@aux@segm{0}{0}{johnson2015_densecap}
\abx@aux@cite{0}{selvaraju2017_gradcam}
\abx@aux@segm{0}{0}{selvaraju2017_gradcam}
\abx@aux@cite{0}{vinyals2015_showtell}
\abx@aux@segm{0}{0}{vinyals2015_showtell}
\abx@aux@cite{0}{johnson2015_densecap}
\abx@aux@segm{0}{0}{johnson2015_densecap}
\@writefile{toc}{\contentsline {paragraph}{Comparative Visualization Examples}{1174}{section*.2433}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.24}{\ignorespaces Qualitative comparison of different visualization methods applied to an image containing both a dog and a cat. (a) Original image. (b) \textbf  {Guided Backpropagation}~\blx@tocontentsinit {0}\cite {springenberg2015_allconv}: Highlights all features that strongly influence the output, but lacks class-specificity. (c) \textbf  {Grad-CAM (ours)}~\blx@tocontentsinit {0}\cite {selvaraju2017_gradcam}: Localizes class-discriminative regions by weighting feature maps based on class gradients. (d) \textbf  {Guided Grad-CAM}: Combines (b) and (c) to produce high-resolution, class-discriminative saliency maps. (e) \textbf  {Occlusion sensitivity}~\blx@tocontentsinit {0}\cite {zeiler2014_visualizing}: Systematically occludes image patches and measures score drop, highlighting regions critical for prediction. (f) Grad-CAM on a deeper ResNet layer: Shows consistent class-relevant focus across architectures. Notably, Grad-CAM (c, f) yields results visually similar to occlusion (e) but is more accurate and is orders of magnitude faster to compute.}}{1174}{figure.caption.2434}\protected@file@percent }
\abx@aux@backref{1394}{springenberg2015_allconv}{0}{1174}{1174}
\abx@aux@backref{1395}{selvaraju2017_gradcam}{0}{1174}{1174}
\abx@aux@backref{1396}{zeiler2014_visualizing}{0}{1174}{1174}
\newlabel{fig:chapter21_gradcam_examples}{{21.24}{1174}{Qualitative comparison of different visualization methods applied to an image containing both a dog and a cat. (a) Original image. (b) \textbf {Guided Backpropagation}~\cite {springenberg2015_allconv}: Highlights all features that strongly influence the output, but lacks class-specificity. (c) \textbf {Grad-CAM (ours)}~\cite {selvaraju2017_gradcam}: Localizes class-discriminative regions by weighting feature maps based on class gradients. (d) \textbf {Guided Grad-CAM}: Combines (b) and (c) to produce high-resolution, class-discriminative saliency maps. (e) \textbf {Occlusion sensitivity}~\cite {zeiler2014_visualizing}: Systematically occludes image patches and measures score drop, highlighting regions critical for prediction. (f) Grad-CAM on a deeper ResNet layer: Shows consistent class-relevant focus across architectures. Notably, Grad-CAM (c, f) yields results visually similar to occlusion (e) but is more accurate and is orders of magnitude faster to compute}{figure.caption.2434}{}}
\BKM@entry{id=821,dest={73756273656374696F6E2E32312E382E32},srcline={851}}{5C3337365C3337375C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C303030735C3030306F5C3030306E5C3030305C3034305C303030425C303030655C303030745C303030775C303030655C303030655C3030306E5C3030305C3034305C303030435C303030415C3030304D5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030475C303030725C303030615C303030645C3030302D5C303030435C303030415C3030304D}
\@writefile{lof}{\contentsline {figure}{\numberline {21.25}{\ignorespaces Visual explanations for image captioning using Grad-CAM~\blx@tocontentsinit {0}\cite {selvaraju2017_gradcam}. Left: Grad-CAM applied to a captioning model~\blx@tocontentsinit {0}\cite {vinyals2015_showtell} highlights the spatial evidence used when generating the sentence ``A man is sitting at a table with a pizza.'' The heatmap localizes relevant objects such as the \emph  {man} and the \emph  {pizza}, providing intuitive support for the generated caption. Right: Grad-CAM applied to a global captioning model conditioned on bounding box-level captions produced by a dense captioning system~\blx@tocontentsinit {0}\cite {johnson2015_densecap}. The highlighted regions correspond to the caption ``A group of people flying kites on a beach,'' showing that Grad-CAM accurately localizes semantically meaningful regions despite not using any box annotations during training.}}{1175}{figure.caption.2435}\protected@file@percent }
\abx@aux@backref{1400}{selvaraju2017_gradcam}{0}{1175}{1175}
\abx@aux@backref{1401}{vinyals2015_showtell}{0}{1175}{1175}
\abx@aux@backref{1402}{johnson2015_densecap}{0}{1175}{1175}
\newlabel{fig:chapter21_gradcam_captioning}{{21.25}{1175}{Visual explanations for image captioning using Grad-CAM~\cite {selvaraju2017_gradcam}. Left: Grad-CAM applied to a captioning model~\cite {vinyals2015_showtell} highlights the spatial evidence used when generating the sentence ``A man is sitting at a table with a pizza.'' The heatmap localizes relevant objects such as the \emph {man} and the \emph {pizza}, providing intuitive support for the generated caption. Right: Grad-CAM applied to a global captioning model conditioned on bounding box-level captions produced by a dense captioning system~\cite {johnson2015_densecap}. The highlighted regions correspond to the caption ``A group of people flying kites on a beach,'' showing that Grad-CAM accurately localizes semantically meaningful regions despite not using any box annotations during training}{figure.caption.2435}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.8.2}Comparison Between CAM and Grad-CAM}{1175}{subsection.21.8.2}\protected@file@percent }
\newlabel{par:chapter21_cam_gradcam_comparison_text}{{21.8.2}{1175}{Comparison Between CAM and Grad-CAM}{subsection.21.8.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {21.1}{\ignorespaces Comparison of CAM and Grad-CAM in terms of architecture, flexibility, and output quality.}}{1175}{table.caption.2436}\protected@file@percent }
\newlabel{tab:chapter21_cam_gradcam_comparison}{{21.1}{1175}{Comparison of CAM and Grad-CAM in terms of architecture, flexibility, and output quality}{table.caption.2436}{}}
\BKM@entry{id=822,dest={73656374696F6E2E32312E39},srcline={893}}{5C3337365C3337375C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030495C3030306E5C303030765C303030655C303030725C303030735C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {paragraph}{From Explanation to Synthesis: A Path Toward Feature Inversion}{1176}{section*.2437}\protected@file@percent }
\newlabel{par:chapter21_lead_to_feature_inversion}{{21.8.2}{1176}{From Explanation to Synthesis: A Path Toward Feature Inversion}{section*.2437}{}}
\@writefile{toc}{\contentsline {section}{\numberline {21.9}Feature Inversion}{1176}{section.21.9}\protected@file@percent }
\newlabel{sec:chapter21_feature_inversion}{{21.9}{1176}{Feature Inversion}{section.21.9}{}}
\@writefile{toc}{\contentsline {paragraph}{Problem Formulation}{1176}{section*.2438}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Comparison to Gradient Ascent}{1176}{section*.2439}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.26}{\ignorespaces Feature inversion optimization: reconstruct an image whose features match those of a target image, optionally constrained by image priors.}}{1177}{figure.caption.2440}\protected@file@percent }
\newlabel{fig:chapter21_feature_inversion_formula}{{21.26}{1177}{Feature inversion optimization: reconstruct an image whose features match those of a target image, optionally constrained by image priors}{figure.caption.2440}{}}
\@writefile{toc}{\contentsline {paragraph}{Effect of Layer Depth}{1177}{section*.2441}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.27}{\ignorespaces Feature inversion examples. Top to bottom: two elephants, banana near an apple. As we invert from deeper layers (left $\rightarrow $ right), texture and color fidelity degrade, but semantic structure is broadly preserved.}}{1177}{figure.caption.2442}\protected@file@percent }
\newlabel{fig:chapter21_feature_inversion_examples}{{21.27}{1177}{Feature inversion examples. Top to bottom: two elephants, banana near an apple. As we invert from deeper layers (left $\rightarrow $ right), texture and color fidelity degrade, but semantic structure is broadly preserved}{figure.caption.2442}{}}
\BKM@entry{id=823,dest={73656374696F6E2E32312E3130},srcline={965}}{5C3337365C3337375C303030445C303030655C303030655C303030705C303030445C303030725C303030655C303030615C3030306D5C3030303A5C3030305C3034305C303030415C3030306D5C303030705C3030306C5C303030695C303030665C303030795C303030695C3030306E5C303030675C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C303030505C303030655C303030725C303030635C303030655C303030705C303030745C303030695C3030306F5C3030306E5C30303073}
\abx@aux@cite{0}{mordvintsev2015_deepdream}
\abx@aux@segm{0}{0}{mordvintsev2015_deepdream}
\@writefile{toc}{\contentsline {paragraph}{Interpretability Insights}{1178}{section*.2443}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Applications}{1178}{section*.2444}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Beyond Feature Inversion}{1178}{section*.2445}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {21.10}DeepDream: Amplifying Neural Perceptions}{1178}{section.21.10}\protected@file@percent }
\newlabel{sec:chapter21_deepdream}{{21.10}{1178}{DeepDream: Amplifying Neural Perceptions}{section.21.10}{}}
\abx@aux@backref{1403}{mordvintsev2015_deepdream}{0}{1178}{1178}
\@writefile{lof}{\contentsline {figure}{\numberline {21.28}{\ignorespaces DeepDream algorithm: Choose image and layer, forward pass to compute activations, backpropagate activations as gradient, update image. Equivalent to maximizing feature norm.}}{1178}{figure.caption.2446}\protected@file@percent }
\newlabel{fig:chapter21_deepdream_process}{{21.28}{1178}{DeepDream algorithm: Choose image and layer, forward pass to compute activations, backpropagate activations as gradient, update image. Equivalent to maximizing feature norm}{figure.caption.2446}{}}
\@writefile{toc}{\contentsline {paragraph}{Optimization Objective}{1179}{section*.2447}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Amplifying Layer-wise Semantics}{1179}{section*.2448}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.29}{\ignorespaces DeepDream on low-level layers: edge filters amplify simple patterns in the sky, yielding fractal-like textures.}}{1179}{figure.caption.2449}\protected@file@percent }
\newlabel{fig:chapter21_deepdream_low_layers}{{21.29}{1179}{DeepDream on low-level layers: edge filters amplify simple patterns in the sky, yielding fractal-like textures}{figure.caption.2449}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21.30}{\ignorespaces DeepDream on high-level layers: dog-like patterns emerge in the clouds as the network amplifies its abstract internal representations.}}{1180}{figure.caption.2450}\protected@file@percent }
\newlabel{fig:chapter21_deepdream_high_layers}{{21.30}{1180}{DeepDream on high-level layers: dog-like patterns emerge in the clouds as the network amplifies its abstract internal representations}{figure.caption.2450}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21.31}{\ignorespaces Examples of DeepDream artifacts: clouds mix with psychedelic animal heads, sky becomes textured with hybrid features like buildings.}}{1180}{figure.caption.2451}\protected@file@percent }
\newlabel{fig:chapter21_deepdream_artifacts}{{21.31}{1180}{Examples of DeepDream artifacts: clouds mix with psychedelic animal heads, sky becomes textured with hybrid features like buildings}{figure.caption.2451}{}}
\@writefile{toc}{\contentsline {paragraph}{Dreaming Deeper}{1181}{section*.2452}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.32}{\ignorespaces Progressive amplification of features using DeepDream. The longer the process runs, the more surreal and abstract the image becomes.}}{1181}{figure.caption.2453}\protected@file@percent }
\newlabel{fig:chapter21_deepdream_progression}{{21.32}{1181}{Progressive amplification of features using DeepDream. The longer the process runs, the more surreal and abstract the image becomes}{figure.caption.2453}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21.33}{\ignorespaces Further examples of DeepDream outputs. Internal concepts from different layers manifest as repeating patterns in generated images.}}{1181}{figure.caption.2454}\protected@file@percent }
\newlabel{fig:chapter21_deepdream_additional}{{21.33}{1181}{Further examples of DeepDream outputs. Internal concepts from different layers manifest as repeating patterns in generated images}{figure.caption.2454}{}}
\abx@aux@cite{0}{gatys2015_texture}
\abx@aux@segm{0}{0}{gatys2015_texture}
\BKM@entry{id=824,dest={73656374696F6E2E32312E3131},srcline={1038}}{5C3337365C3337375C303030545C303030655C303030785C303030745C303030755C303030725C303030655C3030305C3034305C303030535C303030795C3030306E5C303030745C303030685C303030655C303030735C303030695C30303073}
\BKM@entry{id=825,dest={73756273656374696F6E2E32312E31312E31},srcline={1054}}{5C3337365C3337375C303030435C3030306C5C303030615C303030735C303030735C303030695C303030635C303030615C3030306C5C3030305C3034305C303030415C303030705C303030705C303030725C3030306F5C303030615C303030635C303030685C303030655C30303073}
\@writefile{toc}{\contentsline {paragraph}{Interpretability Value}{1182}{section*.2455}\protected@file@percent }
\abx@aux@backref{1404}{gatys2015_texture}{0}{1182}{1182}
\@writefile{toc}{\contentsline {section}{\numberline {21.11}Texture Synthesis}{1182}{section.21.11}\protected@file@percent }
\newlabel{sec:chapter21_texture_synthesis}{{21.11}{1182}{Texture Synthesis}{section.21.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21.34}{\ignorespaces Texture synthesis task overview. Given a small input patch, the goal is to synthesize a larger image that preserves similar local statistics—appearing perceptually consistent without direct repetition.}}{1182}{figure.caption.2456}\protected@file@percent }
\newlabel{fig:chapter21_texture_overview}{{21.34}{1182}{Texture synthesis task overview. Given a small input patch, the goal is to synthesize a larger image that preserves similar local statistics—appearing perceptually consistent without direct repetition}{figure.caption.2456}{}}
\abx@aux@cite{0}{efros1999_texture}
\abx@aux@segm{0}{0}{efros1999_texture}
\abx@aux@cite{0}{wei2000_texture}
\abx@aux@segm{0}{0}{wei2000_texture}
\abx@aux@cite{0}{efros1999_texture}
\abx@aux@segm{0}{0}{efros1999_texture}
\abx@aux@cite{0}{efros1999_texture}
\abx@aux@segm{0}{0}{efros1999_texture}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.11.1}Classical Approaches}{1183}{subsection.21.11.1}\protected@file@percent }
\abx@aux@backref{1405}{efros1999_texture}{0}{1183}{1183}
\abx@aux@backref{1406}{wei2000_texture}{0}{1183}{1183}
\@writefile{lof}{\contentsline {figure}{\numberline {21.35}{\ignorespaces Non-parametric texture synthesis~\blx@tocontentsinit {0}\cite {efros1999_texture}. The algorithm grows the output texture pixel-by-pixel by matching local neighborhoods to those in the source patch using nearest-neighbor search.}}{1183}{figure.caption.2457}\protected@file@percent }
\abx@aux@backref{1408}{efros1999_texture}{0}{1183}{1183}
\newlabel{fig:chapter21_texture_nn}{{21.35}{1183}{Non-parametric texture synthesis~\cite {efros1999_texture}. The algorithm grows the output texture pixel-by-pixel by matching local neighborhoods to those in the source patch using nearest-neighbor search}{figure.caption.2457}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21.36}{\ignorespaces Examples of classical texture synthesis applied to a brick wall and a document fragment. Pixel-based patch matching leads to surprisingly realistic results for locally stationary textures.}}{1183}{figure.caption.2458}\protected@file@percent }
\newlabel{fig:chapter21_classical_texture}{{21.36}{1183}{Examples of classical texture synthesis applied to a brick wall and a document fragment. Pixel-based patch matching leads to surprisingly realistic results for locally stationary textures}{figure.caption.2458}{}}
\BKM@entry{id=826,dest={73756273656374696F6E2E32312E31312E32},srcline={1078}}{5C3337365C3337375C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C303030545C303030655C303030785C303030745C303030755C303030725C303030655C3030305C3034305C303030535C303030795C3030306E5C303030745C303030685C303030655C303030735C303030695C303030735C3030305C3034305C303030765C303030695C303030615C3030305C3034305C303030475C303030725C303030615C3030306D5C3030305C3034305C3030304D5C303030615C303030745C303030725C303030695C303030635C303030655C30303073}
\abx@aux@cite{0}{gatys2015_texture}
\abx@aux@segm{0}{0}{gatys2015_texture}
\@writefile{toc}{\contentsline {paragraph}{Limitations of Pixel Matching}{1184}{section*.2459}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {21.11.2}Neural Texture Synthesis via Gram Matrices}{1184}{subsection.21.11.2}\protected@file@percent }
\abx@aux@backref{1409}{gatys2015_texture}{0}{1184}{1184}
\@writefile{toc}{\contentsline {paragraph}{Constructing the Gram Matrix}{1184}{section*.2460}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.37}{\ignorespaces Constructing the Gram matrix: given feature activations across spatial dimensions, we compute a \( C \times C \) matrix that captures global feature co-occurrence statistics.}}{1184}{figure.caption.2461}\protected@file@percent }
\newlabel{fig:chapter21_gram_matrix}{{21.37}{1184}{Constructing the Gram matrix: given feature activations across spatial dimensions, we compute a \( C \times C \) matrix that captures global feature co-occurrence statistics}{figure.caption.2461}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Gram Matrices?}{1184}{section*.2462}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.38}{\ignorespaces Efficient Gram matrix computation by flattening spatial dimensions: from \( C \times H \times W \) to \( C \times HW \), then multiplying by its transpose.}}{1185}{figure.caption.2463}\protected@file@percent }
\newlabel{fig:chapter21_gram_computation}{{21.38}{1185}{Efficient Gram matrix computation by flattening spatial dimensions: from \( C \times H \times W \) to \( C \times HW \), then multiplying by its transpose}{figure.caption.2463}{}}
\@writefile{toc}{\contentsline {paragraph}{Optimization Pipeline}{1185}{section*.2464}\protected@file@percent }
\abx@aux@cite{0}{gatys2016_stylization}
\abx@aux@segm{0}{0}{gatys2016_stylization}
\abx@aux@cite{0}{johnson2016_perceptual}
\abx@aux@segm{0}{0}{johnson2016_perceptual}
\@writefile{lof}{\contentsline {figure}{\numberline {21.39}{\ignorespaces Full pipeline of neural texture synthesis: from extracting Gram matrices to iterative gradient-based refinement of a noise image to match the desired style.}}{1186}{figure.caption.2465}\protected@file@percent }
\newlabel{fig:chapter21_texture_pipeline}{{21.39}{1186}{Full pipeline of neural texture synthesis: from extracting Gram matrices to iterative gradient-based refinement of a noise image to match the desired style}{figure.caption.2465}{}}
\@writefile{toc}{\contentsline {paragraph}{Effect of Matching Higher Layers}{1186}{section*.2466}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.40}{\ignorespaces Texture reconstructions from matching Gram matrices at various depths. Shallow layers reconstruct local textures; deeper layers capture larger-scale features and structure.}}{1186}{figure.caption.2467}\protected@file@percent }
\newlabel{fig:chapter21_neural_texture_results}{{21.40}{1186}{Texture reconstructions from matching Gram matrices at various depths. Shallow layers reconstruct local textures; deeper layers capture larger-scale features and structure}{figure.caption.2467}{}}
\@writefile{toc}{\contentsline {paragraph}{Impact and Legacy}{1186}{section*.2468}\protected@file@percent }
\abx@aux@backref{1410}{gatys2016_stylization}{0}{1186}{1186}
\BKM@entry{id=827,dest={73656374696F6E2E32312E3132},srcline={1155}}{5C3337365C3337375C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C303030535C303030745C303030795C3030306C5C303030655C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C303030655C30303072}
\BKM@entry{id=828,dest={73756273656374696F6E2E32312E31322E31},srcline={1158}}{5C3337365C3337375C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C303030535C303030745C303030795C3030306C5C303030655C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C303030655C303030725C3030303A5C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030655C3030306E5C303030745C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030535C303030745C303030795C3030306C5C303030655C3030305C3034305C303030465C303030755C303030735C303030695C3030306F5C3030306E}
\abx@aux@backref{1411}{johnson2016_perceptual}{0}{1187}{1187}
\@writefile{toc}{\contentsline {section}{\numberline {21.12}Neural Style Transfer}{1187}{section.21.12}\protected@file@percent }
\newlabel{sec:chapter21_neural_style_transfer}{{21.12}{1187}{Neural Style Transfer}{section.21.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.12.1}Neural Style Transfer: Content and Style Fusion}{1187}{subsection.21.12.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Intuition}{1187}{section*.2469}\protected@file@percent }
\newlabel{par:chapter21_nst_intuition}{{21.12.1}{1187}{Intuition}{section*.2469}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21.41}{\ignorespaces Two optimization objectives: \textbf  {Top—Style (Texture Synthesis)} via Gram matrix matching; \textbf  {Bottom—Content Reconstruction} via feature matching.}}{1187}{figure.caption.2470}\protected@file@percent }
\newlabel{fig:chapter21_nst_dual_objectives}{{21.41}{1187}{Two optimization objectives: \textbf {Top—Style (Texture Synthesis)} via Gram matrix matching; \textbf {Bottom—Content Reconstruction} via feature matching}{figure.caption.2470}{}}
\@writefile{toc}{\contentsline {paragraph}{Optimization Objective}{1187}{section*.2471}\protected@file@percent }
\newlabel{par:chapter21_nst_optimization_objective}{{21.12.1}{1187}{Optimization Objective}{section*.2471}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21.42}{\ignorespaces Neural Style Transfer architecture: Content features are extracted from the content image, and style features (Gram matrices) from the style image. Both guide the optimization of a new output image.}}{1188}{figure.caption.2472}\protected@file@percent }
\newlabel{fig:chapter21_nst_architecture}{{21.42}{1188}{Neural Style Transfer architecture: Content features are extracted from the content image, and style features (Gram matrices) from the style image. Both guide the optimization of a new output image}{figure.caption.2472}{}}
\@writefile{toc}{\contentsline {paragraph}{Optimization via Gradient Descent}{1188}{section*.2473}\protected@file@percent }
\newlabel{par:chapter21_nst_gradient_descent}{{21.12.1}{1188}{Optimization via Gradient Descent}{section*.2473}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21.43}{\ignorespaces Gradient-based optimization: iteratively update the image to minimize content and style loss using gradients from a pretrained CNN.}}{1189}{figure.caption.2474}\protected@file@percent }
\newlabel{fig:chapter21_nst_optimization}{{21.43}{1189}{Gradient-based optimization: iteratively update the image to minimize content and style loss using gradients from a pretrained CNN}{figure.caption.2474}{}}
\@writefile{toc}{\contentsline {paragraph}{Stylization Results}{1190}{section*.2475}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.44}{\ignorespaces A stylization result: the content structure is preserved while adopting textures and colors from the style artwork.}}{1190}{figure.caption.2476}\protected@file@percent }
\newlabel{fig:chapter21_nst_result_1}{{21.44}{1190}{A stylization result: the content structure is preserved while adopting textures and colors from the style artwork}{figure.caption.2476}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21.45}{\ignorespaces Additional examples of Neural Style Transfer across various artworks and content images.}}{1190}{figure.caption.2477}\protected@file@percent }
\newlabel{fig:chapter21_nst_result_2}{{21.45}{1190}{Additional examples of Neural Style Transfer across various artworks and content images}{figure.caption.2477}{}}
\@writefile{toc}{\contentsline {paragraph}{Controlling Style Intensity}{1191}{section*.2478}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.46}{\ignorespaces Effect of changing content-style trade-off: higher style weight yields more aggressive stylization; higher content weight yields better structural fidelity.}}{1191}{figure.caption.2479}\protected@file@percent }
\newlabel{fig:chapter21_nst_content_style_tradeoff}{{21.46}{1191}{Effect of changing content-style trade-off: higher style weight yields more aggressive stylization; higher content weight yields better structural fidelity}{figure.caption.2479}{}}
\@writefile{toc}{\contentsline {paragraph}{Effect of Style Image Scale}{1191}{section*.2480}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.47}{\ignorespaces Effect of style image resizing: larger style image induces small-scale brush strokes; smaller style image encourages transfer of large-scale visual features.}}{1191}{figure.caption.2481}\protected@file@percent }
\newlabel{fig:chapter21_nst_style_scale}{{21.47}{1191}{Effect of style image resizing: larger style image induces small-scale brush strokes; smaller style image encourages transfer of large-scale visual features}{figure.caption.2481}{}}
\@writefile{toc}{\contentsline {paragraph}{Combining Styles}{1192}{section*.2482}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.48}{\ignorespaces Mixed style transfer: combining styles from two different artworks yields visually blended results.}}{1192}{figure.caption.2483}\protected@file@percent }
\newlabel{fig:chapter21_nst_style_mixing}{{21.48}{1192}{Mixed style transfer: combining styles from two different artworks yields visually blended results}{figure.caption.2483}{}}
\@writefile{toc}{\contentsline {paragraph}{Limitations}{1192}{section*.2484}\protected@file@percent }
\BKM@entry{id=829,dest={73756273656374696F6E2E32312E31322E32},srcline={1305}}{5C3337365C3337375C303030465C303030615C303030735C303030745C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C303030535C303030745C303030795C3030306C5C303030655C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C303030655C30303072}
\abx@aux@cite{0}{johnson2016_perceptual}
\abx@aux@segm{0}{0}{johnson2016_perceptual}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.12.2}Fast Neural Style Transfer}{1193}{subsection.21.12.2}\protected@file@percent }
\newlabel{sec:chapter21_fast_style_transfer}{{21.12.2}{1193}{Fast Neural Style Transfer}{subsection.21.12.2}{}}
\abx@aux@backref{1412}{johnson2016_perceptual}{0}{1193}{1193}
\@writefile{toc}{\contentsline {paragraph}{Training Setup}{1193}{section*.2485}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.49}{\ignorespaces Fast style transfer training loop: use perceptual loss to train a feedforward network that performs style transfer in a single pass.}}{1193}{figure.caption.2486}\protected@file@percent }
\newlabel{fig:chapter21_fast_style_transfer_training}{{21.49}{1193}{Fast style transfer training loop: use perceptual loss to train a feedforward network that performs style transfer in a single pass}{figure.caption.2486}{}}
\@writefile{toc}{\contentsline {paragraph}{Key Insight}{1193}{section*.2487}\protected@file@percent }
\abx@aux@cite{0}{ulyanov2017_instance}
\abx@aux@segm{0}{0}{ulyanov2017_instance}
\@writefile{toc}{\contentsline {paragraph}{Stylization Examples}{1194}{section*.2488}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21.50}{\ignorespaces Fast style transfer examples: output images styled in the aesthetics of Van Gogh’s \emph  {Starry Night} and Picasso’s \emph  {The Muse}.}}{1194}{figure.caption.2489}\protected@file@percent }
\newlabel{fig:chapter21_fast_examples}{{21.50}{1194}{Fast style transfer examples: output images styled in the aesthetics of Van Gogh’s \emph {Starry Night} and Picasso’s \emph {The Muse}}{figure.caption.2489}{}}
\@writefile{toc}{\contentsline {paragraph}{Instance Normalization}{1194}{section*.2490}\protected@file@percent }
\abx@aux@backref{1413}{ulyanov2017_instance}{0}{1194}{1194}
\@writefile{lof}{\contentsline {figure}{\numberline {21.51}{\ignorespaces High-quality stylized outputs from fast neural style transfer trained with instance normalization.}}{1194}{figure.caption.2491}\protected@file@percent }
\newlabel{fig:chapter21_instance_norm_results}{{21.51}{1194}{High-quality stylized outputs from fast neural style transfer trained with instance normalization}{figure.caption.2491}{}}
\abx@aux@cite{0}{dumoulin2017_cbn}
\abx@aux@segm{0}{0}{dumoulin2017_cbn}
\abx@aux@cite{0}{hu2024_diffusest}
\abx@aux@segm{0}{0}{hu2024_diffusest}
\abx@aux@cite{0}{rojas2024_sassl}
\abx@aux@segm{0}{0}{rojas2024_sassl}
\abx@aux@cite{0}{wang2023_stylediffusion}
\abx@aux@segm{0}{0}{wang2023_stylediffusion}
\abx@aux@cite{0}{zhang2023_inst}
\abx@aux@segm{0}{0}{zhang2023_inst}
\@writefile{toc}{\contentsline {paragraph}{Conditional Instance Normalization for Multi-Style Transfer}{1195}{section*.2492}\protected@file@percent }
\abx@aux@backref{1414}{dumoulin2017_cbn}{0}{1195}{1195}
\@writefile{lof}{\contentsline {figure}{\numberline {21.52}{\ignorespaces Conditional instance normalization enables one network to perform multiple styles—and interpolate between them.}}{1195}{figure.caption.2493}\protected@file@percent }
\newlabel{fig:chapter21_conditional_style}{{21.52}{1195}{Conditional instance normalization enables one network to perform multiple styles—and interpolate between them}{figure.caption.2493}{}}
\@writefile{toc}{\contentsline {paragraph}{Summary and Emerging Directions}{1195}{section*.2494}\protected@file@percent }
\abx@aux@backref{1415}{hu2024_diffusest}{0}{1195}{1195}
\abx@aux@backref{1416}{rojas2024_sassl}{0}{1195}{1195}
\abx@aux@backref{1417}{wang2023_stylediffusion}{0}{1195}{1195}
\abx@aux@backref{1418}{zhang2023_inst}{0}{1195}{1195}
\BKM@entry{id=830,dest={636861707465722E3232},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030325C303030325C3030303A5C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030535C303030755C303030705C303030655C303030725C303030765C303030695C303030735C303030655C303030645C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\BKM@entry{id=831,dest={73656374696F6E2E32322E31},srcline={10}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030445C303030655C303030665C303030695C3030306E5C303030695C303030745C303030695C3030306F5C3030306E}
\BKM@entry{id=832,dest={73756273656374696F6E2E32322E312E31},srcline={13}}{5C3337365C3337375C303030575C303030685C303030615C303030745C3030305C3034305C303030695C303030735C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030535C303030755C303030705C303030655C303030725C303030765C303030695C303030735C303030655C303030645C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030305C3035305C303030535C303030535C3030304C5C3030305C3035315C3030303F}
\@writefile{toc}{\contentsline {chapter}{\numberline {22}Lecture 22: Self-Supervised Learning}{1196}{chapter.22}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@21}}
\ttl@writefile{ptc}{\ttl@starttoc{default@22}}
\pgfsyspdfmark {pgfid121}{0}{52099153}
\pgfsyspdfmark {pgfid120}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {22.1}Motivation and Definition}{1196}{section.22.1}\protected@file@percent }
\newlabel{sec:chapter22_ssl_intro}{{22.1}{1196}{Motivation and Definition}{section.22.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.1.1}What is Self-Supervised Learning (SSL)?}{1196}{subsection.22.1.1}\protected@file@percent }
\newlabel{sec:chapter22_what_is_ssl}{{22.1.1}{1196}{What is Self-Supervised Learning (SSL)?}{subsection.22.1.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Learning Representations Without Labels}{1196}{section*.2495}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Pretraining Then Transferring}{1196}{section*.2496}\protected@file@percent }
\abx@aux@cite{0}{he2022_mae}
\abx@aux@segm{0}{0}{he2022_mae}
\abx@aux@cite{0}{zhang2016_colorful}
\abx@aux@segm{0}{0}{zhang2016_colorful}
\abx@aux@cite{0}{pathak2016_context}
\abx@aux@segm{0}{0}{pathak2016_context}
\@writefile{lof}{\contentsline {figure}{\numberline {22.1}{\ignorespaces Self-supervised learning via pretext tasks. Top: the model is pretrained using a synthetic task derived from the input data. Bottom: the encoder is transferred to a downstream task with limited supervision. Goal: the pretrain+transfer pipeline outperforms purely supervised training.}}{1197}{figure.caption.2497}\protected@file@percent }
\newlabel{fig:chapter22_ssl_pipeline}{{22.1}{1197}{Self-supervised learning via pretext tasks. Top: the model is pretrained using a synthetic task derived from the input data. Bottom: the encoder is transferred to a downstream task with limited supervision. Goal: the pretrain+transfer pipeline outperforms purely supervised training}{figure.caption.2497}{}}
\@writefile{toc}{\contentsline {paragraph}{Embedding Geometry and Semantic Similarity}{1197}{section*.2498}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Pretext Tasks Work}{1197}{section*.2499}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Categories of Pretext Tasks}{1197}{section*.2500}\protected@file@percent }
\abx@aux@backref{1419}{he2022_mae}{0}{1197}{1197}
\abx@aux@backref{1420}{zhang2016_colorful}{0}{1197}{1197}
\abx@aux@backref{1421}{pathak2016_context}{0}{1197}{1197}
\abx@aux@cite{0}{goodfellow2014_adversarial}
\abx@aux@segm{0}{0}{goodfellow2014_adversarial}
\abx@aux@cite{0}{doersch2015_context}
\abx@aux@segm{0}{0}{doersch2015_context}
\abx@aux@cite{0}{gidaris2018_unsupervised}
\abx@aux@segm{0}{0}{gidaris2018_unsupervised}
\abx@aux@cite{0}{caron2018_deepcluster}
\abx@aux@segm{0}{0}{caron2018_deepcluster}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{he2020_moco}
\abx@aux@segm{0}{0}{he2020_moco}
\abx@aux@cite{0}{arandjelovic2017_look_listen}
\abx@aux@segm{0}{0}{arandjelovic2017_look_listen}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\abx@aux@cite{0}{he2016_resnet}
\abx@aux@segm{0}{0}{he2016_resnet}
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\BKM@entry{id=833,dest={73756273656374696F6E2E32322E312E32},srcline={85}}{5C3337365C3337375C303030575C303030685C303030795C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030535C303030755C303030705C303030655C303030725C303030765C303030695C303030735C303030655C303030645C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030303F}
\abx@aux@backref{1422}{goodfellow2014_adversarial}{0}{1198}{1198}
\abx@aux@backref{1423}{doersch2015_context}{0}{1198}{1198}
\abx@aux@backref{1424}{gidaris2018_unsupervised}{0}{1198}{1198}
\abx@aux@backref{1425}{caron2018_deepcluster}{0}{1198}{1198}
\abx@aux@backref{1426}{chen2020_simclr}{0}{1198}{1198}
\abx@aux@backref{1427}{he2020_moco}{0}{1198}{1198}
\abx@aux@backref{1428}{arandjelovic2017_look_listen}{0}{1198}{1198}
\abx@aux@backref{1429}{radford2021_clip}{0}{1198}{1198}
\@writefile{toc}{\contentsline {paragraph}{Backbones, Augmentations, and Losses}{1198}{section*.2501}\protected@file@percent }
\abx@aux@backref{1430}{he2016_resnet}{0}{1198}{1198}
\abx@aux@backref{1431}{vit2020_transformers}{0}{1198}{1198}
\@writefile{toc}{\contentsline {paragraph}{Summary}{1198}{section*.2502}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {22.1.2}Why Self-Supervised Learning?}{1198}{subsection.22.1.2}\protected@file@percent }
\newlabel{subsec:chapter22_why_ssl}{{22.1.2}{1198}{Why Self-Supervised Learning?}{subsection.22.1.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Supervised Learning is Expensive}{1198}{section*.2503}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{But Unlabeled Data is Free (and Plentiful)}{1198}{section*.2504}\protected@file@percent }
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\abx@aux@cite{0}{brown2020_gpt3}
\abx@aux@segm{0}{0}{brown2020_gpt3}
\BKM@entry{id=834,dest={73756273656374696F6E2E32322E312E33},srcline={101}}{5C3337365C3337375C3030304C5C303030655C303030435C303030755C3030306E5C303030275C303030735C3030305C3034305C303030415C303030495C3030305C3034305C303030435C303030615C3030306B5C303030655C3030303A5C3030305C3034305C303030535C303030535C3030304C5C3030305C3034305C303030615C303030735C3030305C3034305C303030745C303030685C303030655C3030305C3034305C303030425C303030615C303030735C303030655C3030305C3034305C3030304C5C303030615C303030795C303030655C30303072}
\BKM@entry{id=835,dest={73756273656374696F6E2E32322E312E34},srcline={121}}{5C3337365C3337375C303030505C303030725C303030615C303030635C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C303030495C3030306E5C303030745C303030655C303030675C303030725C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030695C3030306E5C303030745C3030306F5C3030305C3034305C303030445C303030655C303030655C303030705C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030505C303030695C303030705C303030655C3030306C5C303030695C3030306E5C303030655C30303073}
\@writefile{toc}{\contentsline {paragraph}{Learning Like Humans}{1199}{section*.2505}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{SSL as the Backbone of Foundation Models}{1199}{section*.2506}\protected@file@percent }
\abx@aux@backref{1432}{radford2021_clip}{0}{1199}{1199}
\abx@aux@backref{1433}{brown2020_gpt3}{0}{1199}{1199}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.1.3}LeCun's AI Cake: SSL as the Base Layer}{1199}{subsection.22.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The Cake Analogy}{1199}{section*.2507}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.2}{\ignorespaces Yann LeCun's "AI Cake" analogy. SSL forms the foundational bulk of learning by leveraging abundant unlabeled data to produce general-purpose representations.}}{1199}{figure.caption.2508}\protected@file@percent }
\newlabel{fig:chapter22_lecun_cake}{{22.2}{1199}{Yann LeCun's "AI Cake" analogy. SSL forms the foundational bulk of learning by leveraging abundant unlabeled data to produce general-purpose representations}{figure.caption.2508}{}}
\@writefile{toc}{\contentsline {paragraph}{Practical Significance}{1199}{section*.2509}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {22.1.4}Practical Integration into Deep Learning Pipelines}{1200}{subsection.22.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How SSL is Used in Practice}{1200}{section*.2510}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Flexible Transfer and Modularity}{1200}{section*.2511}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Strategic Impact and Adoption}{1200}{section*.2512}\protected@file@percent }
\BKM@entry{id=836,dest={73656374696F6E2E32322E32},srcline={141}}{5C3337365C3337375C303030415C3030305C3034305C303030545C303030615C303030785C3030306F5C3030306E5C3030306F5C3030306D5C303030795C3030305C3034305C3030306F5C303030665C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030535C303030755C303030705C303030655C303030725C303030765C303030695C303030735C303030655C303030645C3030305C3034305C303030525C303030655C303030705C303030725C303030655C303030735C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C30303073}
\BKM@entry{id=837,dest={73756273656374696F6E2E32322E322E31},srcline={147}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030745C303030725C303030615C303030735C303030745C303030695C303030765C303030655C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C30303073}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{chen2020_simclrv2}
\abx@aux@segm{0}{0}{chen2020_simclrv2}
\abx@aux@cite{0}{he2020_moco}
\abx@aux@segm{0}{0}{he2020_moco}
\abx@aux@cite{0}{chen2020_improved}
\abx@aux@segm{0}{0}{chen2020_improved}
\abx@aux@cite{0}{chen2021_empiricalstudy}
\abx@aux@segm{0}{0}{chen2021_empiricalstudy}
\abx@aux@cite{0}{mitrovic2020_relic}
\abx@aux@segm{0}{0}{mitrovic2020_relic}
\abx@aux@cite{0}{tomasev2022_relicv2}
\abx@aux@segm{0}{0}{tomasev2022_relicv2}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\abx@aux@cite{0}{dwibedi2021_nnclr}
\abx@aux@segm{0}{0}{dwibedi2021_nnclr}
\BKM@entry{id=838,dest={73756273656374696F6E2E32322E322E32},srcline={164}}{5C3337365C3337375C303030445C303030695C303030735C303030745C303030695C3030306C5C3030306C5C303030615C303030745C303030695C3030306F5C3030306E5C3030302D5C303030425C303030615C303030735C303030655C303030645C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C30303073}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\abx@aux@cite{0}{caron2021_selfsupervised}
\abx@aux@segm{0}{0}{caron2021_selfsupervised}
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\abx@aux@cite{0}{lee2021_cbyol}
\abx@aux@segm{0}{0}{lee2021_cbyol}
\@writefile{toc}{\contentsline {section}{\numberline {22.2}A Taxonomy of Self-Supervised Representation Learning Methods}{1201}{section.22.2}\protected@file@percent }
\newlabel{sec:chapter22_ssl_taxonomy}{{22.2}{1201}{A Taxonomy of Self-Supervised Representation Learning Methods}{section.22.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.2.1}Contrastive Methods}{1201}{subsection.22.2.1}\protected@file@percent }
\newlabel{sec:chapter22_ssl_contrastive}{{22.2.1}{1201}{Contrastive Methods}{subsection.22.2.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Discriminative Representations via Similarity and Dissimilarity}{1201}{section*.2513}\protected@file@percent }
\abx@aux@backref{1434}{chen2020_simclr}{0}{1201}{1201}
\abx@aux@backref{1435}{chen2020_simclrv2}{0}{1201}{1201}
\abx@aux@backref{1436}{chen2021_empiricalstudy}{0}{1201}{1201}
\abx@aux@backref{1437}{chen2020_improved}{0}{1201}{1201}
\abx@aux@backref{1438}{he2020_moco}{0}{1201}{1201}
\abx@aux@backref{1439}{mitrovic2020_relic}{0}{1201}{1201}
\abx@aux@backref{1440}{tomasev2022_relicv2}{0}{1201}{1201}
\abx@aux@backref{1441}{radford2021_clip}{0}{1201}{1201}
\abx@aux@backref{1442}{dwibedi2021_nnclr}{0}{1201}{1201}
\@writefile{toc}{\contentsline {paragraph}{Insight}{1201}{section*.2514}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {22.2.2}Distillation-Based Methods}{1201}{subsection.22.2.2}\protected@file@percent }
\newlabel{sec:chapter22_ssl_distillation}{{22.2.2}{1201}{Distillation-Based Methods}{subsection.22.2.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Teacher-Student Framework without Negatives}{1201}{section*.2515}\protected@file@percent }
\abx@aux@backref{1443}{grill2020_byol}{0}{1201}{1201}
\abx@aux@backref{1444}{chen2021_simsiam}{0}{1201}{1201}
\abx@aux@backref{1445}{caron2021_selfsupervised}{0}{1201}{1201}
\abx@aux@backref{1446}{oquab2023_dinov2}{0}{1201}{1201}
\abx@aux@backref{1447}{lee2021_cbyol}{0}{1201}{1201}
\@writefile{toc}{\contentsline {paragraph}{Insight}{1201}{section*.2516}\protected@file@percent }
\BKM@entry{id=839,dest={73756273656374696F6E2E32322E322E33},srcline={181}}{5C3337365C3337375C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030445C303030655C303030635C3030306F5C303030725C303030725C303030655C3030306C5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C30303073}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\abx@aux@cite{0}{bardes2022_vicreg}
\abx@aux@segm{0}{0}{bardes2022_vicreg}
\abx@aux@cite{0}{ermolov2021_twist}
\abx@aux@segm{0}{0}{ermolov2021_twist}
\BKM@entry{id=840,dest={73756273656374696F6E2E32322E322E34},srcline={196}}{5C3337365C3337375C303030435C3030306C5C303030755C303030735C303030745C303030655C303030725C303030695C3030306E5C303030675C3030302D5C303030425C303030615C303030735C303030655C303030645C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C30303073}
\abx@aux@cite{0}{caron2018_deepcluster}
\abx@aux@segm{0}{0}{caron2018_deepcluster}
\abx@aux@cite{0}{caron2019_deepercluster}
\abx@aux@segm{0}{0}{caron2019_deepercluster}
\abx@aux@cite{0}{caron2020_swav}
\abx@aux@segm{0}{0}{caron2020_swav}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.2.3}Feature Decorrelation Methods}{1202}{subsection.22.2.3}\protected@file@percent }
\newlabel{sec:chapter22_ssl_decorrelation}{{22.2.3}{1202}{Feature Decorrelation Methods}{subsection.22.2.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Promoting Redundancy Reduction}{1202}{section*.2517}\protected@file@percent }
\abx@aux@backref{1448}{zbontar2021_barlow}{0}{1202}{1202}
\abx@aux@backref{1449}{bardes2022_vicreg}{0}{1202}{1202}
\abx@aux@backref{1450}{ermolov2021_twist}{0}{1202}{1202}
\@writefile{toc}{\contentsline {paragraph}{Insight}{1202}{section*.2518}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {22.2.4}Clustering-Based Methods}{1202}{subsection.22.2.4}\protected@file@percent }
\newlabel{sec:chapter22_ssl_clustering}{{22.2.4}{1202}{Clustering-Based Methods}{subsection.22.2.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Learning via Group-Level Semantics}{1202}{section*.2519}\protected@file@percent }
\abx@aux@backref{1451}{caron2018_deepcluster}{0}{1202}{1202}
\abx@aux@backref{1452}{caron2019_deepercluster}{0}{1202}{1202}
\abx@aux@backref{1453}{caron2020_swav}{0}{1202}{1202}
\@writefile{toc}{\contentsline {paragraph}{Insight}{1202}{section*.2520}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {22.1}{\ignorespaces Overview of SSRL Method Families}}{1202}{table.caption.2522}\protected@file@percent }
\newlabel{tab:chapter22_ssrl_taxonomy}{{22.1}{1202}{Overview of SSRL Method Families}{table.caption.2522}{}}
\BKM@entry{id=841,dest={73656374696F6E2E32322E33},srcline={231}}{5C3337365C3337375C303030435C3030306F5C3030306E5C303030745C303030725C303030615C303030735C303030745C303030695C303030765C303030655C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C30303073}
\BKM@entry{id=842,dest={73756273656374696F6E2E32322E332E31},srcline={234}}{5C3337365C3337375C3030304D5C3030306F5C303030745C303030695C303030765C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030725C303030615C303030735C303030745C303030695C303030765C303030655C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\abx@aux@cite{0}{oord2019_representation}
\abx@aux@segm{0}{0}{oord2019_representation}
\@writefile{toc}{\contentsline {section}{\numberline {22.3}Contrastive Methods}{1203}{section.22.3}\protected@file@percent }
\newlabel{sec:chapter22_ssl_contrastive_methods}{{22.3}{1203}{Contrastive Methods}{section.22.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.3.1}Motivation for Contrastive Learning}{1203}{subsection.22.3.1}\protected@file@percent }
\newlabel{subsec:chapter22_contrastive_motivation}{{22.3.1}{1203}{Motivation for Contrastive Learning}{subsection.22.3.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Core Idea}{1203}{section*.2523}\protected@file@percent }
\abx@aux@backref{1454}{oord2019_representation}{0}{1203}{1203}
\@writefile{toc}{\contentsline {paragraph}{Instance Discrimination as a Pretext Task}{1203}{section*.2524}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Avoiding Trivial Solutions}{1203}{section*.2525}\protected@file@percent }
\abx@aux@cite{0}{schroff2015_facenet}
\abx@aux@segm{0}{0}{schroff2015_facenet}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{he2020_moco}
\abx@aux@segm{0}{0}{he2020_moco}
\@writefile{toc}{\contentsline {paragraph}{Scalability and Generalization}{1204}{section*.2526}\protected@file@percent }
\abx@aux@backref{1455}{schroff2015_facenet}{0}{1204}{1204}
\abx@aux@backref{1456}{radford2021_clip}{0}{1204}{1204}
\abx@aux@backref{1457}{chen2020_simclr}{0}{1204}{1204}
\abx@aux@backref{1458}{he2020_moco}{0}{1204}{1204}
\@writefile{toc}{\contentsline {paragraph}{Key Advantages}{1204}{section*.2527}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.3}{\ignorespaces Illustration of contrastive learning for face verification. The model maps augmented images of the \emph  {same person} to nearby embedding vectors (high cosine similarity), while pushing embeddings of \emph  {different people} apart (low cosine similarity). A well-trained model allows verification by thresholding the cosine similarity between face embeddings.}}{1204}{figure.caption.2528}\protected@file@percent }
\newlabel{fig:chapter22_contrastive_loss_idea}{{22.3}{1204}{Illustration of contrastive learning for face verification. The model maps augmented images of the \emph {same person} to nearby embedding vectors (high cosine similarity), while pushing embeddings of \emph {different people} apart (low cosine similarity). A well-trained model allows verification by thresholding the cosine similarity between face embeddings}{figure.caption.2528}{}}
\abx@aux@cite{0}{oord2019_representation}
\abx@aux@segm{0}{0}{oord2019_representation}
\@writefile{toc}{\contentsline {paragraph}{From Semantic Similarity to Objective Formulation}{1205}{section*.2529}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Contrastive Learning as Mutual Information Maximization}{1205}{section*.2530}\protected@file@percent }
\abx@aux@backref{1459}{oord2019_representation}{0}{1205}{1205}
\@writefile{toc}{\contentsline {paragraph}{Towards a Unified Loss Function}{1205}{section*.2531}\protected@file@percent }
\BKM@entry{id=843,dest={73756273656374696F6E2E32322E332E32},srcline={316}}{5C3337365C3337375C3030304F5C303030725C303030695C303030675C303030695C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030495C3030306E5C303030745C303030755C303030695C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030425C303030655C303030685C303030695C3030306E5C303030645C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030725C303030615C303030735C303030745C303030695C303030765C303030655C3030305C3034305C3030304C5C3030306F5C303030735C30303073}
\abx@aux@cite{0}{hadsell2006_dimreduction}
\abx@aux@segm{0}{0}{hadsell2006_dimreduction}
\abx@aux@cite{0}{bekuzarov2022_contrastive_loss}
\abx@aux@segm{0}{0}{bekuzarov2022_contrastive_loss}
\abx@aux@cite{0}{bekuzarov2022_contrastive_loss}
\abx@aux@segm{0}{0}{bekuzarov2022_contrastive_loss}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.3.2}Origin and Intuition Behind Contrastive Loss}{1206}{subsection.22.3.2}\protected@file@percent }
\newlabel{subsec:chapter22_ssl_contrastive_loss_foundation}{{22.3.2}{1206}{Origin and Intuition Behind Contrastive Loss}{subsection.22.3.2}{}}
\@writefile{toc}{\contentsline {paragraph}{From Dimensionality Reduction to Discriminative Embeddings}{1206}{section*.2532}\protected@file@percent }
\abx@aux@backref{1460}{hadsell2006_dimreduction}{0}{1206}{1206}
\@writefile{toc}{\contentsline {paragraph}{Why the Margin Matters}{1206}{section*.2533}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.4}{\ignorespaces Initial state of the embedding space. The anchor point \textcolor {blue}{(blue)} is surrounded by both \textcolor {Black}{black} points (similar) and white points (dissimilar). Arrows illustrate distances: \textcolor {Turquoise}{blue arrows} indicate intra-class similarity (to similar points), while \textcolor {BrickRed}{red arrows} indicate inter-class dissimilarity (to dissimilar points). Figure credit: \blx@tocontentsinit {0}\cite {bekuzarov2022_contrastive_loss}.}}{1206}{figure.caption.2534}\protected@file@percent }
\abx@aux@backref{1462}{bekuzarov2022_contrastive_loss}{0}{1206}{1206}
\newlabel{fig:chapter22_contrastive_margin_intuition}{{22.4}{1206}{Initial state of the embedding space. The anchor point \textcolor {blue}{(blue)} is surrounded by both \textcolor {Black}{black} points (similar) and white points (dissimilar). Arrows illustrate distances: \textcolor {Turquoise}{blue arrows} indicate intra-class similarity (to similar points), while \textcolor {BrickRed}{red arrows} indicate inter-class dissimilarity (to dissimilar points). Figure credit: \cite {bekuzarov2022_contrastive_loss}}{figure.caption.2534}{}}
\@writefile{toc}{\contentsline {paragraph}{A Visual Summary of the Learning Objective}{1206}{section*.2535}\protected@file@percent }
\abx@aux@cite{0}{bekuzarov2022_contrastive_loss}
\abx@aux@segm{0}{0}{bekuzarov2022_contrastive_loss}
\abx@aux@cite{0}{bekuzarov2022_contrastive_loss}
\abx@aux@segm{0}{0}{bekuzarov2022_contrastive_loss}
\@writefile{lof}{\contentsline {figure}{\numberline {22.5}{\ignorespaces Left: Initial configuration where both similar (black) and dissimilar (white) points lie within a margin radius \( m \) of the anchor \textcolor {Aquamarine}{(blue sphere)}. Right: Post-optimization state, where only similar (black) points remain within the margin, and dissimilar (white) points have been pushed outside. Figure credit: \blx@tocontentsinit {0}\cite {bekuzarov2022_contrastive_loss}.}}{1207}{figure.caption.2536}\protected@file@percent }
\abx@aux@backref{1464}{bekuzarov2022_contrastive_loss}{0}{1207}{1207}
\newlabel{fig:chapter22_contrastive_margin_goal}{{22.5}{1207}{Left: Initial configuration where both similar (black) and dissimilar (white) points lie within a margin radius \( m \) of the anchor \textcolor {Aquamarine}{(blue sphere)}. Right: Post-optimization state, where only similar (black) points remain within the margin, and dissimilar (white) points have been pushed outside. Figure credit: \cite {bekuzarov2022_contrastive_loss}}{figure.caption.2536}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Not Use \( \frac  {1}{D_W} \)?}{1207}{section*.2537}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{From Supervision to Self-Supervision}{1207}{section*.2538}\protected@file@percent }
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{he2020_moco}
\abx@aux@segm{0}{0}{he2020_moco}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{kundu2022_contrastive_v7labs}
\abx@aux@segm{0}{0}{kundu2022_contrastive_v7labs}
\abx@aux@cite{0}{kundu2022_contrastive_v7labs}
\abx@aux@segm{0}{0}{kundu2022_contrastive_v7labs}
\abx@aux@backref{1465}{chen2020_simclr}{0}{1208}{1208}
\abx@aux@backref{1466}{he2020_moco}{0}{1208}{1208}
\@writefile{lof}{\contentsline {figure}{\numberline {22.6}{\ignorespaces Data augmentation pipeline in SimCLR: each input image is transformed using a series of operations to create different views (positives). Figure credit: \blx@tocontentsinit {0}\cite {chen2020_simclr}.}}{1208}{figure.caption.2539}\protected@file@percent }
\abx@aux@backref{1468}{chen2020_simclr}{0}{1208}{1208}
\newlabel{fig:chapter22_simclr_augmentations}{{22.6}{1208}{Data augmentation pipeline in SimCLR: each input image is transformed using a series of operations to create different views (positives). Figure credit: \cite {chen2020_simclr}}{figure.caption.2539}{}}
\@writefile{toc}{\contentsline {paragraph}{Triplet Setup: Anchor, Positive, Negative}{1208}{section*.2540}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.7}{\ignorespaces Anchor-positive-negative structure: minimize distance between anchor and positive, maximize from negative. Figure credit: \blx@tocontentsinit {0}\cite {kundu2022_contrastive_v7labs}.}}{1208}{figure.caption.2541}\protected@file@percent }
\abx@aux@backref{1470}{kundu2022_contrastive_v7labs}{0}{1208}{1208}
\newlabel{fig:chapter22_contrastive_triplet_setup}{{22.7}{1208}{Anchor-positive-negative structure: minimize distance between anchor and positive, maximize from negative. Figure credit: \cite {kundu2022_contrastive_v7labs}}{figure.caption.2541}{}}
\BKM@entry{id=844,dest={73756273656374696F6E2E32322E332E33},srcline={411}}{5C3337365C3337375C303030545C303030685C303030655C3030305C3034305C3030304E5C303030545C3030302D5C303030585C303030655C3030306E5C303030745C3030305C3034305C3030304C5C3030306F5C303030735C303030735C3030303A5C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030695C3030307A5C303030655C303030645C3030305C3034305C303030545C303030655C3030306D5C303030705C303030655C303030725C303030615C303030745C303030755C303030725C303030655C3030302D5C303030535C303030635C303030615C3030306C5C303030655C303030645C3030305C3034305C303030435C303030725C3030306F5C303030735C303030735C3030302D5C303030455C3030306E5C303030745C303030725C3030306F5C303030705C30303079}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.3.3}The NT-Xent Loss: Normalized Temperature-Scaled Cross-Entropy}{1209}{subsection.22.3.3}\protected@file@percent }
\newlabel{subsec:chapter22_ssl_nt_xent}{{22.3.3}{1209}{The NT-Xent Loss: Normalized Temperature-Scaled Cross-Entropy}{subsection.22.3.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Overview and Purpose}{1209}{section*.2542}\protected@file@percent }
\abx@aux@backref{1471}{chen2020_simclr}{0}{1209}{1209}
\@writefile{toc}{\contentsline {paragraph}{Pairwise Contrastive Loss: NT-Xent Formulation}{1209}{section*.2543}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Batch Aggregation and the \( \frac  {1}{2N} \) Factor}{1209}{section*.2544}\protected@file@percent }
\abx@aux@cite{0}{anonymous2021_nt_xent}
\abx@aux@segm{0}{0}{anonymous2021_nt_xent}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{anonymous2021_nt_xent}
\abx@aux@segm{0}{0}{anonymous2021_nt_xent}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\@writefile{toc}{\contentsline {paragraph}{The Role of Symmetry}{1210}{section*.2545}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Illustration of the Loss Mechanism}{1210}{section*.2546}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.8}{\ignorespaces Left: Augmentations of the same image form a positive pair. Right: SimCLR’s NT-Xent loss pipeline with encoder \( f(\cdot ) \) and projection head \( g(\cdot ) \). Source: \textbf  {Left}: \blx@tocontentsinit {0}\cite {anonymous2021_nt_xent}, \textbf  {Right}: \blx@tocontentsinit {0}\cite {chen2020_simclr}.}}{1210}{figure.caption.2547}\protected@file@percent }
\abx@aux@backref{1474}{anonymous2021_nt_xent}{0}{1210}{1210}
\abx@aux@backref{1475}{chen2020_simclr}{0}{1210}{1210}
\newlabel{fig:chapter22_ntxent_pipeline}{{22.8}{1210}{Left: Augmentations of the same image form a positive pair. Right: SimCLR’s NT-Xent loss pipeline with encoder \( f(\cdot ) \) and projection head \( g(\cdot ) \). Source: \textbf {Left}: \cite {anonymous2021_nt_xent}, \textbf {Right}: \cite {chen2020_simclr}}{figure.caption.2547}{}}
\abx@aux@cite{0}{anonymous2021_nt_xent}
\abx@aux@segm{0}{0}{anonymous2021_nt_xent}
\abx@aux@cite{0}{anonymous2021_nt_xent}
\abx@aux@segm{0}{0}{anonymous2021_nt_xent}
\@writefile{toc}{\contentsline {paragraph}{Role of the Projection Head}{1211}{section*.2548}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Log-Softmax Intuition}{1211}{section*.2549}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.9}{\ignorespaces Visualizing the objective: positive pair similarity approaches 1; negatives approach -1. Ideally, \(\ell (i, j) \approx 0\) when the numerator and denominator match. Source: \blx@tocontentsinit {0}\cite {anonymous2021_nt_xent}.}}{1211}{figure.caption.2550}\protected@file@percent }
\abx@aux@backref{1477}{anonymous2021_nt_xent}{0}{1211}{1211}
\newlabel{fig:chapter22_ntxent_logsoftmax}{{22.9}{1211}{Visualizing the objective: positive pair similarity approaches 1; negatives approach -1. Ideally, \(\ell (i, j) \approx 0\) when the numerator and denominator match. Source: \cite {anonymous2021_nt_xent}}{figure.caption.2550}{}}
\@writefile{toc}{\contentsline {paragraph}{Summary}{1211}{section*.2551}\protected@file@percent }
\BKM@entry{id=845,dest={73756273656374696F6E2E32322E332E34},srcline={501}}{5C3337365C3337375C303030535C303030695C3030306D5C303030435C3030304C5C303030525C3030303A5C3030305C3034305C303030415C3030305C3034305C303030535C303030695C3030306D5C303030705C3030306C5C303030655C3030305C3034305C303030465C303030725C303030615C3030306D5C303030655C303030775C3030306F5C303030725C3030306B5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030725C303030615C303030735C303030745C303030695C303030765C303030655C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.3.4}SimCLR: A Simple Framework for Contrastive Learning}{1212}{subsection.22.3.4}\protected@file@percent }
\newlabel{subsec:chapter22_ssl_simclr}{{22.3.4}{1212}{SimCLR: A Simple Framework for Contrastive Learning}{subsection.22.3.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Overview}{1212}{section*.2552}\protected@file@percent }
\abx@aux@backref{1478}{chen2020_simclr}{0}{1212}{1212}
\@writefile{toc}{\contentsline {paragraph}{Architecture Components}{1212}{section*.2553}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Design Principles Behind SimCLR}{1212}{section*.2554}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Training Configuration and Stability}{1212}{section*.2555}\protected@file@percent }
\abx@aux@cite{0}{you2017_lars}
\abx@aux@segm{0}{0}{you2017_lars}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@backref{1479}{you2017_lars}{0}{1213}{1213}
\@writefile{toc}{\contentsline {paragraph}{Performance Benchmarks}{1213}{section*.2556}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Visualization of SimCLR Pipeline}{1213}{section*.2557}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.10}{\ignorespaces SimCLR pipeline: each image is augmented into two views. Representations \( {h} \) from the encoder are passed through a projection head \( g(\cdot ) \) to yield \( {z} \), on which the NT-Xent loss is applied. Figure adapted from: \blx@tocontentsinit {0}\cite {chen2020_simclr}.}}{1213}{figure.caption.2558}\protected@file@percent }
\abx@aux@backref{1481}{chen2020_simclr}{0}{1213}{1213}
\newlabel{fig:chapter22_simclr_pipeline}{{22.10}{1213}{SimCLR pipeline: each image is augmented into two views. Representations \( {h} \) from the encoder are passed through a projection head \( g(\cdot ) \) to yield \( {z} \), on which the NT-Xent loss is applied. Figure adapted from: \cite {chen2020_simclr}}{figure.caption.2558}{}}
\abx@aux@cite{0}{he2020_moco}
\abx@aux@segm{0}{0}{he2020_moco}
\BKM@entry{id=846,dest={73756273656374696F6E2E32322E332E35},srcline={567}}{5C3337365C3337375C3030304D5C3030306F5C3030306D5C303030655C3030306E5C303030745C303030755C3030306D5C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030725C303030615C303030735C303030745C3030305C3034305C3030305C3035305C3030304D5C3030306F5C303030435C3030306F5C3030305C303531}
\abx@aux@cite{0}{he2020_moco}
\abx@aux@segm{0}{0}{he2020_moco}
\@writefile{toc}{\contentsline {paragraph}{Limitations and the Road to MoCo}{1214}{section*.2559}\protected@file@percent }
\abx@aux@backref{1482}{he2020_moco}{0}{1214}{1214}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.3.5}Momentum Contrast (MoCo)}{1214}{subsection.22.3.5}\protected@file@percent }
\newlabel{subsec:chapter22_ssl_moco}{{22.3.5}{1214}{Momentum Contrast (MoCo)}{subsection.22.3.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation: Avoiding Large Batch Sizes}{1214}{section*.2560}\protected@file@percent }
\abx@aux@backref{1483}{he2020_moco}{0}{1214}{1214}
\@writefile{toc}{\contentsline {paragraph}{Core Architecture}{1214}{section*.2561}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Contrastive Loss in MoCo}{1214}{section*.2562}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{MoCo Training Pipeline}{1215}{section*.2563}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why MoCo Works: Scale, Stability, and Efficiency}{1215}{section*.2564}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{What the Queue Enables}{1215}{section*.2565}\protected@file@percent }
\abx@aux@cite{0}{he2020_moco}
\abx@aux@segm{0}{0}{he2020_moco}
\abx@aux@cite{0}{he2020_moco}
\abx@aux@segm{0}{0}{he2020_moco}
\abx@aux@cite{0}{he2020_moco}
\abx@aux@segm{0}{0}{he2020_moco}
\abx@aux@cite{0}{he2020_moco}
\abx@aux@segm{0}{0}{he2020_moco}
\@writefile{lof}{\contentsline {figure}{\numberline {22.11}{\ignorespaces Comparison of contrastive learning mechanisms. (a) SimCLR relies on large batches to generate negatives. (b) Memory banks store representations but lack alignment with the current encoder. (c) MoCo uses a momentum encoder and dynamic queue to maintain a large, consistent dictionary. Source:~\blx@tocontentsinit {0}\cite {he2020_moco}.}}{1216}{figure.caption.2566}\protected@file@percent }
\abx@aux@backref{1485}{he2020_moco}{0}{1216}{1216}
\newlabel{fig:chapter22_moco_contrastive_variants}{{22.11}{1216}{Comparison of contrastive learning mechanisms. (a) SimCLR relies on large batches to generate negatives. (b) Memory banks store representations but lack alignment with the current encoder. (c) MoCo uses a momentum encoder and dynamic queue to maintain a large, consistent dictionary. Source:~\cite {he2020_moco}}{figure.caption.2566}{}}
\@writefile{toc}{\contentsline {paragraph}{Momentum Hyperparameter Tuning and Ablation Results}{1216}{section*.2567}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {22.2}{\ignorespaces Impact of momentum coefficient \( m \) on top-1 ImageNet accuracy under linear evaluation. Source:~\blx@tocontentsinit {0}\cite {he2020_moco}.}}{1216}{table.caption.2568}\protected@file@percent }
\abx@aux@backref{1487}{he2020_moco}{0}{1216}{1216}
\newlabel{tab:chapter22_moco_momentum_ablation}{{22.2}{1216}{Impact of momentum coefficient \( m \) on top-1 ImageNet accuracy under linear evaluation. Source:~\cite {he2020_moco}}{table.caption.2568}{}}
\abx@aux@cite{0}{he2020_moco}
\abx@aux@segm{0}{0}{he2020_moco}
\abx@aux@cite{0}{he2020_moco}
\abx@aux@segm{0}{0}{he2020_moco}
\@writefile{toc}{\contentsline {paragraph}{Other Key Ablations and Design Justifications}{1217}{section*.2569}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.12}{\ignorespaces Comparison of three contrastive loss mechanisms on the ImageNet linear classification benchmark using a ResNet-50 backbone. All models share the same pretext task and differ only in their contrastive design. The number of negatives is \( K \) for MoCo and memory bank methods, and \( K - 1 \) for end-to-end approaches (excluding the positive sample). MoCo matches or surpasses the accuracy of both alternatives by combining a large pool of negatives with temporally consistent embeddings—achieving strong performance without relying on massive batches or tolerating stale keys. Source:~\blx@tocontentsinit {0}\cite {he2020_moco}.}}{1217}{figure.caption.2570}\protected@file@percent }
\abx@aux@backref{1489}{he2020_moco}{0}{1217}{1217}
\newlabel{fig:chapter22_moco_mechanism_comparison}{{22.12}{1217}{Comparison of three contrastive loss mechanisms on the ImageNet linear classification benchmark using a ResNet-50 backbone. All models share the same pretext task and differ only in their contrastive design. The number of negatives is \( K \) for MoCo and memory bank methods, and \( K - 1 \) for end-to-end approaches (excluding the positive sample). MoCo matches or surpasses the accuracy of both alternatives by combining a large pool of negatives with temporally consistent embeddings—achieving strong performance without relying on massive batches or tolerating stale keys. Source:~\cite {he2020_moco}}{figure.caption.2570}{}}
\abx@aux@cite{0}{chen2020_improved}
\abx@aux@segm{0}{0}{chen2020_improved}
\@writefile{toc}{\contentsline {paragraph}{Performance and Comparison with SimCLR}{1218}{section*.2571}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {22.3}{\ignorespaces Key Comparison between SimCLR and MoCo.}}{1218}{table.caption.2572}\protected@file@percent }
\newlabel{tab:chapter22_moco_simclr_comparison}{{22.3}{1218}{Key Comparison between SimCLR and MoCo}{table.caption.2572}{}}
\@writefile{toc}{\contentsline {paragraph}{From MoCo v1 to MoCo v2}{1218}{section*.2573}\protected@file@percent }
\abx@aux@backref{1490}{chen2020_improved}{0}{1218}{1218}
\BKM@entry{id=847,dest={73756273656374696F6E2E32322E332E36},srcline={732}}{5C3337365C3337375C3030304D5C3030306F5C303030435C3030306F5C3030305C3034305C303030765C303030325C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304D5C3030306F5C303030435C3030306F5C3030305C3034305C303030765C30303033}
\abx@aux@cite{0}{he2020_moco}
\abx@aux@segm{0}{0}{he2020_moco}
\abx@aux@cite{0}{chen2020_improved}
\abx@aux@segm{0}{0}{chen2020_improved}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{chen2020_improved}
\abx@aux@segm{0}{0}{chen2020_improved}
\abx@aux@cite{0}{chen2020_improved}
\abx@aux@segm{0}{0}{chen2020_improved}
\abx@aux@cite{0}{chen2020_improved}
\abx@aux@segm{0}{0}{chen2020_improved}
\abx@aux@cite{0}{chen2020_improved}
\abx@aux@segm{0}{0}{chen2020_improved}
\abx@aux@cite{0}{he2020_moco}
\abx@aux@segm{0}{0}{he2020_moco}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{chen2020_improved}
\abx@aux@segm{0}{0}{chen2020_improved}
\abx@aux@cite{0}{chen2020_improved}
\abx@aux@segm{0}{0}{chen2020_improved}
\abx@aux@cite{0}{chen2020_improved}
\abx@aux@segm{0}{0}{chen2020_improved}
\abx@aux@cite{0}{chen2020_improved}
\abx@aux@segm{0}{0}{chen2020_improved}
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\abx@aux@cite{0}{chen2021_mocov3}
\abx@aux@segm{0}{0}{chen2021_mocov3}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.3.6}MoCo v2 and MoCo v3}{1219}{subsection.22.3.6}\protected@file@percent }
\newlabel{subec:chapter22_moco_v2_v3}{{22.3.6}{1219}{MoCo v2 and MoCo v3}{subsection.22.3.6}{}}
\@writefile{toc}{\contentsline {paragraph}{From MoCo v1 to v2: Architectural Refinements}{1219}{section*.2574}\protected@file@percent }
\abx@aux@backref{1491}{he2020_moco}{0}{1219}{1219}
\abx@aux@backref{1492}{chen2020_improved}{0}{1219}{1219}
\abx@aux@backref{1493}{chen2020_simclr}{0}{1219}{1219}
\abx@aux@backref{1494}{chen2020_improved}{0}{1219}{1219}
\@writefile{lot}{\contentsline {table}{\numberline {22.4}{\ignorespaces Impact of MLP head and temperature \( \tau \) on ImageNet linear classification accuracy using ResNet-50 trained for 200 epochs. Results reproduced from~\blx@tocontentsinit {0}\cite {chen2020_improved}.}}{1219}{table.caption.2575}\protected@file@percent }
\abx@aux@backref{1496}{chen2020_improved}{0}{1219}{1219}
\newlabel{tab:chapter22_moco_v2_temperature}{{22.4}{1219}{Impact of MLP head and temperature \( \tau \) on ImageNet linear classification accuracy using ResNet-50 trained for 200 epochs. Results reproduced from~\cite {chen2020_improved}}{table.caption.2575}{}}
\abx@aux@backref{1497}{chen2020_improved}{0}{1219}{1219}
\abx@aux@backref{1498}{he2020_moco}{0}{1219}{1219}
\abx@aux@backref{1499}{chen2020_simclr}{0}{1219}{1219}
\abx@aux@backref{1500}{chen2020_improved}{0}{1219}{1219}
\abx@aux@backref{1501}{chen2020_improved}{0}{1219}{1219}
\@writefile{lot}{\contentsline {table}{\numberline {22.5}{\ignorespaces ImageNet linear probing accuracy: MoCo v2 vs. SimCLR. “aug+” includes stronger augmentations such as Gaussian blur and color jitter. Data from~\blx@tocontentsinit {0}\cite {chen2020_improved}.}}{1219}{table.caption.2576}\protected@file@percent }
\abx@aux@backref{1503}{chen2020_improved}{0}{1219}{1219}
\newlabel{tab:chapter22_moco_v2_vs_simclr}{{22.5}{1219}{ImageNet linear probing accuracy: MoCo v2 vs. SimCLR. “aug+” includes stronger augmentations such as Gaussian blur and color jitter. Data from~\cite {chen2020_improved}}{table.caption.2576}{}}
\@writefile{toc}{\contentsline {paragraph}{MoCo v3: Adapting Momentum Contrast to Vision Transformers}{1219}{section*.2577}\protected@file@percent }
\newlabel{par:chapter22_ssl_moco_v3}{{22.3.6}{1219}{MoCo v3: Adapting Momentum Contrast to Vision Transformers}{section*.2577}{}}
\abx@aux@backref{1504}{vit2020_transformers}{0}{1219}{1219}
\abx@aux@backref{1505}{chen2021_mocov3}{0}{1219}{1219}
\abx@aux@cite{0}{chen2021_mocov3}
\abx@aux@segm{0}{0}{chen2021_mocov3}
\abx@aux@cite{0}{chen2021_mocov3}
\abx@aux@segm{0}{0}{chen2021_mocov3}
\@writefile{toc}{\contentsline {paragraph}{Why Symmetric Loss?}{1221}{section*.2578}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Explanation}{1221}{section*.2579}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Performance Highlights}{1221}{section*.2580}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {22.6}{\ignorespaces Linear evaluation accuracy (\%) on ImageNet using various backbones.}}{1221}{table.caption.2581}\protected@file@percent }
\newlabel{tab:chapter22_moco_v3_comparison}{{22.6}{1221}{Linear evaluation accuracy (\%) on ImageNet using various backbones}{table.caption.2581}{}}
\@writefile{lot}{\contentsline {table}{\numberline {22.7}{\ignorespaces MoCo v3 accuracy on ImageNet with larger ViT backbones. Source:~\blx@tocontentsinit {0}\cite {chen2021_mocov3}.}}{1221}{table.caption.2582}\protected@file@percent }
\abx@aux@backref{1507}{chen2021_mocov3}{0}{1221}{1221}
\newlabel{tab:chapter22_mocov3_large_vit}{{22.7}{1221}{MoCo v3 accuracy on ImageNet with larger ViT backbones. Source:~\cite {chen2021_mocov3}}{table.caption.2582}{}}
\BKM@entry{id=848,dest={73756273656374696F6E2E32322E332E37},srcline={886}}{5C3337365C3337375C303030535C303030695C3030306D5C303030435C3030304C5C303030525C3030305C3034305C303030765C303030325C3030303A5C3030305C3034305C303030535C303030635C303030615C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030725C303030615C303030735C303030745C303030695C303030765C303030655C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030535C303030655C3030306D5C303030695C3030302D5C303030535C303030755C303030705C303030655C303030725C303030765C303030695C303030735C303030655C303030645C3030305C3034305C303030535C303030655C303030745C303030745C303030695C3030306E5C303030675C30303073}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{chen2020_simclrv2}
\abx@aux@segm{0}{0}{chen2020_simclrv2}
\abx@aux@cite{0}{chen2020_simclrv2}
\abx@aux@segm{0}{0}{chen2020_simclrv2}
\abx@aux@cite{0}{anonymous2021_nt_xent}
\abx@aux@segm{0}{0}{anonymous2021_nt_xent}
\abx@aux@cite{0}{chen2020_simclrv2}
\abx@aux@segm{0}{0}{chen2020_simclrv2}
\abx@aux@cite{0}{anonymous2021_nt_xent}
\abx@aux@segm{0}{0}{anonymous2021_nt_xent}
\@writefile{toc}{\contentsline {paragraph}{Takeaway}{1222}{section*.2583}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {22.3.7}SimCLR v2: Scaling Contrastive Learning for Semi-Supervised Settings}{1222}{subsection.22.3.7}\protected@file@percent }
\newlabel{subsec:chapter22_ssl_simclrv2}{{22.3.7}{1222}{SimCLR v2: Scaling Contrastive Learning for Semi-Supervised Settings}{subsection.22.3.7}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation and Overview}{1222}{section*.2584}\protected@file@percent }
\abx@aux@backref{1508}{chen2020_simclr}{0}{1222}{1222}
\abx@aux@backref{1509}{chen2020_simclrv2}{0}{1222}{1222}
\@writefile{lof}{\contentsline {figure}{\numberline {22.13}{\ignorespaces SimCLR v2 three-stage semi-supervised training pipeline: unsupervised contrastive pretraining (left), supervised fine-tuning on few labels (center), and distillation to a student network (right). Figure credit: Created by the author, adapted from~\blx@tocontentsinit {0}\cite {chen2020_simclrv2,anonymous2021_nt_xent}.}}{1222}{figure.caption.2585}\protected@file@percent }
\abx@aux@backref{1512}{chen2020_simclrv2}{0}{1222}{1222}
\abx@aux@backref{1513}{anonymous2021_nt_xent}{0}{1222}{1222}
\newlabel{fig:chapter22_simclrv2_pipeline}{{22.13}{1222}{SimCLR v2 three-stage semi-supervised training pipeline: unsupervised contrastive pretraining (left), supervised fine-tuning on few labels (center), and distillation to a student network (right). Figure credit: Created by the author, adapted from~\cite {chen2020_simclrv2,anonymous2021_nt_xent}}{figure.caption.2585}{}}
\@writefile{toc}{\contentsline {paragraph}{Three-Stage Training Framework}{1223}{section*.2586}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Architectural Enhancements and Ablation Insights}{1223}{section*.2587}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Distillation Works}{1223}{section*.2588}\protected@file@percent }
\abx@aux@cite{0}{chen2020_simclrv2}
\abx@aux@segm{0}{0}{chen2020_simclrv2}
\abx@aux@cite{0}{chen2020_simclrv2}
\abx@aux@segm{0}{0}{chen2020_simclrv2}
\abx@aux@cite{0}{chen2020_simclrv2}
\abx@aux@segm{0}{0}{chen2020_simclrv2}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{chen2020_simclrv2}
\abx@aux@segm{0}{0}{chen2020_simclrv2}
\abx@aux@cite{0}{chen2020_simclrv2}
\abx@aux@segm{0}{0}{chen2020_simclrv2}
\abx@aux@cite{0}{chen2020_improved}
\abx@aux@segm{0}{0}{chen2020_improved}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{chen2020_simclrv2}
\abx@aux@segm{0}{0}{chen2020_simclrv2}
\@writefile{toc}{\contentsline {paragraph}{Quantitative Results and Analysis}{1224}{section*.2589}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {22.8}{\ignorespaces \textbf  {Semi-supervised ImageNet classification results.} Top-1 / Top-5 accuracy (\%) using 1\% and 10\% of labels. All SimCLR v2 variants use distillation; smaller models are distilled from the 3$\times $+SK teacher. Adapted from~\blx@tocontentsinit {0}\cite {chen2020_simclrv2}.}}{1224}{table.caption.2590}\protected@file@percent }
\abx@aux@backref{1515}{chen2020_simclrv2}{0}{1224}{1224}
\newlabel{tab:chapter22_simclrv2_sota}{{22.8}{1224}{\textbf {Semi-supervised ImageNet classification results.} Top-1 / Top-5 accuracy (\%) using 1\% and 10\% of labels. All SimCLR v2 variants use distillation; smaller models are distilled from the 3$\times $+SK teacher. Adapted from~\cite {chen2020_simclrv2}}{table.caption.2590}{}}
\abx@aux@backref{1516}{chen2020_simclrv2}{0}{1224}{1224}
\abx@aux@backref{1517}{chen2020_simclr}{0}{1224}{1224}
\abx@aux@backref{1518}{grill2020_byol}{0}{1224}{1224}
\@writefile{lot}{\contentsline {table}{\numberline {22.9}{\ignorespaces Effect of distillation on ImageNet Top-1 accuracy under 1\% and 10\% label regimes. SimCLR v2 achieves strong performance without label-based supervision during distillation. Adapted from~\blx@tocontentsinit {0}\cite {chen2020_simclrv2}.}}{1224}{table.caption.2591}\protected@file@percent }
\abx@aux@backref{1520}{chen2020_simclrv2}{0}{1224}{1224}
\newlabel{tab:chapter22_simclrv2_distill}{{22.9}{1224}{Effect of distillation on ImageNet Top-1 accuracy under 1\% and 10\% label regimes. SimCLR v2 achieves strong performance without label-based supervision during distillation. Adapted from~\cite {chen2020_simclrv2}}{table.caption.2591}{}}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{chen2020_simclrv2}
\abx@aux@segm{0}{0}{chen2020_simclrv2}
\@writefile{lot}{\contentsline {table}{\numberline {22.10}{\ignorespaces Top-1 accuracy (\%) under linear evaluation on ImageNet using frozen backbones. All models use ResNet-50 (1$\times $, no SK) for a fair comparison.}}{1225}{table.caption.2592}\protected@file@percent }
\newlabel{tab:chapter22_simclrv2_linear}{{22.10}{1225}{Top-1 accuracy (\%) under linear evaluation on ImageNet using frozen backbones. All models use ResNet-50 (1$\times $, no SK) for a fair comparison}{table.caption.2592}{}}
\abx@aux@backref{1521}{chen2020_improved}{0}{1225}{1225}
\abx@aux@backref{1522}{chen2020_simclr}{0}{1225}{1225}
\abx@aux@backref{1523}{chen2020_simclrv2}{0}{1225}{1225}
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{1225}{section*.2593}\protected@file@percent }
\abx@aux@backref{1524}{chen2020_simclr}{0}{1225}{1225}
\abx@aux@backref{1525}{chen2020_simclrv2}{0}{1225}{1225}
\BKM@entry{id=849,dest={73756273656374696F6E2E32322E332E38},srcline={1030}}{5C3337365C3337375C303030525C303030655C3030304C5C303030495C303030435C3030303A5C3030305C3034305C303030525C303030655C303030705C303030725C303030655C303030735C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030765C303030695C303030615C3030305C3034305C303030495C3030306E5C303030765C303030615C303030725C303030695C303030615C3030306E5C303030745C3030305C3034305C303030435C303030615C303030755C303030735C303030615C3030306C5C3030305C3034305C3030304D5C303030655C303030635C303030685C303030615C3030306E5C303030695C303030735C3030306D5C30303073}
\abx@aux@cite{0}{mitrovic2020_relic}
\abx@aux@segm{0}{0}{mitrovic2020_relic}
\abx@aux@cite{0}{pearl2009_causality}
\abx@aux@segm{0}{0}{pearl2009_causality}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.3.8}ReLIC: Representation Learning via Invariant Causal Mechanisms}{1226}{subsection.22.3.8}\protected@file@percent }
\newlabel{subsec:chapter22_ssl_relic}{{22.3.8}{1226}{ReLIC: Representation Learning via Invariant Causal Mechanisms}{subsection.22.3.8}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation and Causal Assumptions}{1226}{section*.2594}\protected@file@percent }
\abx@aux@backref{1526}{mitrovic2020_relic}{0}{1226}{1226}
\abx@aux@backref{1527}{pearl2009_causality}{0}{1226}{1226}
\@writefile{toc}{\contentsline {paragraph}{Learning via Invariant Proxy Prediction}{1226}{section*.2595}\protected@file@percent }
\abx@aux@cite{0}{mitrovic2020_relic}
\abx@aux@segm{0}{0}{mitrovic2020_relic}
\abx@aux@cite{0}{mitrovic2020_relic}
\abx@aux@segm{0}{0}{mitrovic2020_relic}
\@writefile{lof}{\contentsline {figure}{\numberline {22.14}{\ignorespaces Causal assumptions and learning objective in ReLIC: representations should yield invariant predictions across style interventions (augmentations). Figure adapted from~\blx@tocontentsinit {0}\cite {mitrovic2020_relic}.}}{1227}{figure.caption.2596}\protected@file@percent }
\abx@aux@backref{1529}{mitrovic2020_relic}{0}{1227}{1227}
\newlabel{fig:chapter22_relic_goal}{{22.14}{1227}{Causal assumptions and learning objective in ReLIC: representations should yield invariant predictions across style interventions (augmentations). Figure adapted from~\cite {mitrovic2020_relic}}{figure.caption.2596}{}}
\@writefile{toc}{\contentsline {paragraph}{Summary}{1227}{section*.2597}\protected@file@percent }
\abx@aux@cite{0}{mitrovic2020_relic}
\abx@aux@segm{0}{0}{mitrovic2020_relic}
\abx@aux@cite{0}{mitrovic2020_relic}
\abx@aux@segm{0}{0}{mitrovic2020_relic}
\@writefile{toc}{\contentsline {paragraph}{From Proxy Tasks to Instance Discrimination}{1228}{section*.2598}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.15}{\ignorespaces Instance discrimination as a universal refinement: each image is treated as its own class, enabling the learning of invariant representations. Adapted from~\blx@tocontentsinit {0}\cite {mitrovic2020_relic}.}}{1228}{figure.caption.2599}\protected@file@percent }
\abx@aux@backref{1531}{mitrovic2020_relic}{0}{1228}{1228}
\newlabel{fig:chapter22_relic_refinement}{{22.15}{1228}{Instance discrimination as a universal refinement: each image is treated as its own class, enabling the learning of invariant representations. Adapted from~\cite {mitrovic2020_relic}}{figure.caption.2599}{}}
\@writefile{toc}{\contentsline {paragraph}{ReLIC Architecture and Training Setup}{1228}{section*.2600}\protected@file@percent }
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{he2020_moco}
\abx@aux@segm{0}{0}{he2020_moco}
\@writefile{toc}{\contentsline {paragraph}{Contrastive and Distributional Loss Terms}{1229}{section*.2601}\protected@file@percent }
\abx@aux@backref{1532}{grill2020_byol}{0}{1229}{1229}
\abx@aux@backref{1533}{he2020_moco}{0}{1229}{1229}
\abx@aux@cite{0}{bossard2014_food101}
\abx@aux@segm{0}{0}{bossard2014_food101}
\abx@aux@cite{0}{bossard2014_food101}
\abx@aux@segm{0}{0}{bossard2014_food101}
\@writefile{toc}{\contentsline {paragraph}{Loss Term 1: Instance-Level Contrastive Learning}{1230}{section*.2602}\protected@file@percent }
\newlabel{par:chapter22_relic_contrastive}{{22.3.8}{1230}{Loss Term 1: Instance-Level Contrastive Learning}{section*.2602}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22.16}{\ignorespaces Contrastive training in ReLIC: positive pairs (connected by green lines) are pulled closer, while negative pairs (connected by red dotted lines) are pushed apart. Arrows indicate pairwise distances in embedding space, measured via cosine similarity. Figure by the author; image samples are from the Food101 dataset~\blx@tocontentsinit {0}\cite {bossard2014_food101}.}}{1231}{figure.caption.2603}\protected@file@percent }
\abx@aux@backref{1535}{bossard2014_food101}{0}{1231}{1231}
\newlabel{fig:chapter22_relic_positive_negatives}{{22.16}{1231}{Contrastive training in ReLIC: positive pairs (connected by green lines) are pulled closer, while negative pairs (connected by red dotted lines) are pushed apart. Arrows indicate pairwise distances in embedding space, measured via cosine similarity. Figure by the author; image samples are from the Food101 dataset~\cite {bossard2014_food101}}{figure.caption.2603}{}}
\abx@aux@cite{0}{xie2019_unsupervised}
\abx@aux@segm{0}{0}{xie2019_unsupervised}
\abx@aux@cite{0}{bossard2014_food101}
\abx@aux@segm{0}{0}{bossard2014_food101}
\abx@aux@cite{0}{bossard2014_food101}
\abx@aux@segm{0}{0}{bossard2014_food101}
\@writefile{toc}{\contentsline {paragraph}{Loss Term 2: KL Regularization for Distributional Invariance}{1232}{section*.2604}\protected@file@percent }
\newlabel{par:chapter22_relic_kl}{{22.3.8}{1232}{Loss Term 2: KL Regularization for Distributional Invariance}{section*.2604}{}}
\abx@aux@backref{1536}{xie2019_unsupervised}{0}{1232}{1232}
\@writefile{lof}{\contentsline {figure}{\numberline {22.17}{\ignorespaces KL regularization in ReLIC encourages the similarity distributions of query embeddings from different augmentations to match. Arrows represent similarity scores from one view to all targets in the other. Figure by the author; image samples are from the Food101 dataset~\blx@tocontentsinit {0}\cite {bossard2014_food101}.}}{1233}{figure.caption.2605}\protected@file@percent }
\abx@aux@backref{1538}{bossard2014_food101}{0}{1233}{1233}
\newlabel{fig:chapter22_relic_kl_figure}{{22.17}{1233}{KL regularization in ReLIC encourages the similarity distributions of query embeddings from different augmentations to match. Arrows represent similarity scores from one view to all targets in the other. Figure by the author; image samples are from the Food101 dataset~\cite {bossard2014_food101}}{figure.caption.2605}{}}
\@writefile{toc}{\contentsline {paragraph}{From Causal Motivation to Loss Construction}{1233}{section*.2606}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ReLIC Objective}{1234}{section*.2607}\protected@file@percent }
\abx@aux@cite{0}{tomasev2022_relicv2}
\abx@aux@segm{0}{0}{tomasev2022_relicv2}
\@writefile{toc}{\contentsline {paragraph}{Architecture and Implementation Details}{1235}{section*.2608}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.18}{\ignorespaces ReLIC training pipeline: dual augmentations, dual encoders, and loss computation with KL regularization. Figure created by the author.}}{1235}{figure.caption.2609}\protected@file@percent }
\newlabel{fig:chapter22_relic_pipeline}{{22.18}{1235}{ReLIC training pipeline: dual augmentations, dual encoders, and loss computation with KL regularization. Figure created by the author}{figure.caption.2609}{}}
\@writefile{toc}{\contentsline {paragraph}{Performance and Evaluation}{1235}{section*.2610}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary and Outlook}{1235}{section*.2611}\protected@file@percent }
\abx@aux@backref{1539}{tomasev2022_relicv2}{0}{1235}{1235}
\BKM@entry{id=850,dest={73756273656374696F6E2E32322E332E39},srcline={1339}}{5C3337365C3337375C303030525C303030655C3030304C5C303030495C303030435C303030765C303030325C3030303A5C3030305C3034305C303030455C3030306E5C303030685C303030615C3030306E5C303030635C303030655C303030645C3030305C3034305C303030495C3030306E5C303030765C303030615C303030725C303030695C303030615C3030306E5C303030745C3030305C3034305C303030525C303030655C303030705C303030725C303030655C303030735C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\abx@aux@cite{0}{mitrovic2020_relic}
\abx@aux@segm{0}{0}{mitrovic2020_relic}
\abx@aux@cite{0}{tomasev2022_relicv2}
\abx@aux@segm{0}{0}{tomasev2022_relicv2}
\abx@aux@cite{0}{tomasev2022_relicv2}
\abx@aux@segm{0}{0}{tomasev2022_relicv2}
\abx@aux@cite{0}{tomasev2022_relicv2}
\abx@aux@segm{0}{0}{tomasev2022_relicv2}
\abx@aux@cite{0}{mitrovic2020_relic}
\abx@aux@segm{0}{0}{mitrovic2020_relic}
\abx@aux@cite{0}{tomasev2022_relicv2}
\abx@aux@segm{0}{0}{tomasev2022_relicv2}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.3.9}ReLICv2: Enhanced Invariant Representation Learning}{1236}{subsection.22.3.9}\protected@file@percent }
\newlabel{subsec:chapter22_ssl_relicv2}{{22.3.9}{1236}{ReLICv2: Enhanced Invariant Representation Learning}{subsection.22.3.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{Motivation: From View Invariance to Causal Robustness}{1236}{section*.2612}\protected@file@percent }
\abx@aux@backref{1540}{mitrovic2020_relic}{0}{1236}{1236}
\abx@aux@backref{1541}{tomasev2022_relicv2}{0}{1236}{1236}
\@writefile{lof}{\contentsline {figure}{\numberline {22.19}{\ignorespaces ReLICv2: Large and small views are generated and optionally passed through an unsupervised saliency mask. Contrastive and invariance losses are computed between each online view and all target large views. Figure credit: \blx@tocontentsinit {0}\cite {tomasev2022_relicv2}.}}{1236}{figure.caption.2613}\protected@file@percent }
\abx@aux@backref{1543}{tomasev2022_relicv2}{0}{1236}{1236}
\newlabel{fig:chapter22_relicv2_approach}{{22.19}{1236}{ReLICv2: Large and small views are generated and optionally passed through an unsupervised saliency mask. Contrastive and invariance losses are computed between each online view and all target large views. Figure credit: \cite {tomasev2022_relicv2}}{figure.caption.2613}{}}
\@writefile{toc}{\contentsline {subsubsection}{Foreground Saliency Masking}{1236}{section*.2614}\protected@file@percent }
\abx@aux@backref{1544}{mitrovic2020_relic}{0}{1236}{1236}
\abx@aux@backref{1545}{tomasev2022_relicv2}{0}{1236}{1236}
\abx@aux@cite{0}{tomasev2022_relicv2}
\abx@aux@segm{0}{0}{tomasev2022_relicv2}
\abx@aux@cite{0}{tomasev2022_relicv2}
\abx@aux@segm{0}{0}{tomasev2022_relicv2}
\abx@aux@cite{0}{mitrovic2020_relic}
\abx@aux@segm{0}{0}{mitrovic2020_relic}
\@writefile{lof}{\contentsline {figure}{\numberline {22.20}{\ignorespaces Foreground saliency masks are estimated without supervision and optionally applied to both large and small views during training. This encourages representations that prioritize object-centric content while discarding background variation. Figure credit: \blx@tocontentsinit {0}\cite {tomasev2022_relicv2}.}}{1237}{figure.caption.2615}\protected@file@percent }
\abx@aux@backref{1547}{tomasev2022_relicv2}{0}{1237}{1237}
\newlabel{fig:chapter22_relicv2_saliency}{{22.20}{1237}{Foreground saliency masks are estimated without supervision and optionally applied to both large and small views during training. This encourages representations that prioritize object-centric content while discarding background variation. Figure credit: \cite {tomasev2022_relicv2}}{figure.caption.2615}{}}
\@writefile{toc}{\contentsline {subsubsection}{Multi-View Learning with Large and Small Crops}{1237}{section*.2616}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{ReLICv2 Objective}{1237}{section*.2617}\protected@file@percent }
\newlabel{subsubsec:chapter22_relicv2_objective}{{22.3.9}{1237}{ReLICv2 Objective}{section*.2617}{}}
\abx@aux@backref{1548}{mitrovic2020_relic}{0}{1237}{1237}
\newlabel{eq:chapter22_relicv2_loss}{{22.1}{1238}{ReLICv2 Objective}{equation.22.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Term 1: Contrastive Log-Likelihood (Large-to-Large)}{1238}{section*.2618}\protected@file@percent }
\newlabel{par:chapter22_relicv2_term1_largelarge}{{22.3.9}{1238}{Term 1: Contrastive Log-Likelihood (Large-to-Large)}{section*.2618}{}}
\@writefile{toc}{\contentsline {paragraph}{Term 2: KL Divergence (Large-to-Large)}{1239}{section*.2619}\protected@file@percent }
\newlabel{par:chapter22_relicv2_term2_kl_largelarge}{{22.3.9}{1239}{Term 2: KL Divergence (Large-to-Large)}{section*.2619}{}}
\@writefile{toc}{\contentsline {paragraph}{Small-to-Large View Consistency Terms}{1240}{section*.2620}\protected@file@percent }
\newlabel{par:chapter22_relicv2_small_views}{{22.3.9}{1240}{Small-to-Large View Consistency Terms}{section*.2620}{}}
\@writefile{toc}{\contentsline {paragraph}{Training Procedure}{1241}{section*.2621}\protected@file@percent }
\newlabel{par:chapter22_relicv2_training}{{22.3.9}{1241}{Training Procedure}{section*.2621}{}}
\@writefile{toc}{\contentsline {paragraph}{Empirical Evaluation and Robustness Analysis}{1242}{section*.2622}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Linear Evaluation Performance}{1242}{section*.2623}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {22.11}{\ignorespaces Linear evaluation accuracy (\%) on ImageNet. ReLICv2 leads across ResNet variants.}}{1242}{table.caption.2624}\protected@file@percent }
\newlabel{tab:chapter22_relicv2_linear_eval}{{22.11}{1242}{Linear evaluation accuracy (\%) on ImageNet. ReLICv2 leads across ResNet variants}{table.caption.2624}{}}
\abx@aux@cite{0}{tomasev2022_relicv2}
\abx@aux@segm{0}{0}{tomasev2022_relicv2}
\abx@aux@cite{0}{tomasev2022_relicv2}
\abx@aux@segm{0}{0}{tomasev2022_relicv2}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{mitrovic2020_relic}
\abx@aux@segm{0}{0}{mitrovic2020_relic}
\abx@aux@cite{0}{tomasev2022_relicv2}
\abx@aux@segm{0}{0}{tomasev2022_relicv2}
\abx@aux@cite{0}{tomasev2022_relicv2}
\abx@aux@segm{0}{0}{tomasev2022_relicv2}
\abx@aux@cite{0}{tomasev2022_relicv2}
\abx@aux@segm{0}{0}{tomasev2022_relicv2}
\@writefile{toc}{\contentsline {paragraph}{Robustness and Out-of-Distribution Generalization}{1243}{section*.2625}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {22.12}{\ignorespaces \textbf  {Robustness and OOD generalization results.} Top-1 accuracy (\%) from linear classifiers trained on frozen ImageNet-pretrained ResNet-50 representations. ImageNet-V2 values are reported for matched frequency (MF), threshold-0.7 (T-0.7), and top images (TI). ImageNet-C accuracy is averaged over 15 corruptions. Adapted from~\blx@tocontentsinit {0}\cite {tomasev2022_relicv2}.}}{1243}{table.caption.2626}\protected@file@percent }
\abx@aux@backref{1550}{tomasev2022_relicv2}{0}{1243}{1243}
\newlabel{tab:chapter22_relicv2_robustness}{{22.12}{1243}{\textbf {Robustness and OOD generalization results.} Top-1 accuracy (\%) from linear classifiers trained on frozen ImageNet-pretrained ResNet-50 representations. ImageNet-V2 values are reported for matched frequency (MF), threshold-0.7 (T-0.7), and top images (TI). ImageNet-C accuracy is averaged over 15 corruptions. Adapted from~\cite {tomasev2022_relicv2}}{table.caption.2626}{}}
\abx@aux@backref{1551}{chen2020_simclr}{0}{1243}{1243}
\abx@aux@backref{1552}{grill2020_byol}{0}{1243}{1243}
\abx@aux@backref{1553}{mitrovic2020_relic}{0}{1243}{1243}
\abx@aux@backref{1554}{tomasev2022_relicv2}{0}{1243}{1243}
\@writefile{toc}{\contentsline {paragraph}{Semantic Clarity and Class-wise Consistency}{1243}{section*.2627}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.21}{\ignorespaces Confusion matrix under linear evaluation. ReLICv2 achieves sharper class boundaries and reduced confusion between semantically similar categories. Figure credit: \blx@tocontentsinit {0}\cite {tomasev2022_relicv2}.}}{1243}{figure.caption.2628}\protected@file@percent }
\abx@aux@backref{1556}{tomasev2022_relicv2}{0}{1243}{1243}
\newlabel{fig:chapter22_relicv2_confusion}{{22.21}{1243}{Confusion matrix under linear evaluation. ReLICv2 achieves sharper class boundaries and reduced confusion between semantically similar categories. Figure credit: \cite {tomasev2022_relicv2}}{figure.caption.2628}{}}
\@writefile{toc}{\contentsline {subsubsection}{Summary}{1244}{section*.2629}\protected@file@percent }
\BKM@entry{id=851,dest={73756273656374696F6E2E32322E332E3130},srcline={1719}}{5C3337365C3337375C303030465C303030755C303030725C303030745C303030685C303030655C303030725C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030725C303030615C303030735C303030745C303030695C303030765C303030655C3030305C3034305C303030495C3030306E5C3030306E5C3030306F5C303030765C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\abx@aux@cite{0}{dwibedi2021_nnclr}
\abx@aux@segm{0}{0}{dwibedi2021_nnclr}
\abx@aux@cite{0}{dwibedi2021_nnclr}
\abx@aux@segm{0}{0}{dwibedi2021_nnclr}
\abx@aux@cite{0}{dwibedi2021_nnclr}
\abx@aux@segm{0}{0}{dwibedi2021_nnclr}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.3.10}Further Contrastive Innovations}{1245}{subsection.22.3.10}\protected@file@percent }
\newlabel{subsec:chapter22_ssl_contrastive_extensions}{{22.3.10}{1245}{Further Contrastive Innovations}{subsection.22.3.10}{}}
\@writefile{toc}{\contentsline {paragraph}{Nearest-Neighbor Contrastive Learning (NNCLR)}{1245}{section*.2630}\protected@file@percent }
\newlabel{par:chapter22_nnclr}{{22.3.10}{1245}{Nearest-Neighbor Contrastive Learning (NNCLR)}{section*.2630}{}}
\abx@aux@backref{1557}{dwibedi2021_nnclr}{0}{1245}{1245}
\@writefile{lof}{\contentsline {figure}{\numberline {22.22}{\ignorespaces Overview of NNCLR training~\blx@tocontentsinit {0}\cite {dwibedi2021_nnclr}. Each query is paired with its nearest neighbor from a support queue. This decouples the definition of positives from augmentation alone and encourages semantic alignment.}}{1245}{figure.caption.2631}\protected@file@percent }
\abx@aux@backref{1559}{dwibedi2021_nnclr}{0}{1245}{1245}
\newlabel{fig:chapter22_nnclr_training}{{22.22}{1245}{Overview of NNCLR training~\cite {dwibedi2021_nnclr}. Each query is paired with its nearest neighbor from a support queue. This decouples the definition of positives from augmentation alone and encourages semantic alignment}{figure.caption.2631}{}}
\abx@aux@cite{0}{hu2021_adco}
\abx@aux@segm{0}{0}{hu2021_adco}
\abx@aux@cite{0}{xiao2021_clsa}
\abx@aux@segm{0}{0}{xiao2021_clsa}
\@writefile{toc}{\contentsline {paragraph}{Adversarial Contrastive Learning (AdCo)}{1246}{section*.2632}\protected@file@percent }
\newlabel{par:chapter22_adco}{{22.3.10}{1246}{Adversarial Contrastive Learning (AdCo)}{section*.2632}{}}
\abx@aux@backref{1560}{hu2021_adco}{0}{1246}{1246}
\@writefile{toc}{\contentsline {paragraph}{Contrastive Learning with Stronger Augmentations (CLSA)}{1246}{section*.2633}\protected@file@percent }
\newlabel{par:chapter22_clsa}{{22.3.10}{1246}{Contrastive Learning with Stronger Augmentations (CLSA)}{section*.2633}{}}
\abx@aux@backref{1561}{xiao2021_clsa}{0}{1246}{1246}
\abx@aux@cite{0}{xiao2021_clsa}
\abx@aux@segm{0}{0}{xiao2021_clsa}
\abx@aux@cite{0}{mitrovic2020_relic}
\abx@aux@segm{0}{0}{mitrovic2020_relic}
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 22.3.10.1: CLSA vs.\ ReLIC: KL Divergence in Perspective}{1247}{section*.2634}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{CLSA: Distributional Distillation Across Augmentation Strength}{1247}{section*.2635}\protected@file@percent }
\abx@aux@backref{1562}{xiao2021_clsa}{0}{1247}{1247}
\@writefile{toc}{\contentsline {paragraph}{ReLICv1: Invariant Prediction Across Augmentations}{1247}{section*.2636}\protected@file@percent }
\abx@aux@backref{1563}{mitrovic2020_relic}{0}{1247}{1247}
\@writefile{toc}{\contentsline {paragraph}{Two KL Terms, Two Philosophies}{1247}{section*.2637}\protected@file@percent }
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{he2020_moco}
\abx@aux@segm{0}{0}{he2020_moco}
\abx@aux@cite{0}{chen2020_improved}
\abx@aux@segm{0}{0}{chen2020_improved}
\abx@aux@cite{0}{chen2021_mocov3}
\abx@aux@segm{0}{0}{chen2021_mocov3}
\abx@aux@cite{0}{dwibedi2021_nnclr}
\abx@aux@segm{0}{0}{dwibedi2021_nnclr}
\abx@aux@cite{0}{hu2021_adco}
\abx@aux@segm{0}{0}{hu2021_adco}
\abx@aux@cite{0}{xiao2021_clsa}
\abx@aux@segm{0}{0}{xiao2021_clsa}
\abx@aux@cite{0}{mitrovic2020_relic}
\abx@aux@segm{0}{0}{mitrovic2020_relic}
\abx@aux@cite{0}{tomasev2022_relicv2}
\abx@aux@segm{0}{0}{tomasev2022_relicv2}
\@writefile{toc}{\contentsline {paragraph}{Summary (CLSA vs. ReLIC)}{1248}{section*.2638}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Comparative Landscape and Emerging Trends}{1248}{section*.2639}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {22.13}{\ignorespaces \textbf  {Comparison of selected self-supervised methods on ImageNet.} Top-1 accuracy from linear evaluation on ResNet-50, unless otherwise noted. All methods use two global views unless stated.}}{1248}{table.caption.2640}\protected@file@percent }
\newlabel{tab:chapter22_contrastive_methods_summary}{{22.13}{1248}{\textbf {Comparison of selected self-supervised methods on ImageNet.} Top-1 accuracy from linear evaluation on ResNet-50, unless otherwise noted. All methods use two global views unless stated}{table.caption.2640}{}}
\abx@aux@backref{1564}{chen2020_simclr}{0}{1248}{1248}
\abx@aux@backref{1565}{he2020_moco}{0}{1248}{1248}
\abx@aux@backref{1566}{chen2020_improved}{0}{1248}{1248}
\abx@aux@backref{1567}{chen2021_mocov3}{0}{1248}{1248}
\abx@aux@backref{1568}{dwibedi2021_nnclr}{0}{1248}{1248}
\abx@aux@backref{1569}{hu2021_adco}{0}{1248}{1248}
\abx@aux@backref{1570}{xiao2021_clsa}{0}{1248}{1248}
\abx@aux@backref{1571}{mitrovic2020_relic}{0}{1248}{1248}
\abx@aux@backref{1572}{tomasev2022_relicv2}{0}{1248}{1248}
\@writefile{toc}{\contentsline {paragraph}{A Transition Toward Natural Supervision}{1248}{section*.2641}\protected@file@percent }
\BKM@entry{id=852,dest={73756273656374696F6E2E32322E332E3131},srcline={1886}}{5C3337365C3337375C303030435C3030304C5C303030495C303030505C3030303A5C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C303030655C303030725C303030615C303030625C3030306C5C303030655C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030735C3030305C3034305C303030665C303030725C3030306F5C3030306D5C3030305C3034305C3030304E5C303030615C303030745C303030755C303030725C303030615C3030306C5C3030305C3034305C3030304C5C303030615C3030306E5C303030675C303030755C303030615C303030675C303030655C3030305C3034305C303030535C303030755C303030705C303030655C303030725C303030765C303030695C303030735C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{clip2021_multimodal}
\abx@aux@segm{0}{0}{clip2021_multimodal}
\abx@aux@cite{0}{clip2021_multimodal}
\abx@aux@segm{0}{0}{clip2021_multimodal}
\abx@aux@cite{0}{clip2021_multimodal}
\abx@aux@segm{0}{0}{clip2021_multimodal}
\abx@aux@cite{0}{vinyals2015_showtell}
\abx@aux@segm{0}{0}{vinyals2015_showtell}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.3.11}CLIP: Learning Transferable Visual Models from Natural Language Supervision}{1249}{subsection.22.3.11}\protected@file@percent }
\newlabel{subsec:chapter22_ssl_clip}{{22.3.11}{1249}{CLIP: Learning Transferable Visual Models from Natural Language Supervision}{subsection.22.3.11}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation: Beyond Fixed Labels}{1249}{section*.2642}\protected@file@percent }
\abx@aux@backref{1573}{clip2021_multimodal}{0}{1249}{1249}
\@writefile{lof}{\contentsline {figure}{\numberline {22.23}{\ignorespaces CLIP learns a shared embedding space for images and text, aligning semantically matching pairs while repelling mismatched ones. Figure adapted from~\blx@tocontentsinit {0}\cite {clip2021_multimodal}.}}{1249}{figure.caption.2643}\protected@file@percent }
\abx@aux@backref{1575}{clip2021_multimodal}{0}{1249}{1249}
\newlabel{fig:chapter22_clip_alignment}{{22.23}{1249}{CLIP learns a shared embedding space for images and text, aligning semantically matching pairs while repelling mismatched ones. Figure adapted from~\cite {clip2021_multimodal}}{figure.caption.2643}{}}
\@writefile{toc}{\contentsline {paragraph}{A Naïve Approach: Caption Prediction}{1249}{section*.2644}\protected@file@percent }
\abx@aux@backref{1576}{vinyals2015_showtell}{0}{1249}{1249}
\abx@aux@cite{0}{clip2021_multimodal}
\abx@aux@segm{0}{0}{clip2021_multimodal}
\abx@aux@cite{0}{clip2021_multimodal}
\abx@aux@segm{0}{0}{clip2021_multimodal}
\@writefile{toc}{\contentsline {paragraph}{Efficiency Comparison: Contrastive vs.\ Predictive Objectives}{1250}{section*.2645}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.24}{\ignorespaces Zero-shot ImageNet classification accuracy under different training objectives. CLIP’s contrastive loss dramatically outperforms alternatives like Bag-of-Words prediction and autoregressive captioning. Figure adapted from~\blx@tocontentsinit {0}\cite {clip2021_multimodal}.}}{1250}{figure.caption.2646}\protected@file@percent }
\abx@aux@backref{1578}{clip2021_multimodal}{0}{1250}{1250}
\newlabel{fig:chapter22_clip_efficiency}{{22.24}{1250}{Zero-shot ImageNet classification accuracy under different training objectives. CLIP’s contrastive loss dramatically outperforms alternatives like Bag-of-Words prediction and autoregressive captioning. Figure adapted from~\cite {clip2021_multimodal}}{figure.caption.2646}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Contrastive Learning Wins}{1251}{section*.2647}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Key Insight}{1251}{section*.2648}\protected@file@percent }
\abx@aux@cite{0}{clip2021_multimodal}
\abx@aux@segm{0}{0}{clip2021_multimodal}
\abx@aux@cite{0}{clip2021_multimodal}
\abx@aux@segm{0}{0}{clip2021_multimodal}
\@writefile{toc}{\contentsline {subsubsection}{CLIP’s Contrastive Training Approach and Loss}{1252}{section*.2649}\protected@file@percent }
\newlabel{subsubsec:chapter22_ssl_clip_loss}{{22.3.11}{1252}{CLIP’s Contrastive Training Approach and Loss}{section*.2649}{}}
\@writefile{toc}{\contentsline {paragraph}{Training Strategy: Paired Alignment at Scale}{1252}{section*.2650}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Symmetric Contrastive Loss}{1252}{section*.2651}\protected@file@percent }
\newlabel{eq:chapter22_clip_similarity}{{22.2}{1252}{Symmetric Contrastive Loss}{equation.22.2}{}}
\newlabel{eq:chapter22_clip_loss}{{22.3}{1252}{Symmetric Contrastive Loss}{equation.22.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22.25}{\ignorespaces Contrastive loss structure in CLIP. For the third image--text pair, CLIP computes similarities between the image and all texts (row) and between the text and all images (column). The objective is to maximize the diagonal element (the correct pair) and suppress off-diagonal similarities. Figure adapted from~\blx@tocontentsinit {0}\cite {clip2021_multimodal}.}}{1253}{figure.caption.2652}\protected@file@percent }
\abx@aux@backref{1580}{clip2021_multimodal}{0}{1253}{1253}
\newlabel{fig:chapter22_clip_loss_matrix}{{22.25}{1253}{Contrastive loss structure in CLIP. For the third image--text pair, CLIP computes similarities between the image and all texts (row) and between the text and all images (column). The objective is to maximize the diagonal element (the correct pair) and suppress off-diagonal similarities. Figure adapted from~\cite {clip2021_multimodal}}{figure.caption.2652}{}}
\@writefile{toc}{\contentsline {paragraph}{Interpretation and Scaling Advantages}{1253}{section*.2653}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Efficient Large-Scale Training}{1253}{section*.2654}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{CLIP Loss Pseudo Code \& Further Explanations}{1254}{section*.2655}\protected@file@percent }
\newlabel{subsec:chapter22_clip_loss}{{22.3.11}{1254}{CLIP Loss Pseudo Code \& Further Explanations}{section*.2655}{}}
\@writefile{toc}{\contentsline {paragraph}{Loss Pseudo Code}{1254}{section*.2656}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Explanation}{1254}{section*.2657}\protected@file@percent }
\abx@aux@cite{0}{clip2021_multimodal}
\abx@aux@segm{0}{0}{clip2021_multimodal}
\abx@aux@cite{0}{clip2021_multimodal}
\abx@aux@segm{0}{0}{clip2021_multimodal}
\@writefile{toc}{\contentsline {paragraph}{Intuition Behind the BCE Terms}{1255}{section*.2658}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{CLIP Experiments and Ablations}{1255}{section*.2659}\protected@file@percent }
\newlabel{subsubsec:chapter22_ssl_clip_experiments}{{22.3.11}{1255}{CLIP Experiments and Ablations}{section*.2659}{}}
\@writefile{toc}{\contentsline {paragraph}{Zero-Shot Performance vs.\ Supervised Models}{1255}{section*.2660}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.26}{\ignorespaces Zero-shot CLIP is competitive with fully supervised linear probes on ImageNet-trained ResNet-50 features. Out of 27 datasets, CLIP wins on 16—including ImageNet itself. Figure adapted from~\blx@tocontentsinit {0}\cite {clip2021_multimodal}.}}{1255}{figure.caption.2661}\protected@file@percent }
\abx@aux@backref{1582}{clip2021_multimodal}{0}{1255}{1255}
\newlabel{fig:chapter22_clip_vs_supervised}{{22.26}{1255}{Zero-shot CLIP is competitive with fully supervised linear probes on ImageNet-trained ResNet-50 features. Out of 27 datasets, CLIP wins on 16—including ImageNet itself. Figure adapted from~\cite {clip2021_multimodal}}{figure.caption.2661}{}}
\abx@aux@cite{0}{clip2021_multimodal}
\abx@aux@segm{0}{0}{clip2021_multimodal}
\abx@aux@cite{0}{clip2021_multimodal}
\abx@aux@segm{0}{0}{clip2021_multimodal}
\@writefile{toc}{\contentsline {paragraph}{Robustness to Natural Distribution Shift}{1256}{section*.2662}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.27}{\ignorespaces CLIP significantly reduces the robustness gap across several distribution shift benchmarks, outperforming supervised baselines. Zero-shot variants reduce the gap by up to 75\%. Figure adapted from~\blx@tocontentsinit {0}\cite {clip2021_multimodal}.}}{1256}{figure.caption.2663}\protected@file@percent }
\abx@aux@backref{1584}{clip2021_multimodal}{0}{1256}{1256}
\newlabel{fig:chapter22_clip_distribution_shift}{{22.27}{1256}{CLIP significantly reduces the robustness gap across several distribution shift benchmarks, outperforming supervised baselines. Zero-shot variants reduce the gap by up to 75\%. Figure adapted from~\cite {clip2021_multimodal}}{figure.caption.2663}{}}
\abx@aux@cite{0}{clip2021_multimodal}
\abx@aux@segm{0}{0}{clip2021_multimodal}
\abx@aux@cite{0}{clip2021_multimodal}
\abx@aux@segm{0}{0}{clip2021_multimodal}
\@writefile{toc}{\contentsline {paragraph}{Linear Probe Evaluation Across Models}{1257}{section*.2664}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.28}{\ignorespaces Linear probing performance across several models. CLIP with Vision Transformer backbones achieves competitive or superior results, while being significantly more compute-efficient than traditional ResNets. Figure adapted from~\blx@tocontentsinit {0}\cite {clip2021_multimodal}.}}{1257}{figure.caption.2665}\protected@file@percent }
\abx@aux@backref{1586}{clip2021_multimodal}{0}{1257}{1257}
\newlabel{fig:chapter22_clip_linear_probe}{{22.28}{1257}{Linear probing performance across several models. CLIP with Vision Transformer backbones achieves competitive or superior results, while being significantly more compute-efficient than traditional ResNets. Figure adapted from~\cite {clip2021_multimodal}}{figure.caption.2665}{}}
\abx@aux@cite{0}{clip2021_multimodal}
\abx@aux@segm{0}{0}{clip2021_multimodal}
\abx@aux@cite{0}{clip2021_multimodal}
\abx@aux@segm{0}{0}{clip2021_multimodal}
\BKM@entry{id=853,dest={73656374696F6E2E32322E34},srcline={2177}}{5C3337365C3337375C303030535C303030655C3030306C5C303030665C3030302D5C303030445C303030695C303030735C303030745C303030695C3030306C5C3030306C5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C30303073}
\BKM@entry{id=854,dest={73756273656374696F6E2E32322E342E31},srcline={2180}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030725C303030615C303030735C303030745C303030695C303030765C303030655C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C30303067}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{he2020_moco}
\abx@aux@segm{0}{0}{he2020_moco}
\abx@aux@cite{0}{clip2021_multimodal}
\abx@aux@segm{0}{0}{clip2021_multimodal}
\@writefile{toc}{\contentsline {paragraph}{Tradeoffs in Dataset-Specific Adaptation}{1258}{section*.2666}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.29}{\ignorespaces Adaptation tradeoff: fine-tuning CLIP on ImageNet improves in-domain performance but may degrade generalization to other datasets. Prompt ensembling or hybrid classifiers offer more balanced solutions. Figure adapted from~\blx@tocontentsinit {0}\cite {clip2021_multimodal}.}}{1258}{figure.caption.2667}\protected@file@percent }
\abx@aux@backref{1588}{clip2021_multimodal}{0}{1258}{1258}
\newlabel{fig:chapter22_clip_adaptation}{{22.29}{1258}{Adaptation tradeoff: fine-tuning CLIP on ImageNet improves in-domain performance but may degrade generalization to other datasets. Prompt ensembling or hybrid classifiers offer more balanced solutions. Figure adapted from~\cite {clip2021_multimodal}}{figure.caption.2667}{}}
\@writefile{toc}{\contentsline {paragraph}{Summary and Practical Takeaways}{1258}{section*.2668}\protected@file@percent }
\BKM@entry{id=855,dest={73756273656374696F6E2E32322E342E32},srcline={2196}}{5C3337365C3337375C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030725C303030615C303030735C303030745C303030695C303030765C303030655C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030445C303030695C303030735C303030745C303030695C3030306C5C3030306C5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{hinton2015_distillation}
\abx@aux@segm{0}{0}{hinton2015_distillation}
\abx@aux@cite{0}{hinton2015_distillation}
\abx@aux@segm{0}{0}{hinton2015_distillation}
\abx@aux@cite{0}{gou2020_kd}
\abx@aux@segm{0}{0}{gou2020_kd}
\@writefile{toc}{\contentsline {section}{\numberline {22.4}Self-Distillation Methods}{1259}{section.22.4}\protected@file@percent }
\newlabel{sec:chapter22_ssl_self_distillation}{{22.4}{1259}{Self-Distillation Methods}{section.22.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.4.1}Limitations of Contrastive Learning}{1259}{subsection.22.4.1}\protected@file@percent }
\newlabel{subsec:chapter22_ssl_contrastive_limitations}{{22.4.1}{1259}{Limitations of Contrastive Learning}{subsection.22.4.1}{}}
\abx@aux@backref{1589}{chen2020_simclr}{0}{1259}{1259}
\abx@aux@backref{1590}{he2020_moco}{0}{1259}{1259}
\abx@aux@backref{1591}{clip2021_multimodal}{0}{1259}{1259}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.4.2}From Contrastive Methods to Self-Distillation}{1259}{subsection.22.4.2}\protected@file@percent }
\newlabel{subsec:chapter22_ssl_transition_to_sd}{{22.4.2}{1259}{From Contrastive Methods to Self-Distillation}{subsection.22.4.2}{}}
\abx@aux@backref{1592}{hinton2015_distillation}{0}{1259}{1259}
\@writefile{toc}{\contentsline {paragraph}{Classical Knowledge Distillation}{1259}{section*.2669}\protected@file@percent }
\newlabel{par:chapter22_kd_classical}{{22.4.2}{1259}{Classical Knowledge Distillation}{section*.2669}{}}
\abx@aux@backref{1593}{gou2020_kd}{0}{1259}{1259}
\abx@aux@backref{1594}{hinton2015_distillation}{0}{1259}{1259}
\abx@aux@cite{0}{gou2020_kd}
\abx@aux@segm{0}{0}{gou2020_kd}
\abx@aux@cite{0}{gou2020_kd}
\abx@aux@segm{0}{0}{gou2020_kd}
\@writefile{lof}{\contentsline {figure}{\numberline {22.30}{\ignorespaces Response-based knowledge distillation~\blx@tocontentsinit {0}\cite {gou2020_kd}. A student network is trained to match the soft output distribution (logits) of a pre-trained teacher.}}{1260}{figure.caption.2670}\protected@file@percent }
\abx@aux@backref{1596}{gou2020_kd}{0}{1260}{1260}
\newlabel{fig:chapter22_kd_response}{{22.30}{1260}{Response-based knowledge distillation~\cite {gou2020_kd}. A student network is trained to match the soft output distribution (logits) of a pre-trained teacher}{figure.caption.2670}{}}
\@writefile{toc}{\contentsline {paragraph}{From Classical KD to Self-Distillation}{1261}{section*.2671}\protected@file@percent }
\newlabel{par:chapter22_kd_transition}{{22.4.2}{1261}{From Classical KD to Self-Distillation}{section*.2671}{}}
\@writefile{toc}{\contentsline {paragraph}{Self-Distillation: Teacher-Free Prediction Alignment}{1261}{section*.2672}\protected@file@percent }
\newlabel{par:chapter22_kd_self}{{22.4.2}{1261}{Self-Distillation: Teacher-Free Prediction Alignment}{section*.2672}{}}
\abx@aux@cite{0}{duc2022_selfkd}
\abx@aux@segm{0}{0}{duc2022_selfkd}
\abx@aux@cite{0}{duc2022_selfkd}
\abx@aux@segm{0}{0}{duc2022_selfkd}
\@writefile{toc}{\contentsline {paragraph}{Cold Start and the Bootstrapping Feedback Loop}{1262}{section*.2673}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.31}{\ignorespaces Left: Classical knowledge distillation with distinct teacher and student networks. Right: Self-distillation using an online and a momentum-updated target network of identical architecture~\blx@tocontentsinit {0}\cite {duc2022_selfkd}.}}{1262}{figure.caption.2674}\protected@file@percent }
\abx@aux@backref{1598}{duc2022_selfkd}{0}{1262}{1262}
\newlabel{fig:chapter22_self_distill}{{22.31}{1262}{Left: Classical knowledge distillation with distinct teacher and student networks. Right: Self-distillation using an online and a momentum-updated target network of identical architecture~\cite {duc2022_selfkd}}{figure.caption.2674}{}}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\@writefile{toc}{\contentsline {paragraph}{Final Representation: What Do We Keep?}{1263}{section*.2675}\protected@file@percent }
\newlabel{par:chapter22_sd_final_network}{{22.4.2}{1263}{Final Representation: What Do We Keep?}{section*.2675}{}}
\abx@aux@backref{1599}{chen2020_simclr}{0}{1263}{1263}
\abx@aux@backref{1600}{grill2020_byol}{0}{1263}{1263}
\@writefile{toc}{\contentsline {paragraph}{Introduction Summary}{1263}{section*.2676}\protected@file@percent }
\BKM@entry{id=856,dest={73756273656374696F6E2E32322E342E33},srcline={2363}}{5C3337365C3337375C303030425C3030306F5C3030306F5C303030745C303030735C303030745C303030725C303030615C303030705C3030305C3034305C303030595C3030306F5C303030755C303030725C3030305C3034305C3030304F5C303030775C3030306E5C3030305C3034305C3030304C5C303030615C303030745C303030655C3030306E5C303030745C3030305C3034305C3030305C3035305C303030425C303030595C3030304F5C3030304C5C3030305C303531}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.4.3}Bootstrap Your Own Latent (BYOL)}{1264}{subsection.22.4.3}\protected@file@percent }
\newlabel{subsec:chapter22_ssl_byol}{{22.4.3}{1264}{Bootstrap Your Own Latent (BYOL)}{subsection.22.4.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation: Learning Without Contrast}{1264}{section*.2677}\protected@file@percent }
\abx@aux@backref{1601}{grill2020_byol}{0}{1264}{1264}
\@writefile{toc}{\contentsline {paragraph}{Architectural Overview}{1264}{section*.2678}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.32}{\ignorespaces BYOL architecture: the online branch predicts the target embedding from a different view of the same image. The target branch is updated via EMA and does not receive gradient updates. Only the online encoder \( f_\theta \) is retained after training. Figure adapted from~\blx@tocontentsinit {0}\cite {grill2020_byol}.}}{1264}{figure.caption.2679}\protected@file@percent }
\abx@aux@backref{1603}{grill2020_byol}{0}{1264}{1264}
\newlabel{fig:chapter22_byol_arch}{{22.32}{1264}{BYOL architecture: the online branch predicts the target embedding from a different view of the same image. The target branch is updated via EMA and does not receive gradient updates. Only the online encoder \( f_\theta \) is retained after training. Figure adapted from~\cite {grill2020_byol}}{figure.caption.2679}{}}
\@writefile{toc}{\contentsline {paragraph}{Mathematical Formulation and Training Objective}{1265}{section*.2680}\protected@file@percent }
\newlabel{par:chapter22_byol_objective}{{22.4.3}{1265}{Mathematical Formulation and Training Objective}{section*.2680}{}}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\@writefile{toc}{\contentsline {paragraph}{Robustness and Empirical Performance}{1266}{section*.2681}\protected@file@percent }
\newlabel{par:chapter22_byol_results}{{22.4.3}{1266}{Robustness and Empirical Performance}{section*.2681}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22.33}{\ignorespaces Left: Top-1 accuracy on ImageNet under varying batch sizes. SimCLR drops sharply below 512, while BYOL remains stable down to 256 and only deteriorates significantly below it. Right: Effect of removing different augmentations from the baseline. In this aspect, BYOL also shows greater robustness than SimCLR. When color distortions were removed from the augmentation pipeline SimCLR performance dropped far more than BYOL. Adapted from~\blx@tocontentsinit {0}\cite {grill2020_byol}.}}{1266}{figure.caption.2682}\protected@file@percent }
\abx@aux@backref{1605}{grill2020_byol}{0}{1266}{1266}
\newlabel{fig:chapter22_byol_batch_aug}{{22.33}{1266}{Left: Top-1 accuracy on ImageNet under varying batch sizes. SimCLR drops sharply below 512, while BYOL remains stable down to 256 and only deteriorates significantly below it. Right: Effect of removing different augmentations from the baseline. In this aspect, BYOL also shows greater robustness than SimCLR. When color distortions were removed from the augmentation pipeline SimCLR performance dropped far more than BYOL. Adapted from~\cite {grill2020_byol}}{figure.caption.2682}{}}
\@writefile{toc}{\contentsline {paragraph}{Linear Evaluation on ImageNet}{1266}{section*.2683}\protected@file@percent }
\newlabel{par:chapter22_byol_lineareval}{{22.4.3}{1266}{Linear Evaluation on ImageNet}{section*.2683}{}}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{henaff2020_cpcv2}
\abx@aux@segm{0}{0}{henaff2020_cpcv2}
\abx@aux@cite{0}{chen2020_improved}
\abx@aux@segm{0}{0}{chen2020_improved}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{tian2021_understanding}
\abx@aux@segm{0}{0}{tian2021_understanding}
\@writefile{lot}{\contentsline {table}{\numberline {22.14}{\ignorespaces Top-1 and Top-5 accuracy on ImageNet under linear evaluation. BYOL outperforms both contrastive and non-contrastive baselines. Adapted from~\blx@tocontentsinit {0}\cite {grill2020_byol}.}}{1267}{table.caption.2684}\protected@file@percent }
\abx@aux@backref{1607}{grill2020_byol}{0}{1267}{1267}
\newlabel{tab:chapter22_byol_lineareval}{{22.14}{1267}{Top-1 and Top-5 accuracy on ImageNet under linear evaluation. BYOL outperforms both contrastive and non-contrastive baselines. Adapted from~\cite {grill2020_byol}}{table.caption.2684}{}}
\abx@aux@backref{1608}{chen2020_simclr}{0}{1267}{1267}
\abx@aux@backref{1609}{henaff2020_cpcv2}{0}{1267}{1267}
\abx@aux@backref{1610}{chen2020_improved}{0}{1267}{1267}
\@writefile{toc}{\contentsline {paragraph}{Semi-Supervised Evaluation}{1267}{section*.2685}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {22.15}{\ignorespaces Semi-supervised ImageNet accuracy under 1\% and 10\% label availability. Adapted from~\blx@tocontentsinit {0}\cite {grill2020_byol}.}}{1267}{table.caption.2686}\protected@file@percent }
\abx@aux@backref{1612}{grill2020_byol}{0}{1267}{1267}
\newlabel{tab:chapter22_byol_semi}{{22.15}{1267}{Semi-supervised ImageNet accuracy under 1\% and 10\% label availability. Adapted from~\cite {grill2020_byol}}{table.caption.2686}{}}
\abx@aux@backref{1613}{chen2020_simclr}{0}{1267}{1267}
\abx@aux@backref{1614}{chen2020_simclr}{0}{1267}{1267}
\@writefile{toc}{\contentsline {paragraph}{Transfer to Downstream Tasks}{1267}{section*.2687}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {22.16}{\ignorespaces Linear evaluation on diverse downstream tasks using a ResNet-50 encoder. Adapted from~\blx@tocontentsinit {0}\cite {grill2020_byol}.}}{1267}{table.caption.2688}\protected@file@percent }
\abx@aux@backref{1616}{grill2020_byol}{0}{1267}{1267}
\newlabel{tab:chapter22_byol_transfer}{{22.16}{1267}{Linear evaluation on diverse downstream tasks using a ResNet-50 encoder. Adapted from~\cite {grill2020_byol}}{table.caption.2688}{}}
\abx@aux@backref{1617}{chen2020_simclr}{0}{1267}{1267}
\@writefile{toc}{\contentsline {paragraph}{Ablation Studies and Collapse Prevention}{1267}{section*.2689}\protected@file@percent }
\newlabel{par:chapter22_byol_ablation}{{22.4.3}{1267}{Ablation Studies and Collapse Prevention}{section*.2689}{}}
\abx@aux@backref{1618}{grill2020_byol}{0}{1267}{1267}
\abx@aux@backref{1619}{chen2021_simsiam}{0}{1267}{1267}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\abx@aux@cite{0}{tian2021_understanding}
\abx@aux@segm{0}{0}{tian2021_understanding}
\abx@aux@cite{0}{tian2021_understanding}
\abx@aux@segm{0}{0}{tian2021_understanding}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\abx@aux@cite{0}{richemond2020_byol_no_batch}
\abx@aux@segm{0}{0}{richemond2020_byol_no_batch}
\abx@aux@backref{1620}{grill2020_byol}{0}{1268}{1268}
\abx@aux@backref{1621}{tian2021_understanding}{0}{1268}{1268}
\abx@aux@backref{1622}{chen2021_simsiam}{0}{1268}{1268}
\abx@aux@backref{1623}{tian2021_understanding}{0}{1268}{1268}
\abx@aux@backref{1624}{tian2021_understanding}{0}{1268}{1268}
\abx@aux@backref{1625}{zbontar2021_barlow}{0}{1268}{1268}
\abx@aux@backref{1626}{richemond2020_byol_no_batch}{0}{1268}{1268}
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{1268}{section*.2690}\protected@file@percent }
\BKM@entry{id=857,dest={73756273656374696F6E2E32322E342E34},srcline={2570}}{5C3337365C3337375C303030535C303030695C3030306D5C303030535C303030695C303030615C3030306D5C3030303A5C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030535C303030755C303030705C303030655C303030725C303030765C303030695C303030735C303030655C303030645C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030575C303030695C303030745C303030685C3030306F5C303030755C303030745C3030305C3034305C3030304E5C303030655C303030675C303030615C303030745C303030695C303030765C303030655C3030305C3034305C303030505C303030615C303030695C303030725C303030735C3030305C3034305C3030306F5C303030725C3030305C3034305C3030304D5C3030306F5C3030306D5C303030655C3030306E5C303030745C303030755C3030306D}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.4.4}SimSiam: Self-Supervised Learning Without Negative Pairs or Momentum}{1269}{subsection.22.4.4}\protected@file@percent }
\newlabel{subsec:chapter22_ssl_simsiam}{{22.4.4}{1269}{SimSiam: Self-Supervised Learning Without Negative Pairs or Momentum}{subsection.22.4.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation: Can Collapse Be Avoided Without Negatives or EMA?}{1269}{section*.2691}\protected@file@percent }
\newlabel{par:chapter22_simsiam_motivation}{{22.4.4}{1269}{Motivation: Can Collapse Be Avoided Without Negatives or EMA?}{section*.2691}{}}
\abx@aux@backref{1627}{chen2021_simsiam}{0}{1269}{1269}
\abx@aux@backref{1628}{grill2020_byol}{0}{1269}{1269}
\@writefile{toc}{\contentsline {paragraph}{Architecture and Symmetric Learning Mechanism}{1269}{section*.2692}\protected@file@percent }
\newlabel{par:chapter22_simsiam_arch}{{22.4.4}{1269}{Architecture and Symmetric Learning Mechanism}{section*.2692}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22.34}{\ignorespaces SimSiam architecture. Two augmented views of the same image are processed by a shared encoder (backbone + projection MLP). A prediction MLP is applied to only one branch, and the other is frozen via a stop-gradient. The model is trained to align predictions with projected features, without using negative pairs or momentum encoders. Adapted from~\blx@tocontentsinit {0}\cite {chen2021_simsiam}.}}{1269}{figure.caption.2693}\protected@file@percent }
\abx@aux@backref{1630}{chen2021_simsiam}{0}{1269}{1269}
\newlabel{fig:chapter22_simsiam_arch}{{22.34}{1269}{SimSiam architecture. Two augmented views of the same image are processed by a shared encoder (backbone + projection MLP). A prediction MLP is applied to only one branch, and the other is frozen via a stop-gradient. The model is trained to align predictions with projected features, without using negative pairs or momentum encoders. Adapted from~\cite {chen2021_simsiam}}{figure.caption.2693}{}}
\@writefile{toc}{\contentsline {paragraph}{SimSiam Training Pseudocode}{1270}{section*.2694}\protected@file@percent }
\newlabel{par:chapter22_simsiam_pseudocode}{{22.4.4}{1270}{SimSiam Training Pseudocode}{section*.2694}{}}
\@writefile{toc}{\contentsline {paragraph}{Gradient Formula and Learning Signal}{1271}{section*.2695}\protected@file@percent }
\newlabel{par:chapter22_simsiam_gradient}{{22.4.4}{1271}{Gradient Formula and Learning Signal}{section*.2695}{}}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\abx@aux@backref{1631}{chen2021_simsiam}{0}{1272}{1272}
\@writefile{toc}{\contentsline {paragraph}{EM-Like Interpretation of SimSiam Training}{1272}{section*.2696}\protected@file@percent }
\newlabel{par:chapter22_simsiam_em}{{22.4.4}{1272}{EM-Like Interpretation of SimSiam Training}{section*.2696}{}}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\@writefile{toc}{\contentsline {paragraph}{Conclusion: Stop-Gradient as a Structural Inductive Bias}{1273}{section*.2697}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Empirical Validation of the Stop-Gradient Mechanism}{1273}{section*.2698}\protected@file@percent }
\newlabel{par:chapter22_simsiam_stopgrad}{{22.4.4}{1273}{Empirical Validation of the Stop-Gradient Mechanism}{section*.2698}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22.35}{\ignorespaces SimSiam with vs.~without stop-gradient. Left: training loss. Middle: per-channel standard deviation of $\ell _2$-normalized output. Right: kNN accuracy as a proxy for representational quality. The absence of stop-gradient causes immediate collapse. Figure adapted from~\blx@tocontentsinit {0}\cite {chen2021_simsiam}.}}{1273}{figure.caption.2699}\protected@file@percent }
\abx@aux@backref{1633}{chen2021_simsiam}{0}{1273}{1273}
\newlabel{fig:chapter22_simsiam_stopgrad}{{22.35}{1273}{SimSiam with vs.~without stop-gradient. Left: training loss. Middle: per-channel standard deviation of $\ell _2$-normalized output. Right: kNN accuracy as a proxy for representational quality. The absence of stop-gradient causes immediate collapse. Figure adapted from~\cite {chen2021_simsiam}}{figure.caption.2699}{}}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\@writefile{toc}{\contentsline {paragraph}{Ablation Studies and Analysis}{1274}{section*.2700}\protected@file@percent }
\newlabel{par:chapter22_simsiam_ablations}{{22.4.4}{1274}{Ablation Studies and Analysis}{section*.2700}{}}
\abx@aux@backref{1634}{chen2021_simsiam}{0}{1274}{1274}
\@writefile{lot}{\contentsline {table}{\numberline {22.17}{\ignorespaces  \textbf  {Effect of modifying the predictor.} Removing the prediction MLP (a) or freezing it (b) causes complete collapse, confirming its essential role in breaking architectural symmetry and guiding learning. Surprisingly, removing learning rate decay (c) slightly improves performance, suggesting that continual plasticity in the prediction head helps match dynamically evolving targets. Adapted from~\blx@tocontentsinit {0}\cite {chen2021_simsiam}. }}{1274}{table.caption.2701}\protected@file@percent }
\abx@aux@backref{1636}{chen2021_simsiam}{0}{1274}{1274}
\newlabel{tab:chapter22_simsiam_pred_mlp}{{22.17}{1274}{\textbf {Effect of modifying the predictor.} Removing the prediction MLP (a) or freezing it (b) causes complete collapse, confirming its essential role in breaking architectural symmetry and guiding learning. Surprisingly, removing learning rate decay (c) slightly improves performance, suggesting that continual plasticity in the prediction head helps match dynamically evolving targets. Adapted from~\cite {chen2021_simsiam}}{table.caption.2701}{}}
\@writefile{lot}{\contentsline {table}{\numberline {22.18}{\ignorespaces  \textbf  {Effect of batch size.} SimSiam maintains high performance across a wide range of batch sizes, highlighting its independence from negative sampling. Performance drops slightly at extreme batch sizes due to SGD inefficiency and overly stable BatchNorm statistics. Adapted from~\blx@tocontentsinit {0}\cite {chen2021_simsiam}. }}{1274}{table.caption.2702}\protected@file@percent }
\abx@aux@backref{1638}{chen2021_simsiam}{0}{1274}{1274}
\newlabel{tab:chapter22_simsiam_batchsize}{{22.18}{1274}{\textbf {Effect of batch size.} SimSiam maintains high performance across a wide range of batch sizes, highlighting its independence from negative sampling. Performance drops slightly at extreme batch sizes due to SGD inefficiency and overly stable BatchNorm statistics. Adapted from~\cite {chen2021_simsiam}}{table.caption.2702}{}}
\@writefile{lot}{\contentsline {table}{\numberline {22.19}{\ignorespaces  \textbf  {Effect of BatchNorm placement in MLP heads.} Batch Normalization (BN) stabilizes training when applied to the hidden layers of the projection and prediction heads. However, applying BN to the final output of the prediction MLP leads to unstable training due to conflicts with the alignment objective of cosine similarity. Adapted from~\blx@tocontentsinit {0}\cite {chen2021_simsiam}. }}{1275}{table.caption.2703}\protected@file@percent }
\abx@aux@backref{1640}{chen2021_simsiam}{0}{1275}{1275}
\newlabel{tab:chapter22_simsiam_bn}{{22.19}{1275}{\textbf {Effect of BatchNorm placement in MLP heads.} Batch Normalization (BN) stabilizes training when applied to the hidden layers of the projection and prediction heads. However, applying BN to the final output of the prediction MLP leads to unstable training due to conflicts with the alignment objective of cosine similarity. Adapted from~\cite {chen2021_simsiam}}{table.caption.2703}{}}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\@writefile{toc}{\contentsline {paragraph}{Comparison to Other Self-Supervised Methods}{1276}{section*.2704}\protected@file@percent }
\newlabel{par:chapter22_simsiam_comparison}{{22.4.4}{1276}{Comparison to Other Self-Supervised Methods}{section*.2704}{}}
\@writefile{lot}{\contentsline {table}{\numberline {22.20}{\ignorespaces Linear evaluation accuracy (\%) on ImageNet for various SSL methods. SimSiam is competitive despite requiring neither negatives nor EMA. Table adapted from~\blx@tocontentsinit {0}\cite {chen2021_simsiam}.}}{1276}{table.caption.2705}\protected@file@percent }
\abx@aux@backref{1642}{chen2021_simsiam}{0}{1276}{1276}
\newlabel{tab:chapter22_simsiam_comparison}{{22.20}{1276}{Linear evaluation accuracy (\%) on ImageNet for various SSL methods. SimSiam is competitive despite requiring neither negatives nor EMA. Table adapted from~\cite {chen2021_simsiam}}{table.caption.2705}{}}
\@writefile{lot}{\contentsline {table}{\numberline {22.21}{\ignorespaces Transfer learning performance on detection and segmentation benchmarks. SimSiam performs competitively across domains. Table adapted from~\blx@tocontentsinit {0}\cite {chen2021_simsiam}.}}{1276}{table.caption.2706}\protected@file@percent }
\abx@aux@backref{1644}{chen2021_simsiam}{0}{1276}{1276}
\newlabel{tab:chapter22_simsiam_transfer}{{22.21}{1276}{Transfer learning performance on detection and segmentation benchmarks. SimSiam performs competitively across domains. Table adapted from~\cite {chen2021_simsiam}}{table.caption.2706}{}}
\@writefile{toc}{\contentsline {paragraph}{Paper Summary}{1276}{section*.2707}\protected@file@percent }
\newlabel{par:chapter22_simsiam_summary}{{22.4.4}{1276}{Paper Summary}{section*.2707}{}}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\BKM@entry{id=858,dest={73756273656374696F6E2E32322E342E35},srcline={2938}}{5C3337365C3337375C303030445C303030495C3030304E5C3030304F5C3030303A5C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030445C303030695C303030735C303030745C303030695C3030306C5C3030306C5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304E5C3030306F5C3030305C3034305C3030304C5C303030615C303030625C303030655C3030306C5C30303073}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{he2020_moco}
\abx@aux@segm{0}{0}{he2020_moco}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@cite{0}{vit2020_transformers}
\abx@aux@segm{0}{0}{vit2020_transformers}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@backref{1645}{dino2021_selfsupervised}{0}{1277}{1277}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.4.5}DINO: Self-Distillation with No Labels}{1277}{subsection.22.4.5}\protected@file@percent }
\newlabel{subsec:chapter22_ssl_dino}{{22.4.5}{1277}{DINO: Self-Distillation with No Labels}{subsection.22.4.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation: From Invariance to Semantic Understanding}{1277}{section*.2708}\protected@file@percent }
\abx@aux@backref{1646}{chen2020_simclr}{0}{1277}{1277}
\abx@aux@backref{1647}{he2020_moco}{0}{1277}{1277}
\abx@aux@backref{1648}{grill2020_byol}{0}{1277}{1277}
\abx@aux@backref{1649}{chen2021_simsiam}{0}{1277}{1277}
\abx@aux@backref{1650}{dino2021_selfsupervised}{0}{1277}{1277}
\abx@aux@backref{1651}{vit2020_transformers}{0}{1277}{1277}
\@writefile{lof}{\contentsline {figure}{\numberline {22.36}{\ignorespaces Self-attention maps from a ViT trained with DINO. The [CLS] token's attention reveals object-aware localization, despite the absence of labels. Figure adapted from~\blx@tocontentsinit {0}\cite {dino2021_selfsupervised}.}}{1277}{figure.caption.2709}\protected@file@percent }
\abx@aux@backref{1653}{dino2021_selfsupervised}{0}{1277}{1277}
\newlabel{fig:chapter22_dino_attention_maps}{{22.36}{1277}{Self-attention maps from a ViT trained with DINO. The [CLS] token's attention reveals object-aware localization, despite the absence of labels. Figure adapted from~\cite {dino2021_selfsupervised}}{figure.caption.2709}{}}
\@writefile{toc}{\contentsline {paragraph}{Self-Distillation Without Labels}{1277}{section*.2710}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Multi-Crop Strategy and View Asymmetry}{1278}{section*.2711}\protected@file@percent }
\newlabel{par:chapter22_dino_multicrop}{{22.4.5}{1278}{Multi-Crop Strategy and View Asymmetry}{section*.2711}{}}
\abx@aux@cite{0}{tsang2022byol_review}
\abx@aux@segm{0}{0}{tsang2022byol_review}
\abx@aux@cite{0}{tsang2022byol_review}
\abx@aux@segm{0}{0}{tsang2022byol_review}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\@writefile{lof}{\contentsline {figure}{\numberline {22.37}{\ignorespaces Multi-crop augmentation in DINO. The student sees both global and local views; the teacher sees only global views. This view asymmetry encourages learning local-to-global consistency. Figure adapted from~\blx@tocontentsinit {0}\cite {tsang2022byol_review}.}}{1279}{figure.caption.2712}\protected@file@percent }
\abx@aux@backref{1655}{tsang2022byol_review}{0}{1279}{1279}
\newlabel{fig:chapter22_dino_crop_views}{{22.37}{1279}{Multi-crop augmentation in DINO. The student sees both global and local views; the teacher sees only global views. This view asymmetry encourages learning local-to-global consistency. Figure adapted from~\cite {tsang2022byol_review}}{figure.caption.2712}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22.38}{\ignorespaces  \textbf  {Self-distillation without labels in DINO.} Two views \( x_1 \) and \( x_2 \) of the same image are used: \( x_2 \) is a \emph  {global} crop seen by the teacher, while \( x_1 \) may be a \emph  {local} crop seen only by the student. Both networks share the same architecture but differ in parameters. The teacher output is \emph  {centered} (mean-subtracted) and \emph  {sharpened} with a low-temperature softmax. The student output is computed at a higher temperature and trained to match the teacher via cross-entropy. Gradients flow only through the student; the teacher is updated via EMA with a cosine-scheduled momentum. Figure adapted from~\blx@tocontentsinit {0}\cite {dino2021_selfsupervised}. }}{1279}{figure.caption.2713}\protected@file@percent }
\abx@aux@backref{1657}{dino2021_selfsupervised}{0}{1279}{1279}
\newlabel{fig:chapter22_dino_self_distill}{{22.38}{1279}{\textbf {Self-distillation without labels in DINO.} Two views \( x_1 \) and \( x_2 \) of the same image are used: \( x_2 \) is a \emph {global} crop seen by the teacher, while \( x_1 \) may be a \emph {local} crop seen only by the student. Both networks share the same architecture but differ in parameters. The teacher output is \emph {centered} (mean-subtracted) and \emph {sharpened} with a low-temperature softmax. The student output is computed at a higher temperature and trained to match the teacher via cross-entropy. Gradients flow only through the student; the teacher is updated via EMA with a cosine-scheduled momentum. Figure adapted from~\cite {dino2021_selfsupervised}}{figure.caption.2713}{}}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\@writefile{toc}{\contentsline {paragraph}{Architectural Backbone: Why Vision Transformers?}{1280}{section*.2714}\protected@file@percent }
\newlabel{par:chapter22_dino_vit}{{22.4.5}{1280}{Architectural Backbone: Why Vision Transformers?}{section*.2714}{}}
\@writefile{toc}{\contentsline {paragraph}{Preventing Collapse with Centering and Sharpening}{1280}{section*.2715}\protected@file@percent }
\newlabel{par:chapter22_dino_centering_sharpening}{{22.4.5}{1280}{Preventing Collapse with Centering and Sharpening}{section*.2715}{}}
\abx@aux@backref{1658}{grill2020_byol}{0}{1280}{1280}
\abx@aux@backref{1659}{chen2021_simsiam}{0}{1280}{1280}
\@writefile{toc}{\contentsline {paragraph}{Asymmetric Distillation Objective}{1281}{section*.2716}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Use Softmax Without Labels?}{1282}{section*.2717}\protected@file@percent }
\newlabel{par:chapter22_dino_softmax_intuition}{{22.4.5}{1282}{Why Use Softmax Without Labels?}{section*.2717}{}}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{chen2021_simsiam}
\abx@aux@segm{0}{0}{chen2021_simsiam}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\@writefile{toc}{\contentsline {paragraph}{No Predictor: Functional Asymmetry Instead of Architectural Tricks}{1283}{section*.2718}\protected@file@percent }
\newlabel{par:chapter22_dino_predictorless}{{22.4.5}{1283}{No Predictor: Functional Asymmetry Instead of Architectural Tricks}{section*.2718}{}}
\abx@aux@backref{1660}{grill2020_byol}{0}{1283}{1283}
\abx@aux@backref{1661}{chen2021_simsiam}{0}{1283}{1283}
\abx@aux@backref{1662}{dino2021_selfsupervised}{0}{1283}{1283}
\abx@aux@backref{1663}{dino2021_selfsupervised}{0}{1283}{1283}
\abx@aux@backref{1664}{dino2021_selfsupervised}{0}{1283}{1283}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\@writefile{toc}{\contentsline {paragraph}{PyTorch-Style Pseudocode and Explanation}{1284}{section*.2719}\protected@file@percent }
\abx@aux@backref{1665}{dino2021_selfsupervised}{0}{1284}{1284}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{chen2020_improved}
\abx@aux@segm{0}{0}{chen2020_improved}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{caron2020_swav}
\abx@aux@segm{0}{0}{caron2020_swav}
\abx@aux@cite{0}{chen2020_improved}
\abx@aux@segm{0}{0}{chen2020_improved}
\abx@aux@cite{0}{caron2020_swav}
\abx@aux@segm{0}{0}{caron2020_swav}
\abx@aux@cite{0}{revaud2019_aploss}
\abx@aux@segm{0}{0}{revaud2019_aploss}
\abx@aux@cite{0}{berman2019_multigrain}
\abx@aux@segm{0}{0}{berman2019_multigrain}
\@writefile{toc}{\contentsline {subsubsection}{Experimental Results and Ablations for DINO}{1285}{section*.2720}\protected@file@percent }
\newlabel{subsubsec:chapter22_dino_experiments}{{22.4.5}{1285}{Experimental Results and Ablations for DINO}{section*.2720}{}}
\@writefile{toc}{\contentsline {paragraph}{Linear and k-NN Evaluation on ImageNet}{1285}{section*.2721}\protected@file@percent }
\abx@aux@backref{1666}{chen2020_simclr}{0}{1285}{1285}
\abx@aux@backref{1667}{chen2020_improved}{0}{1285}{1285}
\abx@aux@backref{1668}{zbontar2021_barlow}{0}{1285}{1285}
\abx@aux@backref{1669}{grill2020_byol}{0}{1285}{1285}
\abx@aux@backref{1670}{caron2020_swav}{0}{1285}{1285}
\abx@aux@backref{1671}{chen2020_improved}{0}{1285}{1285}
\abx@aux@backref{1672}{caron2020_swav}{0}{1285}{1285}
\@writefile{lot}{\contentsline {table}{\numberline {22.22}{\ignorespaces Top-1 accuracy on ImageNet for linear and k-NN evaluations using different self-supervised methods and architectures. DINO achieves state-of-the-art results, especially with small-patch ViTs.}}{1285}{table.caption.2722}\protected@file@percent }
\newlabel{tab:chapter22_dino_imagenet_linear_knn}{{22.22}{1285}{Top-1 accuracy on ImageNet for linear and k-NN evaluations using different self-supervised methods and architectures. DINO achieves state-of-the-art results, especially with small-patch ViTs}{table.caption.2722}{}}
\@writefile{toc}{\contentsline {paragraph}{Transfer to Retrieval and Segmentation Tasks}{1285}{section*.2723}\protected@file@percent }
\abx@aux@backref{1673}{revaud2019_aploss}{0}{1285}{1285}
\abx@aux@backref{1674}{berman2019_multigrain}{0}{1285}{1285}
\abx@aux@cite{0}{jabri2020_stc}
\abx@aux@segm{0}{0}{jabri2020_stc}
\abx@aux@cite{0}{lai2020_mast}
\abx@aux@segm{0}{0}{lai2020_mast}
\abx@aux@cite{0}{oh2019_stm}
\abx@aux@segm{0}{0}{oh2019_stm}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@backref{1675}{jabri2020_stc}{0}{1286}{1286}
\abx@aux@backref{1676}{lai2020_mast}{0}{1286}{1286}
\abx@aux@backref{1677}{oh2019_stm}{0}{1286}{1286}
\@writefile{toc}{\contentsline {paragraph}{Ablation: Emergent Object Segmentation via Self-Attention}{1286}{section*.2724}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.39}{\ignorespaces \textbf  {Self-attention from the final ViT layer using \texttt  {[CLS]} token queries.} DINO (left) produces object-aligned attention maps that are sharp and coherent across different heads. The supervised ViT (right), while still focusing on relevant regions, exhibits more fragmented and class-discriminative attention. Adapted from~\blx@tocontentsinit {0}\cite {dino2021_selfsupervised}.}}{1286}{figure.caption.2725}\protected@file@percent }
\abx@aux@backref{1679}{dino2021_selfsupervised}{0}{1286}{1286}
\newlabel{fig:chapter22_dino_attention_maps}{{22.39}{1286}{\textbf {Self-attention from the final ViT layer using \texttt {[CLS]} token queries.} DINO (left) produces object-aligned attention maps that are sharp and coherent across different heads. The supervised ViT (right), while still focusing on relevant regions, exhibits more fragmented and class-discriminative attention. Adapted from~\cite {dino2021_selfsupervised}}{figure.caption.2725}{}}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\@writefile{toc}{\contentsline {paragraph}{Ablation: Semantic Structure from Unlabeled Data}{1287}{section*.2726}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.40}{\ignorespaces \textbf  {t-SNE projection of class-wise averaged \texttt  {[CLS]} features from DINO.} Car-related categories (e.g., \texttt  {minivan}, \texttt  {sports car}) form a compact super-cluster in the learned representation space, despite no access to labels during training. Adapted from~\blx@tocontentsinit {0}\cite {dino2021_selfsupervised}.}}{1287}{figure.caption.2727}\protected@file@percent }
\abx@aux@backref{1681}{dino2021_selfsupervised}{0}{1287}{1287}
\newlabel{fig:chapter22_dino_tsne_embeddings}{{22.40}{1287}{\textbf {t-SNE projection of class-wise averaged \texttt {[CLS]} features from DINO.} Car-related categories (e.g., \texttt {minivan}, \texttt {sports car}) form a compact super-cluster in the learned representation space, despite no access to labels during training. Adapted from~\cite {dino2021_selfsupervised}}{figure.caption.2727}{}}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\@writefile{toc}{\contentsline {paragraph}{Ablation: Teacher Update Strategies}{1288}{section*.2728}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.41}{\ignorespaces Top-1 k-NN accuracy on ImageNet during training. \textbf  {Left:} EMA teacher consistently outperforms the student. \textbf  {Right:} Momentum updates outperform all other teacher variants. Reproduced from \blx@tocontentsinit {0}\cite {dino2021_selfsupervised}.}}{1288}{figure.caption.2729}\protected@file@percent }
\abx@aux@backref{1683}{dino2021_selfsupervised}{0}{1288}{1288}
\newlabel{fig:chapter22_dino_teacher_update}{{22.41}{1288}{Top-1 k-NN accuracy on ImageNet during training. \textbf {Left:} EMA teacher consistently outperforms the student. \textbf {Right:} Momentum updates outperform all other teacher variants. Reproduced from \cite {dino2021_selfsupervised}}{figure.caption.2729}{}}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\@writefile{toc}{\contentsline {paragraph}{Ablation: Collapse Prevention via Centering and Sharpening}{1289}{section*.2730}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.42}{\ignorespaces Collapse analysis. \textbf  {Left:} Teacher entropy remains high without sharpening. \textbf  {Right:} KL divergence between teacher and student only increases when both centering and sharpening are used. Reproduced from \blx@tocontentsinit {0}\cite {dino2021_selfsupervised}.}}{1289}{figure.caption.2731}\protected@file@percent }
\abx@aux@backref{1685}{dino2021_selfsupervised}{0}{1289}{1289}
\newlabel{fig:chapter22_dino_collapse}{{22.42}{1289}{Collapse analysis. \textbf {Left:} Teacher entropy remains high without sharpening. \textbf {Right:} KL divergence between teacher and student only increases when both centering and sharpening are used. Reproduced from \cite {dino2021_selfsupervised}}{figure.caption.2731}{}}
\@writefile{toc}{\contentsline {paragraph}{Ablation: Patch Size and Inference Throughput}{1289}{section*.2732}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.43}{\ignorespaces Effect of patch size on accuracy and throughput. Smaller patches improve accuracy, but slow down inference. Reproduced from \blx@tocontentsinit {0}\cite {dino2021_selfsupervised}.}}{1289}{figure.caption.2733}\protected@file@percent }
\abx@aux@backref{1687}{dino2021_selfsupervised}{0}{1289}{1289}
\newlabel{fig:chapter22_dino_patch_size}{{22.43}{1289}{Effect of patch size on accuracy and throughput. Smaller patches improve accuracy, but slow down inference. Reproduced from \cite {dino2021_selfsupervised}}{figure.caption.2733}{}}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\@writefile{toc}{\contentsline {paragraph}{Ablation: Batch Size Effects}{1290}{section*.2734}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {22.23}{\ignorespaces Effect of batch size on top-1 k-NN accuracy for DINO models trained for 100 epochs without multi-crop. Reproduced from \blx@tocontentsinit {0}\cite {dino2021_selfsupervised}.}}{1290}{table.caption.2735}\protected@file@percent }
\abx@aux@backref{1689}{dino2021_selfsupervised}{0}{1290}{1290}
\newlabel{tab:chapter22_dino_batchsize}{{22.23}{1290}{Effect of batch size on top-1 k-NN accuracy for DINO models trained for 100 epochs without multi-crop. Reproduced from \cite {dino2021_selfsupervised}}{table.caption.2735}{}}
\@writefile{toc}{\contentsline {paragraph}{Ablation: Multi-Crop Augmentation and Resource Tradeoffs}{1290}{section*.2736}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {22.24}{\ignorespaces Effect of multi-crop augmentation on linear top-1 accuracy, training time, and peak memory usage for ViT-S/16 models. Data reproduced from \blx@tocontentsinit {0}\cite {dino2021_selfsupervised}.}}{1290}{table.caption.2737}\protected@file@percent }
\abx@aux@backref{1691}{dino2021_selfsupervised}{0}{1290}{1290}
\newlabel{tab:chapter22_dino_multicrop}{{22.24}{1290}{Effect of multi-crop augmentation on linear top-1 accuracy, training time, and peak memory usage for ViT-S/16 models. Data reproduced from \cite {dino2021_selfsupervised}}{table.caption.2737}{}}
\@writefile{toc}{\contentsline {paragraph}{Paper Conclusion}{1290}{section*.2738}\protected@file@percent }
\BKM@entry{id=859,dest={73756273656374696F6E2E32322E342E36},srcline={3406}}{5C3337365C3337375C303030445C303030495C3030304E5C3030304F5C303030765C303030325C3030303A5C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030525C3030306F5C303030625C303030755C303030735C303030745C3030305C3034305C303030565C303030695C303030735C303030755C303030615C3030306C5C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C303030735C3030305C3034305C303030575C303030695C303030745C303030685C3030306F5C303030755C303030745C3030305C3034305C303030535C303030755C303030705C303030655C303030725C303030765C303030695C303030735C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.4.6}DINOv2: Learning Robust Visual Features Without Supervision}{1291}{subsection.22.4.6}\protected@file@percent }
\newlabel{subsec:chapter22_dinov2_intro}{{22.4.6}{1291}{DINOv2: Learning Robust Visual Features Without Supervision}{subsection.22.4.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Background and Motivation}{1291}{section*.2739}\protected@file@percent }
\abx@aux@backref{1692}{oquab2023_dinov2}{0}{1291}{1291}
\abx@aux@backref{1693}{dino2021_selfsupervised}{0}{1291}{1291}
\@writefile{lof}{\contentsline {figure}{\numberline {22.44}{\ignorespaces First three PCA components of ViT patch embeddings visualized as RGB heatmaps. Each column juxtaposes conceptually related images: birds and airplanes (a), elephants and statues (b), horses in photo and sketch (c), and cars in photo and sketch (d). Matched parts across object categories or visual styles receive similar embeddings. Adapted from \blx@tocontentsinit {0}\cite {oquab2023_dinov2}.}}{1291}{figure.caption.2740}\protected@file@percent }
\abx@aux@backref{1695}{oquab2023_dinov2}{0}{1291}{1291}
\newlabel{fig:chapter22_dinov2_pca}{{22.44}{1291}{First three PCA components of ViT patch embeddings visualized as RGB heatmaps. Each column juxtaposes conceptually related images: birds and airplanes (a), elephants and statues (b), horses in photo and sketch (c), and cars in photo and sketch (d). Matched parts across object categories or visual styles receive similar embeddings. Adapted from \cite {oquab2023_dinov2}}{figure.caption.2740}{}}
\@writefile{toc}{\contentsline {paragraph}{Emergent Semantic Structure Without Labels}{1291}{section*.2741}\protected@file@percent }
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\@writefile{toc}{\contentsline {paragraph}{Scaling Training through Architectural and Data Efficiency}{1292}{section*.2742}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Data Processing in DINOv2}{1292}{section*.2743}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.45}{\ignorespaces Overview of the DINOv2 data processing pipeline. Images from curated and uncurated sources are embedded with a frozen ViT-H/14 model. Near-duplicate images are removed, and uncurated data are aligned to curated anchors via embedding similarity to form the LVD-142M dataset. Adapted from \blx@tocontentsinit {0}\cite {oquab2023_dinov2}.}}{1292}{figure.caption.2744}\protected@file@percent }
\abx@aux@backref{1697}{oquab2023_dinov2}{0}{1292}{1292}
\newlabel{fig:chapter22_dinov2_preprocessing}{{22.45}{1292}{Overview of the DINOv2 data processing pipeline. Images from curated and uncurated sources are embedded with a frozen ViT-H/14 model. Near-duplicate images are removed, and uncurated data are aligned to curated anchors via embedding similarity to form the LVD-142M dataset. Adapted from \cite {oquab2023_dinov2}}{figure.caption.2744}{}}
\abx@aux@cite{0}{pizzi2022_sscd}
\abx@aux@segm{0}{0}{pizzi2022_sscd}
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\abx@aux@cite{0}{pizzi2022_sscd}
\abx@aux@segm{0}{0}{pizzi2022_sscd}
\abx@aux@backref{1698}{pizzi2022_sscd}{0}{1293}{1293}
\@writefile{toc}{\contentsline {subsubsection}{SSCD: A Self-Supervised Descriptor for Image Copy Detection}{1293}{section*.2745}\protected@file@percent }
\newlabel{subsec:chapter22_sscd}{{22.4.6}{1293}{SSCD: A Self-Supervised Descriptor for Image Copy Detection}{section*.2745}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation for Copy Detection in DINOv2}{1293}{section*.2746}\protected@file@percent }
\newlabel{par:chapter22_sscd_motivation}{{22.4.6}{1293}{Motivation for Copy Detection in DINOv2}{section*.2746}{}}
\abx@aux@backref{1699}{oquab2023_dinov2}{0}{1293}{1293}
\abx@aux@backref{1700}{pizzi2022_sscd}{0}{1293}{1293}
\abx@aux@cite{0}{pizzi2022_sscd}
\abx@aux@segm{0}{0}{pizzi2022_sscd}
\abx@aux@cite{0}{pizzi2022_sscd}
\abx@aux@segm{0}{0}{pizzi2022_sscd}
\@writefile{lof}{\contentsline {figure}{\numberline {22.46}{\ignorespaces SSCD architecture overview. Based on SimCLR, it introduces entropy regularization, mixed-image-aware contrastive loss, and inference-time score normalization. Adapted from \blx@tocontentsinit {0}\cite {pizzi2022_sscd}.}}{1295}{figure.caption.2747}\protected@file@percent }
\abx@aux@backref{1702}{pizzi2022_sscd}{0}{1295}{1295}
\newlabel{fig:chapter22_sscd_arch}{{22.46}{1295}{SSCD architecture overview. Based on SimCLR, it introduces entropy regularization, mixed-image-aware contrastive loss, and inference-time score normalization. Adapted from \cite {pizzi2022_sscd}}{figure.caption.2747}{}}
\@writefile{toc}{\contentsline {paragraph}{Core Architecture and Augmentations}{1296}{section*.2748}\protected@file@percent }
\newlabel{par:chapter22_sscd_core}{{22.4.6}{1296}{Core Architecture and Augmentations}{section*.2748}{}}
\@writefile{toc}{\contentsline {paragraph}{Post-Processing via Whitening and Synergy with Training}{1296}{section*.2749}\protected@file@percent }
\newlabel{par:chapter22_sscd_whitening}{{22.4.6}{1296}{Post-Processing via Whitening and Synergy with Training}{section*.2749}{}}
\@writefile{toc}{\contentsline {paragraph}{Augmentation Pipeline for Real-World Tampering}{1297}{section*.2750}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Loss Formulation with Entropy and Mixed Positives}{1298}{section*.2751}\protected@file@percent }
\newlabel{par:chapter22_sscd_loss}{{22.4.6}{1298}{Loss Formulation with Entropy and Mixed Positives}{section*.2751}{}}
\abx@aux@cite{0}{pizzi2022_sscd}
\abx@aux@segm{0}{0}{pizzi2022_sscd}
\abx@aux@cite{0}{pizzi2022_sscd}
\abx@aux@segm{0}{0}{pizzi2022_sscd}
\@writefile{lof}{\contentsline {figure}{\numberline {22.47}{\ignorespaces \textbf  {Ablation: Entropy Regularization Improves Thresholdability.} Histogram of squared $\ell _2$ distances between descriptor pairs on DISC2021. The x-axis shows pairwise squared distance; the y-axis indicates frequency. \textcolor {RoyalBlue}{Blue} curves represent \emph  {positive pairs} (augmented or composite views of the same image); \textcolor {Red}{Red} curves show \emph  {hardest negatives} (non-matching nearest neighbors). \textbf  {Top:} Without entropy regularization (SimCLR), positives and negatives heavily overlap, making global thresholding unreliable. \textbf  {Bottom:} With KoLeo entropy regularization, many \textcolor {RoyalBlue}{positives} concentrate below 0.6, while \textcolor {Red}{negatives} are pushed outward, peaking near 1.1. Despite a long-tailed positive distribution ($\sim $35\% beyond 1.15), the scarcity of negatives below 0.6 enables a global threshold (e.g., $\tau = 0.6$) to recover most matches—achieving high \emph  {recall}. However, due to midrange overlap, \emph  {precision remains moderate}. This balance favors high-recall use cases such as large-scale pre-filtering, where false positives are tolerable. The separation shown here directly improves micro Average Precision (µAP), which reflects how well a fixed threshold can distinguish matches from non-matches across the entire dataset. Adapted from~\blx@tocontentsinit {0}\cite {pizzi2022_sscd}.}}{1299}{figure.caption.2752}\protected@file@percent }
\abx@aux@backref{1704}{pizzi2022_sscd}{0}{1299}{1299}
\newlabel{fig:chapter20_sscd_entropy_ablation}{{22.47}{1299}{\textbf {Ablation: Entropy Regularization Improves Thresholdability.} Histogram of squared $\ell _2$ distances between descriptor pairs on DISC2021. The x-axis shows pairwise squared distance; the y-axis indicates frequency. \textcolor {RoyalBlue}{Blue} curves represent \emph {positive pairs} (augmented or composite views of the same image); \textcolor {Red}{Red} curves show \emph {hardest negatives} (non-matching nearest neighbors). \textbf {Top:} Without entropy regularization (SimCLR), positives and negatives heavily overlap, making global thresholding unreliable. \textbf {Bottom:} With KoLeo entropy regularization, many \textcolor {RoyalBlue}{positives} concentrate below 0.6, while \textcolor {Red}{negatives} are pushed outward, peaking near 1.1. Despite a long-tailed positive distribution ($\sim $35\% beyond 1.15), the scarcity of negatives below 0.6 enables a global threshold (e.g., $\tau = 0.6$) to recover most matches—achieving high \emph {recall}. However, due to midrange overlap, \emph {precision remains moderate}. This balance favors high-recall use cases such as large-scale pre-filtering, where false positives are tolerable. The separation shown here directly improves micro Average Precision (µAP), which reflects how well a fixed threshold can distinguish matches from non-matches across the entire dataset. Adapted from~\cite {pizzi2022_sscd}}{figure.caption.2752}{}}
\abx@aux@cite{0}{he2022_mae}
\abx@aux@segm{0}{0}{he2022_mae}
\abx@aux@cite{0}{zhou2022_ibot}
\abx@aux@segm{0}{0}{zhou2022_ibot}
\@writefile{toc}{\contentsline {paragraph}{Empirical Results and Impact}{1300}{section*.2753}\protected@file@percent }
\abx@aux@backref{1705}{he2022_mae}{0}{1300}{1300}
\abx@aux@backref{1706}{zhou2022_ibot}{0}{1300}{1300}
\abx@aux@cite{0}{he2022_mae}
\abx@aux@segm{0}{0}{he2022_mae}
\abx@aux@cite{0}{he2022_mae}
\abx@aux@segm{0}{0}{he2022_mae}
\abx@aux@cite{0}{he2022_mae}
\abx@aux@segm{0}{0}{he2022_mae}
\@writefile{toc}{\contentsline {subsubsection}{Masked Autoencoders (MAE): Scalable Vision Learners}{1301}{section*.2754}\protected@file@percent }
\newlabel{subsubsec:chapter22_mae}{{22.4.6}{1301}{Masked Autoencoders (MAE): Scalable Vision Learners}{section*.2754}{}}
\abx@aux@backref{1707}{he2022_mae}{0}{1301}{1301}
\@writefile{lof}{\contentsline {figure}{\numberline {22.48}{\ignorespaces MAE architecture and objective. A large fraction of image patches (e.g., 75\%) is masked, and the encoder processes only the visible subset. Mask tokens are inserted post-encoder and processed by a lightweight decoder to reconstruct the full image. The decoder is discarded after pre-training (Adapted from \blx@tocontentsinit {0}\cite {he2022_mae}).}}{1301}{figure.caption.2755}\protected@file@percent }
\abx@aux@backref{1709}{he2022_mae}{0}{1301}{1301}
\newlabel{fig:chapter22_mae_objective}{{22.48}{1301}{MAE architecture and objective. A large fraction of image patches (e.g., 75\%) is masked, and the encoder processes only the visible subset. Mask tokens are inserted post-encoder and processed by a lightweight decoder to reconstruct the full image. The decoder is discarded after pre-training (Adapted from \cite {he2022_mae})}{figure.caption.2755}{}}
\@writefile{toc}{\contentsline {paragraph}{Asymmetric Architecture and High-Ratio Masking}{1302}{section*.2756}\protected@file@percent }
\newlabel{par:chapter22_mae_architecture}{{22.4.6}{1302}{Asymmetric Architecture and High-Ratio Masking}{section*.2756}{}}
\abx@aux@cite{0}{he2022_mae}
\abx@aux@segm{0}{0}{he2022_mae}
\abx@aux@cite{0}{he2022_mae}
\abx@aux@segm{0}{0}{he2022_mae}
\@writefile{lof}{\contentsline {figure}{\numberline {22.49}{\ignorespaces MAE achieves optimal performance with a high masking ratio (75\%), benefiting both fine-tuning and linear probing. A low masking ratio (e.g., 25–50\%) results in diminished linear separability, as the task becomes too easy (Adapted from \blx@tocontentsinit {0}\cite {he2022_mae}).}}{1303}{figure.caption.2757}\protected@file@percent }
\abx@aux@backref{1711}{he2022_mae}{0}{1303}{1303}
\newlabel{fig:chapter22_mae_masking_ratio}{{22.49}{1303}{MAE achieves optimal performance with a high masking ratio (75\%), benefiting both fine-tuning and linear probing. A low masking ratio (e.g., 25–50\%) results in diminished linear separability, as the task becomes too easy (Adapted from \cite {he2022_mae})}{figure.caption.2757}{}}
\abx@aux@cite{0}{he2022_mae}
\abx@aux@segm{0}{0}{he2022_mae}
\abx@aux@cite{0}{he2022_mae}
\abx@aux@segm{0}{0}{he2022_mae}
\abx@aux@cite{0}{he2022_mae}
\abx@aux@segm{0}{0}{he2022_mae}
\abx@aux@cite{0}{he2022_mae}
\abx@aux@segm{0}{0}{he2022_mae}
\@writefile{toc}{\contentsline {paragraph}{Empirical Results and Qualitative Analysis}{1304}{section*.2758}\protected@file@percent }
\newlabel{par:chapter22_mae_results}{{22.4.6}{1304}{Empirical Results and Qualitative Analysis}{section*.2758}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22.50}{\ignorespaces Example reconstructions on ImageNet validation images with 80\% masking. For each triplet: masked input (left), MAE reconstruction (center), and ground truth (right). While visible patches are not reconstructed, the model infers plausible global structure (Adapted from \blx@tocontentsinit {0}\cite {he2022_mae}).}}{1304}{figure.caption.2759}\protected@file@percent }
\abx@aux@backref{1713}{he2022_mae}{0}{1304}{1304}
\newlabel{fig:chapter22_mae_imagenet_recons}{{22.50}{1304}{Example reconstructions on ImageNet validation images with 80\% masking. For each triplet: masked input (left), MAE reconstruction (center), and ground truth (right). While visible patches are not reconstructed, the model infers plausible global structure (Adapted from \cite {he2022_mae})}{figure.caption.2759}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22.51}{\ignorespaces MAE generalizes well to out-of-distribution samples. When applied to COCO images, a model pretrained only on ImageNet produces semantically coherent reconstructions, even when the outputs differ from the ground truth. (Adapted from \blx@tocontentsinit {0}\cite {he2022_mae})}}{1304}{figure.caption.2760}\protected@file@percent }
\abx@aux@backref{1715}{he2022_mae}{0}{1304}{1304}
\newlabel{fig:chapter22_mae_coco_recons}{{22.51}{1304}{MAE generalizes well to out-of-distribution samples. When applied to COCO images, a model pretrained only on ImageNet produces semantically coherent reconstructions, even when the outputs differ from the ground truth. (Adapted from \cite {he2022_mae})}{figure.caption.2760}{}}
\abx@aux@cite{0}{he2022_mae}
\abx@aux@segm{0}{0}{he2022_mae}
\abx@aux@cite{0}{he2022_mae}
\abx@aux@segm{0}{0}{he2022_mae}
\abx@aux@cite{0}{he2022_mae}
\abx@aux@segm{0}{0}{he2022_mae}
\abx@aux@cite{0}{he2022_mae}
\abx@aux@segm{0}{0}{he2022_mae}
\@writefile{lof}{\contentsline {figure}{\numberline {22.52}{\ignorespaces Reconstructions under increasingly aggressive masking. MAE remains robust up to 95\% masking, producing plausible if blurry results, suggesting strong generalization from sparse inputs (Adapted from \blx@tocontentsinit {0}\cite {he2022_mae}).}}{1305}{figure.caption.2761}\protected@file@percent }
\abx@aux@backref{1717}{he2022_mae}{0}{1305}{1305}
\newlabel{fig:chapter22_mae_high_masking}{{22.52}{1305}{Reconstructions under increasingly aggressive masking. MAE remains robust up to 95\% masking, producing plausible if blurry results, suggesting strong generalization from sparse inputs (Adapted from \cite {he2022_mae})}{figure.caption.2761}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22.53}{\ignorespaces Comparison of masking strategies. Random masking (left) produces harder prediction tasks and better representations than block-wise (center) or grid-wise (right) masking, which leak low-level structure (Adapted from \blx@tocontentsinit {0}\cite {he2022_mae}).}}{1305}{figure.caption.2762}\protected@file@percent }
\abx@aux@backref{1719}{he2022_mae}{0}{1305}{1305}
\newlabel{fig:chapter22_mae_masking_strategies}{{22.53}{1305}{Comparison of masking strategies. Random masking (left) produces harder prediction tasks and better representations than block-wise (center) or grid-wise (right) masking, which leak low-level structure (Adapted from \cite {he2022_mae})}{figure.caption.2762}{}}
\abx@aux@cite{0}{he2022_mae}
\abx@aux@segm{0}{0}{he2022_mae}
\abx@aux@cite{0}{zhou2022_ibot}
\abx@aux@segm{0}{0}{zhou2022_ibot}
\abx@aux@backref{1720}{he2022_mae}{0}{1306}{1306}
\@writefile{lot}{\contentsline {table}{\numberline {22.25}{\ignorespaces Key findings from MAE ablation studies. Accuracy differences are reported on ImageNet-1K for ViT-L pretrained for 800 epochs. Optimal settings are bolded.}}{1306}{table.caption.2763}\protected@file@percent }
\newlabel{tab:chapter22_mae_ablation_summary}{{22.25}{1306}{Key findings from MAE ablation studies. Accuracy differences are reported on ImageNet-1K for ViT-L pretrained for 800 epochs. Optimal settings are bolded}{table.caption.2763}{}}
\abx@aux@cite{0}{zhou2022_ibot}
\abx@aux@segm{0}{0}{zhou2022_ibot}
\abx@aux@cite{0}{zhou2022_ibot}
\abx@aux@segm{0}{0}{zhou2022_ibot}
\abx@aux@cite{0}{zhou2022_ibot}
\abx@aux@segm{0}{0}{zhou2022_ibot}
\abx@aux@backref{1721}{zhou2022_ibot}{0}{1307}{1307}
\@writefile{toc}{\contentsline {subsubsection}{iBOT: Masked Image Modeling with Self-Distillation}{1307}{section*.2764}\protected@file@percent }
\newlabel{subsec:chapter22_ibot}{{22.4.6}{1307}{iBOT: Masked Image Modeling with Self-Distillation}{section*.2764}{}}
\abx@aux@backref{1722}{zhou2022_ibot}{0}{1307}{1307}
\@writefile{lof}{\contentsline {figure}{\numberline {22.54}{\ignorespaces iBOT trains a student network to match the soft outputs of a momentum-updated teacher on both class and patch tokens. Only the student receives masked inputs; the teacher operates on full images. Predictions are made in feature space via projection heads and cross-entropy loss (Adapted from \blx@tocontentsinit {0}\cite {zhou2022_ibot}).}}{1307}{figure.caption.2765}\protected@file@percent }
\abx@aux@backref{1724}{zhou2022_ibot}{0}{1307}{1307}
\newlabel{fig:chapter22_ibot_architecture}{{22.54}{1307}{iBOT trains a student network to match the soft outputs of a momentum-updated teacher on both class and patch tokens. Only the student receives masked inputs; the teacher operates on full images. Predictions are made in feature space via projection heads and cross-entropy loss (Adapted from \cite {zhou2022_ibot})}{figure.caption.2765}{}}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\@writefile{toc}{\contentsline {paragraph}{iBOT Loss Function and Self-Distillation Objective}{1309}{section*.2766}\protected@file@percent }
\newlabel{par:chapter22_ibot_objective}{{22.4.6}{1309}{iBOT Loss Function and Self-Distillation Objective}{section*.2766}{}}
\abx@aux@backref{1725}{dino2021_selfsupervised}{0}{1309}{1309}
\abx@aux@cite{0}{dino2021_selfsupervised}
\abx@aux@segm{0}{0}{dino2021_selfsupervised}
\abx@aux@backref{1726}{dino2021_selfsupervised}{0}{1310}{1310}
\abx@aux@cite{0}{zhou2022_ibot}
\abx@aux@segm{0}{0}{zhou2022_ibot}
\abx@aux@cite{0}{zhou2022_ibot}
\abx@aux@segm{0}{0}{zhou2022_ibot}
\@writefile{toc}{\contentsline {paragraph}{iBOT Training Procedure}{1312}{section*.2767}\protected@file@percent }
\newlabel{par:chapter22_ibot_training}{{22.4.6}{1312}{iBOT Training Procedure}{section*.2767}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22.55}{\ignorespaces iBOT training procedure in PyTorch-style pseudocode form, illustrating the computation of both the masked image modeling loss and the [CLS] token self-distillation loss. This schematic captures the core logic of token-level and global-level supervision using a momentum-updated teacher and centering/sharpening techniques. Figure adapted from~\blx@tocontentsinit {0}\cite {zhou2022_ibot}.}}{1312}{figure.caption.2768}\protected@file@percent }
\abx@aux@backref{1728}{zhou2022_ibot}{0}{1312}{1312}
\newlabel{fig:chapter22_ibot_pseudocode}{{22.55}{1312}{iBOT training procedure in PyTorch-style pseudocode form, illustrating the computation of both the masked image modeling loss and the [CLS] token self-distillation loss. This schematic captures the core logic of token-level and global-level supervision using a momentum-updated teacher and centering/sharpening techniques. Figure adapted from~\cite {zhou2022_ibot}}{figure.caption.2768}{}}
\abx@aux@cite{0}{zhou2022_ibot}
\abx@aux@segm{0}{0}{zhou2022_ibot}
\abx@aux@cite{0}{zhou2022_ibot}
\abx@aux@segm{0}{0}{zhou2022_ibot}
\abx@aux@cite{0}{zhou2022_ibot}
\abx@aux@segm{0}{0}{zhou2022_ibot}
\abx@aux@cite{0}{zhou2022_ibot}
\abx@aux@segm{0}{0}{zhou2022_ibot}
\@writefile{toc}{\contentsline {paragraph}{Empirical Results and Evaluation}{1313}{section*.2769}\protected@file@percent }
\newlabel{par:chapter22_ibot_results}{{22.4.6}{1313}{Empirical Results and Evaluation}{section*.2769}{}}
\@writefile{lot}{\contentsline {table}{\numberline {22.26}{\ignorespaces ImageNet-1K classification results. iBOT achieves state-of-the-art linear probing accuracy while maintaining strong fine-tuned performance. (Adapted from \blx@tocontentsinit {0}\cite {zhou2022_ibot})}}{1313}{table.caption.2770}\protected@file@percent }
\abx@aux@backref{1730}{zhou2022_ibot}{0}{1313}{1313}
\newlabel{tab:chapter22_ibot_imagenet_classification}{{22.26}{1313}{ImageNet-1K classification results. iBOT achieves state-of-the-art linear probing accuracy while maintaining strong fine-tuned performance. (Adapted from \cite {zhou2022_ibot})}{table.caption.2770}{}}
\@writefile{lot}{\contentsline {table}{\numberline {22.27}{\ignorespaces Semi-supervised learning on ImageNet-1K with 1\% and 10\% labels. iBOT exhibits high label efficiency in low-shot transfer. (Adapted from \blx@tocontentsinit {0}\cite {zhou2022_ibot})}}{1313}{table.caption.2771}\protected@file@percent }
\abx@aux@backref{1732}{zhou2022_ibot}{0}{1313}{1313}
\newlabel{tab:chapter22_ibot_semi_supervised}{{22.27}{1313}{Semi-supervised learning on ImageNet-1K with 1\% and 10\% labels. iBOT exhibits high label efficiency in low-shot transfer. (Adapted from \cite {zhou2022_ibot})}{table.caption.2771}{}}
\abx@aux@cite{0}{zhou2022_ibot}
\abx@aux@segm{0}{0}{zhou2022_ibot}
\abx@aux@cite{0}{zhou2022_ibot}
\abx@aux@segm{0}{0}{zhou2022_ibot}
\abx@aux@cite{0}{zhou2022_ibot}
\abx@aux@segm{0}{0}{zhou2022_ibot}
\abx@aux@cite{0}{zhou2022_ibot}
\abx@aux@segm{0}{0}{zhou2022_ibot}
\@writefile{lot}{\contentsline {table}{\numberline {22.28}{\ignorespaces Unsupervised clustering on ImageNet-1K. iBOT improves upon DINO across all clustering metrics. (Adapted from \blx@tocontentsinit {0}\cite {zhou2022_ibot})}}{1314}{table.caption.2772}\protected@file@percent }
\abx@aux@backref{1734}{zhou2022_ibot}{0}{1314}{1314}
\newlabel{tab:chapter22_ibot_unsupervised_clustering}{{22.28}{1314}{Unsupervised clustering on ImageNet-1K. iBOT improves upon DINO across all clustering metrics. (Adapted from \cite {zhou2022_ibot})}{table.caption.2772}{}}
\@writefile{lot}{\contentsline {table}{\numberline {22.29}{\ignorespaces Transfer results on COCO (object detection and instance segmentation) and ADE20K (semantic segmentation). iBOT outperforms both BEiT and DINO across all evaluated tasks. (Adapted from~\blx@tocontentsinit {0}\cite {zhou2022_ibot})}}{1314}{table.caption.2773}\protected@file@percent }
\abx@aux@backref{1736}{zhou2022_ibot}{0}{1314}{1314}
\newlabel{tab:chapter22_ibot_dense_tasks}{{22.29}{1314}{Transfer results on COCO (object detection and instance segmentation) and ADE20K (semantic segmentation). iBOT outperforms both BEiT and DINO across all evaluated tasks. (Adapted from~\cite {zhou2022_ibot})}{table.caption.2773}{}}
\abx@aux@cite{0}{zhou2022_ibot}
\abx@aux@segm{0}{0}{zhou2022_ibot}
\abx@aux@cite{0}{zhou2022_ibot}
\abx@aux@segm{0}{0}{zhou2022_ibot}
\abx@aux@cite{0}{zhou2022_ibot}
\abx@aux@segm{0}{0}{zhou2022_ibot}
\abx@aux@cite{0}{zhou2022_ibot}
\abx@aux@segm{0}{0}{zhou2022_ibot}
\abx@aux@cite{0}{zhou2022_ibot}
\abx@aux@segm{0}{0}{zhou2022_ibot}
\@writefile{toc}{\contentsline {paragraph}{Ablation Studies and Component Analysis}{1315}{section*.2774}\protected@file@percent }
\newlabel{par:chapter22_ibot_ablations}{{22.4.6}{1315}{Ablation Studies and Component Analysis}{section*.2774}{}}
\@writefile{lot}{\contentsline {table}{\numberline {22.30}{\ignorespaces Ablation on patch-level loss. Removing the masked patch loss reduces linear accuracy, showing that local token supervision is critical for representation quality. (Adapted from \blx@tocontentsinit {0}\cite {zhou2022_ibot})}}{1315}{table.caption.2775}\protected@file@percent }
\abx@aux@backref{1738}{zhou2022_ibot}{0}{1315}{1315}
\newlabel{tab:chapter22_ibot_patch_loss}{{22.30}{1315}{Ablation on patch-level loss. Removing the masked patch loss reduces linear accuracy, showing that local token supervision is critical for representation quality. (Adapted from \cite {zhou2022_ibot})}{table.caption.2775}{}}
\@writefile{lot}{\contentsline {table}{\numberline {22.31}{\ignorespaces Ablation on projection heads. Using a shared head across tokens performs on par with a dual-head setup while maintaining simplicity. (Adapted from \blx@tocontentsinit {0}\cite {zhou2022_ibot})}}{1315}{table.caption.2776}\protected@file@percent }
\abx@aux@backref{1740}{zhou2022_ibot}{0}{1315}{1315}
\newlabel{tab:chapter22_ibot_head_ablation}{{22.31}{1315}{Ablation on projection heads. Using a shared head across tokens performs on par with a dual-head setup while maintaining simplicity. (Adapted from \cite {zhou2022_ibot})}{table.caption.2776}{}}
\abx@aux@backref{1741}{zhou2022_ibot}{0}{1315}{1315}
\abx@aux@cite{0}{caron2020_swav}
\abx@aux@segm{0}{0}{caron2020_swav}
\@writefile{toc}{\contentsline {paragraph}{iBOT vs. DINO: Paving the Way for DINOv2}{1316}{section*.2777}\protected@file@percent }
\newlabel{par:chapter22_ibot_vs_dino}{{22.4.6}{1316}{iBOT vs. DINO: Paving the Way for DINOv2}{section*.2777}{}}
\abx@aux@backref{1742}{caron2020_swav}{0}{1316}{1316}
\@writefile{toc}{\contentsline {paragraph}{Sinkhorn--Knopp Centering in DINOv2}{1317}{section*.2778}\protected@file@percent }
\newlabel{par:chapter22_dinov2_sinkhorn}{{22.4.6}{1317}{Sinkhorn--Knopp Centering in DINOv2}{section*.2778}{}}
\@writefile{toc}{\contentsline {paragraph}{Sinkhorn--Knopp Algorithm (NumPy Pseudocode)}{1319}{section*.2779}\protected@file@percent }
\newlabel{par:chapter22_dinov2_sinkhorn_pseudocode}{{22.4.6}{1319}{Sinkhorn--Knopp Algorithm (NumPy Pseudocode)}{section*.2779}{}}
\@writefile{toc}{\contentsline {paragraph}{Explanation and DINOv2 Motivation}{1320}{section*.2780}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Toy Example: The Fair Project Manager}{1320}{section*.2781}\protected@file@percent }
\newlabel{par:chapter22_dinov2_sinkhorn_example}{{22.4.6}{1320}{Toy Example: The Fair Project Manager}{section*.2781}{}}
\@writefile{toc}{\contentsline {paragraph}{Connection to DINOv2}{1322}{section*.2782}\protected@file@percent }
\newlabel{par:chapter22_dinov2_sinkhorn_connection}{{22.4.6}{1322}{Connection to DINOv2}{section*.2782}{}}
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\@writefile{toc}{\contentsline {paragraph}{Linear Evaluation on ImageNet and Comparison to Prior Work}{1323}{section*.2783}\protected@file@percent }
\newlabel{par:chapter22_dinov2_classification}{{22.4.6}{1323}{Linear Evaluation on ImageNet and Comparison to Prior Work}{section*.2783}{}}
\@writefile{lot}{\contentsline {table}{\numberline {22.32}{\ignorespaces Linear evaluation and $k$-NN classification accuracy on ImageNet-1k. All methods use frozen features. DINOv2 outperforms prior self-supervised models and approaches the performance of large-scale weakly supervised models (Adapted from~\blx@tocontentsinit {0}\cite {oquab2023_dinov2}).}}{1323}{table.caption.2784}\protected@file@percent }
\abx@aux@backref{1744}{oquab2023_dinov2}{0}{1323}{1323}
\newlabel{tab:chapter22_dinov2_linear_comparison}{{22.32}{1323}{Linear evaluation and $k$-NN classification accuracy on ImageNet-1k. All methods use frozen features. DINOv2 outperforms prior self-supervised models and approaches the performance of large-scale weakly supervised models (Adapted from~\cite {oquab2023_dinov2})}{table.caption.2784}{}}
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\@writefile{toc}{\contentsline {paragraph}{Ablation of Design Modifications from iBOT to DINOv2}{1324}{section*.2785}\protected@file@percent }
\newlabel{par:chapter22_dinov2_ablation_training}{{22.4.6}{1324}{Ablation of Design Modifications from iBOT to DINOv2}{section*.2785}{}}
\@writefile{lot}{\contentsline {table}{\numberline {22.33}{\ignorespaces Stepwise training ablation from iBOT to DINOv2 using ViT-L/14 pretrained on ImageNet-22k. Colored deltas show improvement (\textcolor {green!50!black}{green}) or degradation (\textcolor {red}{red}) from the previous step (adapted from \blx@tocontentsinit {0}\cite {oquab2023_dinov2}).}}{1324}{table.caption.2786}\protected@file@percent }
\abx@aux@backref{1746}{oquab2023_dinov2}{0}{1324}{1324}
\newlabel{tab:chapter22_dinov2_training_ablation}{{22.33}{1324}{Stepwise training ablation from iBOT to DINOv2 using ViT-L/14 pretrained on ImageNet-22k. Colored deltas show improvement (\textcolor {green!50!black}{green}) or degradation (\textcolor {red}{red}) from the previous step (adapted from \cite {oquab2023_dinov2})}{table.caption.2786}{}}
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\@writefile{toc}{\contentsline {paragraph}{Ablation of Pretraining Data: LVD-142M vs ImageNet-22k}{1325}{section*.2787}\protected@file@percent }
\newlabel{par:chapter22_dinov2_data_ablation}{{22.4.6}{1325}{Ablation of Pretraining Data: LVD-142M vs ImageNet-22k}{section*.2787}{}}
\@writefile{lot}{\contentsline {table}{\numberline {22.34}{\ignorespaces Comparison of different pretraining data sources. LVD-142M leads to stronger generalization across diverse tasks while maintaining high ImageNet-1k accuracy (Adapted from \blx@tocontentsinit {0}\cite {oquab2023_dinov2}).}}{1325}{table.caption.2788}\protected@file@percent }
\abx@aux@backref{1748}{oquab2023_dinov2}{0}{1325}{1325}
\newlabel{tab:chapter22_dinov2_data_sources}{{22.34}{1325}{Comparison of different pretraining data sources. LVD-142M leads to stronger generalization across diverse tasks while maintaining high ImageNet-1k accuracy (Adapted from \cite {oquab2023_dinov2})}{table.caption.2788}{}}
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\@writefile{toc}{\contentsline {paragraph}{Effectiveness of Knowledge Distillation from DINOv2}{1326}{section*.2789}\protected@file@percent }
\newlabel{par:chapter22_dinov2_distillation}{{22.4.6}{1326}{Effectiveness of Knowledge Distillation from DINOv2}{section*.2789}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22.56}{\ignorespaces Effectiveness of knowledge distillation from DINOv2. A ViT-L/14 student distilled from a frozen ViT-g/14 teacher outperforms the same architecture trained from scratch. On some benchmarks, it even matches or exceeds the teacher’s own performance (Adapted from \blx@tocontentsinit {0}\cite {oquab2023_dinov2}).}}{1326}{figure.caption.2790}\protected@file@percent }
\abx@aux@backref{1750}{oquab2023_dinov2}{0}{1326}{1326}
\newlabel{fig:chapter22_dino_kd}{{22.56}{1326}{Effectiveness of knowledge distillation from DINOv2. A ViT-L/14 student distilled from a frozen ViT-g/14 teacher outperforms the same architecture trained from scratch. On some benchmarks, it even matches or exceeds the teacher’s own performance (Adapted from \cite {oquab2023_dinov2})}{figure.caption.2790}{}}
\@writefile{toc}{\contentsline {paragraph}{Transfer to Diverse Visual Tasks}{1327}{section*.2791}\protected@file@percent }
\newlabel{par:chapter22_dinov2_generalization}{{22.4.6}{1327}{Transfer to Diverse Visual Tasks}{section*.2791}{}}
\BKM@entry{id=860,dest={73756273656374696F6E2E32322E342E37},srcline={4731}}{5C3337365C3337375C303030445C303030495C3030304E5C3030304F5C303030765C303030335C3030303A5C3030305C3034305C303030515C303030755C303030695C303030635C3030306B5C3030305C3034305C3030304F5C303030765C303030655C303030725C303030765C303030695C303030655C30303077}
\abx@aux@cite{0}{he2020_moco}
\abx@aux@segm{0}{0}{he2020_moco}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{caron2021_selfsupervised}
\abx@aux@segm{0}{0}{caron2021_selfsupervised}
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.4.7}DINOv3: Quick Overview}{1328}{subsection.22.4.7}\protected@file@percent }
\newlabel{subsec:chapter22_dinov3}{{22.4.7}{1328}{DINOv3: Quick Overview}{subsection.22.4.7}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation}{1328}{section*.2792}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{What DINOv3 changes}{1328}{section*.2793}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Position in the SSL landscape}{1329}{section*.2794}\protected@file@percent }
\abx@aux@backref{1751}{caron2021_selfsupervised}{0}{1329}{1329}
\abx@aux@backref{1752}{grill2020_byol}{0}{1329}{1329}
\abx@aux@backref{1753}{he2020_moco}{0}{1329}{1329}
\abx@aux@backref{1754}{oquab2023_dinov2}{0}{1329}{1329}
\@writefile{toc}{\contentsline {paragraph}{Practical gains}{1329}{section*.2795}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{From Self-Distillation to Clustering-Based Objectives}{1329}{section*.2796}\protected@file@percent }
\newlabel{par:chapter22_transition_swav}{{22.4.7}{1329}{From Self-Distillation to Clustering-Based Objectives}{section*.2796}{}}
\BKM@entry{id=861,dest={73656374696F6E2E32322E35},srcline={4777}}{5C3337365C3337375C303030435C3030306C5C303030755C303030735C303030745C303030655C303030725C303030695C3030306E5C303030675C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C30303073}
\BKM@entry{id=862,dest={73756273656374696F6E2E32322E352E31},srcline={4780}}{5C3337365C3337375C303030535C303030775C303030415C303030565C3030303A5C3030305C3034305C3030304F5C3030306E5C3030306C5C303030695C3030306E5C303030655C3030305C3034305C303030435C3030306C5C303030755C303030735C303030745C303030655C303030725C303030695C3030306E5C303030675C3030305C3034305C303030765C303030695C303030615C3030305C3034305C303030535C303030775C303030615C303030705C303030705C303030655C303030645C3030305C3034305C303030415C303030735C303030735C303030695C303030675C3030306E5C3030306D5C303030655C3030306E5C303030745C30303073}
\abx@aux@cite{0}{caron2020_swav}
\abx@aux@segm{0}{0}{caron2020_swav}
\abx@aux@cite{0}{caron2020_swav}
\abx@aux@segm{0}{0}{caron2020_swav}
\abx@aux@cite{0}{caron2020_swav}
\abx@aux@segm{0}{0}{caron2020_swav}
\@writefile{toc}{\contentsline {section}{\numberline {22.5}Clustering Methods}{1330}{section.22.5}\protected@file@percent }
\newlabel{sec:chapter22_clustering_ssl_methods}{{22.5}{1330}{Clustering Methods}{section.22.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.5.1}SwAV: Online Clustering via Swapped Assignments}{1330}{subsection.22.5.1}\protected@file@percent }
\newlabel{subsec:chapter22_swav}{{22.5.1}{1330}{SwAV: Online Clustering via Swapped Assignments}{subsection.22.5.1}{}}
\@writefile{toc}{\contentsline {paragraph}{From Contrastive Bottlenecks to Clustering-Based Self-Supervision}{1330}{section*.2797}\protected@file@percent }
\abx@aux@backref{1755}{caron2020_swav}{0}{1330}{1330}
\@writefile{lof}{\contentsline {figure}{\numberline {22.57}{\ignorespaces Overview of SwAV’s online clustering strategy. Each augmented view is projected to a shared prototype space. The model predicts the cluster assignment of one view using features from another, using balanced soft assignments computed with the Sinkhorn-Knopp algorithm. (Adapted from \blx@tocontentsinit {0}\cite {caron2020_swav})}}{1330}{figure.caption.2798}\protected@file@percent }
\abx@aux@backref{1757}{caron2020_swav}{0}{1330}{1330}
\newlabel{fig:chapter22_swav_overview}{{22.57}{1330}{Overview of SwAV’s online clustering strategy. Each augmented view is projected to a shared prototype space. The model predicts the cluster assignment of one view using features from another, using balanced soft assignments computed with the Sinkhorn-Knopp algorithm. (Adapted from \cite {caron2020_swav})}{figure.caption.2798}{}}
\@writefile{toc}{\contentsline {subsubsection}{Architecture and Training Pipeline}{1331}{section*.2799}\protected@file@percent }
\newlabel{subsubsec:chapter22_swav_architecture}{{22.5.1}{1331}{Architecture and Training Pipeline}{section*.2799}{}}
\@writefile{toc}{\contentsline {paragraph}{Multi-crop Augmentation and Swapped Prediction}{1331}{section*.2800}\protected@file@percent }
\newlabel{par:chapter22_swav_multicrop}{{22.5.1}{1331}{Multi-crop Augmentation and Swapped Prediction}{section*.2800}{}}
\abx@aux@cite{0}{caron2020_swav}
\abx@aux@segm{0}{0}{caron2020_swav}
\abx@aux@cite{0}{caron2020_swav}
\abx@aux@segm{0}{0}{caron2020_swav}
\@writefile{toc}{\contentsline {paragraph}{Training Objective and Prototype Updates}{1332}{section*.2801}\protected@file@percent }
\newlabel{par:chapter22_swav_prototypes}{{22.5.1}{1332}{Training Objective and Prototype Updates}{section*.2801}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22.58}{\ignorespaces \textbf  {Swapped prediction in SwAV.} Two augmented views \(x_s\) and \(x_t\) yield embeddings \(z_s\) and \(z_t\). A soft cluster code \(q_s\) is computed from \(z_s\) using Sinkhorn–Knopp, while \(z_t\) is trained to match \(q_s\) via softmax over the prototypes. Swapping roles symmetrizes the loss (Adapted from~\blx@tocontentsinit {0}\cite {caron2020_swav}).}}{1333}{figure.caption.2802}\protected@file@percent }
\abx@aux@backref{1759}{caron2020_swav}{0}{1333}{1333}
\newlabel{fig:chapter22_swav_pipeline}{{22.58}{1333}{\textbf {Swapped prediction in SwAV.} Two augmented views \(x_s\) and \(x_t\) yield embeddings \(z_s\) and \(z_t\). A soft cluster code \(q_s\) is computed from \(z_s\) using Sinkhorn–Knopp, while \(z_t\) is trained to match \(q_s\) via softmax over the prototypes. Swapping roles symmetrizes the loss (Adapted from~\cite {caron2020_swav})}{figure.caption.2802}{}}
\@writefile{toc}{\contentsline {paragraph}{Summary}{1333}{section*.2803}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Empirical Results and Key Findings}{1333}{section*.2804}\protected@file@percent }
\newlabel{subsubsec:chapter22_swav_results}{{22.5.1}{1333}{Empirical Results and Key Findings}{section*.2804}{}}
\abx@aux@cite{0}{caron2020_swav}
\abx@aux@segm{0}{0}{caron2020_swav}
\abx@aux@cite{0}{caron2020_swav}
\abx@aux@segm{0}{0}{caron2020_swav}
\@writefile{toc}{\contentsline {paragraph}{Benchmarking on ImageNet}{1334}{section*.2805}\protected@file@percent }
\abx@aux@backref{1760}{caron2020_swav}{0}{1334}{1334}
\@writefile{toc}{\contentsline {paragraph}{Transfer to Downstream Tasks}{1334}{section*.2806}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Training Efficiency and Accessibility}{1334}{section*.2807}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Ablation Highlights}{1334}{section*.2808}\protected@file@percent }
\abx@aux@backref{1761}{caron2020_swav}{0}{1334}{1334}
\@writefile{toc}{\contentsline {paragraph}{Impact and Legacy}{1334}{section*.2809}\protected@file@percent }
\BKM@entry{id=863,dest={73656374696F6E2E32322E36},srcline={4940}}{5C3337365C3337375C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030445C303030655C303030635C3030306F5C303030725C303030725C303030655C3030306C5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C303030655C303030745C303030685C3030306F5C303030645C30303073}
\BKM@entry{id=864,dest={73756273656374696F6E2E32322E362E31},srcline={4943}}{5C3337365C3337375C303030425C303030615C303030725C3030306C5C3030306F5C303030775C3030305C3034305C303030545C303030775C303030695C3030306E5C303030735C3030303A5C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C303030445C303030655C303030635C3030306F5C303030725C303030725C303030655C3030306C5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030775C303030695C303030745C303030685C3030306F5C303030755C303030745C3030305C3034305C3030304E5C303030655C303030675C303030615C303030745C303030695C303030765C303030655C30303073}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\@writefile{toc}{\contentsline {section}{\numberline {22.6}Feature Decorrelation Methods}{1335}{section.22.6}\protected@file@percent }
\newlabel{sec:chapter22_feature_decorrelation_ssl_methods}{{22.6}{1335}{Feature Decorrelation Methods}{section.22.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.6.1}Barlow Twins: Feature Decorrelation without Negatives}{1335}{subsection.22.6.1}\protected@file@percent }
\newlabel{subsec:chapter22_barlow_twins}{{22.6.1}{1335}{Barlow Twins: Feature Decorrelation without Negatives}{subsection.22.6.1}{}}
\abx@aux@backref{1762}{zbontar2021_barlow}{0}{1335}{1335}
\@writefile{lof}{\contentsline {figure}{\numberline {22.59}{\ignorespaces Overview of Barlow Twins. Two augmented views of the same image are processed through a shared encoder and projector. The method computes a cross-correlation matrix across the batch and minimizes a loss that enforces invariance (diagonal entries close to 1) and decorrelation (off-diagonal entries close to 0). Adapted from~\blx@tocontentsinit {0}\cite {zbontar2021_barlow}.}}{1335}{figure.caption.2810}\protected@file@percent }
\abx@aux@backref{1764}{zbontar2021_barlow}{0}{1335}{1335}
\newlabel{fig:chapter22_barlowtwins_pipeline}{{22.59}{1335}{Overview of Barlow Twins. Two augmented views of the same image are processed through a shared encoder and projector. The method computes a cross-correlation matrix across the batch and minimizes a loss that enforces invariance (diagonal entries close to 1) and decorrelation (off-diagonal entries close to 0). Adapted from~\cite {zbontar2021_barlow}}{figure.caption.2810}{}}
\@writefile{toc}{\contentsline {paragraph}{Method Overview}{1335}{section*.2811}\protected@file@percent }
\newlabel{par:chapter22_barlowtwins_overview}{{22.6.1}{1335}{Method Overview}{section*.2811}{}}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\@writefile{toc}{\contentsline {paragraph}{Redundancy Reduction Loss}{1336}{section*.2812}\protected@file@percent }
\newlabel{par:chapter22_barlowtwins_loss}{{22.6.1}{1336}{Redundancy Reduction Loss}{section*.2812}{}}
\abx@aux@backref{1765}{zbontar2021_barlow}{0}{1336}{1336}
\@writefile{toc}{\contentsline {paragraph}{Practical Details}{1336}{section*.2814}\protected@file@percent }
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\@writefile{toc}{\contentsline {subsubsection}{Empirical Results and Ablation Studies}{1337}{section*.2815}\protected@file@percent }
\newlabel{subsubsec:chapter22_barlowtwins_results}{{22.6.1}{1337}{Empirical Results and Ablation Studies}{section*.2815}{}}
\@writefile{toc}{\contentsline {paragraph}{Linear Evaluation on ImageNet}{1337}{section*.2816}\protected@file@percent }
\abx@aux@backref{1766}{zbontar2021_barlow}{0}{1337}{1337}
\@writefile{lot}{\contentsline {table}{\numberline {22.35}{\ignorespaces \textbf  {ImageNet-1K linear evaluation (ResNet-50).} Barlow Twins performs competitively with state-of-the-art SSL methods. Adapted from~\blx@tocontentsinit {0}\cite {zbontar2021_barlow}.}}{1337}{table.caption.2817}\protected@file@percent }
\abx@aux@backref{1768}{zbontar2021_barlow}{0}{1337}{1337}
\newlabel{tab:chapter22_barlowtwins_imagenet}{{22.35}{1337}{\textbf {ImageNet-1K linear evaluation (ResNet-50).} Barlow Twins performs competitively with state-of-the-art SSL methods. Adapted from~\cite {zbontar2021_barlow}}{table.caption.2817}{}}
\@writefile{toc}{\contentsline {paragraph}{Transfer Learning Performance}{1337}{section*.2818}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {22.36}{\ignorespaces \textbf  {Transfer learning benchmarks.} Performance is measured using linear classifiers trained on frozen features. Adapted from~\blx@tocontentsinit {0}\cite {zbontar2021_barlow}.}}{1337}{table.caption.2819}\protected@file@percent }
\abx@aux@backref{1770}{zbontar2021_barlow}{0}{1337}{1337}
\newlabel{tab:chapter22_barlowtwins_transfer}{{22.36}{1337}{\textbf {Transfer learning benchmarks.} Performance is measured using linear classifiers trained on frozen features. Adapted from~\cite {zbontar2021_barlow}}{table.caption.2819}{}}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\@writefile{toc}{\contentsline {paragraph}{Ablation Studies}{1338}{section*.2820}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {22.37}{\ignorespaces \textbf  {Ablation results.} Removing either loss component or normalization significantly degrades accuracy. Adapted from~\blx@tocontentsinit {0}\cite {zbontar2021_barlow}.}}{1338}{table.caption.2821}\protected@file@percent }
\abx@aux@backref{1772}{zbontar2021_barlow}{0}{1338}{1338}
\newlabel{tab:chapter22_barlowtwins_ablation}{{22.37}{1338}{\textbf {Ablation results.} Removing either loss component or normalization significantly degrades accuracy. Adapted from~\cite {zbontar2021_barlow}}{table.caption.2821}{}}
\@writefile{toc}{\contentsline {paragraph}{Batch Size Robustness}{1338}{section*.2822}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.60}{\ignorespaces \textbf  {Effect of batch size.} Barlow Twins retains high accuracy at small batch sizes, unlike SimCLR and BYOL. (Adapted from~\blx@tocontentsinit {0}\cite {zbontar2021_barlow})}}{1338}{figure.caption.2823}\protected@file@percent }
\abx@aux@backref{1774}{zbontar2021_barlow}{0}{1338}{1338}
\newlabel{fig:chapter22_barlowtwins_batchsize}{{22.60}{1338}{\textbf {Effect of batch size.} Barlow Twins retains high accuracy at small batch sizes, unlike SimCLR and BYOL. (Adapted from~\cite {zbontar2021_barlow})}{figure.caption.2823}{}}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\@writefile{toc}{\contentsline {paragraph}{Effect of Projector Dimensionality}{1339}{section*.2824}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.61}{\ignorespaces \textbf  {Effect of embedding dimensionality.} Barlow Twins scales well with projector width, unlike SimCLR and BYOL (Adapted from~\blx@tocontentsinit {0}\cite {zbontar2021_barlow}).}}{1339}{figure.caption.2825}\protected@file@percent }
\abx@aux@backref{1776}{zbontar2021_barlow}{0}{1339}{1339}
\newlabel{fig:chapter22_barlowtwins_projectiondim}{{22.61}{1339}{\textbf {Effect of embedding dimensionality.} Barlow Twins scales well with projector width, unlike SimCLR and BYOL (Adapted from~\cite {zbontar2021_barlow})}{figure.caption.2825}{}}
\@writefile{toc}{\contentsline {paragraph}{Sensitivity to Augmentations}{1339}{section*.2826}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.62}{\ignorespaces \textbf  {Sensitivity to augmentation strength.} Barlow Twins is more robust than SimCLR but less stable than BYOL (Adapted from~\blx@tocontentsinit {0}\cite {zbontar2021_barlow}).}}{1339}{figure.caption.2827}\protected@file@percent }
\abx@aux@backref{1778}{zbontar2021_barlow}{0}{1339}{1339}
\newlabel{fig:chapter22_barlowtwins_augmentations}{{22.62}{1339}{\textbf {Sensitivity to augmentation strength.} Barlow Twins is more robust than SimCLR but less stable than BYOL (Adapted from~\cite {zbontar2021_barlow})}{figure.caption.2827}{}}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\@writefile{toc}{\contentsline {paragraph}{Hyperparameter Stability}{1340}{section*.2828}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22.63}{\ignorespaces \textbf  {Effect of redundancy weight \( \lambda \).} Barlow Twins maintains stable accuracy over a wide range of redundancy loss weights (Adapted from~\blx@tocontentsinit {0}\cite {zbontar2021_barlow}).}}{1340}{figure.caption.2829}\protected@file@percent }
\abx@aux@backref{1780}{zbontar2021_barlow}{0}{1340}{1340}
\newlabel{fig:chapter22_barlowtwins_lambda}{{22.63}{1340}{\textbf {Effect of redundancy weight \( \lambda \).} Barlow Twins maintains stable accuracy over a wide range of redundancy loss weights (Adapted from~\cite {zbontar2021_barlow})}{figure.caption.2829}{}}
\@writefile{toc}{\contentsline {paragraph}{Summary and Outlook}{1340}{section*.2830}\protected@file@percent }
\BKM@entry{id=865,dest={73756273656374696F6E2E32322E362E32},srcline={5171}}{5C3337365C3337375C303030565C303030495C303030435C303030525C303030655C303030675C3030303A5C3030305C3034305C303030565C303030615C303030725C303030695C303030615C3030306E5C303030635C303030655C3030302D5C303030495C3030306E5C303030765C303030615C303030725C303030695C303030615C3030306E5C303030635C303030655C3030302D5C303030435C3030306F5C303030765C303030615C303030725C303030695C303030615C3030306E5C303030635C303030655C3030305C3034305C303030525C303030655C303030675C303030755C3030306C5C303030615C303030725C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{bardes2022_vicreg}
\abx@aux@segm{0}{0}{bardes2022_vicreg}
\abx@aux@cite{0}{bardes2022_vicreg}
\abx@aux@segm{0}{0}{bardes2022_vicreg}
\abx@aux@cite{0}{bardes2022_vicreg}
\abx@aux@segm{0}{0}{bardes2022_vicreg}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.6.2}VICReg: Variance-Invariance-Covariance Regularization}{1341}{subsection.22.6.2}\protected@file@percent }
\newlabel{subsec:chapter22_vicreg}{{22.6.2}{1341}{VICReg: Variance-Invariance-Covariance Regularization}{subsection.22.6.2}{}}
\abx@aux@backref{1781}{bardes2022_vicreg}{0}{1341}{1341}
\@writefile{lof}{\contentsline {figure}{\numberline {22.64}{\ignorespaces \textbf  {VICReg architecture.} A batch of images \( I \) is augmented into two views \( X, X' \), encoded into intermediate features \( Y, Y' \), and passed through an expander MLP to yield final embeddings \( Z, Z' \). The model minimizes three terms: a distance loss to align embeddings, a variance loss to ensure feature spread, and a covariance loss to decorrelate features (Adapted from~\blx@tocontentsinit {0}\cite {bardes2022_vicreg}).}}{1341}{figure.caption.2831}\protected@file@percent }
\abx@aux@backref{1783}{bardes2022_vicreg}{0}{1341}{1341}
\newlabel{fig:chapter22_vicreg_pipeline}{{22.64}{1341}{\textbf {VICReg architecture.} A batch of images \( I \) is augmented into two views \( X, X' \), encoded into intermediate features \( Y, Y' \), and passed through an expander MLP to yield final embeddings \( Z, Z' \). The model minimizes three terms: a distance loss to align embeddings, a variance loss to ensure feature spread, and a covariance loss to decorrelate features (Adapted from~\cite {bardes2022_vicreg})}{figure.caption.2831}{}}
\@writefile{toc}{\contentsline {subsubsection}{Invariance Term: Similarity Loss}{1342}{section*.2832}\protected@file@percent }
\newlabel{subsubsec:chapter22_vicreg_similarity}{{22.6.2}{1342}{Invariance Term: Similarity Loss}{section*.2832}{}}
\@writefile{toc}{\contentsline {subsubsection}{Variance Term: Spread Preservation}{1343}{section*.2833}\protected@file@percent }
\newlabel{subsubsec:chapter22_vicreg_variance}{{22.6.2}{1343}{Variance Term: Spread Preservation}{section*.2833}{}}
\@writefile{toc}{\contentsline {subsubsection}{Covariance Term: Redundancy Reduction}{1343}{section*.2834}\protected@file@percent }
\newlabel{subsubsec:chapter22_vicreg_covariance}{{22.6.2}{1343}{Covariance Term: Redundancy Reduction}{section*.2834}{}}
\abx@aux@cite{0}{bardes2022_vicreg}
\abx@aux@segm{0}{0}{bardes2022_vicreg}
\abx@aux@cite{0}{bardes2022_vicreg}
\abx@aux@segm{0}{0}{bardes2022_vicreg}
\abx@aux@cite{0}{bardes2022_vicreg}
\abx@aux@segm{0}{0}{bardes2022_vicreg}
\abx@aux@cite{0}{bardes2022_vicreg}
\abx@aux@segm{0}{0}{bardes2022_vicreg}
\abx@aux@cite{0}{he2020_moco}
\abx@aux@segm{0}{0}{he2020_moco}
\abx@aux@cite{0}{misra2019_pirl}
\abx@aux@segm{0}{0}{misra2019_pirl}
\abx@aux@cite{0}{chen2020_simclr}
\abx@aux@segm{0}{0}{chen2020_simclr}
\abx@aux@cite{0}{chen2020_improved}
\abx@aux@segm{0}{0}{chen2020_improved}
\abx@aux@cite{0}{grill2020_byol}
\abx@aux@segm{0}{0}{grill2020_byol}
\abx@aux@cite{0}{zbontar2021_barlow}
\abx@aux@segm{0}{0}{zbontar2021_barlow}
\abx@aux@cite{0}{bardes2022_vicreg}
\abx@aux@segm{0}{0}{bardes2022_vicreg}
\abx@aux@cite{0}{caron2020_swav}
\abx@aux@segm{0}{0}{caron2020_swav}
\abx@aux@cite{0}{bardes2022_vicreg}
\abx@aux@segm{0}{0}{bardes2022_vicreg}
\@writefile{toc}{\contentsline {subsubsection}{Implementation Details and Empirical Evaluation}{1345}{section*.2835}\protected@file@percent }
\newlabel{subsubsec:chapter22_vicreg_results}{{22.6.2}{1345}{Implementation Details and Empirical Evaluation}{section*.2835}{}}
\@writefile{toc}{\contentsline {paragraph}{Training Setup}{1345}{section*.2836}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Linear Evaluation on ImageNet}{1345}{section*.2837}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {22.38}{\ignorespaces \textbf  {Linear probing accuracy on ImageNet using ResNet-50.} VICReg performs competitively without contrastive negatives, stop-gradient tricks, or teacher networks. Adapted from~\blx@tocontentsinit {0}\cite {bardes2022_vicreg}.}}{1345}{table.caption.2838}\protected@file@percent }
\abx@aux@backref{1785}{bardes2022_vicreg}{0}{1345}{1345}
\newlabel{tab:chapter22_vicreg_linear}{{22.38}{1345}{\textbf {Linear probing accuracy on ImageNet using ResNet-50.} VICReg performs competitively without contrastive negatives, stop-gradient tricks, or teacher networks. Adapted from~\cite {bardes2022_vicreg}}{table.caption.2838}{}}
\@writefile{toc}{\contentsline {paragraph}{Transfer Learning Performance}{1345}{section*.2839}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {22.39}{\ignorespaces \textbf  {Transfer learning benchmarks.} Top-1 accuracy or mAP from linear classifiers trained on frozen ResNet-50 features. Results for VICReg and other methods are adapted from~\blx@tocontentsinit {0}\cite {bardes2022_vicreg}. The top-3 methods per column are underlined.}}{1345}{table.caption.2840}\protected@file@percent }
\abx@aux@backref{1787}{bardes2022_vicreg}{0}{1345}{1345}
\newlabel{tab:chapter22_vicreg_transfer}{{22.39}{1345}{\textbf {Transfer learning benchmarks.} Top-1 accuracy or mAP from linear classifiers trained on frozen ResNet-50 features. Results for VICReg and other methods are adapted from~\cite {bardes2022_vicreg}. The top-3 methods per column are underlined}{table.caption.2840}{}}
\abx@aux@backref{1788}{he2020_moco}{0}{1345}{1345}
\abx@aux@backref{1789}{misra2019_pirl}{0}{1345}{1345}
\abx@aux@backref{1790}{chen2020_simclr}{0}{1345}{1345}
\abx@aux@backref{1791}{chen2020_improved}{0}{1345}{1345}
\abx@aux@backref{1792}{grill2020_byol}{0}{1345}{1345}
\abx@aux@backref{1793}{zbontar2021_barlow}{0}{1345}{1345}
\abx@aux@backref{1794}{bardes2022_vicreg}{0}{1345}{1345}
\abx@aux@backref{1795}{caron2020_swav}{0}{1345}{1345}
\abx@aux@cite{0}{bardes2022_vicreg}
\abx@aux@segm{0}{0}{bardes2022_vicreg}
\abx@aux@cite{0}{bardes2022_vicreg}
\abx@aux@segm{0}{0}{bardes2022_vicreg}
\abx@aux@cite{0}{bardes2022_vicreg}
\abx@aux@segm{0}{0}{bardes2022_vicreg}
\abx@aux@cite{0}{bardes2022_vicreg}
\abx@aux@segm{0}{0}{bardes2022_vicreg}
\@writefile{toc}{\contentsline {paragraph}{Robustness to Batch Size}{1346}{section*.2841}\protected@file@percent }
\abx@aux@backref{1796}{bardes2022_vicreg}{0}{1346}{1346}
\@writefile{toc}{\contentsline {paragraph}{Summary of Empirical Results}{1346}{section*.2842}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Ablation Studies and Objective Decomposition}{1346}{section*.2843}\protected@file@percent }
\newlabel{subsubsec:chapter22_vicreg_ablations}{{22.6.2}{1346}{Ablation Studies and Objective Decomposition}{section*.2843}{}}
\@writefile{toc}{\contentsline {paragraph}{Effect of Removing Loss Terms}{1346}{section*.2844}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {22.40}{\ignorespaces \textbf  {Effect of VICReg loss term combinations on collapse and accuracy.} Models are pretrained for 100 epochs on ImageNet using a ResNet-50 backbone and evaluated by linear probing. Following~\blx@tocontentsinit {0}\cite {bardes2022_vicreg}, collapse is defined as the standard deviation across embedding dimensions approaching zero. Note that the full VICReg model achieves 73.2\% top-1 accuracy after 1000 epochs (see Table~1 in~\blx@tocontentsinit {0}\cite {bardes2022_vicreg}); these 100-epoch results are used for efficient ablation. Default loss weights are \( \lambda _s = 25 \) (similarity), \( \lambda _v = 25 \) (variance), and \( \lambda _c = 1 \) (covariance).}}{1346}{table.caption.2845}\protected@file@percent }
\abx@aux@backref{1799}{bardes2022_vicreg}{0}{1346}{1346}
\abx@aux@backref{1800}{bardes2022_vicreg}{0}{1346}{1346}
\newlabel{tab:chapter22_vicreg_ablation_terms}{{22.40}{1346}{\textbf {Effect of VICReg loss term combinations on collapse and accuracy.} Models are pretrained for 100 epochs on ImageNet using a ResNet-50 backbone and evaluated by linear probing. Following~\cite {bardes2022_vicreg}, collapse is defined as the standard deviation across embedding dimensions approaching zero. Note that the full VICReg model achieves 73.2\% top-1 accuracy after 1000 epochs (see Table~1 in~\cite {bardes2022_vicreg}); these 100-epoch results are used for efficient ablation. Default loss weights are \( \lambda _s = 25 \) (similarity), \( \lambda _v = 25 \) (variance), and \( \lambda _c = 1 \) (covariance)}{table.caption.2845}{}}
\@writefile{toc}{\contentsline {paragraph}{Architectural Robustness}{1347}{section*.2846}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Comparison with Whitening-Based Methods}{1347}{section*.2847}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Ablation Summary}{1347}{section*.2848}\protected@file@percent }
\BKM@entry{id=866,dest={73656374696F6E2E32322E37},srcline={5464}}{5C3337365C3337375C303030415C303030645C303030615C303030705C303030745C303030695C3030306E5C303030675C3030305C3034305C303030535C303030535C3030304C5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030445C3030306F5C303030775C3030306E5C303030735C303030745C303030725C303030655C303030615C3030306D5C3030305C3034305C303030545C303030615C303030735C3030306B5C30303073}
\BKM@entry{id=867,dest={73756273656374696F6E2E32322E372E31},srcline={5472}}{5C3337365C3337375C303030415C3030306C5C303030695C303030675C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030425C303030615C303030635C3030306B5C303030625C3030306F5C3030306E5C303030655C3030305C3034305C303030535C303030745C303030725C303030755C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030545C303030615C303030735C3030306B5C3030305C3034305C303030445C303030655C3030306D5C303030615C3030306E5C303030645C30303073}
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\@writefile{toc}{\contentsline {section}{\numberline {22.7}Adapting SSL to Downstream Tasks}{1348}{section.22.7}\protected@file@percent }
\newlabel{sec:chapter22_adapting_ssl}{{22.7}{1348}{Adapting SSL to Downstream Tasks}{section.22.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.7.1}Aligning Backbone Structure with Task Demands}{1348}{subsection.22.7.1}\protected@file@percent }
\newlabel{subsec:chapter22_ssrl_backbone_task}{{22.7.1}{1348}{Aligning Backbone Structure with Task Demands}{subsection.22.7.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Masked Image Modeling: Prioritizing Spatial Detail}{1348}{section*.2849}\protected@file@percent }
\abx@aux@backref{1801}{oquab2023_dinov2}{0}{1348}{1348}
\@writefile{toc}{\contentsline {paragraph}{Contrastive and Clustering Methods: Emphasizing Semantic Structure}{1348}{section*.2850}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Hybrid Approaches: Balancing Spatial and Semantic Information}{1348}{section*.2851}\protected@file@percent }
\abx@aux@backref{1802}{oquab2023_dinov2}{0}{1348}{1348}
\BKM@entry{id=868,dest={73756273656374696F6E2E32322E372E32},srcline={5519}}{5C3337365C3337375C303030445C303030615C303030745C303030615C3030305C3034305C303030445C303030695C303030735C303030745C303030725C303030695C303030625C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030445C3030306F5C3030306D5C303030615C303030695C3030306E5C3030305C3034305C303030535C303030685C303030695C303030665C303030745C3030305C3034305C303030435C3030306F5C3030306E5C303030735C303030695C303030645C303030655C303030725C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {paragraph}{Recommended Usage}{1349}{section*.2852}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {22.7.2}Data Distribution and Domain Shift Considerations}{1349}{subsection.22.7.2}\protected@file@percent }
\newlabel{subsec:chapter22_ssrl_data_shift}{{22.7.2}{1349}{Data Distribution and Domain Shift Considerations}{subsection.22.7.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Diagnosing Domain Shift}{1349}{section*.2853}\protected@file@percent }
\BKM@entry{id=869,dest={73656374696F6E2E32322E38},srcline={5557}}{5C3337365C3337375C303030465C303030695C3030306E5C303030655C3030302D5C303030545C303030755C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030535C303030755C303030705C303030655C303030725C303030765C303030695C303030735C303030655C303030645C3030305C3034305C303030425C303030615C303030635C3030306B5C303030625C3030306F5C3030306E5C303030655C30303073}
\BKM@entry{id=870,dest={73756273656374696F6E2E32322E382E31},srcline={5574}}{5C3337365C3337375C303030435C303030685C3030306F5C3030306F5C303030735C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C3030305C3034305C303030415C303030645C303030615C303030705C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030535C303030745C303030725C303030615C303030745C303030655C303030675C303030795C3030303A5C3030305C3034305C303030445C303030615C303030745C303030615C3030302C5C3030305C3034305C303030445C3030306F5C3030306D5C303030615C303030695C3030306E5C3030302C5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030435C3030306F5C303030735C30303074}
\@writefile{toc}{\contentsline {paragraph}{Should We Try Multiple Backbones?}{1350}{section*.2854}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{1350}{section*.2855}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {22.8}Fine-Tuning Self-Supervised Backbones}{1350}{section.22.8}\protected@file@percent }
\newlabel{sec:chapter22_finetuning_ssl}{{22.8}{1350}{Fine-Tuning Self-Supervised Backbones}{section.22.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.8.1}Choosing an Adaptation Strategy: Data, Domain, and Cost}{1350}{subsection.22.8.1}\protected@file@percent }
\newlabel{subsec:chapter22_ssrl_adaptation_strategy}{{22.8.1}{1350}{Choosing an Adaptation Strategy: Data, Domain, and Cost}{subsection.22.8.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{(1) Linear Probing and Lightweight Heads}{1350}{section*.2856}\protected@file@percent }
\abx@aux@cite{0}{liu2024_dora}
\abx@aux@segm{0}{0}{liu2024_dora}
\@writefile{toc}{\contentsline {subsubsection}{(2) Parameter-Efficient Fine-Tuning (PEFT)}{1351}{section*.2857}\protected@file@percent }
\newlabel{subsubsec:chapter22_peft}{{22.8.1}{1351}{(2) Parameter-Efficient Fine-Tuning (PEFT)}{section*.2857}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Use PEFT?}{1351}{section*.2858}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Common PEFT Strategies}{1351}{section*.2859}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {22.41}{\ignorespaces \textbf  {Comparison of PEFT methods.} All strategies update fewer than 5\% of parameters, enabling adaptation of large models under resource constraints.}}{1351}{table.caption.2860}\protected@file@percent }
\newlabel{tab:chapter22_peft_summary}{{22.41}{1351}{\textbf {Comparison of PEFT methods.} All strategies update fewer than 5\% of parameters, enabling adaptation of large models under resource constraints}{table.caption.2860}{}}
\@writefile{toc}{\contentsline {paragraph}{Practical Considerations}{1351}{section*.2861}\protected@file@percent }
\abx@aux@backref{1803}{liu2024_dora}{0}{1352}{1352}
\@writefile{toc}{\contentsline {subsubsection}{(3) Progressive Unfreezing and LP-FT}{1352}{section*.2862}\protected@file@percent }
\newlabel{subsubsec:chapter22_progressive_lpft}{{22.8.1}{1352}{(3) Progressive Unfreezing and LP-FT}{section*.2862}{}}
\@writefile{toc}{\contentsline {paragraph}{Progressive Unfreezing}{1352}{section*.2863}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Linear-Probe-Then-Fine-Tune (LP-FT)}{1352}{section*.2864}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{(4) Full Fine-Tuning (FFT)}{1352}{section*.2865}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{(5) Continued Self-Supervised Pretraining (C-SSL)}{1352}{section*.2866}\protected@file@percent }
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\abx@aux@cite{0}{gadre2023_datacomp}
\abx@aux@segm{0}{0}{gadre2023_datacomp}
\@writefile{toc}{\contentsline {paragraph}{Why Curation Beats Raw Scale}{1353}{section*.2867}\protected@file@percent }
\abx@aux@backref{1804}{oquab2023_dinov2}{0}{1353}{1353}
\abx@aux@backref{1805}{gadre2023_datacomp}{0}{1353}{1353}
\@writefile{toc}{\contentsline {paragraph}{Curation Workflow: Practical Steps}{1353}{section*.2868}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Illustrative Case Study: Learning Artistic Style}{1354}{section*.2869}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Challenge: Content-Biased Representations}{1354}{section*.2870}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{C-SSL Solution: Re-centering on Style}{1354}{section*.2871}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Outcome: Efficient Style Recognition}{1354}{section*.2872}\protected@file@percent }
\abx@aux@cite{0}{liu2024_dora}
\abx@aux@segm{0}{0}{liu2024_dora}
\@writefile{toc}{\contentsline {paragraph}{Summary: Fine-Tuning Strategies for Self-Supervised Models}{1355}{section*.2873}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {22.42}{\ignorespaces \textbf  {Comparison of Fine-Tuning Strategies.} Tradeoffs across flexibility, compute, etc.}}{1355}{table.caption.2874}\protected@file@percent }
\newlabel{tab:chapter22_ft_summary}{{22.42}{1355}{\textbf {Comparison of Fine-Tuning Strategies.} Tradeoffs across flexibility, compute, etc}{table.caption.2874}{}}
\abx@aux@backref{1806}{liu2024_dora}{0}{1355}{1355}
\BKM@entry{id=871,dest={73756273656374696F6E2E32322E382E32},srcline={5819}}{5C3337365C3337375C3030304C5C303030695C3030306E5C303030655C303030615C303030725C3030305C3034305C303030505C303030725C3030306F5C303030625C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304D5C3030304C5C303030505C3030305C3034305C303030485C303030655C303030615C303030645C3030305C3034305C303030415C303030645C303030615C303030705C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{oquab2023_dinov2}
\abx@aux@segm{0}{0}{oquab2023_dinov2}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.8.2}Linear Probing and MLP Head Adaptation}{1356}{subsection.22.8.2}\protected@file@percent }
\newlabel{subsec:chapter22_linear_probing}{{22.8.2}{1356}{Linear Probing and MLP Head Adaptation}{subsection.22.8.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Purpose and Motivation}{1356}{section*.2875}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Application Procedure}{1356}{section*.2876}\protected@file@percent }
\abx@aux@backref{1807}{oquab2023_dinov2}{0}{1356}{1356}
\abx@aux@cite{0}{ericsson2021_selfsup}
\abx@aux@segm{0}{0}{ericsson2021_selfsup}
\abx@aux@cite{0}{zhai2019_largescale}
\abx@aux@segm{0}{0}{zhai2019_largescale}
\abx@aux@cite{0}{shuttleworth2024_loraillusion}
\abx@aux@segm{0}{0}{shuttleworth2024_loraillusion}
\@writefile{toc}{\contentsline {paragraph}{Hyperparameter Recommendations}{1357}{section*.2877}\protected@file@percent }
\abx@aux@backref{1808}{ericsson2021_selfsup}{0}{1357}{1357}
\abx@aux@backref{1809}{zhai2019_largescale}{0}{1357}{1357}
\@writefile{toc}{\contentsline {paragraph}{Practical Tips and Diagnostic Insights}{1357}{section*.2878}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{When to Escalate to LoRA or Other PEFT Techniques}{1357}{section*.2879}\protected@file@percent }
\abx@aux@backref{1810}{shuttleworth2024_loraillusion}{0}{1357}{1357}
\@writefile{toc}{\contentsline {paragraph}{Best Practices}{1358}{section*.2880}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Empirical Signal for Escalation}{1358}{section*.2881}\protected@file@percent }
\BKM@entry{id=872,dest={73756273656374696F6E2E32322E382E33},srcline={5929}}{5C3337365C3337375C3030304C5C3030306F5C303030775C3030302D5C303030525C303030615C3030306E5C3030306B5C3030305C3034305C303030415C303030645C303030615C303030705C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C3030304C5C3030306F5C303030525C303030415C3030305C3035315C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C303030655C30303072}
\abx@aux@cite{0}{hu2021_lora}
\abx@aux@segm{0}{0}{hu2021_lora}
\abx@aux@cite{0}{hu2021_lora}
\abx@aux@segm{0}{0}{hu2021_lora}
\abx@aux@cite{0}{shuttleworth2024_loraillusion}
\abx@aux@segm{0}{0}{shuttleworth2024_loraillusion}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.8.3}Low-Rank Adaptation (LoRA) for Efficient Transfer}{1359}{subsection.22.8.3}\protected@file@percent }
\newlabel{subsec:chapter22_lora_adaptation}{{22.8.3}{1359}{Low-Rank Adaptation (LoRA) for Efficient Transfer}{subsection.22.8.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation and Intuition}{1359}{section*.2882}\protected@file@percent }
\abx@aux@backref{1811}{hu2021_lora}{0}{1359}{1359}
\@writefile{toc}{\contentsline {paragraph}{Mechanism}{1359}{section*.2883}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Initialization and Forward Pass}{1359}{section*.2884}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The Role of the Scaling Factor \( \alpha / r \)}{1359}{section*.2885}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Tuning \( \alpha \) in Practice}{1359}{section*.2886}\protected@file@percent }
\abx@aux@cite{0}{shuttleworth2024_loraillusion}
\abx@aux@segm{0}{0}{shuttleworth2024_loraillusion}
\abx@aux@cite{0}{hu2021_lora}
\abx@aux@segm{0}{0}{hu2021_lora}
\abx@aux@cite{0}{hu2021_lora}
\abx@aux@segm{0}{0}{hu2021_lora}
\abx@aux@cite{0}{hu2021_lora}
\abx@aux@segm{0}{0}{hu2021_lora}
\abx@aux@cite{0}{lialin2024_scaling}
\abx@aux@segm{0}{0}{lialin2024_scaling}
\abx@aux@cite{0}{hu2021_lora}
\abx@aux@segm{0}{0}{hu2021_lora}
\@writefile{toc}{\contentsline {paragraph}{Empirical Findings and Low-Rank Capacity}{1360}{section*.2887}\protected@file@percent }
\abx@aux@backref{1812}{hu2021_lora}{0}{1360}{1360}
\abx@aux@backref{1813}{shuttleworth2024_loraillusion}{0}{1360}{1360}
\abx@aux@backref{1814}{shuttleworth2024_loraillusion}{0}{1360}{1360}
\@writefile{toc}{\contentsline {paragraph}{Inference-Time Behavior}{1360}{section*.2888}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Advantages of LoRA}{1360}{section*.2889}\protected@file@percent }
\abx@aux@backref{1815}{hu2021_lora}{0}{1360}{1360}
\abx@aux@backref{1816}{hu2021_lora}{0}{1360}{1360}
\@writefile{toc}{\contentsline {paragraph}{Recommended Hyperparameters}{1360}{section*.2890}\protected@file@percent }
\abx@aux@backref{1817}{hu2021_lora}{0}{1360}{1360}
\abx@aux@backref{1818}{lialin2024_scaling}{0}{1360}{1360}
\abx@aux@backref{1819}{hu2021_lora}{0}{1360}{1360}
\abx@aux@cite{0}{hayou2024_loraplus}
\abx@aux@segm{0}{0}{hayou2024_loraplus}
\abx@aux@cite{0}{liu2024_dora}
\abx@aux@segm{0}{0}{liu2024_dora}
\abx@aux@cite{0}{zhang2023_adalora}
\abx@aux@segm{0}{0}{zhang2023_adalora}
\abx@aux@cite{0}{valipour2022_dylora}
\abx@aux@segm{0}{0}{valipour2022_dylora}
\abx@aux@cite{0}{meng2025_pissa}
\abx@aux@segm{0}{0}{meng2025_pissa}
\@writefile{toc}{\contentsline {paragraph}{Example: PyTorch-style LoRA Setup}{1361}{section*.2891}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{When LoRA Is Not Enough}{1361}{section*.2892}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Variants and Extensions}{1361}{section*.2893}\protected@file@percent }
\abx@aux@backref{1820}{hayou2024_loraplus}{0}{1361}{1361}
\abx@aux@backref{1821}{liu2024_dora}{0}{1361}{1361}
\abx@aux@cite{0}{kalajdzievski2023_rs}
\abx@aux@segm{0}{0}{kalajdzievski2023_rs}
\abx@aux@cite{0}{huang2024_allora}
\abx@aux@segm{0}{0}{huang2024_allora}
\abx@aux@backref{1822}{meng2025_pissa}{0}{1362}{1362}
\abx@aux@backref{1823}{valipour2022_dylora}{0}{1362}{1362}
\abx@aux@backref{1824}{zhang2023_adalora}{0}{1362}{1362}
\abx@aux@backref{1825}{huang2024_allora}{0}{1362}{1362}
\abx@aux@backref{1826}{kalajdzievski2023_rs}{0}{1362}{1362}
\@writefile{toc}{\contentsline {paragraph}{Why They Matter}{1362}{section*.2894}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{1362}{section*.2895}\protected@file@percent }
\BKM@entry{id=873,dest={73756273656374696F6E2E32322E382E34},srcline={6100}}{5C3337365C3337375C303030505C303030725C3030306F5C303030675C303030725C303030655C303030735C303030735C303030695C303030765C303030655C3030305C3034305C303030555C3030306E5C303030665C303030725C303030655C303030655C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C303030505C3030302D5C303030465C30303054}
\abx@aux@cite{0}{kumar2022_finetuning}
\abx@aux@segm{0}{0}{kumar2022_finetuning}
\@writefile{toc}{\contentsline {subsection}{\numberline {22.8.4}Progressive Unfreezing and LP-FT}{1363}{subsection.22.8.4}\protected@file@percent }
\newlabel{subsec:chapter22_progressive_unfreezing}{{22.8.4}{1363}{Progressive Unfreezing and LP-FT}{subsection.22.8.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation}{1363}{section*.2896}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Progressive Unfreezing: Controlled Backbone Adaptation}{1363}{section*.2897}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example Schedule}{1363}{section*.2898}\protected@file@percent }
\abx@aux@backref{1827}{kumar2022_finetuning}{0}{1363}{1363}
\@writefile{toc}{\contentsline {paragraph}{LP-FT: Linear Probing Followed by Full Fine-Tuning}{1363}{section*.2899}\protected@file@percent }
\abx@aux@cite{0}{kumar2022_finetuning}
\abx@aux@segm{0}{0}{kumar2022_finetuning}
\@writefile{toc}{\contentsline {paragraph}{Best Use Cases}{1364}{section*.2900}\protected@file@percent }
\abx@aux@backref{1828}{kumar2022_finetuning}{0}{1364}{1364}
\@writefile{toc}{\contentsline {paragraph}{Decision Guidelines}{1364}{section*.2901}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{1364}{section*.2902}\protected@file@percent }
\BKM@entry{id=874,dest={636861707465722E3233},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030325C303030335C3030303A5C3030305C3034305C303030335C303030445C3030305C3034305C303030765C303030695C303030735C303030695C3030306F5C3030306E}
\BKM@entry{id=875,dest={73656374696F6E2E32332E31},srcline={10}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030335C303030445C3030305C3034305C303030505C303030655C303030725C303030635C303030655C303030705C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C303030725C3030306F5C3030306D5C3030305C3034305C303030325C303030445C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C30303073}
\@writefile{toc}{\contentsline {chapter}{\numberline {23}Lecture 23: 3D vision}{1365}{chapter.23}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@22}}
\ttl@writefile{ptc}{\ttl@starttoc{default@23}}
\pgfsyspdfmark {pgfid133}{0}{52099153}
\pgfsyspdfmark {pgfid132}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {23.1}Introduction to 3D Perception from 2D Images}{1365}{section.23.1}\protected@file@percent }
\newlabel{sec:chapter23_intro_3d}{{23.1}{1365}{Introduction to 3D Perception from 2D Images}{section.23.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23.1}{\ignorespaces Left: Inferring 3D shape (voxel grid) from a single 2D image. Right: Classifying an object from its 3D representation.}}{1365}{figure.caption.2903}\protected@file@percent }
\newlabel{fig:chapter23_3d_tasks}{{23.1}{1365}{Left: Inferring 3D shape (voxel grid) from a single 2D image. Right: Classifying an object from its 3D representation}{figure.caption.2903}{}}
\BKM@entry{id=876,dest={73756273656374696F6E2E32332E312E31},srcline={23}}{5C3337365C3337375C303030435C3030306F5C303030725C303030655C3030305C3034305C303030545C303030615C303030735C3030306B5C303030735C3030305C3034305C303030695C3030306E5C3030305C3034305C303030335C303030445C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E}
\BKM@entry{id=877,dest={73756273656374696F6E2E32332E312E32},srcline={35}}{5C3337365C3337375C303030335C303030445C3030305C3034305C303030525C303030655C303030705C303030725C303030655C303030735C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=878,dest={73656374696F6E2E32332E32},srcline={50}}{5C3337365C3337375C303030505C303030725C303030655C303030645C303030695C303030635C303030745C303030695C3030306E5C303030675C3030305C3034305C303030445C303030655C303030705C303030745C303030685C3030305C3034305C3030304D5C303030615C303030705C303030735C3030305C3034305C303030665C303030725C3030306F5C3030306D5C3030305C3034305C303030525C303030475C303030425C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {23.1.1}Core Tasks in 3D Vision}{1366}{subsection.23.1.1}\protected@file@percent }
\newlabel{subsec:chapter23_3d_tasks}{{23.1.1}{1366}{Core Tasks in 3D Vision}{subsection.23.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {23.1.2}3D Representations}{1366}{subsection.23.1.2}\protected@file@percent }
\newlabel{subsec:chapter23_3d_representations}{{23.1.2}{1366}{3D Representations}{subsection.23.1.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {23.2}Predicting Depth Maps from RGB Images}{1366}{section.23.2}\protected@file@percent }
\newlabel{sec:chapter23_depth_prediction}{{23.2}{1366}{Predicting Depth Maps from RGB Images}{section.23.2}{}}
\abx@aux@cite{0}{ranftl2022_midas}
\abx@aux@segm{0}{0}{ranftl2022_midas}
\abx@aux@cite{0}{lee2019_bts}
\abx@aux@segm{0}{0}{lee2019_bts}
\abx@aux@cite{0}{ranftl2021_dpt}
\abx@aux@segm{0}{0}{ranftl2021_dpt}
\@writefile{lof}{\contentsline {figure}{\numberline {23.2}{\ignorespaces Naive approach to monocular depth prediction using a fully convolutional encoder-decoder network with pixelwise $\ell _2$ loss.}}{1367}{figure.caption.2904}\protected@file@percent }
\newlabel{fig:chapter23_naive_depth}{{23.2}{1367}{Naive approach to monocular depth prediction using a fully convolutional encoder-decoder network with pixelwise $\ell _2$ loss}{figure.caption.2904}{}}
\abx@aux@backref{1829}{ranftl2022_midas}{0}{1367}{1367}
\abx@aux@backref{1830}{lee2019_bts}{0}{1367}{1367}
\abx@aux@backref{1831}{ranftl2021_dpt}{0}{1367}{1367}
\@writefile{toc}{\contentsline {paragraph}{Loss Function and the Limitations of Absolute Depth Regression}{1368}{section*.2905}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Scale-Depth Ambiguity and the Need for Invariant Losses}{1368}{section*.2906}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.3}{\ignorespaces Scale-depth ambiguity in monocular images: a nearby small toy cat and a distant real cat can produce indistinguishable 2D projections.}}{1368}{figure.caption.2907}\protected@file@percent }
\newlabel{fig:chapter23_scale_ambiguity}{{23.3}{1368}{Scale-depth ambiguity in monocular images: a nearby small toy cat and a distant real cat can produce indistinguishable 2D projections}{figure.caption.2907}{}}
\abx@aux@cite{0}{eigen2014_depth}
\abx@aux@segm{0}{0}{eigen2014_depth}
\abx@aux@cite{0}{eigen2014_depth}
\abx@aux@segm{0}{0}{eigen2014_depth}
\@writefile{toc}{\contentsline {paragraph}{Scale-Invariant Log-Depth Loss}{1369}{section*.2908}\protected@file@percent }
\newlabel{par:scale_invariant_depth}{{23.2}{1369}{Scale-Invariant Log-Depth Loss}{section*.2908}{}}
\abx@aux@backref{1832}{eigen2014_depth}{0}{1369}{1369}
\@writefile{toc}{\contentsline {paragraph}{Pairwise Interpretation}{1369}{section*.2909}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Weighted Loss for Training}{1369}{section*.2910}\protected@file@percent }
\abx@aux@backref{1833}{eigen2014_depth}{0}{1369}{1369}
\abx@aux@cite{0}{ranftl2022_midas}
\abx@aux@segm{0}{0}{ranftl2022_midas}
\abx@aux@cite{0}{ranftl2021_dpt}
\abx@aux@segm{0}{0}{ranftl2021_dpt}
\@writefile{toc}{\contentsline {paragraph}{Why a Single Global Scale Correction Suffices}{1370}{section*.2911}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Scale and Shift-Invariant Losses in MiDaS and DPT}{1370}{section*.2912}\protected@file@percent }
\abx@aux@backref{1834}{ranftl2022_midas}{0}{1370}{1370}
\abx@aux@backref{1835}{ranftl2021_dpt}{0}{1370}{1370}
\@writefile{toc}{\contentsline {paragraph}{Robust Trimmed MAE and Multi-Scale Gradient Losses}{1370}{section*.2913}\protected@file@percent }
\BKM@entry{id=879,dest={73656374696F6E2E32332E33},srcline={198}}{5C3337365C3337375C303030535C303030755C303030725C303030665C303030615C303030635C303030655C3030305C3034305C3030304E5C3030306F5C303030725C3030306D5C303030615C3030306C5C303030735C3030305C3034305C303030615C303030735C3030305C3034305C303030615C3030305C3034305C303030335C303030445C3030305C3034305C303030525C303030655C303030705C303030725C303030655C303030735C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {paragraph}{Summary}{1371}{section*.2914}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {23.3}Surface Normals as a 3D Representation}{1372}{section.23.3}\protected@file@percent }
\newlabel{sec:chapter23_surface_normals}{{23.3}{1372}{Surface Normals as a 3D Representation}{section.23.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Visualizing Normals}{1372}{section*.2915}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.4}{\ignorespaces Visualizing surface normals: blue indicates upward-facing normals (e.g., floor, bed top), red and green indicate horizontal orientations. Mixed colors reflect diagonal or mixed-direction normals.}}{1372}{figure.caption.2916}\protected@file@percent }
\newlabel{fig:chapter23_surface_normals}{{23.4}{1372}{Visualizing surface normals: blue indicates upward-facing normals (e.g., floor, bed top), red and green indicate horizontal orientations. Mixed colors reflect diagonal or mixed-direction normals}{figure.caption.2916}{}}
\@writefile{toc}{\contentsline {paragraph}{Learning Surface Normals}{1372}{section*.2917}\protected@file@percent }
\BKM@entry{id=880,dest={73656374696F6E2E32332E34},srcline={230}}{5C3337365C3337375C303030565C3030306F5C303030785C303030655C3030306C5C3030305C3034305C303030475C303030725C303030695C303030645C30303073}
\@writefile{toc}{\contentsline {paragraph}{Multi-Task Learning}{1373}{section*.2918}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Limitations}{1373}{section*.2919}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {23.4}Voxel Grids}{1373}{section.23.4}\protected@file@percent }
\newlabel{sec:chapter23_voxel_grids}{{23.4}{1373}{Voxel Grids}{section.23.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23.5}{\ignorespaces Voxel grids: Pros and cons. Left: regular grid and simplicity of representation. Right: loss of detail in high-frequency regions such as armrests of the sofa, due to limited resolution.}}{1373}{figure.caption.2920}\protected@file@percent }
\newlabel{fig:chapter23_voxel_grid}{{23.5}{1373}{Voxel grids: Pros and cons. Left: regular grid and simplicity of representation. Right: loss of detail in high-frequency regions such as armrests of the sofa, due to limited resolution}{figure.caption.2920}{}}
\@writefile{toc}{\contentsline {paragraph}{Advantages}{1373}{section*.2921}\protected@file@percent }
\abx@aux@cite{0}{wu2015_shapenets}
\abx@aux@segm{0}{0}{wu2015_shapenets}
\abx@aux@cite{0}{wu2015_shapenets}
\abx@aux@segm{0}{0}{wu2015_shapenets}
\@writefile{toc}{\contentsline {paragraph}{Limitations}{1374}{section*.2922}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{3D Convolutional Processing}{1374}{section*.2923}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.6}{\ignorespaces Processing voxel inputs using 3D convolutions for shape classification. The input is a $30 \times 30 \times 30$ occupancy grid representing the 3D shape. The network applies successive 3D convolutions: a $6 \times 6 \times 6$ kernel producing a $48 \times 13 \times 13 \times 13$ feature volume, followed by a $5 \times 5 \times 5$ kernel yielding $160 \times 5 \times 5 \times 5$, and a $4 \times 4 \times 4$ kernel producing $512 \times 2 \times 2 \times 2$. A fully connected layer maps the final volume to class scores. Adapted from Wu et al.~\blx@tocontentsinit {0}\cite {wu2015_shapenets}.}}{1374}{figure.caption.2924}\protected@file@percent }
\abx@aux@backref{1837}{wu2015_shapenets}{0}{1374}{1374}
\newlabel{fig:chapter23_voxel3d_pipeline}{{23.6}{1374}{Processing voxel inputs using 3D convolutions for shape classification. The input is a $30 \times 30 \times 30$ occupancy grid representing the 3D shape. The network applies successive 3D convolutions: a $6 \times 6 \times 6$ kernel producing a $48 \times 13 \times 13 \times 13$ feature volume, followed by a $5 \times 5 \times 5$ kernel yielding $160 \times 5 \times 5 \times 5$, and a $4 \times 4 \times 4$ kernel producing $512 \times 2 \times 2 \times 2$. A fully connected layer maps the final volume to class scores. Adapted from Wu et al.~\cite {wu2015_shapenets}}{figure.caption.2924}{}}
\@writefile{toc}{\contentsline {paragraph}{Application Example: Image-to-Voxel Prediction}{1374}{section*.2925}\protected@file@percent }
\newlabel{par:chapter23_voxel_application}{{23.4}{1374}{Application Example: Image-to-Voxel Prediction}{section*.2925}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23.7}{\ignorespaces Image-to-voxel inference pipeline. A $3 \times 112 \times 112$ RGB image is processed by a 2D CNN to extract features, which are lifted into a latent 3D voxel grid and refined by 3D convolutions. The output is an occupancy grid representing the shape of the object—in this case, an armchair.}}{1375}{figure.caption.2926}\protected@file@percent }
\newlabel{fig:chapter23_voxel_from_image}{{23.7}{1375}{Image-to-voxel inference pipeline. A $3 \times 112 \times 112$ RGB image is processed by a 2D CNN to extract features, which are lifted into a latent 3D voxel grid and refined by 3D convolutions. The output is an occupancy grid representing the shape of the object—in this case, an armchair}{figure.caption.2926}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23.8}{\ignorespaces Voxel representation memory usage scales cubically with resolution. A $V \times V \times V$ grid storing 32-bit floats grows rapidly: a $256^3$ grid requires roughly 64\,MB, while a $1024^3$ grid exceeds 4\,GB. This cubic growth severely limits the feasibility of high-resolution volumetric models, motivating the exploration of more memory-efficient 3D representations.}}{1375}{figure.caption.2927}\protected@file@percent }
\newlabel{fig:chapter23_voxel_memory}{{23.8}{1375}{Voxel representation memory usage scales cubically with resolution. A $V \times V \times V$ grid storing 32-bit floats grows rapidly: a $256^3$ grid requires roughly 64\,MB, while a $1024^3$ grid exceeds 4\,GB. This cubic growth severely limits the feasibility of high-resolution volumetric models, motivating the exploration of more memory-efficient 3D representations}{figure.caption.2927}{}}
\BKM@entry{id=881,dest={73756273656374696F6E2E32332E342E31},srcline={294}}{5C3337365C3337375C303030535C303030635C303030615C3030306C5C303030695C3030306E5C303030675C3030305C3034305C303030565C3030306F5C303030785C303030655C3030306C5C3030305C3034305C303030475C303030725C303030695C303030645C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304F5C303030635C303030745C303030725C303030655C303030655C30303073}
\abx@aux@cite{0}{tatarchenko2017_ogn}
\abx@aux@segm{0}{0}{tatarchenko2017_ogn}
\abx@aux@cite{0}{tatarchenko2017_ogn}
\abx@aux@segm{0}{0}{tatarchenko2017_ogn}
\abx@aux@cite{0}{tatarchenko2017_ogn}
\abx@aux@segm{0}{0}{tatarchenko2017_ogn}
\@writefile{toc}{\contentsline {subsection}{\numberline {23.4.1}Scaling Voxel Grids with Octrees}{1376}{subsection.23.4.1}\protected@file@percent }
\newlabel{subsec:chapter23_octrees}{{23.4.1}{1376}{Scaling Voxel Grids with Octrees}{subsection.23.4.1}{}}
\abx@aux@backref{1838}{tatarchenko2017_ogn}{0}{1376}{1376}
\@writefile{toc}{\contentsline {paragraph}{Octrees: Intuition and Structure}{1376}{section*.2928}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{From Dense to Adaptive}{1376}{section*.2929}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Octree Generating Networks (OGNs)}{1376}{section*.2930}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Surface-Driven Efficiency}{1376}{section*.2931}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.9}{\ignorespaces Octree-based shape reconstruction from OGN \blx@tocontentsinit {0}\cite {tatarchenko2017_ogn}. Left: initial coarse level (\(32^3\), blue) captures the basic structure of the car. Middle: refinement to level 2 (\(64^3\), green) improves resolution near surfaces such as wheels and front grill. Right: full octree refinement to level 3 (\(128^3\), brown) adds fine details and completes the car’s geometry while avoiding redundant subdivisions in empty space.}}{1377}{figure.caption.2932}\protected@file@percent }
\abx@aux@backref{1840}{tatarchenko2017_ogn}{0}{1377}{1377}
\newlabel{fig:chapter23_octree_car}{{23.9}{1377}{Octree-based shape reconstruction from OGN \cite {tatarchenko2017_ogn}. Left: initial coarse level (\(32^3\), blue) captures the basic structure of the car. Middle: refinement to level 2 (\(64^3\), green) improves resolution near surfaces such as wheels and front grill. Right: full octree refinement to level 3 (\(128^3\), brown) adds fine details and completes the car’s geometry while avoiding redundant subdivisions in empty space}{figure.caption.2932}{}}
\@writefile{toc}{\contentsline {paragraph}{Why and How Octrees Work}{1377}{section*.2933}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Predicting Subdivision with Neural Networks}{1377}{section*.2934}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Training with Supervised Supervision}{1378}{section*.2935}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why It Works}{1378}{section*.2936}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Limitations and Motivation for Point-Based Methods}{1378}{section*.2937}\protected@file@percent }
\BKM@entry{id=882,dest={73656374696F6E2E32332E35},srcline={396}}{5C3337365C3337375C303030505C3030306F5C303030695C3030306E5C303030745C3030305C3034305C303030435C3030306C5C3030306F5C303030755C303030645C30303073}
\abx@aux@cite{0}{fan2017_pointset}
\abx@aux@segm{0}{0}{fan2017_pointset}
\abx@aux@cite{0}{fan2017_pointset}
\abx@aux@segm{0}{0}{fan2017_pointset}
\@writefile{toc}{\contentsline {section}{\numberline {23.5}Point Clouds}{1379}{section.23.5}\protected@file@percent }
\newlabel{sec:chapter23_pointclouds}{{23.5}{1379}{Point Clouds}{section.23.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23.10}{\ignorespaces Airplane represented as a point cloud, from \blx@tocontentsinit {0}\cite {fan2017_pointset}. Fine structures such as wings and tail are densely sampled, while coarse regions like the fuselage use fewer points.}}{1379}{figure.caption.2938}\protected@file@percent }
\abx@aux@backref{1842}{fan2017_pointset}{0}{1379}{1379}
\newlabel{fig:chapter23_pointcloud_airplane}{{23.10}{1379}{Airplane represented as a point cloud, from \cite {fan2017_pointset}. Fine structures such as wings and tail are densely sampled, while coarse regions like the fuselage use fewer points}{figure.caption.2938}{}}
\@writefile{toc}{\contentsline {paragraph}{Advantages}{1379}{section*.2939}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Limitations}{1379}{section*.2940}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Rendering}{1379}{section*.2941}\protected@file@percent }
\BKM@entry{id=883,dest={73756273656374696F6E2E32332E352E31},srcline={434}}{5C3337365C3337375C303030505C3030306F5C303030695C3030306E5C303030745C3030305C3034305C303030435C3030306C5C3030306F5C303030755C303030645C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C303030725C3030306F5C3030306D5C3030305C3034305C303030615C3030305C3034305C303030535C303030695C3030306E5C303030675C3030306C5C303030655C3030305C3034305C303030495C3030306D5C303030615C303030675C30303065}
\abx@aux@cite{0}{fan2017_pointset}
\abx@aux@segm{0}{0}{fan2017_pointset}
\@writefile{toc}{\contentsline {paragraph}{Applications}{1380}{section*.2942}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {23.5.1}Point Cloud Generation from a Single Image}{1380}{subsection.23.5.1}\protected@file@percent }
\newlabel{subsec:pointcloud_fan}{{23.5.1}{1380}{Point Cloud Generation from a Single Image}{subsection.23.5.1}{}}
\abx@aux@backref{1843}{fan2017_pointset}{0}{1380}{1380}
\@writefile{toc}{\contentsline {paragraph}{Architecture Overview}{1380}{section*.2943}\protected@file@percent }
\abx@aux@cite{0}{fan2017_pointset}
\abx@aux@segm{0}{0}{fan2017_pointset}
\abx@aux@cite{0}{fan2017_pointset}
\abx@aux@segm{0}{0}{fan2017_pointset}
\@writefile{lof}{\contentsline {figure}{\numberline {23.11}{\ignorespaces Dual-branch point cloud generation network of Fan et al.~\blx@tocontentsinit {0}\cite {fan2017_pointset}. A 2D CNN encodes the input image into spatial features. The fully-connected branch (red) predicts \(P_1\) global points, while the convolutional branch (blue) predicts \(P_2\) points per spatial location, yielding \(H'W'P_2\) local points. Their union forms the final output \(\hat  {S}\).}}{1381}{figure.caption.2944}\protected@file@percent }
\abx@aux@backref{1845}{fan2017_pointset}{0}{1381}{1381}
\newlabel{fig:chapter23_fan_pointsetnet}{{23.11}{1381}{Dual-branch point cloud generation network of Fan et al.~\cite {fan2017_pointset}. A 2D CNN encodes the input image into spatial features. The fully-connected branch (red) predicts \(P_1\) global points, while the convolutional branch (blue) predicts \(P_2\) points per spatial location, yielding \(H'W'P_2\) local points. Their union forms the final output \(\hat {S}\)}{figure.caption.2944}{}}
\@writefile{toc}{\contentsline {paragraph}{Architectural Motivation}{1381}{section*.2945}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Loss Function: Chamfer Distance}{1382}{section*.2946}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.12}{\ignorespaces Chamfer distance (forward term): for each predicted point \( \textcolor {blue}{x \in S_1} \), find its nearest ground-truth match \( \textcolor {orange}{y \in S_2} \).}}{1382}{figure.caption.2947}\protected@file@percent }
\newlabel{fig:chapter23_chamfer_forward}{{23.12}{1382}{Chamfer distance (forward term): for each predicted point \( \textcolor {blue}{x \in S_1} \), find its nearest ground-truth match \( \textcolor {orange}{y \in S_2} \)}{figure.caption.2947}{}}
\BKM@entry{id=884,dest={73756273656374696F6E2E32332E352E32},srcline={529}}{5C3337365C3337375C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030306F5C3030306E5C3030305C3034305C303030505C3030306F5C303030695C3030306E5C303030745C3030305C3034305C303030435C3030306C5C3030306F5C303030755C303030645C303030735C3030303A5C3030305C3034305C303030505C3030306F5C303030695C3030306E5C303030745C3030304E5C303030655C303030745C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030565C303030615C303030725C303030695C303030615C3030306E5C303030745C30303073}
\abx@aux@cite{0}{qi2017_pointnet}
\abx@aux@segm{0}{0}{qi2017_pointnet}
\@writefile{lof}{\contentsline {figure}{\numberline {23.13}{\ignorespaces Chamfer distance (backward term): for each ground-truth point \( \textcolor {orange}{y \in S_2} \), find the nearest predicted point \( \textcolor {blue}{x \in S_1} \) to ensure coverage.}}{1383}{figure.caption.2948}\protected@file@percent }
\newlabel{fig:chapter23_chamfer_backward}{{23.13}{1383}{Chamfer distance (backward term): for each ground-truth point \( \textcolor {orange}{y \in S_2} \), find the nearest predicted point \( \textcolor {blue}{x \in S_1} \) to ensure coverage}{figure.caption.2948}{}}
\@writefile{toc}{\contentsline {paragraph}{Intuition and Impact}{1383}{section*.2949}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {23.5.2}Learning on Point Clouds: PointNet and Variants}{1383}{subsection.23.5.2}\protected@file@percent }
\newlabel{subsec:chapter23_pointnet_variants}{{23.5.2}{1383}{Learning on Point Clouds: PointNet and Variants}{subsection.23.5.2}{}}
\abx@aux@backref{1846}{qi2017_pointnet}{0}{1383}{1383}
\@writefile{toc}{\contentsline {paragraph}{Core Design: Set-Invariance via Shared MLP and Symmetric Pooling}{1383}{section*.2950}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.14}{\ignorespaces PointNet architecture (Qi et al., 2017). The classification network (left) takes \(n\) points, applies input and feature transformations, aggregates via max-pooling, and outputs class scores for \(k\) categories. The segmentation network (right) extends this by concatenating per-point and global features to predict labels at each point. MLP layer sizes are shown in brackets; BatchNorm+ReLU is used at each layer, and Dropout appears in the final FC layer for classification.}}{1384}{figure.caption.2951}\protected@file@percent }
\newlabel{fig:pointnet_arch}{{23.14}{1384}{PointNet architecture (Qi et al., 2017). The classification network (left) takes \(n\) points, applies input and feature transformations, aggregates via max-pooling, and outputs class scores for \(k\) categories. The segmentation network (right) extends this by concatenating per-point and global features to predict labels at each point. MLP layer sizes are shown in brackets; BatchNorm+ReLU is used at each layer, and Dropout appears in the final FC layer for classification}{figure.caption.2951}{}}
\@writefile{toc}{\contentsline {paragraph}{Pose Normalization via T-Net Modules}{1384}{section*.2952}\protected@file@percent }
\BKM@entry{id=885,dest={73756273656374696F6E2E32332E352E33},srcline={598}}{5C3337365C3337375C303030505C3030306F5C303030695C3030306E5C303030745C3030304E5C303030655C303030745C3030302B5C3030302B5C3030303A5C3030305C3034305C303030485C303030695C303030655C303030725C303030615C303030725C303030635C303030685C303030695C303030635C303030615C3030306C5C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030306F5C3030306E5C3030305C3034305C303030505C3030306F5C303030695C3030306E5C303030745C3030305C3034305C303030435C3030306C5C3030306F5C303030755C303030645C30303073}
\abx@aux@cite{0}{qi2017_pointnet}
\abx@aux@segm{0}{0}{qi2017_pointnet}
\abx@aux@cite{0}{qi2017_pointnetplusplus}
\abx@aux@segm{0}{0}{qi2017_pointnetplusplus}
\@writefile{toc}{\contentsline {paragraph}{Hierarchical Reasoning via Iterative Refinement}{1385}{section*.2953}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Legacy and Evolution}{1385}{section*.2954}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {23.5.3}PointNet++: Hierarchical Feature Learning on Point Clouds}{1385}{subsection.23.5.3}\protected@file@percent }
\newlabel{subsec:chapter23_pointnetpp}{{23.5.3}{1385}{PointNet++: Hierarchical Feature Learning on Point Clouds}{subsection.23.5.3}{}}
\abx@aux@backref{1847}{qi2017_pointnet}{0}{1385}{1385}
\abx@aux@backref{1848}{qi2017_pointnetplusplus}{0}{1385}{1385}
\abx@aux@cite{0}{qi2017_pointnetplusplus}
\abx@aux@segm{0}{0}{qi2017_pointnetplusplus}
\abx@aux@cite{0}{qi2017_pointnetplusplus}
\abx@aux@segm{0}{0}{qi2017_pointnetplusplus}
\abx@aux@cite{0}{qi2017_pointnetplusplus}
\abx@aux@segm{0}{0}{qi2017_pointnetplusplus}
\@writefile{lof}{\contentsline {figure}{\numberline {23.15}{\ignorespaces Hierarchical feature abstraction in PointNet++~\blx@tocontentsinit {0}\cite {qi2017_pointnetplusplus}. Local regions are formed via sampling and grouping, then encoded by mini-PointNets. Higher abstraction levels operate on increasingly larger receptive fields.}}{1386}{figure.caption.2955}\protected@file@percent }
\abx@aux@backref{1850}{qi2017_pointnetplusplus}{0}{1386}{1386}
\newlabel{fig:chapter23_pointnetpp_idea}{{23.15}{1386}{Hierarchical feature abstraction in PointNet++~\cite {qi2017_pointnetplusplus}. Local regions are formed via sampling and grouping, then encoded by mini-PointNets. Higher abstraction levels operate on increasingly larger receptive fields}{figure.caption.2955}{}}
\@writefile{toc}{\contentsline {paragraph}{Density-Adaptive Grouping and Robustness}{1386}{section*.2956}\protected@file@percent }
\abx@aux@backref{1851}{qi2017_pointnetplusplus}{0}{1386}{1386}
\abx@aux@cite{0}{qi2017_pointnetplusplus}
\abx@aux@segm{0}{0}{qi2017_pointnetplusplus}
\abx@aux@cite{0}{qi2017_pointnetplusplus}
\abx@aux@segm{0}{0}{qi2017_pointnetplusplus}
\@writefile{lof}{\contentsline {figure}{\numberline {23.16}{\ignorespaces Multi-scale and multi-resolution grouping strategies from PointNet++~\blx@tocontentsinit {0}\cite {qi2017_pointnetplusplus}. (a) \textbf  {Multi-Scale Grouping (MSG)}: for each centroid, multiple neighborhoods at different radii are constructed and processed by parallel mini-PointNets; their features are concatenated to form a scale-robust representation. (b) \textbf  {Multi-Resolution Grouping (MRG)}: combines coarse features propagated from previous abstraction levels with fine features extracted from raw points at the current level, allowing efficient adaptation to non-uniform sampling densities.}}{1387}{figure.caption.2957}\protected@file@percent }
\abx@aux@backref{1853}{qi2017_pointnetplusplus}{0}{1387}{1387}
\newlabel{fig:chapter23_pointnetplusplus_msg_mrg}{{23.16}{1387}{Multi-scale and multi-resolution grouping strategies from PointNet++~\cite {qi2017_pointnetplusplus}. (a) \textbf {Multi-Scale Grouping (MSG)}: for each centroid, multiple neighborhoods at different radii are constructed and processed by parallel mini-PointNets; their features are concatenated to form a scale-robust representation. (b) \textbf {Multi-Resolution Grouping (MRG)}: combines coarse features propagated from previous abstraction levels with fine features extracted from raw points at the current level, allowing efficient adaptation to non-uniform sampling densities}{figure.caption.2957}{}}
\@writefile{toc}{\contentsline {paragraph}{Feature Propagation for Dense Prediction}{1387}{section*.2958}\protected@file@percent }
\abx@aux@cite{0}{qian2022_pointnext}
\abx@aux@segm{0}{0}{qian2022_pointnext}
\abx@aux@cite{0}{wang2019_dgcnn}
\abx@aux@segm{0}{0}{wang2019_dgcnn}
\abx@aux@cite{0}{zhao2021_point}
\abx@aux@segm{0}{0}{zhao2021_point}
\abx@aux@cite{0}{wu2022_point}
\abx@aux@segm{0}{0}{wu2022_point}
\@writefile{toc}{\contentsline {paragraph}{Summary and Impact}{1388}{section*.2959}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Extensions and Improvements}{1388}{section*.2960}\protected@file@percent }
\abx@aux@backref{1854}{qian2022_pointnext}{0}{1388}{1388}
\abx@aux@backref{1855}{wang2019_dgcnn}{0}{1388}{1388}
\abx@aux@backref{1856}{wu2022_point}{0}{1388}{1388}
\abx@aux@backref{1857}{zhao2021_point}{0}{1388}{1388}
\@writefile{toc}{\contentsline {paragraph}{Toward Structured Representations}{1388}{section*.2961}\protected@file@percent }
\BKM@entry{id=886,dest={73656374696F6E2E32332E36},srcline={688}}{5C3337365C3337375C303030545C303030725C303030695C303030615C3030306E5C303030675C3030306C5C303030655C3030305C3034305C3030304D5C303030655C303030735C303030685C303030655C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030335C303030445C3030305C3034305C303030535C303030685C303030615C303030705C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {section}{\numberline {23.6}Triangle Meshes for 3D Shape Modeling}{1389}{section.23.6}\protected@file@percent }
\newlabel{sec:chapter23_triangle_meshes}{{23.6}{1389}{Triangle Meshes for 3D Shape Modeling}{section.23.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Advantages of Triangle Meshes}{1389}{section*.2962}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.17}{\ignorespaces Left: A schematic triangle mesh with explicit vertices and faces. Right: A dolphin mesh reconstructed from real-world geometry. Adapted from lecture slides.}}{1389}{figure.caption.2963}\protected@file@percent }
\newlabel{fig:chapter23_triangle_mesh_intro}{{23.17}{1389}{Left: A schematic triangle mesh with explicit vertices and faces. Right: A dolphin mesh reconstructed from real-world geometry. Adapted from lecture slides}{figure.caption.2963}{}}
\BKM@entry{id=887,dest={73756273656374696F6E2E32332E362E31},srcline={714}}{5C3337365C3337375C303030505C303030695C303030785C303030655C3030306C5C303030325C3030304D5C303030655C303030735C303030685C3030303A5C3030305C3034305C303030505C303030725C303030655C303030645C303030695C303030635C303030745C303030695C3030306E5C303030675C3030305C3034305C303030545C303030725C303030695C303030615C3030306E5C303030675C3030306C5C303030655C3030305C3034305C3030304D5C303030655C303030735C303030685C303030655C30303073}
\abx@aux@cite{0}{wang2018_pixel2mesh}
\abx@aux@segm{0}{0}{wang2018_pixel2mesh}
\@writefile{toc}{\contentsline {subsection}{\numberline {23.6.1}Pixel2Mesh: Predicting Triangle Meshes}{1390}{subsection.23.6.1}\protected@file@percent }
\newlabel{subsec:chapter23_pixel2mesh}{{23.6.1}{1390}{Pixel2Mesh: Predicting Triangle Meshes}{subsection.23.6.1}{}}
\abx@aux@backref{1858}{wang2018_pixel2mesh}{0}{1390}{1390}
\@writefile{toc}{\contentsline {paragraph}{Pre-Pixel2Mesh Landscape}{1390}{section*.2964}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Core Proposition}{1390}{section*.2965}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Key Innovations}{1390}{section*.2966}\protected@file@percent }
\abx@aux@cite{0}{smith2019_geometric}
\abx@aux@segm{0}{0}{smith2019_geometric}
\abx@aux@backref{1859}{smith2019_geometric}{0}{1391}{1391}
\@writefile{toc}{\contentsline {paragraph}{High-Level Pipeline}{1391}{section*.2967}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.18}{\ignorespaces Pixel2Mesh architecture overview. Starting from a coarse ellipsoid mesh, the model applies a sequence of mesh deformation blocks, each guided by per vertex extracted 2D image-aligned features and processed via graph convolutions. Between deformation blocks, \emph  {graph unpooling} operations increase mesh resolution by inserting new vertices at edge midpoints, preserving surface shape while enabling finer geometric detail in subsequent refinements. As the mesh evolves, vertex projections better align with informative regions in the image, improving both feature sampling and deformation accuracy.}}{1392}{figure.caption.2968}\protected@file@percent }
\newlabel{fig:chapter23_pixel2mesh_overview}{{23.18}{1392}{Pixel2Mesh architecture overview. Starting from a coarse ellipsoid mesh, the model applies a sequence of mesh deformation blocks, each guided by per vertex extracted 2D image-aligned features and processed via graph convolutions. Between deformation blocks, \emph {graph unpooling} operations increase mesh resolution by inserting new vertices at edge midpoints, preserving surface shape while enabling finer geometric detail in subsequent refinements. As the mesh evolves, vertex projections better align with informative regions in the image, improving both feature sampling and deformation accuracy}{figure.caption.2968}{}}
\@writefile{toc}{\contentsline {paragraph}{Graph-Based Feature Learning}{1392}{section*.2969}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.19}{\ignorespaces Graph convolution on meshes: each vertex aggregates features from its 1-ring neighbors using shared learnable weights.}}{1393}{figure.caption.2970}\protected@file@percent }
\newlabel{fig:chapter23_graph_convolution}{{23.19}{1393}{Graph convolution on meshes: each vertex aggregates features from its 1-ring neighbors using shared learnable weights}{figure.caption.2970}{}}
\@writefile{toc}{\contentsline {paragraph}{Predicting Vertex Positions via Graph Projection}{1393}{section*.2971}\protected@file@percent }
\abx@aux@cite{0}{wang2018_pixel2mesh}
\abx@aux@segm{0}{0}{wang2018_pixel2mesh}
\abx@aux@cite{0}{wang2018_pixel2mesh}
\abx@aux@segm{0}{0}{wang2018_pixel2mesh}
\@writefile{toc}{\contentsline {paragraph}{Edge-Based Graph Unpooling for Mesh Resolution Refinement}{1394}{section*.2972}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.20}{\ignorespaces Graph unpooling in Pixel2Mesh (adapted from \blx@tocontentsinit {0}\cite {wang2018_pixel2mesh}): (a) New vertices (black) are inserted at edge midpoints and connected via dashed edges. (b) Face-based unpooling leads to irregular vertex degrees and topological imbalance, while edge-based unpooling preserves mesh regularity and uniform structure.}}{1394}{figure.caption.2973}\protected@file@percent }
\abx@aux@backref{1861}{wang2018_pixel2mesh}{0}{1394}{1394}
\newlabel{fig:chapter23_graph_unpooling}{{23.20}{1394}{Graph unpooling in Pixel2Mesh (adapted from \cite {wang2018_pixel2mesh}): (a) New vertices (black) are inserted at edge midpoints and connected via dashed edges. (b) Face-based unpooling leads to irregular vertex degrees and topological imbalance, while edge-based unpooling preserves mesh regularity and uniform structure}{figure.caption.2973}{}}
\@writefile{toc}{\contentsline {paragraph}{Image-to-Mesh Feature Alignment}{1395}{section*.2974}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.21}{\ignorespaces Bilinear interpolation retrieves CNN features at non-integer projected positions. This mechanism resembles RoIAlign and allows smooth vertex-to-image alignment.}}{1395}{figure.caption.2975}\protected@file@percent }
\newlabel{fig:chapter23_interpolation_reminder}{{23.21}{1395}{Bilinear interpolation retrieves CNN features at non-integer projected positions. This mechanism resembles RoIAlign and allows smooth vertex-to-image alignment}{figure.caption.2975}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23.22}{\ignorespaces Image feature alignment: each mesh vertex is projected onto the input image and associated with interpolated CNN features. These features are fused with graph features and passed to the GCN.}}{1396}{figure.caption.2976}\protected@file@percent }
\newlabel{fig:chapter23_pixel2mesh_alignment}{{23.22}{1396}{Image feature alignment: each mesh vertex is projected onto the input image and associated with interpolated CNN features. These features are fused with graph features and passed to the GCN}{figure.caption.2976}{}}
\@writefile{toc}{\contentsline {paragraph}{Primary Objective: Chamfer Distance (Vertex-to-Vertex)}{1397}{section*.2978}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Laplacian Smoothness Loss}{1397}{section*.2979}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Edge Length Regularization}{1398}{section*.2980}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Normal Consistency Loss}{1398}{section*.2981}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Total Loss}{1398}{section*.2982}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Limitations and Future Directions}{1399}{section*.2983}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Enrichment 23.6.1.1: Differentiable Surface Sampling in GEOMetrics}{1399}{section*.2984}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Surface-to-Surface Comparison with Differentiable Sampling}{1399}{section*.2985}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Point-to-Surface Loss and Fine-Tuning}{1399}{section*.2986}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Differentiable Surface Sampling via Reparameterization}{1400}{section*.2987}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.23}{\ignorespaces Differentiable surface-aware Chamfer loss in GEOMetrics. Thousands of points are sampled online from predicted and ground-truth mesh surfaces using area-weighted triangle selection and barycentric coordinates. The resulting loss provides uniform supervision across the entire surface and allows gradients to flow through the reparameterized sampling process.}}{1400}{figure.caption.2988}\protected@file@percent }
\newlabel{fig:chapter23_chamfer_mesh_loss}{{23.23}{1400}{Differentiable surface-aware Chamfer loss in GEOMetrics. Thousands of points are sampled online from predicted and ground-truth mesh surfaces using area-weighted triangle selection and barycentric coordinates. The resulting loss provides uniform supervision across the entire surface and allows gradients to flow through the reparameterized sampling process}{figure.caption.2988}{}}
\@writefile{toc}{\contentsline {paragraph}{Complete Loss Formulation in GEOMetrics}{1400}{section*.2989}\protected@file@percent }
\abx@aux@cite{0}{wen2019_pixel2meshplusplus}
\abx@aux@segm{0}{0}{wen2019_pixel2meshplusplus}
\abx@aux@cite{0}{gkioxari2020_meshrcnn}
\abx@aux@segm{0}{0}{gkioxari2020_meshrcnn}
\abx@aux@cite{0}{wen2019_pixel2meshplusplus}
\abx@aux@segm{0}{0}{wen2019_pixel2meshplusplus}
\@writefile{toc}{\contentsline {paragraph}{Adaptive Mesh Refinement via Face Splitting}{1401}{section*.2990}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Advantages Over Vertex-Based Supervision}{1401}{section*.2991}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Limitations of Pixel2Mesh and the Motivation for Successor Models}{1401}{section*.2992}\protected@file@percent }
\newlabel{subsubsec:chapter23_pixel2mesh_limitations}{{23.6.1}{1401}{Limitations of Pixel2Mesh and the Motivation for Successor Models}{section*.2992}{}}
\abx@aux@backref{1862}{wen2019_pixel2meshplusplus}{0}{1401}{1401}
\abx@aux@backref{1863}{gkioxari2020_meshrcnn}{0}{1401}{1401}
\@writefile{toc}{\contentsline {paragraph}{Single-View Ambiguity and 2.5D Reconstruction}{1401}{section*.2993}\protected@file@percent }
\abx@aux@backref{1864}{wen2019_pixel2meshplusplus}{0}{1401}{1401}
\abx@aux@cite{0}{gkioxari2020_meshrcnn}
\abx@aux@segm{0}{0}{gkioxari2020_meshrcnn}
\abx@aux@cite{0}{wen2019_pixel2meshplusplus}
\abx@aux@segm{0}{0}{wen2019_pixel2meshplusplus}
\abx@aux@cite{0}{smith2019_geometric}
\abx@aux@segm{0}{0}{smith2019_geometric}
\abx@aux@cite{0}{gkioxari2020_meshrcnn}
\abx@aux@segm{0}{0}{gkioxari2020_meshrcnn}
\@writefile{toc}{\contentsline {paragraph}{Topological Rigidity and the Genus-0 Constraint}{1402}{section*.2994}\protected@file@percent }
\abx@aux@backref{1865}{gkioxari2020_meshrcnn}{0}{1402}{1402}
\@writefile{toc}{\contentsline {paragraph}{Surface-Level Supervision and Over-Smoothing Limitations}{1402}{section*.2995}\protected@file@percent }
\abx@aux@backref{1866}{wen2019_pixel2meshplusplus}{0}{1402}{1402}
\abx@aux@backref{1867}{smith2019_geometric}{0}{1402}{1402}
\@writefile{toc}{\contentsline {paragraph}{Domain Shift and Real-World Generalization}{1402}{section*.2996}\protected@file@percent }
\abx@aux@backref{1868}{gkioxari2020_meshrcnn}{0}{1402}{1402}
\abx@aux@cite{0}{wen2019_pixel2meshplusplus}
\abx@aux@segm{0}{0}{wen2019_pixel2meshplusplus}
\abx@aux@cite{0}{smith2019_geometric}
\abx@aux@segm{0}{0}{smith2019_geometric}
\abx@aux@cite{0}{gkioxari2020_meshrcnn}
\abx@aux@segm{0}{0}{gkioxari2020_meshrcnn}
\@writefile{toc}{\contentsline {paragraph}{Summary and Takeaways}{1403}{section*.2997}\protected@file@percent }
\abx@aux@backref{1869}{wen2019_pixel2meshplusplus}{0}{1403}{1403}
\abx@aux@backref{1870}{smith2019_geometric}{0}{1403}{1403}
\abx@aux@backref{1871}{gkioxari2020_meshrcnn}{0}{1403}{1403}
\BKM@entry{id=888,dest={73756273656374696F6E2E32332E362E32},srcline={1109}}{5C3337365C3337375C3030304D5C303030655C303030735C303030685C3030305C3034305C303030525C3030302D5C303030435C3030304E5C3030304E5C3030303A5C3030305C3034305C303030545C3030306F5C303030705C3030306F5C3030306C5C3030306F5C303030675C303030795C3030302D5C303030415C303030775C303030615C303030725C303030655C3030305C3034305C3030304D5C303030655C303030735C303030685C3030305C3034305C303030525C303030655C303030635C3030306F5C3030306E5C303030735C303030745C303030725C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C303030725C3030306F5C3030306D5C3030305C3034305C303030525C303030655C303030615C3030306C5C3030302D5C303030575C3030306F5C303030725C3030306C5C303030645C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C30303073}
\abx@aux@cite{0}{gkioxari2020_meshrcnn}
\abx@aux@segm{0}{0}{gkioxari2020_meshrcnn}
\abx@aux@cite{0}{he2017_maskrcnn}
\abx@aux@segm{0}{0}{he2017_maskrcnn}
\abx@aux@cite{0}{he2017_maskrcnn}
\abx@aux@segm{0}{0}{he2017_maskrcnn}
\@writefile{toc}{\contentsline {subsection}{\numberline {23.6.2}Mesh R-CNN: Topology-Aware Mesh Reconstruction from Real-World Images}{1404}{subsection.23.6.2}\protected@file@percent }
\newlabel{subsec:chapter23_meshrcnn}{{23.6.2}{1404}{Mesh R-CNN: Topology-Aware Mesh Reconstruction from Real-World Images}{subsection.23.6.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation and Key Ideas}{1404}{section*.2998}\protected@file@percent }
\abx@aux@backref{1872}{gkioxari2020_meshrcnn}{0}{1404}{1404}
\@writefile{lof}{\contentsline {figure}{\numberline {23.24}{\ignorespaces Mesh R-CNN augments Mask R-CNN~\blx@tocontentsinit {0}\cite {he2017_maskrcnn} to move from 2D instance segmentation to 3D shape prediction. The pipeline proceeds from 2D object detection to voxel prediction and finally to mesh refinement.}}{1404}{figure.caption.2999}\protected@file@percent }
\abx@aux@backref{1874}{he2017_maskrcnn}{0}{1404}{1404}
\newlabel{fig:chapter23_meshrcnn_overview1}{{23.24}{1404}{Mesh R-CNN augments Mask R-CNN~\cite {he2017_maskrcnn} to move from 2D instance segmentation to 3D shape prediction. The pipeline proceeds from 2D object detection to voxel prediction and finally to mesh refinement}{figure.caption.2999}{}}
\abx@aux@cite{0}{gkioxari2020_meshrcnn}
\abx@aux@segm{0}{0}{gkioxari2020_meshrcnn}
\abx@aux@cite{0}{gkioxari2020_meshrcnn}
\abx@aux@segm{0}{0}{gkioxari2020_meshrcnn}
\@writefile{toc}{\contentsline {paragraph}{Mask R-CNN as Backbone for 2D Instance Segmentation}{1405}{section*.3000}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The Mesh Prediction Head: A Hybrid Voxel-to-Mesh Strategy}{1405}{section*.3001}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.25}{\ignorespaces \textbf  {System overview of Mesh R-CNN.} The architecture extends Mask R-CNN by adding a 3D shape prediction head. For each detected object, the voxel branch first predicts a coarse 3D occupancy grid aligned with the camera frustum. This voxel scaffold is then converted into a watertight mesh via the \texttt  {cubify} operation and refined through a cascade of graph-based mesh deformation stages (similarly to \ref {subsec:chapter23_pixel2mesh}). Each refinement step incorporates both 3D geometry and 2D image features. \emph  {Figure reproduced from Gkioxari et al.\ \blx@tocontentsinit {0}\cite {gkioxari2020_meshrcnn}.}}}{1406}{figure.caption.3002}\protected@file@percent }
\abx@aux@backref{1876}{gkioxari2020_meshrcnn}{0}{1406}{1406}
\newlabel{fig:meshrcnn_overview}{{23.25}{1406}{\textbf {System overview of Mesh R-CNN.} The architecture extends Mask R-CNN by adding a 3D shape prediction head. For each detected object, the voxel branch first predicts a coarse 3D occupancy grid aligned with the camera frustum. This voxel scaffold is then converted into a watertight mesh via the \texttt {cubify} operation and refined through a cascade of graph-based mesh deformation stages (similarly to \ref {subsec:chapter23_pixel2mesh}). Each refinement step incorporates both 3D geometry and 2D image features. \emph {Figure reproduced from Gkioxari et al.\ \cite {gkioxari2020_meshrcnn}.}}{figure.caption.3002}{}}
\@writefile{toc}{\contentsline {subsubsection}{The Voxel Branch for Topological Flexibility}{1406}{section*.3003}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Perspective-Aware Voxel Grid via Camera Frustum Alignment}{1406}{section*.3004}\protected@file@percent }
\abx@aux@cite{0}{ksimek2013_intrinsic}
\abx@aux@segm{0}{0}{ksimek2013_intrinsic}
\abx@aux@cite{0}{gkioxari2020_meshrcnn}
\abx@aux@segm{0}{0}{gkioxari2020_meshrcnn}
\abx@aux@cite{0}{ksimek2013_intrinsic}
\abx@aux@segm{0}{0}{ksimek2013_intrinsic}
\abx@aux@cite{0}{gkioxari2020_meshrcnn}
\abx@aux@segm{0}{0}{gkioxari2020_meshrcnn}
\@writefile{lof}{\contentsline {figure}{\numberline {23.26}{\ignorespaces \textbf  {Frustum-aligned prediction space in Mesh R-CNN.} Rather than predicting occupancy in a uniform world-aligned cube, Mesh R-CNN defines voxel predictions in a space aligned with the image plane. This is achieved by applying the camera intrinsics matrix \( K \) during voxel warping; applying \( K^{-1} \) transforms the voxel coordinates back into 3D world space. The result is a truncated frustum bounded by near and far depth planes (\( z_{\text  {near}}, z_{\text  {far}} \)), centered on the detected object. This frustum-aware grid mirrors the actual viewing volume of the camera, ensuring that voxel resolution is higher for nearby regions and coarser at greater depths—naturally encoding perspective and spatial priors. For background on camera intrinsics and perspective projection, see~\blx@tocontentsinit {0}\cite {ksimek2013_intrinsic}. \emph  {Figure adapted from Gkioxari et al.\ \blx@tocontentsinit {0}\cite {gkioxari2020_meshrcnn}.}}}{1407}{figure.caption.3005}\protected@file@percent }
\abx@aux@backref{1879}{ksimek2013_intrinsic}{0}{1407}{1407}
\abx@aux@backref{1880}{gkioxari2020_meshrcnn}{0}{1407}{1407}
\newlabel{fig:chapter23_meshrcnn_prediction_space}{{23.26}{1407}{\textbf {Frustum-aligned prediction space in Mesh R-CNN.} Rather than predicting occupancy in a uniform world-aligned cube, Mesh R-CNN defines voxel predictions in a space aligned with the image plane. This is achieved by applying the camera intrinsics matrix \( K \) during voxel warping; applying \( K^{-1} \) transforms the voxel coordinates back into 3D world space. The result is a truncated frustum bounded by near and far depth planes (\( z_{\text {near}}, z_{\text {far}} \)), centered on the detected object. This frustum-aware grid mirrors the actual viewing volume of the camera, ensuring that voxel resolution is higher for nearby regions and coarser at greater depths—naturally encoding perspective and spatial priors. For background on camera intrinsics and perspective projection, see~\cite {ksimek2013_intrinsic}. \emph {Figure adapted from Gkioxari et al.\ \cite {gkioxari2020_meshrcnn}.}}{figure.caption.3005}{}}
\abx@aux@cite{0}{gkioxari2020_meshrcnn}
\abx@aux@segm{0}{0}{gkioxari2020_meshrcnn}
\abx@aux@cite{0}{gkioxari2020_meshrcnn}
\abx@aux@segm{0}{0}{gkioxari2020_meshrcnn}
\abx@aux@backref{1881}{gkioxari2020_meshrcnn}{0}{1409}{1409}
\abx@aux@backref{1882}{gkioxari2020_meshrcnn}{0}{1409}{1409}
\@writefile{toc}{\contentsline {paragraph}{Summary and Advantages}{1409}{section*.3006}\protected@file@percent }
\abx@aux@cite{0}{gkioxari2020_meshrcnn}
\abx@aux@segm{0}{0}{gkioxari2020_meshrcnn}
\@writefile{toc}{\contentsline {subsubsection}{Mesh Refinement Branch: Image-Guided Graph Deformation}{1410}{section*.3007}\protected@file@percent }
\newlabel{subsubsec:chapter23_meshrcnn_refinement}{{23.6.2}{1410}{Mesh Refinement Branch: Image-Guided Graph Deformation}{section*.3007}{}}
\@writefile{toc}{\contentsline {paragraph}{Fixed-Topology Refinement Pipeline}{1410}{section*.3008}\protected@file@percent }
\abx@aux@backref{1883}{gkioxari2020_meshrcnn}{0}{1410}{1410}
\@writefile{toc}{\contentsline {paragraph}{Loss Functions}{1410}{section*.3009}\protected@file@percent }
\abx@aux@cite{0}{gkioxari2020_meshrcnn}
\abx@aux@segm{0}{0}{gkioxari2020_meshrcnn}
\abx@aux@backref{1884}{gkioxari2020_meshrcnn}{0}{1411}{1411}
\@writefile{toc}{\contentsline {paragraph}{Summary}{1411}{section*.3010}\protected@file@percent }
\abx@aux@cite{0}{chang2015_shapenet}
\abx@aux@segm{0}{0}{chang2015_shapenet}
\abx@aux@cite{0}{sun2018_pix3d}
\abx@aux@segm{0}{0}{sun2018_pix3d}
\@writefile{toc}{\contentsline {subsubsection}{Experiments and Ablations}{1412}{section*.3011}\protected@file@percent }
\newlabel{subsec:chapter23_meshrcnn_experiments}{{23.6.2}{1412}{Experiments and Ablations}{section*.3011}{}}
\@writefile{toc}{\contentsline {paragraph}{Datasets}{1412}{section*.3012}\protected@file@percent }
\abx@aux@backref{1885}{chang2015_shapenet}{0}{1412}{1412}
\abx@aux@backref{1886}{sun2018_pix3d}{0}{1412}{1412}
\@writefile{toc}{\contentsline {paragraph}{Evaluation Metrics}{1412}{section*.3013}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Key Results on ShapeNet}{1412}{section*.3014}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.27}{\ignorespaces Qualitative ShapeNet comparisons. While Pixel2Mesh+ fails to represent holes due to spherical initialization, Mesh R-CNN produces topologically faithful reconstructions for chairs, tables, and other perforated objects (Adapted from ICCV 2019 talk).}}{1412}{figure.caption.3015}\protected@file@percent }
\newlabel{fig:chapter23_shapenet_holes}{{23.27}{1412}{Qualitative ShapeNet comparisons. While Pixel2Mesh+ fails to represent holes due to spherical initialization, Mesh R-CNN produces topologically faithful reconstructions for chairs, tables, and other perforated objects (Adapted from ICCV 2019 talk)}{figure.caption.3015}{}}
\@writefile{toc}{\contentsline {paragraph}{Key Results on Pix3D}{1413}{section*.3016}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.28}{\ignorespaces Qualitative Pix3D reconstructions. Mesh R-CNN successfully captures complex scene structures including desks, tables, and bookshelves.}}{1413}{figure.caption.3017}\protected@file@percent }
\newlabel{fig:chapter23_pix3d_qualitative}{{23.28}{1413}{Qualitative Pix3D reconstructions. Mesh R-CNN successfully captures complex scene structures including desks, tables, and bookshelves}{figure.caption.3017}{}}
\@writefile{toc}{\contentsline {paragraph}{Amodal Completion}{1413}{section*.3018}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.29}{\ignorespaces Amodal shape completion: Mesh R-CNN reconstructs full geometry despite occlusion (e.g., sofa behind dog and chair).}}{1413}{figure.caption.3019}\protected@file@percent }
\newlabel{fig:chapter23_amodal_completion}{{23.29}{1413}{Amodal shape completion: Mesh R-CNN reconstructs full geometry despite occlusion (e.g., sofa behind dog and chair)}{figure.caption.3019}{}}
\abx@aux@cite{0}{gkioxari2020_meshrcnn}
\abx@aux@segm{0}{0}{gkioxari2020_meshrcnn}
\abx@aux@cite{0}{gkioxari2020_meshrcnn}
\abx@aux@segm{0}{0}{gkioxari2020_meshrcnn}
\@writefile{toc}{\contentsline {paragraph}{Failure Modes}{1414}{section*.3020}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.30}{\ignorespaces Failure case: segmentation noise in 2D leads to missing geometry in the 3D mesh.}}{1414}{figure.caption.3021}\protected@file@percent }
\newlabel{fig:chapter23_failure_modes}{{23.30}{1414}{Failure case: segmentation noise in 2D leads to missing geometry in the 3D mesh}{figure.caption.3021}{}}
\@writefile{toc}{\contentsline {paragraph}{Ablation Studies}{1414}{section*.3022}\protected@file@percent }
\abx@aux@backref{1887}{gkioxari2020_meshrcnn}{0}{1414}{1414}
\abx@aux@backref{1888}{gkioxari2020_meshrcnn}{0}{1414}{1414}
\abx@aux@cite{0}{gkioxari2020_meshrcnn}
\abx@aux@segm{0}{0}{gkioxari2020_meshrcnn}
\abx@aux@cite{0}{gkioxari2020_meshrcnn}
\abx@aux@segm{0}{0}{gkioxari2020_meshrcnn}
\abx@aux@cite{0}{wang2018_pixel2mesh}
\abx@aux@segm{0}{0}{wang2018_pixel2mesh}
\abx@aux@cite{0}{gkioxari2020_meshrcnn}
\abx@aux@segm{0}{0}{gkioxari2020_meshrcnn}
\abx@aux@cite{0}{gkioxari2020_meshrcnn}
\abx@aux@segm{0}{0}{gkioxari2020_meshrcnn}
\abx@aux@cite{0}{wang2018_pixel2mesh}
\abx@aux@segm{0}{0}{wang2018_pixel2mesh}
\abx@aux@cite{0}{wang2018_pixel2mesh}
\abx@aux@segm{0}{0}{wang2018_pixel2mesh}
\abx@aux@backref{1889}{gkioxari2020_meshrcnn}{0}{1415}{1415}
\abx@aux@backref{1890}{gkioxari2020_meshrcnn}{0}{1415}{1415}
\abx@aux@backref{1891}{wang2018_pixel2mesh}{0}{1415}{1415}
\abx@aux@backref{1892}{gkioxari2020_meshrcnn}{0}{1415}{1415}
\abx@aux@backref{1893}{gkioxari2020_meshrcnn}{0}{1415}{1415}
\abx@aux@backref{1894}{wang2018_pixel2mesh}{0}{1415}{1415}
\abx@aux@backref{1895}{wang2018_pixel2mesh}{0}{1415}{1415}
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{1415}{section*.3023}\protected@file@percent }
\BKM@entry{id=889,dest={73656374696F6E2E32332E37},srcline={1511}}{5C3337365C3337375C303030495C3030306D5C303030705C3030306C5C303030695C303030635C303030695C303030745C3030305C3034305C303030535C303030755C303030725C303030665C303030615C303030635C303030655C3030305C3034305C303030525C303030655C303030705C303030725C303030655C303030735C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\abx@aux@cite{0}{mescheder2019_occupancy}
\abx@aux@segm{0}{0}{mescheder2019_occupancy}
\abx@aux@cite{0}{park2019_deepsdf}
\abx@aux@segm{0}{0}{park2019_deepsdf}
\@writefile{toc}{\contentsline {section}{\numberline {23.7}Implicit Surface Representations}{1416}{section.23.7}\protected@file@percent }
\newlabel{sec:chapter23_implicit_surfaces}{{23.7}{1416}{Implicit Surface Representations}{section.23.7}{}}
\@writefile{toc}{\contentsline {paragraph}{From Discrete to Continuous Geometry}{1416}{section*.3024}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Occupancy Fields vs.\ Signed Distance Functions}{1416}{section*.3025}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.31}{\ignorespaces Left: explicit triangle mesh. Right: corresponding implicit field where the decision boundary \( f(\mathbf  {x}) = 0.5 \) defines the surface.}}{1416}{figure.caption.3026}\protected@file@percent }
\newlabel{fig:chapter23_implicit_overview}{{23.31}{1416}{Left: explicit triangle mesh. Right: corresponding implicit field where the decision boundary \( f(\mathbf {x}) = 0.5 \) defines the surface}{figure.caption.3026}{}}
\@writefile{toc}{\contentsline {paragraph}{Neural Implicit Models}{1416}{section*.3027}\protected@file@percent }
\abx@aux@backref{1896}{mescheder2019_occupancy}{0}{1416}{1416}
\abx@aux@backref{1897}{park2019_deepsdf}{0}{1416}{1416}
\BKM@entry{id=890,dest={73756273656374696F6E2E32332E372E31},srcline={1539}}{5C3337365C3337375C3030304D5C303030755C3030306C5C303030745C303030695C3030302D5C303030535C303030635C303030615C3030306C5C303030655C3030305C3034305C303030495C303030735C3030306F5C303030535C303030755C303030725C303030665C303030615C303030635C303030655C3030305C3034305C303030455C303030785C303030745C303030725C303030615C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030305C3035305C3030304D5C303030495C303030535C303030455C3030305C303531}
\abx@aux@cite{0}{mescheder2019_occupancy}
\abx@aux@segm{0}{0}{mescheder2019_occupancy}
\abx@aux@cite{0}{lorensen1987_marchingcubes}
\abx@aux@segm{0}{0}{lorensen1987_marchingcubes}
\@writefile{toc}{\contentsline {paragraph}{Why Surface Extraction Is Required}{1417}{section*.3028}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {23.7.1}Multi-Scale IsoSurface Extraction (MISE)}{1417}{subsection.23.7.1}\protected@file@percent }
\newlabel{subsec:chapter23_mise}{{23.7.1}{1417}{Multi-Scale IsoSurface Extraction (MISE)}{subsection.23.7.1}{}}
\abx@aux@backref{1898}{mescheder2019_occupancy}{0}{1417}{1417}
\abx@aux@backref{1899}{lorensen1987_marchingcubes}{0}{1417}{1417}
\abx@aux@cite{0}{mescheder2019_occupancy}
\abx@aux@segm{0}{0}{mescheder2019_occupancy}
\abx@aux@cite{0}{mescheder2019_occupancy}
\abx@aux@segm{0}{0}{mescheder2019_occupancy}
\BKM@entry{id=891,dest={73756273656374696F6E2E32332E372E32},srcline={1575}}{5C3337365C3337375C303030495C3030306D5C303030705C3030306C5C303030695C303030635C303030695C303030745C3030305C3034305C303030535C303030755C303030725C303030665C303030615C303030635C303030655C3030305C3034305C303030415C303030645C303030765C303030615C3030306E5C303030745C303030615C303030675C303030655C303030735C3030305C3034305C3030305C3034365C3030305C3034305C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{lof}{\contentsline {figure}{\numberline {23.32}{\ignorespaces Multi-resolution surface extraction in Occupancy Networks \blx@tocontentsinit {0}\cite {mescheder2019_occupancy}. The function is queried across a hierarchical voxel grid, refined at boundaries, and meshed via Marching Cubes.}}{1418}{figure.caption.3029}\protected@file@percent }
\abx@aux@backref{1901}{mescheder2019_occupancy}{0}{1418}{1418}
\newlabel{fig:chapter23_implicit_meshing}{{23.32}{1418}{Multi-resolution surface extraction in Occupancy Networks \cite {mescheder2019_occupancy}. The function is queried across a hierarchical voxel grid, refined at boundaries, and meshed via Marching Cubes}{figure.caption.3029}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {23.7.2}Implicit Surface Advantages \& Limitations}{1418}{subsection.23.7.2}\protected@file@percent }
\newlabel{subsec:chapter23_implicit_sufrace_advantages_limitations}{{23.7.2}{1418}{Implicit Surface Advantages \& Limitations}{subsection.23.7.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Advantages}{1418}{section*.3030}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Limitations}{1418}{section*.3031}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Relation to Octrees and Voxel Refinement}{1418}{section*.3032}\protected@file@percent }
\BKM@entry{id=892,dest={73656374696F6E2E32332E38},srcline={1599}}{5C3337365C3337375C303030475C303030655C3030306E5C303030655C303030725C303030615C3030306C5C3030305C3034305C303030335C303030445C3030305C3034305C303030545C3030306F5C303030705C303030695C303030635C30303073}
\BKM@entry{id=893,dest={73756273656374696F6E2E32332E382E31},srcline={1604}}{5C3337365C3337375C303030535C303030685C303030615C303030705C303030655C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C303030735C3030306F5C3030306E5C3030305C3034305C3030304D5C303030655C303030745C303030725C303030695C303030635C30303073}
\abx@aux@cite{0}{tatarchenko2019_singleview}
\abx@aux@segm{0}{0}{tatarchenko2019_singleview}
\@writefile{toc}{\contentsline {section}{\numberline {23.8}General 3D Topics}{1419}{section.23.8}\protected@file@percent }
\newlabel{sec:chapter23_general_3d_topics}{{23.8}{1419}{General 3D Topics}{section.23.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {23.8.1}Shape Comparison Metrics}{1419}{subsection.23.8.1}\protected@file@percent }
\newlabel{subsec:chapter23_shape_metrics}{{23.8.1}{1419}{Shape Comparison Metrics}{subsection.23.8.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Voxel IoU: Intuitive but Limited}{1419}{section*.3033}\protected@file@percent }
\abx@aux@backref{1902}{tatarchenko2019_singleview}{0}{1419}{1419}
\@writefile{lof}{\contentsline {figure}{\numberline {23.33}{\ignorespaces Limitations of 3D IoU: (Left) Summary of common pitfalls. (Right) Visualization of the metric failing to capture geometry differences in a kite example.}}{1419}{figure.caption.3034}\protected@file@percent }
\newlabel{fig:chapter23_voxel_iou}{{23.33}{1419}{Limitations of 3D IoU: (Left) Summary of common pitfalls. (Right) Visualization of the metric failing to capture geometry differences in a kite example}{figure.caption.3034}{}}
\@writefile{toc}{\contentsline {paragraph}{Chamfer Distance: Simple and Effective}{1419}{section*.3035}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.34}{\ignorespaces Comparison of Chamfer Distance (CD) against a ground-truth table. Both predicted shapes are structurally similar chairs that share the same flat seat base as the table, differing primarily in their back support. The chair with shorter back support (right) receives a lower CD of 0.15, as its geometry more closely matches the table. The chair with taller back support (left) receives a higher CD of 0.21, despite the backrest being the only mismatch. This illustrates CD’s sensitivity to peripheral outliers: small localized differences—far from the main shape—can disproportionately inflate the score.}}{1420}{figure.caption.3036}\protected@file@percent }
\newlabel{fig:chapter23_chamfer_outlier}{{23.34}{1420}{Comparison of Chamfer Distance (CD) against a ground-truth table. Both predicted shapes are structurally similar chairs that share the same flat seat base as the table, differing primarily in their back support. The chair with shorter back support (right) receives a lower CD of 0.15, as its geometry more closely matches the table. The chair with taller back support (left) receives a higher CD of 0.21, despite the backrest being the only mismatch. This illustrates CD’s sensitivity to peripheral outliers: small localized differences—far from the main shape—can disproportionately inflate the score}{figure.caption.3036}{}}
\@writefile{toc}{\contentsline {paragraph}{F1 Score: Thresholded Surface Accuracy}{1420}{section*.3037}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.35}{\ignorespaces F1 score-based shape evaluation: More robust to outliers and informative across different geometric scales.}}{1420}{figure.caption.3038}\protected@file@percent }
\newlabel{fig:chapter23_f1_example}{{23.35}{1420}{F1 score-based shape evaluation: More robust to outliers and informative across different geometric scales}{figure.caption.3038}{}}
\BKM@entry{id=894,dest={73756273656374696F6E2E32332E382E32},srcline={1665}}{5C3337365C3337375C303030435C303030615C3030306D5C303030655C303030725C303030615C3030305C3034305C303030435C3030306F5C3030306F5C303030725C303030645C303030695C3030306E5C303030615C303030745C303030655C303030735C3030303A5C3030305C3034305C303030435C303030615C3030306E5C3030306F5C3030306E5C303030695C303030635C303030615C3030306C5C3030305C3034305C303030765C303030735C3030302E5C3030305C3034305C303030565C303030695C303030655C303030775C3030302D5C303030415C3030306C5C303030695C303030675C3030306E5C303030655C30303064}
\abx@aux@cite{0}{shin2018_pixelsvoxelsviews}
\abx@aux@segm{0}{0}{shin2018_pixelsvoxelsviews}
\@writefile{toc}{\contentsline {paragraph}{Threshold Sensitivity}{1421}{section*.3039}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.36}{\ignorespaces Comparative behavior of shape metrics across thresholds. F1 curves often reveal performance differences that CD and IoU miss.}}{1421}{figure.caption.3040}\protected@file@percent }
\newlabel{fig:chapter23_f1_vs_metrics}{{23.36}{1421}{Comparative behavior of shape metrics across thresholds. F1 curves often reveal performance differences that CD and IoU miss}{figure.caption.3040}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {23.8.2}Camera Coordinates: Canonical vs.\ View-Aligned}{1421}{subsection.23.8.2}\protected@file@percent }
\newlabel{subsec:chapter23_coordinate_systems}{{23.8.2}{1421}{Camera Coordinates: Canonical vs.\ View-Aligned}{subsection.23.8.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23.37}{\ignorespaces Canonical (mid column) vs view-aligned (last column) coordinate systems for chair reconstruction. The latter preserves direct alignment with the input image (first column).}}{1421}{figure.caption.3041}\protected@file@percent }
\newlabel{fig:chapter23_coord_systems}{{23.37}{1421}{Canonical (mid column) vs view-aligned (last column) coordinate systems for chair reconstruction. The latter preserves direct alignment with the input image (first column)}{figure.caption.3041}{}}
\@writefile{toc}{\contentsline {paragraph}{Canonical Coordinates}{1421}{section*.3042}\protected@file@percent }
\BKM@entry{id=895,dest={73756273656374696F6E2E32332E382E33},srcline={1698}}{5C3337365C3337375C303030335C303030445C3030305C3034305C303030445C303030615C303030745C303030615C303030735C303030655C303030745C30303073}
\@writefile{toc}{\contentsline {paragraph}{View Coordinates}{1422}{section*.3043}\protected@file@percent }
\abx@aux@backref{1903}{shin2018_pixelsvoxelsviews}{0}{1422}{1422}
\@writefile{lof}{\contentsline {figure}{\numberline {23.38}{\ignorespaces Feature alignment: View coordinates maintain spatial consistency between input features and predicted geometry.}}{1422}{figure.caption.3044}\protected@file@percent }
\newlabel{fig:chapter23_view_alignment}{{23.38}{1422}{Feature alignment: View coordinates maintain spatial consistency between input features and predicted geometry}{figure.caption.3044}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23.39}{\ignorespaces Generalization gap: Canonical prediction overfits to training shapes. View-aligned models perform better on novel objects and categories.}}{1422}{figure.caption.3045}\protected@file@percent }
\newlabel{fig:chapter23_view_generalization}{{23.39}{1422}{Generalization gap: Canonical prediction overfits to training shapes. View-aligned models perform better on novel objects and categories}{figure.caption.3045}{}}
\@writefile{toc}{\contentsline {paragraph}{Conclusion}{1422}{section*.3046}\protected@file@percent }
\abx@aux@cite{0}{chang2015_shapenet}
\abx@aux@segm{0}{0}{chang2015_shapenet}
\abx@aux@cite{0}{sun2018_pix3d}
\abx@aux@segm{0}{0}{sun2018_pix3d}
\@writefile{toc}{\contentsline {subsection}{\numberline {23.8.3}3D Datasets}{1423}{subsection.23.8.3}\protected@file@percent }
\newlabel{subsec:chapter23_datasets}{{23.8.3}{1423}{3D Datasets}{subsection.23.8.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Core Benchmarks for Single-View Reconstruction}{1423}{section*.3047}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ShapeNet}{1423}{section*.3048}\protected@file@percent }
\abx@aux@backref{1904}{chang2015_shapenet}{0}{1423}{1423}
\@writefile{toc}{\contentsline {paragraph}{Pix3D}{1423}{section*.3049}\protected@file@percent }
\abx@aux@backref{1905}{sun2018_pix3d}{0}{1423}{1423}
\@writefile{lof}{\contentsline {figure}{\numberline {23.40}{\ignorespaces Comparison of ShapeNet (left) and Pix3D (right). ShapeNet offers synthetic scale and geometric cleanliness; Pix3D provides real-world variation and appearance realism.}}{1423}{figure.caption.3050}\protected@file@percent }
\newlabel{fig:chapter23_datasets}{{23.40}{1423}{Comparison of ShapeNet (left) and Pix3D (right). ShapeNet offers synthetic scale and geometric cleanliness; Pix3D provides real-world variation and appearance realism}{figure.caption.3050}{}}
\abx@aux@cite{0}{reizenstein2021_co3d}
\abx@aux@segm{0}{0}{reizenstein2021_co3d}
\abx@aux@cite{0}{deitke2023_objaverse}
\abx@aux@segm{0}{0}{deitke2023_objaverse}
\abx@aux@cite{0}{dai2017_scannet}
\abx@aux@segm{0}{0}{dai2017_scannet}
\abx@aux@cite{0}{koch2019_abc}
\abx@aux@segm{0}{0}{koch2019_abc}
\abx@aux@cite{0}{wu2015_shapenets}
\abx@aux@segm{0}{0}{wu2015_shapenets}
\abx@aux@cite{0}{mo2019_partnet}
\abx@aux@segm{0}{0}{mo2019_partnet}
\abx@aux@cite{0}{ahmadyan2021_objectron}
\abx@aux@segm{0}{0}{ahmadyan2021_objectron}
\BKM@entry{id=896,dest={73656374696F6E2E32332E39},srcline={1781}}{5C3337365C3337375C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C303030525C303030615C303030645C303030695C303030615C3030306E5C303030635C303030655C3030305C3034305C303030465C303030695C303030655C3030306C5C303030645C303030735C3030305C3034305C3030305C3035305C3030304E5C303030655C303030525C303030465C3030305C303531}
\BKM@entry{id=897,dest={73756273656374696F6E2E32332E392E31},srcline={1784}}{5C3337365C3337375C303030505C303030725C3030306F5C303030625C3030306C5C303030655C3030306D5C3030305C3034305C303030535C303030655C303030745C303030755C303030705C3030303A5C3030305C3034305C3030304E5C3030306F5C303030765C303030655C3030306C5C3030305C3034305C303030565C303030695C303030655C303030775C3030305C3034305C303030535C303030795C3030306E5C303030745C303030685C303030655C303030735C303030695C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C3030304B5C3030306E5C3030306F5C303030775C3030306E5C3030305C3034305C303030435C303030615C3030306D5C303030655C303030725C303030615C30303073}
\@writefile{toc}{\contentsline {paragraph}{Training Strategy}{1424}{section*.3051}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{CO3D: Common Objects in 3D}{1424}{section*.3052}\protected@file@percent }
\abx@aux@backref{1906}{reizenstein2021_co3d}{0}{1424}{1424}
\@writefile{toc}{\contentsline {paragraph}{Objaverse and Objaverse-XL}{1424}{section*.3053}\protected@file@percent }
\abx@aux@backref{1907}{deitke2023_objaverse}{0}{1424}{1424}
\@writefile{toc}{\contentsline {paragraph}{ScanNet}{1424}{section*.3054}\protected@file@percent }
\abx@aux@backref{1908}{dai2017_scannet}{0}{1424}{1424}
\@writefile{toc}{\contentsline {paragraph}{Supplementary Datasets}{1424}{section*.3055}\protected@file@percent }
\abx@aux@backref{1909}{koch2019_abc}{0}{1424}{1424}
\abx@aux@backref{1910}{wu2015_shapenets}{0}{1424}{1424}
\abx@aux@backref{1911}{mo2019_partnet}{0}{1424}{1424}
\abx@aux@backref{1912}{ahmadyan2021_objectron}{0}{1424}{1424}
\@writefile{toc}{\contentsline {paragraph}{Summary}{1424}{section*.3056}\protected@file@percent }
\abx@aux@cite{0}{seitz2006_multiview}
\abx@aux@segm{0}{0}{seitz2006_multiview}
\abx@aux@cite{0}{zhang2001_shape_motion}
\abx@aux@segm{0}{0}{zhang2001_shape_motion}
\abx@aux@cite{0}{schonberger2016_structure}
\abx@aux@segm{0}{0}{schonberger2016_structure}
\@writefile{toc}{\contentsline {section}{\numberline {23.9}Neural Radiance Fields (NeRF)}{1425}{section.23.9}\protected@file@percent }
\newlabel{sec:chapter23_nerf}{{23.9}{1425}{Neural Radiance Fields (NeRF)}{section.23.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {23.9.1}Problem Setup: Novel View Synthesis with Known Cameras}{1425}{subsection.23.9.1}\protected@file@percent }
\newlabel{subsec:chapter23_nerf_setup}{{23.9.1}{1425}{Problem Setup: Novel View Synthesis with Known Cameras}{subsection.23.9.1}{}}
\@writefile{toc}{\contentsline {paragraph}{What Is Novel View Synthesis?}{1425}{section*.3057}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.41}{\ignorespaces NeRF performs novel view synthesis: given images from known viewpoints, it renders unseen views (example: Lego bulldozer).}}{1425}{figure.caption.3058}\protected@file@percent }
\newlabel{fig:chapter23_nerf_view_synthesis}{{23.41}{1425}{NeRF performs novel view synthesis: given images from known viewpoints, it renders unseen views (example: Lego bulldozer)}{figure.caption.3058}{}}
\@writefile{toc}{\contentsline {paragraph}{Limitations of Traditional Novel View Synthesis Pipelines}{1425}{section*.3059}\protected@file@percent }
\abx@aux@backref{1913}{schonberger2016_structure}{0}{1425}{1425}
\abx@aux@backref{1914}{seitz2006_multiview}{0}{1425}{1425}
\abx@aux@backref{1915}{zhang2001_shape_motion}{0}{1425}{1425}
\abx@aux@cite{0}{seitz2006_multiview}
\abx@aux@segm{0}{0}{seitz2006_multiview}
\abx@aux@cite{0}{zhang2015_mvs_illum}
\abx@aux@segm{0}{0}{zhang2015_mvs_illum}
\abx@aux@cite{0}{aanaes2016_mvs}
\abx@aux@segm{0}{0}{aanaes2016_mvs}
\abx@aux@cite{0}{zhang2001_shape_motion}
\abx@aux@segm{0}{0}{zhang2001_shape_motion}
\abx@aux@cite{0}{seitz2006_multiview}
\abx@aux@segm{0}{0}{seitz2006_multiview}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{nerf_survey2023}
\abx@aux@segm{0}{0}{nerf_survey2023}
\BKM@entry{id=898,dest={73756273656374696F6E2E32332E392E32},srcline={1830}}{5C3337365C3337375C303030415C3030305C3034305C3030304E5C303030655C303030775C3030305C3034305C303030505C303030615C303030725C303030615C303030645C303030695C303030675C3030306D5C3030303A5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C303030465C303030695C303030655C3030306C5C303030645C30303073}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@backref{1916}{seitz2006_multiview}{0}{1426}{1426}
\abx@aux@backref{1917}{zhang2015_mvs_illum}{0}{1426}{1426}
\abx@aux@backref{1918}{aanaes2016_mvs}{0}{1426}{1426}
\abx@aux@backref{1919}{zhang2001_shape_motion}{0}{1426}{1426}
\abx@aux@backref{1920}{seitz2006_multiview}{0}{1426}{1426}
\abx@aux@backref{1921}{nerf_survey2023}{0}{1426}{1426}
\abx@aux@backref{1922}{mildenhall2020_nerf}{0}{1426}{1426}
\@writefile{toc}{\contentsline {subsection}{\numberline {23.9.2}A New Paradigm: Neural Fields}{1426}{subsection.23.9.2}\protected@file@percent }
\newlabel{subsec:chapter23_nerf_neuralfields}{{23.9.2}{1426}{A New Paradigm: Neural Fields}{subsection.23.9.2}{}}
\abx@aux@backref{1923}{mildenhall2020_nerf}{0}{1426}{1426}
\@writefile{toc}{\contentsline {paragraph}{Scene Representation}{1426}{section*.3060}\protected@file@percent }
\BKM@entry{id=899,dest={73756273656374696F6E2E32332E392E33},srcline={1855}}{5C3337365C3337375C303030435C303030615C3030306D5C303030655C303030725C303030615C3030305C3034305C303030505C303030615C303030725C303030615C3030306D5C303030655C303030745C303030655C303030725C303030735C3030305C3034305C303030615C303030735C3030305C3034305C303030615C3030305C3034305C303030505C303030725C303030655C303030725C303030655C303030715C303030755C303030695C303030735C303030695C303030745C30303065}
\abx@aux@cite{0}{schonberger2016_structure}
\abx@aux@segm{0}{0}{schonberger2016_structure}
\@writefile{toc}{\contentsline {paragraph}{Why Only Direction Matters}{1427}{section*.3061}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Training Supervision: From Images to Rays}{1427}{section*.3062}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {23.9.3}Camera Parameters as a Prerequisite}{1427}{subsection.23.9.3}\protected@file@percent }
\newlabel{subsec:chapter23_nerf_camera_params}{{23.9.3}{1427}{Camera Parameters as a Prerequisite}{subsection.23.9.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Recovering Camera Parameters via SfM}{1428}{section*.3063}\protected@file@percent }
\abx@aux@backref{1924}{schonberger2016_structure}{0}{1428}{1428}
\BKM@entry{id=900,dest={73756273656374696F6E2E32332E392E34},srcline={1902}}{5C3337365C3337375C303030565C3030306F5C3030306C5C303030755C3030306D5C303030655C3030305C3034305C303030525C303030655C3030306E5C303030645C303030655C303030725C303030695C3030306E5C303030675C3030303A5C3030305C3034305C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030525C303030615C303030795C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C303030505C303030695C303030785C303030655C3030306C5C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {23.9.4}Volume Rendering: From Rays to Pixels}{1429}{subsection.23.9.4}\protected@file@percent }
\newlabel{subsec:chapter23_nerf_volume_rendering}{{23.9.4}{1429}{Volume Rendering: From Rays to Pixels}{subsection.23.9.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23.42}{\ignorespaces Volume rendering setup from the pinhole camera model. Light reflects off surfaces in the scene and enters the camera through a pinhole, projecting onto a virtual image plane. The key questions at each sampled 3D point are: (1) how much light does it emit? and (2) how opaque is it? Points on the object (e.g., car) emit colored light and are opaque (\(\sigma = 1\)), while empty space emits no light and is fully transparent (\(\sigma = 0\)).}}{1429}{figure.caption.3064}\protected@file@percent }
\newlabel{fig:chapter23_nerf_pinhole}{{23.42}{1429}{Volume rendering setup from the pinhole camera model. Light reflects off surfaces in the scene and enters the camera through a pinhole, projecting onto a virtual image plane. The key questions at each sampled 3D point are: (1) how much light does it emit? and (2) how opaque is it? Points on the object (e.g., car) emit colored light and are opaque (\(\sigma = 1\)), while empty space emits no light and is fully transparent (\(\sigma = 0\))}{figure.caption.3064}{}}
\@writefile{toc}{\contentsline {paragraph}{Discretizing the Rendering Equation: Stratified Sampling and Alpha Compositing}{1431}{section*.3065}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.43}{\ignorespaces Discrete volume rendering along ray \( \mathbf  {r}(t) = \mathbf  {o} + t\mathbf  {d} \). The network predicts density \( \sigma _i \) and color \( \mathbf  {c}_i \) at \( N \) points sampled along the ray. Each segment’s contribution is attenuated by accumulated transmittance \( T_i \) and its own opacity \( \alpha _i \).}}{1432}{figure.caption.3066}\protected@file@percent }
\newlabel{fig:chapter23_nerf_volumerendering}{{23.43}{1432}{Discrete volume rendering along ray \( \mathbf {r}(t) = \mathbf {o} + t\mathbf {d} \). The network predicts density \( \sigma _i \) and color \( \mathbf {c}_i \) at \( N \) points sampled along the ray. Each segment’s contribution is attenuated by accumulated transmittance \( T_i \) and its own opacity \( \alpha _i \)}{figure.caption.3066}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23.44}{\ignorespaces Discrete approximation of volume rendering along a ray \( \mathbf  {r}(t) = \mathbf  {o} + t\mathbf  {d} \). The network samples \( N = 4 \) points at depths \( t_1, t_2, t_3, t_4 \) within the interval \( [t_n, t_f] \), spaced by intervals \( \delta _i = t_{i+1} - t_i \). For each point \( \mathbf  {r}(t_i) \), the MLP predicts a density \( \sigma _i \) and view-dependent color \( \mathbf  {c}_i \). These are combined via alpha compositing to produce an estimated pixel color \( \hat  {\mathcal  {C}}(\mathbf  {r}) \), which is supervised (i.e., compared to $\mathcal  {C}(\mathbf  {r})$) to match the ground truth image color via \( \ell _2 \) loss.}}{1432}{figure.caption.3067}\protected@file@percent }
\newlabel{fig:chapter23_nerf_sampling}{{23.44}{1432}{Discrete approximation of volume rendering along a ray \( \mathbf {r}(t) = \mathbf {o} + t\mathbf {d} \). The network samples \( N = 4 \) points at depths \( t_1, t_2, t_3, t_4 \) within the interval \( [t_n, t_f] \), spaced by intervals \( \delta _i = t_{i+1} - t_i \). For each point \( \mathbf {r}(t_i) \), the MLP predicts a density \( \sigma _i \) and view-dependent color \( \mathbf {c}_i \). These are combined via alpha compositing to produce an estimated pixel color \( \hat {\mathcal {C}}(\mathbf {r}) \), which is supervised (i.e., compared to $\mathcal {C}(\mathbf {r})$) to match the ground truth image color via \( \ell _2 \) loss}{figure.caption.3067}{}}
\@writefile{toc}{\contentsline {paragraph}{From Pixel Color to Supervised 3D Sampling}{1433}{section*.3068}\protected@file@percent }
\newlabel{par:chapter23_nerf_pixel_supervision}{{23.9.4}{1433}{From Pixel Color to Supervised 3D Sampling}{section*.3068}{}}
\@writefile{toc}{\contentsline {paragraph}{Hierarchical Sampling: Coarse-to-Fine Supervision and Loss}{1433}{section*.3069}\protected@file@percent }
\newlabel{par:chapter23_nerf_hierarchical_sampling}{{23.9.4}{1433}{Hierarchical Sampling: Coarse-to-Fine Supervision and Loss}{section*.3069}{}}
\@writefile{toc}{\contentsline {paragraph}{Why This Works: Learning Geometry from Pixel Colors}{1434}{section*.3070}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{A Differentiable Rendering Engine for View Synthesis}{1434}{section*.3071}\protected@file@percent }
\BKM@entry{id=901,dest={73756273656374696F6E2E32332E392E35},srcline={2157}}{5C3337365C3337375C303030505C303030725C303030615C303030635C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C303030495C3030306D5C303030705C3030306C5C303030655C3030306D5C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030445C303030655C303030745C303030615C303030695C3030306C5C30303073}
\abx@aux@cite{0}{rahaman2019_spectralbias}
\abx@aux@segm{0}{0}{rahaman2019_spectralbias}
\abx@aux@cite{0}{ramasinghe2022_coordmlp}
\abx@aux@segm{0}{0}{ramasinghe2022_coordmlp}
\abx@aux@cite{0}{rahaman2019_spectralbias}
\abx@aux@segm{0}{0}{rahaman2019_spectralbias}
\abx@aux@cite{0}{ramasinghe2022_coordmlp}
\abx@aux@segm{0}{0}{ramasinghe2022_coordmlp}
\@writefile{toc}{\contentsline {subsection}{\numberline {23.9.5}Practical Implementation Details}{1435}{subsection.23.9.5}\protected@file@percent }
\newlabel{subsec:chapter23_practical_nerf}{{23.9.5}{1435}{Practical Implementation Details}{subsection.23.9.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{Positional Encoding for High-Frequency Detail}{1435}{section*.3072}\protected@file@percent }
\newlabel{subsubsec:chapter23_nerf_positional_encoding}{{23.9.5}{1435}{Positional Encoding for High-Frequency Detail}{section*.3072}{}}
\abx@aux@backref{1925}{rahaman2019_spectralbias}{0}{1435}{1435}
\abx@aux@backref{1926}{ramasinghe2022_coordmlp}{0}{1435}{1435}
\abx@aux@backref{1927}{rahaman2019_spectralbias}{0}{1435}{1435}
\abx@aux@backref{1928}{ramasinghe2022_coordmlp}{0}{1435}{1435}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{tancik2020_fourierfeatures}
\abx@aux@segm{0}{0}{tancik2020_fourierfeatures}
\abx@aux@backref{1929}{mildenhall2020_nerf}{0}{1436}{1436}
\abx@aux@backref{1930}{tancik2020_fourierfeatures}{0}{1436}{1436}
\@writefile{toc}{\contentsline {paragraph}{Intuition Behind Positional Encoding}{1436}{section*.3073}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Network Architecture and Functional Mapping}{1437}{section*.3074}\protected@file@percent }
\newlabel{subsec:chapter23_architecture}{{23.9.5}{1437}{Network Architecture and Functional Mapping}{section*.3074}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23.45}{\ignorespaces NeRF network architecture. Positional encodings are applied to both position and viewing direction inputs. The MLP first predicts volume density and intermediate features from position, then conditions RGB color on the viewing direction.}}{1437}{figure.caption.3075}\protected@file@percent }
\newlabel{fig:chapter23_nerf_architecture}{{23.45}{1437}{NeRF network architecture. Positional encodings are applied to both position and viewing direction inputs. The MLP first predicts volume density and intermediate features from position, then conditions RGB color on the viewing direction}{figure.caption.3075}{}}
\@writefile{toc}{\contentsline {subsubsection}{Training vs. Inference: Pixel-Level Supervision and Scene Reconstruction}{1438}{section*.3076}\protected@file@percent }
\newlabel{subsubsec:chapter23_nerf_training_inference}{{23.9.5}{1438}{Training vs. Inference: Pixel-Level Supervision and Scene Reconstruction}{section*.3076}{}}
\@writefile{toc}{\contentsline {paragraph}{Training Procedure}{1438}{section*.3077}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Inference Procedure}{1439}{section*.3078}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Pixel-Level Supervision Works}{1439}{section*.3079}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Outlook}{1439}{section*.3080}\protected@file@percent }
\BKM@entry{id=902,dest={73756273656374696F6E2E32332E392E36},srcline={2281}}{5C3337365C3337375C303030455C303030785C303030705C303030655C303030725C303030695C3030306D5C303030655C3030306E5C303030745C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030415C303030625C3030306C5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030535C303030745C303030755C303030645C303030695C303030655C30303073}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{sitzmann2019_srn}
\abx@aux@segm{0}{0}{sitzmann2019_srn}
\abx@aux@cite{0}{lombardi2019_neuralvolumes}
\abx@aux@segm{0}{0}{lombardi2019_neuralvolumes}
\abx@aux@cite{0}{mildenhall2019_llff}
\abx@aux@segm{0}{0}{mildenhall2019_llff}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\@writefile{toc}{\contentsline {subsection}{\numberline {23.9.6}Experiments and Ablation Studies}{1440}{subsection.23.9.6}\protected@file@percent }
\newlabel{subsec:chapter23_nerf_experiments_ablation}{{23.9.6}{1440}{Experiments and Ablation Studies}{subsection.23.9.6}{}}
\abx@aux@backref{1931}{mildenhall2020_nerf}{0}{1440}{1440}
\@writefile{toc}{\contentsline {subsubsection}{Quantitative and Qualitative Evaluation}{1440}{section*.3081}\protected@file@percent }
\newlabel{subsubsec:chapter23_nerf_quantitative}{{23.9.6}{1440}{Quantitative and Qualitative Evaluation}{section*.3081}{}}
\abx@aux@backref{1932}{mildenhall2020_nerf}{0}{1440}{1440}
\abx@aux@backref{1933}{sitzmann2019_srn}{0}{1440}{1440}
\abx@aux@backref{1934}{lombardi2019_neuralvolumes}{0}{1440}{1440}
\abx@aux@backref{1935}{mildenhall2019_llff}{0}{1440}{1440}
\abx@aux@backref{1936}{mildenhall2020_nerf}{0}{1440}{1440}
\@writefile{lot}{\contentsline {table}{\numberline {23.1}{\ignorespaces Quantitative comparison on the \emph  {Realistic Synthetic 360$^\circ $} dataset~\blx@tocontentsinit {0}\cite {mildenhall2020_nerf}. NeRF achieves the highest performance across all metrics, demonstrating superior geometric reconstruction, perceptual realism, and high-frequency detail.}}{1440}{table.caption.3082}\protected@file@percent }
\abx@aux@backref{1938}{mildenhall2020_nerf}{0}{1440}{1440}
\newlabel{tab:chapter23_nerf_quantitative}{{23.1}{1440}{Quantitative comparison on the \emph {Realistic Synthetic 360$^\circ $} dataset~\cite {mildenhall2020_nerf}. NeRF achieves the highest performance across all metrics, demonstrating superior geometric reconstruction, perceptual realism, and high-frequency detail}{table.caption.3082}{}}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\@writefile{lof}{\contentsline {figure}{\numberline {23.46}{\ignorespaces Qualitative comparisons on held-out views from the Realistic Synthetic 360$^\circ $ dataset~\blx@tocontentsinit {0}\cite {mildenhall2020_nerf}. NeRF recovers intricate structures and materials (e.g., Lego gears, Microphone grille) and captures non-Lambertian effects. In contrast, LLFF exhibits ghosting and aliasing, while SRN and NV yield blurred or distorted geometry.}}{1441}{figure.caption.3083}\protected@file@percent }
\abx@aux@backref{1940}{mildenhall2020_nerf}{0}{1441}{1441}
\newlabel{fig:chapter23_nerf_comparisons}{{23.46}{1441}{Qualitative comparisons on held-out views from the Realistic Synthetic 360$^\circ $ dataset~\cite {mildenhall2020_nerf}. NeRF recovers intricate structures and materials (e.g., Lego gears, Microphone grille) and captures non-Lambertian effects. In contrast, LLFF exhibits ghosting and aliasing, while SRN and NV yield blurred or distorted geometry}{figure.caption.3083}{}}
\@writefile{toc}{\contentsline {subsubsection}{Ablation Studies}{1441}{section*.3084}\protected@file@percent }
\newlabel{subsubsec:chapter23_nerf_ablation}{{23.9.6}{1441}{Ablation Studies}{section*.3084}{}}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\@writefile{lot}{\contentsline {table}{\numberline {23.2}{\ignorespaces Ablation study from~\blx@tocontentsinit {0}\cite {mildenhall2020_nerf}. Each row disables or modifies one component of the full model. PE = Positional Encoding, VD = View Dependence, H = Hierarchical Sampling. All metrics averaged across 8 scenes.}}{1442}{table.caption.3085}\protected@file@percent }
\abx@aux@backref{1942}{mildenhall2020_nerf}{0}{1442}{1442}
\newlabel{tab:chapter23_nerf_ablation}{{23.2}{1442}{Ablation study from~\cite {mildenhall2020_nerf}. Each row disables or modifies one component of the full model. PE = Positional Encoding, VD = View Dependence, H = Hierarchical Sampling. All metrics averaged across 8 scenes}{table.caption.3085}{}}
\BKM@entry{id=903,dest={73756273656374696F6E2E32332E392E37},srcline={2381}}{5C3337365C3337375C3030304C5C303030695C3030306D5C303030695C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030745C303030685C303030655C3030305C3034305C3030304F5C303030725C303030695C303030675C303030695C3030306E5C303030615C3030306C5C3030305C3034305C3030304E5C303030655C303030525C303030465C3030305C3034305C303030415C303030725C303030635C303030685C303030695C303030745C303030655C303030635C303030745C303030755C303030725C30303065}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{fridovichkeil2022_plenoxels}
\abx@aux@segm{0}{0}{fridovichkeil2022_plenoxels}
\abx@aux@cite{0}{mueller2022_instantngp}
\abx@aux@segm{0}{0}{mueller2022_instantngp}
\abx@aux@cite{0}{chen2022_tensorf}
\abx@aux@segm{0}{0}{chen2022_tensorf}
\@writefile{lof}{\contentsline {figure}{\numberline {23.47}{\ignorespaces Ablation visualization from~\blx@tocontentsinit {0}\cite {mildenhall2020_nerf}. Without view dependence, specular highlights disappear, e.g., on the bulldozer tread. Without positional encoding, the model fails to recover high-frequency geometry and textures, leading to blurred reconstructions.}}{1443}{figure.caption.3086}\protected@file@percent }
\abx@aux@backref{1944}{mildenhall2020_nerf}{0}{1443}{1443}
\newlabel{fig:chapter23_nerf_ablation_visual}{{23.47}{1443}{Ablation visualization from~\cite {mildenhall2020_nerf}. Without view dependence, specular highlights disappear, e.g., on the bulldozer tread. Without positional encoding, the model fails to recover high-frequency geometry and textures, leading to blurred reconstructions}{figure.caption.3086}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {23.9.7}Limitations of the Original NeRF Architecture}{1443}{subsection.23.9.7}\protected@file@percent }
\newlabel{subsec:nerf_limitations}{{23.9.7}{1443}{Limitations of the Original NeRF Architecture}{subsection.23.9.7}{}}
\abx@aux@backref{1945}{mildenhall2020_nerf}{0}{1443}{1443}
\abx@aux@backref{1946}{fridovichkeil2022_plenoxels}{0}{1443}{1443}
\abx@aux@backref{1947}{mueller2022_instantngp}{0}{1443}{1443}
\abx@aux@backref{1948}{chen2022_tensorf}{0}{1443}{1443}
\abx@aux@cite{0}{pumarola2021_dnerf}
\abx@aux@segm{0}{0}{pumarola2021_dnerf}
\abx@aux@cite{0}{tancik2022_blocknerf}
\abx@aux@segm{0}{0}{tancik2022_blocknerf}
\abx@aux@cite{0}{mildenhall2022_rawnerf}
\abx@aux@segm{0}{0}{mildenhall2022_rawnerf}
\abx@aux@cite{0}{lazova2023_controlnerf}
\abx@aux@segm{0}{0}{lazova2023_controlnerf}
\abx@aux@cite{0}{jang2023_nerfshop}
\abx@aux@segm{0}{0}{jang2023_nerfshop}
\abx@aux@backref{1949}{pumarola2021_dnerf}{0}{1444}{1444}
\abx@aux@backref{1950}{tancik2022_blocknerf}{0}{1444}{1444}
\abx@aux@backref{1951}{mildenhall2022_rawnerf}{0}{1444}{1444}
\abx@aux@backref{1952}{lazova2023_controlnerf}{0}{1444}{1444}
\abx@aux@backref{1953}{jang2023_nerfshop}{0}{1444}{1444}
\@writefile{toc}{\contentsline {paragraph}{Summary}{1444}{section*.3093}\protected@file@percent }
\BKM@entry{id=904,dest={73656374696F6E2A2E33303934},srcline={2418}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030335C3030302E5C303030315C303030305C3030303A5C3030305C3034305C3030304E5C303030655C303030525C303030465C3030303A5C3030305C3034305C303030415C303030635C303030635C303030655C3030306C5C303030655C303030725C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030525C303030655C303030705C303030725C303030655C303030735C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030525C303030655C303030765C303030695C303030735C303030695C3030306F5C3030306E5C30303073}
\BKM@entry{id=905,dest={73656374696F6E2A2E33303936},srcline={2434}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030335C3030302E5C303030315C303030305C3030302E5C303030315C3030303A5C3030305C3034305C303030505C3030306C5C303030655C3030306E5C3030306F5C303030785C303030655C3030306C5C303030735C3030303A5C3030305C3034305C303030535C303030705C303030615C303030725C303030735C303030655C3030305C3034305C303030565C3030306F5C303030785C303030655C3030306C5C3030305C3034305C303030475C303030725C303030695C303030645C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030535C303030705C303030685C303030655C303030725C303030695C303030635C303030615C3030306C5C3030305C3034305C303030485C303030615C303030725C3030306D5C3030306F5C3030306E5C303030695C303030635C30303073}
\abx@aux@cite{0}{fridovichkeil2022_plenoxels}
\abx@aux@segm{0}{0}{fridovichkeil2022_plenoxels}
\abx@aux@cite{0}{fridovichkeil2022_plenoxels}
\abx@aux@segm{0}{0}{fridovichkeil2022_plenoxels}
\abx@aux@cite{0}{fridovichkeil2022_plenoxels}
\abx@aux@segm{0}{0}{fridovichkeil2022_plenoxels}
\@writefile{toc}{\contentsline {section}{Enrichment 23.10: NeRF: Acceleration and Representation Revisions}{1445}{section*.3094}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Explicit Voxel and Point Grid Representations}{1445}{section*.3095}\protected@file@percent }
\newlabel{par:chapter23_nerf_voxel_grids}{{23.10}{1445}{Explicit Voxel and Point Grid Representations}{section*.3095}{}}
\@writefile{toc}{\contentsline {subsection}{Enrichment 23.10.1: Plenoxels: Sparse Voxel Grids with Spherical Harmonics}{1445}{section*.3096}\protected@file@percent }
\newlabel{enr:sybsec_chapter23_plenoxels}{{23.10.1}{1445}{\color {ocre}Enrichment \thesubsection : Plenoxels: Sparse Voxel Grids with Spherical Harmonics}{section*.3096}{}}
\abx@aux@backref{1954}{fridovichkeil2022_plenoxels}{0}{1445}{1445}
\abx@aux@cite{0}{fridovichkeil2022_plenoxels}
\abx@aux@segm{0}{0}{fridovichkeil2022_plenoxels}
\abx@aux@cite{0}{fridovichkeil2022_plenoxels}
\abx@aux@segm{0}{0}{fridovichkeil2022_plenoxels}
\@writefile{lof}{\contentsline {figure}{\numberline {23.48}{\ignorespaces Overview of the Plenoxel model, adapted from~\blx@tocontentsinit {0}\cite {fridovichkeil2022_plenoxels}. (a) A sparse voxel grid stores SH coefficients and densities at each corner. (b) Sampled points along rays interpolate these values. (c) Volume rendering integrates color and opacity. (d) Grid parameters are optimized via a reconstruction loss and regularization.}}{1446}{figure.caption.3097}\protected@file@percent }
\abx@aux@backref{1956}{fridovichkeil2022_plenoxels}{0}{1446}{1446}
\newlabel{fig:chapter23_plenoxels_model}{{23.48}{1446}{Overview of the Plenoxel model, adapted from~\cite {fridovichkeil2022_plenoxels}. (a) A sparse voxel grid stores SH coefficients and densities at each corner. (b) Sampled points along rays interpolate these values. (c) Volume rendering integrates color and opacity. (d) Grid parameters are optimized via a reconstruction loss and regularization}{figure.caption.3097}{}}
\@writefile{toc}{\contentsline {paragraph}{Inference and Volume Rendering.}{1446}{section*.3098}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Training via Reconstruction Loss.}{1446}{section*.3099}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why Spherical Harmonics?}{1446}{section*.3100}\protected@file@percent }
\BKM@entry{id=906,dest={73656374696F6E2A2E33313034},srcline={2493}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030335C3030302E5C303030315C303030305C3030302E5C303030325C3030303A5C3030305C3034305C303030445C303030565C303030475C3030304F5C3030303A5C3030305C3034305C303030445C303030695C303030725C303030655C303030635C303030745C3030305C3034305C3030304F5C303030705C303030745C303030695C3030306D5C303030695C3030307A5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030306F5C3030306E5C3030305C3034305C303030445C303030655C3030306E5C303030735C303030655C3030305C3034305C303030565C3030306F5C303030785C303030655C3030306C5C3030305C3034305C303030475C303030725C303030695C303030645C30303073}
\abx@aux@cite{0}{sun2022_directvoxelgrid}
\abx@aux@segm{0}{0}{sun2022_directvoxelgrid}
\@writefile{lof}{\contentsline {figure}{\numberline {23.49}{\ignorespaces Training comparison between NeRF and Plenoxels~\blx@tocontentsinit {0}\cite {fridovichkeil2022_plenoxels}. Plenoxels reconstruct meaningful geometry within a minute, while NeRF requires tens of minutes for similar fidelity.}}{1447}{figure.caption.3101}\protected@file@percent }
\abx@aux@backref{1958}{fridovichkeil2022_plenoxels}{0}{1447}{1447}
\newlabel{fig:chapter23_nerf_vs_plenoxels}{{23.49}{1447}{Training comparison between NeRF and Plenoxels~\cite {fridovichkeil2022_plenoxels}. Plenoxels reconstruct meaningful geometry within a minute, while NeRF requires tens of minutes for similar fidelity}{figure.caption.3101}{}}
\@writefile{toc}{\contentsline {paragraph}{Fast Convergence via Coarse-to-Fine Refinement.}{1447}{section*.3102}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Core Insight and Tradeoffs.}{1447}{section*.3103}\protected@file@percent }
\abx@aux@cite{0}{sun2022_directvoxelgrid}
\abx@aux@segm{0}{0}{sun2022_directvoxelgrid}
\abx@aux@cite{0}{sun2022_directvoxelgrid}
\abx@aux@segm{0}{0}{sun2022_directvoxelgrid}
\@writefile{toc}{\contentsline {subsection}{Enrichment 23.10.2: DVGO: Direct Optimization on Dense Voxel Grids}{1448}{section*.3104}\protected@file@percent }
\newlabel{enr:subsec_chapter23_dvgo}{{23.10.2}{1448}{\color {ocre}Enrichment \thesubsection : DVGO: Direct Optimization on Dense Voxel Grids}{section*.3104}{}}
\abx@aux@backref{1959}{sun2022_directvoxelgrid}{0}{1448}{1448}
\@writefile{lof}{\contentsline {figure}{\numberline {23.50}{\ignorespaces DVGO framework overview, adapted from~\blx@tocontentsinit {0}\cite {sun2022_directvoxelgrid}. A ray is cast and sampled at 3D points. Trilinear interpolation retrieves density and appearance features from a dense voxel grid. A lightweight MLP decodes RGB values. Differentiable volume rendering is used for supervision.}}{1448}{figure.caption.3105}\protected@file@percent }
\abx@aux@backref{1961}{sun2022_directvoxelgrid}{0}{1448}{1448}
\newlabel{fig:chapter23_dvgo_model}{{23.50}{1448}{DVGO framework overview, adapted from~\cite {sun2022_directvoxelgrid}. A ray is cast and sampled at 3D points. Trilinear interpolation retrieves density and appearance features from a dense voxel grid. A lightweight MLP decodes RGB values. Differentiable volume rendering is used for supervision}{figure.caption.3105}{}}
\@writefile{toc}{\contentsline {paragraph}{Rendering Pipeline}{1448}{section*.3106}\protected@file@percent }
\newlabel{par:chapter23_dvgo_rendering}{{23.10.2}{1448}{Rendering Pipeline}{section*.3106}{}}
\abx@aux@cite{0}{sun2022_directvoxelgrid}
\abx@aux@segm{0}{0}{sun2022_directvoxelgrid}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\@writefile{toc}{\contentsline {paragraph}{Coarse-to-Fine Upsampling and Fine Detail Reconstruction}{1449}{section*.3107}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Foreground–Background Decomposition}{1449}{section*.3108}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{DVGOv2 Improvements}{1449}{section*.3109}\protected@file@percent }
\newlabel{par:chapter23_dvgo_v2}{{23.10.2}{1449}{DVGOv2 Improvements}{section*.3109}{}}
\abx@aux@backref{1962}{sun2022_directvoxelgrid}{0}{1449}{1449}
\abx@aux@backref{1963}{barron2022_mipnerf360}{0}{1449}{1449}
\@writefile{toc}{\contentsline {paragraph}{Comparison to Plenoxels and NeRF}{1449}{section*.3110}\protected@file@percent }
\abx@aux@cite{0}{sun2022_directvoxelgridv2}
\abx@aux@segm{0}{0}{sun2022_directvoxelgridv2}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\@writefile{toc}{\contentsline {paragraph}{Efficiency and Tradeoffs}{1450}{section*.3111}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Performance Across Scene Types}{1450}{section*.3112}\protected@file@percent }
\abx@aux@backref{1964}{sun2022_directvoxelgridv2}{0}{1450}{1450}
\@writefile{lot}{\contentsline {table}{\numberline {23.3}{\ignorespaces Results on Synthetic-NeRF~\blx@tocontentsinit {0}\cite {mildenhall2020_nerf}.}}{1450}{table.caption.3113}\protected@file@percent }
\abx@aux@backref{1966}{mildenhall2020_nerf}{0}{1450}{1450}
\newlabel{tab:chapter23_dvgo_synthetic}{{23.3}{1450}{Results on Synthetic-NeRF~\cite {mildenhall2020_nerf}}{table.caption.3113}{}}
\abx@aux@backref{1967}{mildenhall2020_nerf}{0}{1450}{1450}
\@writefile{lot}{\contentsline {table}{\numberline {23.4}{\ignorespaces LLFF benchmark results. DVGOv2 achieves strong accuracy with fast training and compact grids.}}{1450}{table.caption.3114}\protected@file@percent }
\newlabel{tab:chapter23_dvgo_llff}{{23.4}{1450}{LLFF benchmark results. DVGOv2 achieves strong accuracy with fast training and compact grids}{table.caption.3114}{}}
\@writefile{lot}{\contentsline {table}{\numberline {23.5}{\ignorespaces Results on unbounded inward-facing Tanks\&Temples dataset.}}{1451}{table.caption.3115}\protected@file@percent }
\newlabel{tab:chapter23_dvgo_tanks}{{23.5}{1451}{Results on unbounded inward-facing Tanks\&Temples dataset}{table.caption.3115}{}}
\@writefile{lot}{\contentsline {table}{\numberline {23.6}{\ignorespaces Results on mip-NeRF 360 dataset. (*) denotes longer schedule with cuboid contraction.}}{1451}{table.caption.3116}\protected@file@percent }
\newlabel{tab:chapter23_dvgo_mip360}{{23.6}{1451}{Results on mip-NeRF 360 dataset. (*) denotes longer schedule with cuboid contraction}{table.caption.3116}{}}
\@writefile{toc}{\contentsline {paragraph}{Final Remarks}{1451}{section*.3117}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Hash-Based Feature Grid Representations}{1451}{section*.3118}\protected@file@percent }
\newlabel{par:chapter23_nerf_hash_grids}{{23.10}{1451}{Hash-Based Feature Grid Representations}{section*.3118}{}}
\@writefile{toc}{\contentsline {subparagraph}{Core Tradeoff.}{1451}{subparagraph*.3119}\protected@file@percent }
\BKM@entry{id=907,dest={73656374696F6E2A2E33313230},srcline={2690}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030335C3030302E5C303030315C303030305C3030302E5C303030335C3030303A5C3030305C3034305C303030495C3030306E5C303030735C303030745C303030615C3030306E5C303030745C3030302D5C3030304E5C303030475C303030505C3030303A5C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C303030735C303030635C303030615C3030306C5C303030655C3030305C3034305C303030485C303030615C303030735C303030685C3030305C3034305C303030455C3030306E5C303030635C3030306F5C303030645C303030695C3030306E5C303030675C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030525C303030655C303030615C3030306C5C3030302D5C303030545C303030695C3030306D5C303030655C3030305C3034305C3030304E5C303030655C303030525C30303046}
\abx@aux@cite{0}{mueller2022_instantngp}
\abx@aux@segm{0}{0}{mueller2022_instantngp}
\@writefile{toc}{\contentsline {subsection}{Enrichment 23.10.3: Instant-NGP: Multiscale Hash Encoding for Real-Time NeRF}{1452}{section*.3120}\protected@file@percent }
\newlabel{enr:subsec_chapter23_instant_ngp}{{23.10.3}{1452}{\color {ocre}Enrichment \thesubsection : Instant-NGP: Multiscale Hash Encoding for Real-Time NeRF}{section*.3120}{}}
\abx@aux@backref{1968}{mueller2022_instantngp}{0}{1452}{1452}
\@writefile{toc}{\contentsline {paragraph}{Multiscale Hash Encoding}{1452}{section*.3121}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Motivation and Benefits}{1452}{section*.3122}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Hash Function and Learning Dynamics}{1453}{section*.3123}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Fast MLP Decoder and View Conditioning}{1454}{section*.3124}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.51}{\ignorespaces Instant-NGP architecture. Input points are encoded with multi-resolution hash grids, passed to a fused MLP along with viewing direction, and output density and color are used for volume rendering.}}{1454}{figure.caption.3125}\protected@file@percent }
\newlabel{fig:chapter23_instantngp_model}{{23.51}{1454}{Instant-NGP architecture. Input points are encoded with multi-resolution hash grids, passed to a fused MLP along with viewing direction, and output density and color are used for volume rendering}{figure.caption.3125}{}}
\@writefile{toc}{\contentsline {paragraph}{Occupancy Grid Acceleration}{1455}{section*.3126}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Training and Inference}{1455}{section*.3127}\protected@file@percent }
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{liu2020_nsvf}
\abx@aux@segm{0}{0}{liu2020_nsvf}
\abx@aux@cite{0}{barron2021_mipnerf}
\abx@aux@segm{0}{0}{barron2021_mipnerf}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{liu2020_nsvf}
\abx@aux@segm{0}{0}{liu2020_nsvf}
\abx@aux@cite{0}{barron2021_mipnerf}
\abx@aux@segm{0}{0}{barron2021_mipnerf}
\abx@aux@cite{0}{mueller2022_instantngp}
\abx@aux@segm{0}{0}{mueller2022_instantngp}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{liu2020_nsvf}
\abx@aux@segm{0}{0}{liu2020_nsvf}
\abx@aux@cite{0}{barron2021_mipnerf}
\abx@aux@segm{0}{0}{barron2021_mipnerf}
\abx@aux@cite{0}{mueller2022_instantngp}
\abx@aux@segm{0}{0}{mueller2022_instantngp}
\BKM@entry{id=908,dest={73656374696F6E2A2E33313330},srcline={2880}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030335C3030302E5C303030315C303030305C3030302E5C303030345C3030303A5C3030305C3034305C3030304E5C303030655C303030725C303030665C303030615C303030635C303030745C3030306F5C3030303A5C3030305C3034305C3030304D5C303030655C303030725C303030675C303030695C3030306E5C303030675C3030305C3034305C303030495C3030306E5C303030735C303030745C303030615C3030306E5C303030745C3030302D5C3030304E5C303030475C303030505C3030305C3034305C3030305C3034365C3030305C3034305C3030304E5C303030525C3030305C3034305C303030505C303030695C303030705C303030655C3030306C5C303030695C3030306E5C303030655C30303073}
\abx@aux@cite{0}{tancik2023_nerfacto}
\abx@aux@segm{0}{0}{tancik2023_nerfacto}
\@writefile{toc}{\contentsline {paragraph}{Advantages and Limitations}{1456}{section*.3128}\protected@file@percent }
\abx@aux@backref{1969}{mildenhall2020_nerf}{0}{1456}{1456}
\abx@aux@backref{1970}{liu2020_nsvf}{0}{1456}{1456}
\abx@aux@backref{1971}{barron2021_mipnerf}{0}{1456}{1456}
\@writefile{lot}{\contentsline {table}{\numberline {23.7}{\ignorespaces PSNR comparison on the eight synthetic scenes from the NeRF dataset. Instant-NGP (Hash) achieves top quality within seconds to minutes, outperforming NeRF~\blx@tocontentsinit {0}\cite {mildenhall2020_nerf} and NSVF~\blx@tocontentsinit {0}\cite {liu2020_nsvf}, and approaching or exceeding mip-NeRF~\blx@tocontentsinit {0}\cite {barron2021_mipnerf}. Data from~\blx@tocontentsinit {0}\cite {mueller2022_instantngp}.}}{1456}{table.caption.3129}\protected@file@percent }
\abx@aux@backref{1976}{mildenhall2020_nerf}{0}{1456}{1456}
\abx@aux@backref{1977}{liu2020_nsvf}{0}{1456}{1456}
\abx@aux@backref{1978}{barron2021_mipnerf}{0}{1456}{1456}
\abx@aux@backref{1979}{mueller2022_instantngp}{0}{1456}{1456}
\newlabel{tab:instantngp_psnr}{{23.7}{1456}{PSNR comparison on the eight synthetic scenes from the NeRF dataset. Instant-NGP (Hash) achieves top quality within seconds to minutes, outperforming NeRF~\cite {mildenhall2020_nerf} and NSVF~\cite {liu2020_nsvf}, and approaching or exceeding mip-NeRF~\cite {barron2021_mipnerf}. Data from~\cite {mueller2022_instantngp}}{table.caption.3129}{}}
\abx@aux@cite{0}{tancik2023_nerfacto}
\abx@aux@segm{0}{0}{tancik2023_nerfacto}
\abx@aux@cite{0}{tancik2023_nerfacto}
\abx@aux@segm{0}{0}{tancik2023_nerfacto}
\abx@aux@cite{0}{tancik2023_nerfacto}
\abx@aux@segm{0}{0}{tancik2023_nerfacto}
\abx@aux@cite{0}{tancik2023_nerfacto}
\abx@aux@segm{0}{0}{tancik2023_nerfacto}
\@writefile{toc}{\contentsline {subsection}{Enrichment 23.10.4: Nerfacto: Merging Instant-NGP \& NR Pipelines}{1457}{section*.3130}\protected@file@percent }
\newlabel{enr:subsec_chapter23_nerfacto}{{23.10.4}{1457}{\color {ocre}Enrichment \thesubsection : Nerfacto: Merging Instant-NGP \& NR Pipelines}{section*.3130}{}}
\abx@aux@backref{1980}{tancik2023_nerfacto}{0}{1457}{1457}
\@writefile{lof}{\contentsline {figure}{\numberline {23.52}{\ignorespaces Nerfacto pipeline~\blx@tocontentsinit {0}\cite {tancik2023_nerfacto}. Hash-encoded 3D features and auxiliary 2D features are fused before MLP decoding. The network is trained using RGB, geometric, and semantic losses.}}{1457}{figure.caption.3131}\protected@file@percent }
\abx@aux@backref{1982}{tancik2023_nerfacto}{0}{1457}{1457}
\newlabel{fig:chapter23_nerfacto_pipeline}{{23.52}{1457}{Nerfacto pipeline~\cite {tancik2023_nerfacto}. Hash-encoded 3D features and auxiliary 2D features are fused before MLP decoding. The network is trained using RGB, geometric, and semantic losses}{figure.caption.3131}{}}
\BKM@entry{id=909,dest={73656374696F6E2A2E33313335},srcline={2945}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030335C3030302E5C303030315C303030305C3030302E5C303030355C3030303A5C3030305C3034305C303030545C303030655C3030306E5C303030735C3030306F5C303030525C303030465C3030303A5C3030305C3034305C303030545C303030655C3030306E5C303030735C3030306F5C303030725C3030302D5C303030465C303030615C303030635C303030745C3030306F5C303030725C303030695C3030307A5C303030655C303030645C3030305C3034305C303030465C303030695C303030655C3030306C5C303030645C30303073}
\abx@aux@cite{0}{chen2022_tensorf}
\abx@aux@segm{0}{0}{chen2022_tensorf}
\@writefile{lof}{\contentsline {figure}{\numberline {23.53}{\ignorespaces Volume rendering output of Nerfacto~\blx@tocontentsinit {0}\cite {tancik2023_nerfacto}. Despite real-time training, the model recovers sharp surfaces and textures.}}{1458}{figure.caption.3132}\protected@file@percent }
\abx@aux@backref{1984}{tancik2023_nerfacto}{0}{1458}{1458}
\newlabel{fig:chapter23_nerfacto_field}{{23.53}{1458}{Volume rendering output of Nerfacto~\cite {tancik2023_nerfacto}. Despite real-time training, the model recovers sharp surfaces and textures}{figure.caption.3132}{}}
\@writefile{toc}{\contentsline {paragraph}{Applications and Design Goals}{1458}{section*.3133}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Core Insight and Tradeoffs}{1458}{section*.3134}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Enrichment 23.10.5: TensoRF: Tensor-Factorized Fields}{1459}{section*.3135}\protected@file@percent }
\newlabel{enr:subsec_chapter23_tensorf}{{23.10.5}{1459}{\color {ocre}Enrichment \thesubsection : TensoRF: Tensor-Factorized Fields}{section*.3135}{}}
\abx@aux@backref{1985}{chen2022_tensorf}{0}{1459}{1459}
\@writefile{toc}{\contentsline {paragraph}{Overview}{1459}{section*.3136}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Radiance Field Decomposition via Tensor Approximation}{1459}{section*.3137}\protected@file@percent }
\newlabel{par:chapter23_tensorf_decomp}{{23.10.5}{1459}{Radiance Field Decomposition via Tensor Approximation}{section*.3137}{}}
\@writefile{toc}{\contentsline {paragraph}{Vector--Matrix (VM) Decomposition}{1459}{section*.3138}\protected@file@percent }
\newlabel{par:chapter23_tensorf_vm}{{23.10.5}{1459}{Vector--Matrix (VM) Decomposition}{section*.3138}{}}
\@writefile{toc}{\contentsline {paragraph}{Interpolation: From Discrete Grids to Continuous Coordinates}{1460}{section*.3139}\protected@file@percent }
\newlabel{par:chapter23_tensorf_interpolation}{{23.10.5}{1460}{Interpolation: From Discrete Grids to Continuous Coordinates}{section*.3139}{}}
\@writefile{toc}{\contentsline {paragraph}{Differentiability and Training Efficiency}{1460}{section*.3140}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Geometry: View-Independent Density Estimation}{1461}{section*.3141}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Appearance: View-Dependent Color Prediction}{1461}{section*.3142}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Comparison to CP Decomposition}{1461}{section*.3143}\protected@file@percent }
\abx@aux@cite{0}{chen2022_tensorf}
\abx@aux@segm{0}{0}{chen2022_tensorf}
\abx@aux@cite{0}{chen2022_tensorf}
\abx@aux@segm{0}{0}{chen2022_tensorf}
\abx@aux@cite{0}{chen2022_tensorf}
\abx@aux@segm{0}{0}{chen2022_tensorf}
\abx@aux@cite{0}{chen2022_tensorf}
\abx@aux@segm{0}{0}{chen2022_tensorf}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{liu2020_nsvf}
\abx@aux@segm{0}{0}{liu2020_nsvf}
\abx@aux@cite{0}{fridovichkeil2022_plenoxels}
\abx@aux@segm{0}{0}{fridovichkeil2022_plenoxels}
\abx@aux@cite{0}{sun2022_directvoxelgrid}
\abx@aux@segm{0}{0}{sun2022_directvoxelgrid}
\@writefile{toc}{\contentsline {paragraph}{Summary}{1462}{section*.3144}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.54}{\ignorespaces TensoRF VM architecture~\blx@tocontentsinit {0}\cite {chen2022_tensorf}. Each 3D point \( \mathbf  {x} \) is reconstructed from axis-aligned vector–matrix components. Density is predicted additively; color is produced from appearance features and view direction using a shallow decoder.}}{1462}{figure.caption.3145}\protected@file@percent }
\abx@aux@backref{1987}{chen2022_tensorf}{0}{1462}{1462}
\newlabel{fig:chapter23_tensorf_overview}{{23.54}{1462}{TensoRF VM architecture~\cite {chen2022_tensorf}. Each 3D point \( \mathbf {x} \) is reconstructed from axis-aligned vector–matrix components. Density is predicted additively; color is produced from appearance features and view direction using a shallow decoder}{figure.caption.3145}{}}
\@writefile{toc}{\contentsline {paragraph}{Quantitative Comparison}{1462}{section*.3146}\protected@file@percent }
\abx@aux@cite{0}{chen2022_tensorf}
\abx@aux@segm{0}{0}{chen2022_tensorf}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{fridovichkeil2022_plenoxels}
\abx@aux@segm{0}{0}{fridovichkeil2022_plenoxels}
\abx@aux@cite{0}{sun2022_directvoxelgrid}
\abx@aux@segm{0}{0}{sun2022_directvoxelgrid}
\abx@aux@cite{0}{liu2020_nsvf}
\abx@aux@segm{0}{0}{liu2020_nsvf}
\abx@aux@cite{0}{chen2022_tensorf}
\abx@aux@segm{0}{0}{chen2022_tensorf}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{fridovichkeil2022_plenoxels}
\abx@aux@segm{0}{0}{fridovichkeil2022_plenoxels}
\abx@aux@cite{0}{sun2022_directvoxelgrid}
\abx@aux@segm{0}{0}{sun2022_directvoxelgrid}
\abx@aux@cite{0}{liu2020_nsvf}
\abx@aux@segm{0}{0}{liu2020_nsvf}
\BKM@entry{id=910,dest={73656374696F6E2A2E33313530},srcline={3148}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030335C3030302E5C303030315C303030305C3030302E5C303030365C3030303A5C3030305C3034305C3030304D5C303030695C303030705C3030302D5C3030304E5C303030655C303030525C303030465C3030303A5C3030305C3034305C303030415C3030306E5C303030745C303030695C3030302D5C303030415C3030306C5C303030695C303030615C303030735C303030655C303030645C3030305C3034305C303030525C303030615C303030645C303030695C303030615C3030306E5C303030635C303030655C3030305C3034305C303030465C303030695C303030655C3030306C5C303030645C30303073}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\@writefile{lot}{\contentsline {table}{\numberline {23.8}{\ignorespaces Quantitative results from~\blx@tocontentsinit {0}\cite {chen2022_tensorf}. TensoRF (VM-192) achieves strong PSNR and SSIM with orders-of-magnitude faster training and smaller model size than most voxel-based methods.}}{1463}{table.caption.3147}\protected@file@percent }
\abx@aux@backref{1989}{chen2022_tensorf}{0}{1463}{1463}
\newlabel{tab:chapter23_tensorf_metrics}{{23.8}{1463}{Quantitative results from~\cite {chen2022_tensorf}. TensoRF (VM-192) achieves strong PSNR and SSIM with orders-of-magnitude faster training and smaller model size than most voxel-based methods}{table.caption.3147}{}}
\abx@aux@backref{1990}{mildenhall2020_nerf}{0}{1463}{1463}
\abx@aux@backref{1991}{liu2020_nsvf}{0}{1463}{1463}
\abx@aux@backref{1992}{fridovichkeil2022_plenoxels}{0}{1463}{1463}
\abx@aux@backref{1993}{sun2022_directvoxelgrid}{0}{1463}{1463}
\@writefile{toc}{\contentsline {paragraph}{Qualitative Results}{1463}{section*.3148}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.55}{\ignorespaces Qualitative results from~\blx@tocontentsinit {0}\cite {chen2022_tensorf}. TensoRF (VM-192) recovers finer geometric and appearance details compared to NeRF~\blx@tocontentsinit {0}\cite {mildenhall2020_nerf}, Plenoxels~\blx@tocontentsinit {0}\cite {fridovichkeil2022_plenoxels}, DVGO~\blx@tocontentsinit {0}\cite {sun2022_directvoxelgrid}, and NSVF~\blx@tocontentsinit {0}\cite {liu2020_nsvf}.}}{1463}{figure.caption.3149}\protected@file@percent }
\abx@aux@backref{1999}{chen2022_tensorf}{0}{1463}{1463}
\abx@aux@backref{2000}{mildenhall2020_nerf}{0}{1463}{1463}
\abx@aux@backref{2001}{fridovichkeil2022_plenoxels}{0}{1463}{1463}
\abx@aux@backref{2002}{sun2022_directvoxelgrid}{0}{1463}{1463}
\abx@aux@backref{2003}{liu2020_nsvf}{0}{1463}{1463}
\newlabel{fig:chapter23_tensorf_results}{{23.55}{1463}{Qualitative results from~\cite {chen2022_tensorf}. TensoRF (VM-192) recovers finer geometric and appearance details compared to NeRF~\cite {mildenhall2020_nerf}, Plenoxels~\cite {fridovichkeil2022_plenoxels}, DVGO~\cite {sun2022_directvoxelgrid}, and NSVF~\cite {liu2020_nsvf}}{figure.caption.3149}{}}
\@writefile{toc}{\contentsline {subsection}{Enrichment 23.10.6: Mip-NeRF: Anti-Aliased Radiance Fields}{1463}{section*.3150}\protected@file@percent }
\newlabel{enr:subsec_chapter23_mipnerf}{{23.10.6}{1463}{\color {ocre}Enrichment \thesubsection : Mip-NeRF: Anti-Aliased Radiance Fields}{section*.3150}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation: scale ambiguity and aliasing}{1463}{section*.3151}\protected@file@percent }
\abx@aux@backref{2004}{mildenhall2020_nerf}{0}{1463}{1463}
\abx@aux@cite{0}{barron2021_mipnerf}
\abx@aux@segm{0}{0}{barron2021_mipnerf}
\abx@aux@cite{0}{barron2021_mipnerf}
\abx@aux@segm{0}{0}{barron2021_mipnerf}
\@writefile{lof}{\contentsline {figure}{\numberline {23.56}{\ignorespaces Aliasing in NeRF~\blx@tocontentsinit {0}\cite {barron2021_mipnerf}. (a) NeRF trained on high-res images suffers aliasing at lower resolutions or when zooming. (b) Multi-scale training with NeRF only partially fixes this. (c) Mip-NeRF yields less aliasing in its renderings across all scales. (d) Ground truth.}}{1464}{figure.caption.3152}\protected@file@percent }
\abx@aux@backref{2006}{barron2021_mipnerf}{0}{1464}{1464}
\newlabel{fig:chapter23_nerf_aliasing}{{23.56}{1464}{Aliasing in NeRF~\cite {barron2021_mipnerf}. (a) NeRF trained on high-res images suffers aliasing at lower resolutions or when zooming. (b) Multi-scale training with NeRF only partially fixes this. (c) Mip-NeRF yields less aliasing in its renderings across all scales. (d) Ground truth}{figure.caption.3152}{}}
\@writefile{toc}{\contentsline {paragraph}{From pixels to cones}{1464}{section*.3153}\protected@file@percent }
\abx@aux@cite{0}{barron2021_mipnerf}
\abx@aux@segm{0}{0}{barron2021_mipnerf}
\abx@aux@cite{0}{barron2021_mipnerf}
\abx@aux@segm{0}{0}{barron2021_mipnerf}
\@writefile{toc}{\contentsline {paragraph}{Why cones are divided into frustums}{1465}{section*.3154}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.57}{\ignorespaces Volume coverage ambiguity~\blx@tocontentsinit {0}\cite {barron2021_mipnerf}. NeRF samples points along rays (dots), which can alias across resolutions. Mip-NeRF casts cones and integrates over the entire volume seen by a pixel (trapezoids), resolving ambiguity and encoding scale.}}{1465}{figure.caption.3155}\protected@file@percent }
\abx@aux@backref{2008}{barron2021_mipnerf}{0}{1465}{1465}
\newlabel{fig:chapter23_mipnerf_pov}{{23.57}{1465}{Volume coverage ambiguity~\cite {barron2021_mipnerf}. NeRF samples points along rays (dots), which can alias across resolutions. Mip-NeRF casts cones and integrates over the entire volume seen by a pixel (trapezoids), resolving ambiguity and encoding scale}{figure.caption.3155}{}}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\@writefile{toc}{\contentsline {paragraph}{From frustums to a pixel’s color}{1466}{section*.3156}\protected@file@percent }
\abx@aux@backref{2009}{mildenhall2020_nerf}{0}{1466}{1466}
\@writefile{toc}{\contentsline {paragraph}{From pixels to cones}{1466}{section*.3157}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Approximating the footprint as a disk}{1466}{section*.3158}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Frustum geometry and indicator function}{1467}{section*.3159}\protected@file@percent }
\newlabel{eq:chapter23_mipnerf_indicator}{{23.1}{1467}{Frustum geometry and indicator function}{equation.23.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Expected positional encoding over a frustum}{1469}{section*.3160}\protected@file@percent }
\newlabel{eq:chapter23_mipnerf_expected_encoding}{{23.2}{1469}{Expected positional encoding over a frustum}{equation.23.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Intuition}{1469}{section*.3161}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Moment-matched Gaussian approximation}{1470}{section*.3162}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Frustum-centric coordinates}{1470}{subparagraph*.3163}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Marginal depth distribution $p(t)$}{1471}{section*.3164}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Mean depth $\mu _t$}{1472}{section*.3165}\protected@file@percent }
\newlabel{eq:chapter23_mipnerf_expectation_def}{{23.3}{1472}{Mean depth $\mu _t$}{equation.23.3}{}}
\newlabel{eq:chapter23_mipnerf_mu_t_exact}{{23.5}{1472}{Mean depth $\mu _t$}{equation.23.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Stable reparameterization of $\mu _t$}{1472}{section*.3166}\protected@file@percent }
\newlabel{eq:chapter23_mipnerf_mu_t_stable}{{23.6}{1472}{Stable reparameterization of $\mu _t$}{equation.23.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Axial variance $\sigma _t^2$}{1472}{section*.3167}\protected@file@percent }
\newlabel{eq:chapter23_mipnerf_axial_variance}{{23.7}{1472}{Axial variance $\sigma _t^2$}{equation.23.7}{}}
\newlabel{eq:chapter23_mipnerf_second_moment_t}{{23.9}{1472}{Axial variance $\sigma _t^2$}{equation.23.9}{}}
\@writefile{toc}{\contentsline {paragraph}{Stable reparameterization of $\sigma _t^2$}{1473}{section*.3168}\protected@file@percent }
\newlabel{eq:chapter23_mipnerf_sigma_t}{{23.10}{1473}{Stable reparameterization of $\sigma _t^2$}{equation.23.10}{}}
\@writefile{toc}{\contentsline {paragraph}{Radial (perpendicular) variance $\sigma _r^2$}{1473}{section*.3169}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Step 1: Conditional second moment at fixed depth}{1473}{subparagraph*.3170}\protected@file@percent }
\newlabel{eq:chapter23_mipnerf_conditional_x2}{{23.13}{1474}{Step 1: Conditional second moment at fixed depth}{equation.23.13}{}}
\@writefile{toc}{\contentsline {subparagraph}{Step 2: Averaging over depth}{1474}{subparagraph*.3171}\protected@file@percent }
\newlabel{eq:chapter23_mipnerf_sigma_r2}{{23.16}{1475}{Step 2: Averaging over depth}{equation.23.16}{}}
\newlabel{eq:chapter23_mipnerf_sigma_r2_stable}{{23.17}{1475}{Step 2: Averaging over depth}{equation.23.17}{}}
\@writefile{toc}{\contentsline {paragraph}{Moment-Matched Gaussian in World Space}{1475}{section*.3172}\protected@file@percent }
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\newlabel{eq:chapter23_mipnerf_gaussian}{{23.18}{1477}{Moment-Matched Gaussian in World Space}{equation.23.18}{}}
\@writefile{toc}{\contentsline {paragraph}{Rewriting positional encoding as Fourier features}{1477}{section*.3173}\protected@file@percent }
\abx@aux@backref{2010}{mildenhall2020_nerf}{0}{1477}{1477}
\@writefile{toc}{\contentsline {paragraph}{Fourier matrix formulation}{1477}{section*.3174}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Architecture \& Implementation Details}{1480}{section*.3175}\protected@file@percent }
\newlabel{subsubsec:chapter23_mipnerf_architecture}{{23.10.6}{1480}{Architecture \& Implementation Details}{section*.3175}{}}
\@writefile{toc}{\contentsline {paragraph}{Cone tracing and interval IPE features}{1480}{section*.3176}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Single multiscale MLP with hierarchical sampling}{1480}{section*.3177}\protected@file@percent }
\abx@aux@cite{0}{zhang2019_blurpool}
\abx@aux@segm{0}{0}{zhang2019_blurpool}
\@writefile{toc}{\contentsline {paragraph}{Training objective}{1481}{section*.3178}\protected@file@percent }
\newlabel{eq:chapter23_mipnerf_objective}{{{17}}{1481}{Training objective}{AMS.3179}{}}
\@writefile{toc}{\contentsline {paragraph}{Smoothed importance sampling for the fine pass}{1481}{section*.3180}\protected@file@percent }
\abx@aux@cite{0}{barron2021_mipnerf}
\abx@aux@segm{0}{0}{barron2021_mipnerf}
\abx@aux@cite{0}{barron2021_mipnerf}
\abx@aux@segm{0}{0}{barron2021_mipnerf}
\abx@aux@backref{2011}{zhang2019_blurpool}{0}{1482}{1482}
\newlabel{eq:chapter23_mipnerf_blurpool_pdf}{{{18}}{1482}{Smoothed importance sampling for the fine pass}{AMS.3181}{}}
\@writefile{toc}{\contentsline {subparagraph}{Implementation Details.}{1482}{subparagraph*.3182}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Benefits over NeRF}{1482}{section*.3183}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.58}{\ignorespaces PE vs.\ IPE in Mip-NeRF}}{1483}{figure.caption.3184}\protected@file@percent }
\abx@aux@backref{2013}{barron2021_mipnerf}{0}{1483}{1483}
\newlabel{fig:chapter23_mipnerf_pe_vs_ipe}{{23.58}{1483}{PE vs.\ IPE in Mip-NeRF}{figure.caption.3184}{}}
\abx@aux@cite{0}{barron2021_mipnerf}
\abx@aux@segm{0}{0}{barron2021_mipnerf}
\abx@aux@cite{0}{barron2021_mipnerf}
\abx@aux@segm{0}{0}{barron2021_mipnerf}
\abx@aux@cite{0}{barron2021_mipnerf}
\abx@aux@segm{0}{0}{barron2021_mipnerf}
\abx@aux@cite{0}{barron2021_mipnerf}
\abx@aux@segm{0}{0}{barron2021_mipnerf}
\abx@aux@cite{0}{barron2021_mipnerf}
\abx@aux@segm{0}{0}{barron2021_mipnerf}
\@writefile{toc}{\contentsline {subsubsection}{Results and Ablations}{1484}{section*.3185}\protected@file@percent }
\newlabel{subsec:chapter23_mipnerf_results}{{23.10.6}{1484}{Results and Ablations}{section*.3185}{}}
\@writefile{toc}{\contentsline {paragraph}{Quantitative performance}{1484}{section*.3186}\protected@file@percent }
\abx@aux@backref{2014}{barron2021_mipnerf}{0}{1484}{1484}
\@writefile{lot}{\contentsline {table}{\numberline {23.9}{\ignorespaces Quantitative results of Mip-NeRF and ablations}}{1484}{table.caption.3187}\protected@file@percent }
\abx@aux@backref{2016}{barron2021_mipnerf}{0}{1484}{1484}
\newlabel{tab:chapter23_mipnerf_results}{{23.9}{1484}{Quantitative results of Mip-NeRF and ablations}{table.caption.3187}{}}
\@writefile{toc}{\contentsline {paragraph}{Qualitative performance}{1484}{section*.3188}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.59}{\ignorespaces Mip-NeRF vs NeRF visual results}}{1484}{figure.caption.3189}\protected@file@percent }
\abx@aux@backref{2018}{barron2021_mipnerf}{0}{1484}{1484}
\newlabel{fig:chapter23_mipnerf_vs_nerf}{{23.59}{1484}{Mip-NeRF vs NeRF visual results}{figure.caption.3189}{}}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{barron2023_zipnerf}
\abx@aux@segm{0}{0}{barron2023_zipnerf}
\abx@aux@cite{0}{hu2023_trimiprf}
\abx@aux@segm{0}{0}{hu2023_trimiprf}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\BKM@entry{id=911,dest={73656374696F6E2A2E33313934},srcline={4261}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030335C3030302E5C303030315C303030305C3030302E5C303030375C3030303A5C3030305C3034305C3030304E5C303030655C303030755C303030535C3030303A5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C303030495C3030306D5C303030705C3030306C5C303030695C303030635C303030695C303030745C3030305C3034305C303030535C303030755C303030725C303030665C303030615C303030635C303030655C303030735C3030305C3034305C303030625C303030795C3030305C3034305C303030565C3030306F5C3030306C5C303030755C3030306D5C303030655C3030305C3034305C303030525C303030655C3030306E5C303030645C303030655C303030725C303030695C3030306E5C30303067}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\@writefile{toc}{\contentsline {paragraph}{Ablation insights (following table~\ref {tab:chapter23_mipnerf_results})}{1485}{section*.3190}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Generalization to unseen scales}{1485}{section*.3191}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Limitations and Downsides}{1485}{section*.3192}\protected@file@percent }
\newlabel{subsec:chapter23_mipnerf_limitations}{{23.10.6}{1485}{Limitations and Downsides}{section*.3192}{}}
\@writefile{toc}{\contentsline {subsubsection}{Notable Works Building on Mip-NeRF}{1485}{section*.3193}\protected@file@percent }
\newlabel{subsec:chapter23_mipnerf_influence}{{23.10.6}{1485}{Notable Works Building on Mip-NeRF}{section*.3193}{}}
\abx@aux@backref{2019}{barron2022_mipnerf360}{0}{1485}{1485}
\abx@aux@backref{2020}{barron2023_zipnerf}{0}{1485}{1485}
\abx@aux@backref{2021}{hu2023_trimiprf}{0}{1485}{1485}
\abx@aux@backref{2022}{kerbl2023_3dgaussiansplatting}{0}{1485}{1485}
\abx@aux@cite{0}{yariv2020_idr}
\abx@aux@segm{0}{0}{yariv2020_idr}
\abx@aux@cite{0}{wang2021_neus}
\abx@aux@segm{0}{0}{wang2021_neus}
\abx@aux@cite{0}{wang2021_neus}
\abx@aux@segm{0}{0}{wang2021_neus}
\abx@aux@cite{0}{wang2021_neus}
\abx@aux@segm{0}{0}{wang2021_neus}
\@writefile{toc}{\contentsline {subsection}{Enrichment 23.10.7: NeuS: Neural Implicit Surfaces by Volume Rendering}{1486}{section*.3194}\protected@file@percent }
\newlabel{enr:subsec_chapter23_neus}{{23.10.7}{1486}{\color {ocre}Enrichment \thesubsection : NeuS: Neural Implicit Surfaces by Volume Rendering}{section*.3194}{}}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{1486}{section*.3195}\protected@file@percent }
\newlabel{subsec:chapter23_neus_motivation}{{23.10.7}{1486}{Motivation}{section*.3195}{}}
\abx@aux@backref{2023}{mildenhall2020_nerf}{0}{1486}{1486}
\abx@aux@backref{2024}{yariv2020_idr}{0}{1486}{1486}
\abx@aux@backref{2025}{wang2021_neus}{0}{1486}{1486}
\@writefile{lof}{\contentsline {figure}{\numberline {23.60}{\ignorespaces \textbf  {Surface vs. volume rendering in neural scene reconstruction.} (a) Conceptual differences. (b) Bamboo planter example: IDR fills the interior despite a smooth surface, NeRF preserves hollowness but exhibits surface noise, NeuS avoids both issues by combining SDF-based surfaces with volumetric rendering. Image credit:~\blx@tocontentsinit {0}\cite {wang2021_neus}.}}{1486}{figure.caption.3196}\protected@file@percent }
\abx@aux@backref{2027}{wang2021_neus}{0}{1486}{1486}
\newlabel{fig:chapter23_neus_surface_vs_volume_rendering}{{23.60}{1486}{\textbf {Surface vs. volume rendering in neural scene reconstruction.} (a) Conceptual differences. (b) Bamboo planter example: IDR fills the interior despite a smooth surface, NeRF preserves hollowness but exhibits surface noise, NeuS avoids both issues by combining SDF-based surfaces with volumetric rendering. Image credit:~\cite {wang2021_neus}}{figure.caption.3196}{}}
\abx@aux@cite{0}{wang2021_neus}
\abx@aux@segm{0}{0}{wang2021_neus}
\abx@aux@cite{0}{wang2021_neus}
\abx@aux@segm{0}{0}{wang2021_neus}
\@writefile{toc}{\contentsline {subsubsection}{Method}{1487}{section*.3197}\protected@file@percent }
\newlabel{subsec:chapter23_neus_method}{{23.10.7}{1487}{Method}{section*.3197}{}}
\@writefile{toc}{\contentsline {paragraph}{Scene representation and rendering objective}{1487}{section*.3198}\protected@file@percent }
\abx@aux@backref{2028}{wang2021_neus}{0}{1487}{1487}
\@writefile{toc}{\contentsline {paragraph}{From SDF to volume rendering}{1487}{section*.3199}\protected@file@percent }
\abx@aux@backref{2029}{wang2021_neus}{0}{1487}{1487}
\abx@aux@cite{0}{wang2021_neus}
\abx@aux@segm{0}{0}{wang2021_neus}
\abx@aux@cite{0}{wang2021_neus}
\abx@aux@segm{0}{0}{wang2021_neus}
\@writefile{toc}{\contentsline {paragraph}{Na\"{i}ve SDF$\to $density conversion and its bias}{1488}{section*.3200}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.61}{\ignorespaces \textbf  {Weight bias vs.\ unbiased construction.} (a) Naïve approach: The blue curve shows the SDF \(f(t)\), whose zero-crossing marks the true surface location. The brown curve is the density \(\sigma (t) = \phi _s(f_\theta (\mathbf  {r}(t)))\), which is largest when \(f(t)\) is near zero. The green curve is the weight \(w(t) = T(t)\,\sigma (t)\). Because the transmittance \(T(t)\) has already decayed by the time \(\sigma (t)\) reaches its peak, the product \(w(t)\) achieves its maximum \emph  {before} the blue zero-crossing—producing a biased surface estimate. (b) NeuS: By redefining the effective density \(\rho (t)\) so that \(T(t)\) matches the logistic CDF \(\Phi _s(f(t))\) in the first-order SDF approximation, the decay of \(T(t)\) and the growth of \(\rho (t)\) are balanced. This alignment causes the green weight \(w(t) = T(t)\,\rho (t)\) to peak exactly at the blue zero-crossing of \(f(t)\), eliminating bias while retaining occlusion handling. Source:~\blx@tocontentsinit {0}\cite {wang2021_neus}.}}{1489}{figure.caption.3201}\protected@file@percent }
\abx@aux@backref{2031}{wang2021_neus}{0}{1489}{1489}
\newlabel{fig:chapter23_neus_biased_vs_unbiased_weights_sdf}{{23.61}{1489}{\textbf {Weight bias vs.\ unbiased construction.} (a) Naïve approach: The blue curve shows the SDF \(f(t)\), whose zero-crossing marks the true surface location. The brown curve is the density \(\sigma (t) = \phi _s(f_\theta (\mathbf {r}(t)))\), which is largest when \(f(t)\) is near zero. The green curve is the weight \(w(t) = T(t)\,\sigma (t)\). Because the transmittance \(T(t)\) has already decayed by the time \(\sigma (t)\) reaches its peak, the product \(w(t)\) achieves its maximum \emph {before} the blue zero-crossing—producing a biased surface estimate. (b) NeuS: By redefining the effective density \(\rho (t)\) so that \(T(t)\) matches the logistic CDF \(\Phi _s(f(t))\) in the first-order SDF approximation, the decay of \(T(t)\) and the growth of \(\rho (t)\) are balanced. This alignment causes the green weight \(w(t) = T(t)\,\rho (t)\) to peak exactly at the blue zero-crossing of \(f(t)\), eliminating bias while retaining occlusion handling. Source:~\cite {wang2021_neus}}{figure.caption.3201}{}}
\@writefile{toc}{\contentsline {paragraph}{A direct unbiased weighting that fails occlusion}{1489}{section*.3202}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Derivation of the NeuS Weight Function for the Single-Plane Case}{1490}{section*.3203}\protected@file@percent }
\newlabel{subsubsec:chapter23_neus_singleplane}{{23.10.7}{1490}{Derivation of the NeuS Weight Function for the Single-Plane Case}{section*.3203}{}}
\@writefile{toc}{\contentsline {paragraph}{Step 1: Geometric Setup}{1490}{section*.3204}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 2: Normal and Incidence Angle}{1490}{section*.3205}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 3: SDF properties (geometry and intuition)}{1490}{section*.3206}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 4: SDF Evolution Along the Ray}{1491}{section*.3207}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 5: Local linearization near the surface}{1491}{section*.3208}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 6: Direct unbiased weight construction}{1492}{section*.3209}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 7: Derivative-of-CDF Identity}{1493}{section*.3210}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 8: Interpretation as Soft Visibility}{1493}{section*.3211}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Step 9: Embedding into Volume Rendering}{1493}{section*.3212}\protected@file@percent }
\abx@aux@cite{0}{wang2021_neus}
\abx@aux@segm{0}{0}{wang2021_neus}
\abx@aux@cite{0}{wang2021_neus}
\abx@aux@segm{0}{0}{wang2021_neus}
\@writefile{toc}{\contentsline {subsubsection}{Multi-Surface Generalization}{1494}{section*.3213}\protected@file@percent }
\newlabel{subsubsec:chapter23_neus_multisurface}{{23.10.7}{1494}{Multi-Surface Generalization}{section*.3213}{}}
\@writefile{toc}{\contentsline {paragraph}{Enforcing Physical Validity}{1494}{section*.3214}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Intuition}{1494}{section*.3215}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Weights Construction Summary}{1494}{section*.3216}\protected@file@percent }
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\@writefile{lof}{\contentsline {figure}{\numberline {23.62}{\ignorespaces \textbf  {Multiple intersections.} The NeuS weight assigns probability only on \emph  {entering} segments (\(df/dt<0\)); on \emph  {exiting} segments (\(df/dt>0\)) the derived opaque density is clipped to zero. This preserves visibility ordering. Image credit:~\blx@tocontentsinit {0}\cite {wang2021_neus}.}}{1495}{figure.caption.3217}\protected@file@percent }
\abx@aux@backref{2033}{wang2021_neus}{0}{1495}{1495}
\newlabel{fig:chapter23_neus_weight_distribution_multi_surface}{{23.62}{1495}{\textbf {Multiple intersections.} The NeuS weight assigns probability only on \emph {entering} segments (\(df/dt<0\)); on \emph {exiting} segments (\(df/dt>0\)) the derived opaque density is clipped to zero. This preserves visibility ordering. Image credit:~\cite {wang2021_neus}}{figure.caption.3217}{}}
\@writefile{toc}{\contentsline {paragraph}{Discretization}{1495}{section*.3218}\protected@file@percent }
\abx@aux@backref{2034}{mildenhall2020_nerf}{0}{1495}{1495}
\abx@aux@cite{0}{gropp2020implicit}
\abx@aux@segm{0}{0}{gropp2020implicit}
\@writefile{toc}{\contentsline {paragraph}{Training}{1496}{section*.3222}\protected@file@percent }
\abx@aux@backref{2035}{gropp2020implicit}{0}{1496}{1496}
\@writefile{toc}{\contentsline {paragraph}{Training stabilization via geometry initialization}{1497}{section*.3227}\protected@file@percent }
\abx@aux@cite{0}{yariv2020_idr}
\abx@aux@segm{0}{0}{yariv2020_idr}
\abx@aux@cite{0}{oechsle2021_unisurf}
\abx@aux@segm{0}{0}{oechsle2021_unisurf}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{schonberger2016_structure}
\abx@aux@segm{0}{0}{schonberger2016_structure}
\@writefile{toc}{\contentsline {subsubsection}{Experiments and Ablations}{1498}{section*.3228}\protected@file@percent }
\newlabel{subsec:chapter23_neus_experiments}{{23.10.7}{1498}{Experiments and Ablations}{section*.3228}{}}
\@writefile{toc}{\contentsline {paragraph}{Experimental setup}{1498}{section*.3229}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Quantitative results}{1498}{section*.3230}\protected@file@percent }
\abx@aux@backref{2036}{yariv2020_idr}{0}{1498}{1498}
\abx@aux@backref{2037}{oechsle2021_unisurf}{0}{1498}{1498}
\abx@aux@backref{2038}{mildenhall2020_nerf}{0}{1498}{1498}
\abx@aux@backref{2039}{schonberger2016_structure}{0}{1498}{1498}
\@writefile{lot}{\contentsline {table}{\numberline {23.10}{\ignorespaces Quantitative evaluation on the DTU dataset. NeuS achieves the lowest Chamfer distance both with and without mask supervision. COLMAP results use trim=0.}}{1498}{table.caption.3231}\protected@file@percent }
\newlabel{tab:chapter23_neus_dtu}{{23.10}{1498}{Quantitative evaluation on the DTU dataset. NeuS achieves the lowest Chamfer distance both with and without mask supervision. COLMAP results use trim=0}{table.caption.3231}{}}
\@writefile{toc}{\contentsline {paragraph}{Qualitative comparisons}{1499}{section*.3232}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.63}{\ignorespaces Comparison of surface reconstruction with mask supervision. NeuS generates the most accurate surfaces. IDR produces smooth but wrong geometry, while NeRF captures geometry but with many artifacts.}}{1499}{figure.caption.3233}\protected@file@percent }
\newlabel{fig:chapter23_neus_mask}{{23.63}{1499}{Comparison of surface reconstruction with mask supervision. NeuS generates the most accurate surfaces. IDR produces smooth but wrong geometry, while NeRF captures geometry but with many artifacts}{figure.caption.3233}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23.64}{\ignorespaces Comparison of surface reconstruction without mask supervision. NeuS is robust, while NeRF introduces artifacts and COLMAP removes parts of the object or hallucinates noise.}}{1499}{figure.caption.3234}\protected@file@percent }
\newlabel{fig:chapter23_neus_nomask}{{23.64}{1499}{Comparison of surface reconstruction without mask supervision. NeuS is robust, while NeRF introduces artifacts and COLMAP removes parts of the object or hallucinates noise}{figure.caption.3234}{}}
\@writefile{toc}{\contentsline {paragraph}{Ablation studies}{1500}{section*.3235}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {23.11}{\ignorespaces Ablation studies on DTU. Removing the Eikonal loss or geometric initialization substantially harms reconstruction accuracy.}}{1500}{table.caption.3236}\protected@file@percent }
\newlabel{tab:chapter23_neus_ablation}{{23.11}{1500}{Ablation studies on DTU. Removing the Eikonal loss or geometric initialization substantially harms reconstruction accuracy}{table.caption.3236}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23.65}{\ignorespaces Qualitative ablations. NeuS requires all components—S-density, Eikonal regularization, and geometry initialization—for faithful reconstruction.}}{1500}{figure.caption.3237}\protected@file@percent }
\newlabel{fig:chapter23_neus_ablation}{{23.65}{1500}{Qualitative ablations. NeuS requires all components—S-density, Eikonal regularization, and geometry initialization—for faithful reconstruction}{figure.caption.3237}{}}
\@writefile{toc}{\contentsline {paragraph}{Limitations and Related Work}{1500}{section*.3238}\protected@file@percent }
\abx@aux@cite{0}{yariv2020_idr}
\abx@aux@segm{0}{0}{yariv2020_idr}
\abx@aux@cite{0}{yariv2021_volsdf}
\abx@aux@segm{0}{0}{yariv2021_volsdf}
\abx@aux@cite{0}{oechsle2021_unisurf}
\abx@aux@segm{0}{0}{oechsle2021_unisurf}
\abx@aux@cite{0}{wang2022_hf_neus}
\abx@aux@segm{0}{0}{wang2022_hf_neus}
\abx@aux@cite{0}{wang2023_pet_neus}
\abx@aux@segm{0}{0}{wang2023_pet_neus}
\abx@aux@cite{0}{liu2023_neudf}
\abx@aux@segm{0}{0}{liu2023_neudf}
\abx@aux@cite{0}{esposito2022_kiloneus}
\abx@aux@segm{0}{0}{esposito2022_kiloneus}
\abx@aux@cite{0}{wang2023_neus2}
\abx@aux@segm{0}{0}{wang2023_neus2}
\abx@aux@cite{0}{yu2024_gsdf}
\abx@aux@segm{0}{0}{yu2024_gsdf}
\BKM@entry{id=912,dest={73656374696F6E2A2E33323339},srcline={4919}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030335C3030302E5C303030315C303030305C3030302E5C303030385C3030303A5C3030305C3034305C303030505C3030306F5C303030695C3030306E5C303030745C3030302D5C3030304E5C303030655C303030525C303030465C3030303A5C3030305C3034305C303030505C3030306F5C303030695C3030306E5C303030745C3030302D5C303030625C303030615C303030735C303030655C303030645C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C303030525C303030615C303030645C303030695C303030615C3030306E5C303030635C303030655C3030305C3034305C303030465C303030695C303030655C3030306C5C303030645C30303073}
\abx@aux@cite{0}{xu2022_pointnerf}
\abx@aux@segm{0}{0}{xu2022_pointnerf}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@backref{2040}{yariv2020_idr}{0}{1501}{1501}
\abx@aux@backref{2041}{yariv2021_volsdf}{0}{1501}{1501}
\abx@aux@backref{2042}{oechsle2021_unisurf}{0}{1501}{1501}
\abx@aux@backref{2043}{wang2022_hf_neus}{0}{1501}{1501}
\abx@aux@backref{2044}{wang2023_pet_neus}{0}{1501}{1501}
\abx@aux@backref{2045}{liu2023_neudf}{0}{1501}{1501}
\abx@aux@backref{2046}{esposito2022_kiloneus}{0}{1501}{1501}
\abx@aux@backref{2047}{wang2023_neus2}{0}{1501}{1501}
\abx@aux@backref{2048}{yu2024_gsdf}{0}{1501}{1501}
\abx@aux@cite{0}{schonberger2016_structure}
\abx@aux@segm{0}{0}{schonberger2016_structure}
\abx@aux@cite{0}{xu2022_pointnerf}
\abx@aux@segm{0}{0}{xu2022_pointnerf}
\abx@aux@cite{0}{xu2022_pointnerf}
\abx@aux@segm{0}{0}{xu2022_pointnerf}
\@writefile{toc}{\contentsline {subsection}{Enrichment 23.10.8: Point-NeRF: Point-based Neural Radiance Fields}{1502}{section*.3239}\protected@file@percent }
\newlabel{enr:subsec_chapter23_pointnerf}{{23.10.8}{1502}{\color {ocre}Enrichment \thesubsection : Point-NeRF: Point-based Neural Radiance Fields}{section*.3239}{}}
\abx@aux@backref{2049}{xu2022_pointnerf}{0}{1502}{1502}
\abx@aux@backref{2050}{mildenhall2020_nerf}{0}{1502}{1502}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{1502}{section*.3240}\protected@file@percent }
\newlabel{subsubsec:chapter23_pointnerf_motivation}{{23.10.8}{1502}{Motivation}{section*.3240}{}}
\abx@aux@backref{2051}{schonberger2016_structure}{0}{1502}{1502}
\@writefile{lof}{\contentsline {figure}{\numberline {23.66}{\ignorespaces Point-NeRF efficiently reconstructs fine details (e.g., leaf structures) in tens of minutes, unlike NeRF which requires days of optimization. It can also initialize from raw COLMAP point clouds and refine them via pruning and growing. Credit: \blx@tocontentsinit {0}\cite {xu2022_pointnerf}.}}{1502}{figure.caption.3241}\protected@file@percent }
\abx@aux@backref{2053}{xu2022_pointnerf}{0}{1502}{1502}
\newlabel{fig:chapter23_pointnerf_fast_details}{{23.66}{1502}{Point-NeRF efficiently reconstructs fine details (e.g., leaf structures) in tens of minutes, unlike NeRF which requires days of optimization. It can also initialize from raw COLMAP point clouds and refine them via pruning and growing. Credit: \cite {xu2022_pointnerf}}{figure.caption.3241}{}}
\@writefile{toc}{\contentsline {subsubsection}{Method}{1502}{section*.3242}\protected@file@percent }
\newlabel{subsec:chapter23_pointnerf_method}{{23.10.8}{1502}{Method}{section*.3242}{}}
\@writefile{toc}{\contentsline {paragraph}{Overview}{1502}{section*.3243}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Ray setup and sampling (as in NeRF)}{1504}{section*.3244}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Local neighbor query (surface-aware shading)}{1504}{section*.3245}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Local feature regression (Eq.~3)}{1505}{section*.3246}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Radiance aggregation (Eqs.~4–5)}{1505}{section*.3247}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Density aggregation (Eqs.~6–7)}{1505}{section*.3248}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Putting the pieces together}{1505}{section*.3249}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{End-to-end optimization objective}{1505}{section*.3250}\protected@file@percent }
\abx@aux@cite{0}{xu2022_pointnerf}
\abx@aux@segm{0}{0}{xu2022_pointnerf}
\abx@aux@cite{0}{xu2022_pointnerf}
\abx@aux@segm{0}{0}{xu2022_pointnerf}
\abx@aux@cite{0}{jensen2014_dtu}
\abx@aux@segm{0}{0}{jensen2014_dtu}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{knapitsch2017_tanks}
\abx@aux@segm{0}{0}{knapitsch2017_tanks}
\abx@aux@cite{0}{dai2017_scannet}
\abx@aux@segm{0}{0}{dai2017_scannet}
\@writefile{toc}{\contentsline {paragraph}{Topology edits during refinement (per paper)}{1506}{section*.3251}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Architecture and Implementation Details}{1506}{section*.3252}\protected@file@percent }
\newlabel{subsubsec:chapter23_pointnerf_architecture}{{23.10.8}{1506}{Architecture and Implementation Details}{section*.3252}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23.67}{\ignorespaces Point-NeRF optimization: dashed lines indicate gradient updates during initialization and per-scene finetuning. Credit: \blx@tocontentsinit {0}\cite {xu2022_pointnerf}.}}{1506}{figure.caption.3253}\protected@file@percent }
\abx@aux@backref{2055}{xu2022_pointnerf}{0}{1506}{1506}
\newlabel{fig:chapter23_pointnerf_optimization}{{23.67}{1506}{Point-NeRF optimization: dashed lines indicate gradient updates during initialization and per-scene finetuning. Credit: \cite {xu2022_pointnerf}}{figure.caption.3253}{}}
\abx@aux@cite{0}{jensen2014_dtu}
\abx@aux@segm{0}{0}{jensen2014_dtu}
\abx@aux@cite{0}{chen2021_mvsnerf}
\abx@aux@segm{0}{0}{chen2021_mvsnerf}
\abx@aux@cite{0}{jensen2014_dtu}
\abx@aux@segm{0}{0}{jensen2014_dtu}
\abx@aux@cite{0}{chen2021_mvsnerf}
\abx@aux@segm{0}{0}{chen2021_mvsnerf}
\abx@aux@cite{0}{yu2020_pixelnerf}
\abx@aux@segm{0}{0}{yu2020_pixelnerf}
\abx@aux@cite{0}{chen2021_mvsnerf}
\abx@aux@segm{0}{0}{chen2021_mvsnerf}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{schonberger2016_structure}
\abx@aux@segm{0}{0}{schonberger2016_structure}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{schonberger2016_structure}
\abx@aux@segm{0}{0}{schonberger2016_structure}
\abx@aux@cite{0}{aliev2020_npbg}
\abx@aux@segm{0}{0}{aliev2020_npbg}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\abx@aux@cite{0}{liu2020_nsvf}
\abx@aux@segm{0}{0}{liu2020_nsvf}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\@writefile{toc}{\contentsline {subsubsection}{Experiments and Ablations}{1507}{section*.3254}\protected@file@percent }
\newlabel{subsubsec:chapter23_pointnerf_experiments}{{23.10.8}{1507}{Experiments and Ablations}{section*.3254}{}}
\abx@aux@backref{2056}{jensen2014_dtu}{0}{1507}{1507}
\abx@aux@backref{2057}{mildenhall2020_nerf}{0}{1507}{1507}
\abx@aux@backref{2058}{knapitsch2017_tanks}{0}{1507}{1507}
\abx@aux@backref{2059}{dai2017_scannet}{0}{1507}{1507}
\@writefile{lot}{\contentsline {table}{\numberline {23.12}{\ignorespaces DTU~\blx@tocontentsinit {0}\cite {jensen2014_dtu} (novel-view setting of~\blx@tocontentsinit {0}\cite {chen2021_mvsnerf}). Subscripts indicate training iterations.}}{1507}{table.caption.3255}\protected@file@percent }
\abx@aux@backref{2062}{jensen2014_dtu}{0}{1507}{1507}
\abx@aux@backref{2063}{chen2021_mvsnerf}{0}{1507}{1507}
\abx@aux@backref{2064}{yu2020_pixelnerf}{0}{1507}{1507}
\abx@aux@backref{2065}{chen2021_mvsnerf}{0}{1507}{1507}
\abx@aux@backref{2066}{wang2021_ibrnet}{0}{1507}{1507}
\abx@aux@backref{2067}{mildenhall2020_nerf}{0}{1507}{1507}
\@writefile{toc}{\contentsline {paragraph}{DTU}{1507}{section*.3256}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {23.13}{\ignorespaces NeRF-Synthetic~\blx@tocontentsinit {0}\cite {mildenhall2020_nerf}. Point-NeRF matches NeRF at \(20\text  {K}\) steps and surpasses it at \(200\text  {K}\). “col” denotes initialization from COLMAP~\blx@tocontentsinit {0}\cite {schonberger2016_structure}.}}{1507}{table.caption.3257}\protected@file@percent }
\abx@aux@backref{2070}{mildenhall2020_nerf}{0}{1507}{1507}
\abx@aux@backref{2071}{schonberger2016_structure}{0}{1507}{1507}
\abx@aux@backref{2072}{aliev2020_npbg}{0}{1507}{1507}
\abx@aux@backref{2073}{mildenhall2020_nerf}{0}{1507}{1507}
\abx@aux@backref{2074}{wang2021_ibrnet}{0}{1507}{1507}
\abx@aux@backref{2075}{liu2020_nsvf}{0}{1507}{1507}
\@writefile{toc}{\contentsline {paragraph}{NeRF-Synthetic}{1507}{section*.3258}\protected@file@percent }
\abx@aux@cite{0}{knapitsch2017_tanks}
\abx@aux@segm{0}{0}{knapitsch2017_tanks}
\abx@aux@cite{0}{dai2017_scannet}
\abx@aux@segm{0}{0}{dai2017_scannet}
\abx@aux@cite{0}{liu2020_nsvf}
\abx@aux@segm{0}{0}{liu2020_nsvf}
\abx@aux@cite{0}{liu2020_nsvf}
\abx@aux@segm{0}{0}{liu2020_nsvf}
\abx@aux@cite{0}{schonberger2016_structure}
\abx@aux@segm{0}{0}{schonberger2016_structure}
\@writefile{lof}{\contentsline {figure}{\numberline {23.68}{\ignorespaces Qualitative comparisons on NeRF-Synthetic~\blx@tocontentsinit {0}\cite {mildenhall2020_nerf}. Subscripts indicate training iterations. Point-NeRF captures thin structures (e.g., the rope) and converges much faster than NeRF.}}{1508}{figure.caption.3259}\protected@file@percent }
\abx@aux@backref{2077}{mildenhall2020_nerf}{0}{1508}{1508}
\newlabel{fig:chapter23_pointnerf_visual_comparisons}{{23.68}{1508}{Qualitative comparisons on NeRF-Synthetic~\cite {mildenhall2020_nerf}. Subscripts indicate training iterations. Point-NeRF captures thin structures (e.g., the rope) and converges much faster than NeRF}{figure.caption.3259}{}}
\@writefile{toc}{\contentsline {paragraph}{Tanks\&Temples and ScanNet}{1508}{section*.3260}\protected@file@percent }
\abx@aux@backref{2078}{knapitsch2017_tanks}{0}{1508}{1508}
\abx@aux@backref{2079}{dai2017_scannet}{0}{1508}{1508}
\abx@aux@backref{2080}{liu2020_nsvf}{0}{1508}{1508}
\abx@aux@backref{2081}{liu2020_nsvf}{0}{1508}{1508}
\@writefile{toc}{\contentsline {paragraph}{Initialization from external COLMAP clouds}{1508}{section*.3261}\protected@file@percent }
\abx@aux@backref{2082}{schonberger2016_structure}{0}{1508}{1508}
\abx@aux@cite{0}{schonberger2016_structure}
\abx@aux@segm{0}{0}{schonberger2016_structure}
\abx@aux@cite{0}{schonberger2016_structure}
\abx@aux@segm{0}{0}{schonberger2016_structure}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\@writefile{lof}{\contentsline {figure}{\numberline {23.69}{\ignorespaces Pruning \& growing (P\&G) ablation. Removing outliers and adding anchors in high-opacity, under-anchored regions improves both geometry and rendering for predicted points and COLMAP-initialized points.}}{1509}{figure.caption.3262}\protected@file@percent }
\newlabel{fig:chapter23_pointnerf_pg_ablation}{{23.69}{1509}{Pruning \& growing (P\&G) ablation. Removing outliers and adding anchors in high-opacity, under-anchored regions improves both geometry and rendering for predicted points and COLMAP-initialized points}{figure.caption.3262}{}}
\@writefile{lot}{\contentsline {table}{\numberline {23.14}{\ignorespaces Effect of pruning \& growing (paper’s Table~4). COLMAP: initialization from~\blx@tocontentsinit {0}\cite {schonberger2016_structure}.}}{1509}{table.caption.3263}\protected@file@percent }
\abx@aux@backref{2084}{schonberger2016_structure}{0}{1509}{1509}
\newlabel{tab:chapter23_pointnerf_pg}{{23.14}{1509}{Effect of pruning \& growing (paper’s Table~4). COLMAP: initialization from~\cite {schonberger2016_structure}}{table.caption.3263}{}}
\@writefile{toc}{\contentsline {paragraph}{Feature-initialization ablation (paper’s Table 5)}{1509}{section*.3264}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Limitations}{1509}{section*.3265}\protected@file@percent }
\newlabel{subsubsec:chapter23_pointnerf_limitations}{{23.10.8}{1509}{Limitations}{section*.3265}{}}
\@writefile{toc}{\contentsline {paragraph}{Outlook toward 3D Gaussian Splatting.}{1510}{section*.3266}\protected@file@percent }
\abx@aux@backref{2085}{kerbl2023_3dgaussiansplatting}{0}{1510}{1510}
\BKM@entry{id=913,dest={73656374696F6E2A2E33323637},srcline={5270}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030335C3030302E5C303030315C303030305C3030302E5C303030395C3030303A5C3030305C3034305C303030335C303030445C3030305C3034305C303030475C303030615C303030755C303030735C303030735C303030695C303030615C3030306E5C3030305C3034305C303030535C303030705C3030306C5C303030615C303030745C303030745C303030695C3030306E5C303030675C3030303A5C3030305C3034305C303030525C303030545C3030305C3034305C303030525C303030615C303030645C303030695C303030615C3030306E5C303030635C303030655C3030305C3034305C303030465C303030695C303030655C3030306C5C303030645C3030305C3034305C303030525C303030655C3030306E5C303030645C303030655C303030725C303030695C3030306E5C30303067}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@cite{0}{hf_gs_blog}
\abx@aux@segm{0}{0}{hf_gs_blog}
\abx@aux@cite{0}{hf_gs_blog}
\abx@aux@segm{0}{0}{hf_gs_blog}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\@writefile{toc}{\contentsline {subsection}{Enrichment 23.10.9: 3D Gaussian Splatting: RT Radiance Field Rendering}{1511}{section*.3267}\protected@file@percent }
\newlabel{enr:subsec_chapter23_gaussian_splatting}{{23.10.9}{1511}{\color {ocre}Enrichment \thesubsection : 3D Gaussian Splatting: RT Radiance Field Rendering}{section*.3267}{}}
\@writefile{toc}{\contentsline {subsubsection}{Motivation and big picture}{1511}{section*.3268}\protected@file@percent }
\newlabel{subsubsec:chapter23_gaussian_splatting_motivation}{{23.10.9}{1511}{Motivation and big picture}{section*.3268}{}}
\@writefile{toc}{\contentsline {paragraph}{Context and objective}{1511}{section*.3269}\protected@file@percent }
\abx@aux@backref{2086}{kerbl2023_3dgaussiansplatting}{0}{1511}{1511}
\@writefile{lof}{\contentsline {figure}{\numberline {23.70}{\ignorespaces \textbf  {From triangles to Gaussians} Instead of rasterizing mesh triangles, 3DGS renders many anisotropic 3D Gaussians; in screen space these appear as ellipses (borders shown for clarity). Illustration adapted from the Hugging Face overview \blx@tocontentsinit {0}\cite {hf_gs_blog}.}}{1511}{figure.caption.3270}\protected@file@percent }
\abx@aux@backref{2088}{hf_gs_blog}{0}{1511}{1511}
\newlabel{fig:chapter23_gaussian_eg_gaussians}{{23.70}{1511}{\textbf {From triangles to Gaussians} Instead of rasterizing mesh triangles, 3DGS renders many anisotropic 3D Gaussians; in screen space these appear as ellipses (borders shown for clarity). Illustration adapted from the Hugging Face overview \cite {hf_gs_blog}}{figure.caption.3270}{}}
\@writefile{toc}{\contentsline {paragraph}{Key idea}{1511}{section*.3271}\protected@file@percent }
\abx@aux@backref{2089}{kerbl2023_3dgaussiansplatting}{0}{1511}{1511}
\@writefile{toc}{\contentsline {paragraph}{Core terms}{1511}{section*.3272}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How 3DGS uses Gaussians}{1511}{section*.3273}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{From 3D to 2D footprints}{1512}{section*.3274}\protected@file@percent }
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\@writefile{toc}{\contentsline {paragraph}{View-dependent color with spherical harmonics}{1513}{section*.3275}\protected@file@percent }
\abx@aux@backref{2090}{kerbl2023_3dgaussiansplatting}{0}{1513}{1513}
\@writefile{toc}{\contentsline {paragraph}{Rasterize and composite}{1513}{section*.3276}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why this design is effective}{1513}{section*.3277}\protected@file@percent }
\abx@aux@backref{2091}{kerbl2023_3dgaussiansplatting}{0}{1513}{1513}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\@writefile{toc}{\contentsline {subsubsection}{3D Gaussian Splatting stages}{1514}{section*.3278}\protected@file@percent }
\newlabel{enr:subsubsec_chapter23_3dgs_overview}{{23.10.9}{1514}{3D Gaussian Splatting stages}{section*.3278}{}}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\@writefile{lof}{\contentsline {figure}{\numberline {23.71}{\ignorespaces \textbf  {Optimization pipeline} Initialization from a sparse SfM point cloud, followed by interleaved gradient-based optimization and adaptive density control using a fast, tile-based differentiable renderer. Once trained, the representation supports RT novel-view navigation. Adapted from \blx@tocontentsinit {0}\cite {kerbl2023_3dgaussiansplatting}.}}{1515}{figure.caption.3279}\protected@file@percent }
\abx@aux@backref{2093}{kerbl2023_3dgaussiansplatting}{0}{1515}{1515}
\newlabel{fig:chapter23_gs_pipeline}{{23.71}{1515}{\textbf {Optimization pipeline} Initialization from a sparse SfM point cloud, followed by interleaved gradient-based optimization and adaptive density control using a fast, tile-based differentiable renderer. Once trained, the representation supports RT novel-view navigation. Adapted from \cite {kerbl2023_3dgaussiansplatting}}{figure.caption.3279}{}}
\@writefile{toc}{\contentsline {subsubsection}{Representation and parameterization}{1515}{section*.3280}\protected@file@percent }
\newlabel{enr:subsubsec_chapter23_representation}{{23.10.9}{1515}{Representation and parameterization}{section*.3280}{}}
\@writefile{toc}{\contentsline {paragraph}{From mean–covariance to a renderable primitive}{1515}{section*.3281}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Initialization and the need for valid, optimizable covariances}{1515}{section*.3282}\protected@file@percent }
\abx@aux@backref{2094}{kerbl2023_3dgaussiansplatting}{0}{1515}{1515}
\@writefile{toc}{\contentsline {paragraph}{Choosing a geometry parameterization for valid optimization}{1516}{section*.3283}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Intuition and practical knobs for $R$ and $S$}{1516}{section*.3284}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Opacity as a direct parameter}{1516}{section*.3285}\protected@file@percent }
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@backref{2095}{kerbl2023_3dgaussiansplatting}{0}{1517}{1517}
\@writefile{lof}{\contentsline {figure}{\numberline {23.72}{\ignorespaces \textbf  {Spherical–harmonics basis intuition.} Each panel shows a real spherical harmonic $Y_{\ell m}(\theta ,\phi )$ on the unit sphere. Red/blue indicate positive/negative values and white shows nodal sets (zeros). The degree $\ell $ controls the number of latitudinal (polar) bands, while $|m|$ controls the number of longitudinal (azimuthal) oscillations. Changing the sign of $m$ rotates the pattern around the vertical axis without altering its node counts. Small glyphs on the $\ell =1$ row mark whether variation is with $\phi $ (horizontal, around the equator) or with $\theta $ (vertical, pole to equator). }}{1517}{figure.caption.3286}\protected@file@percent }
\newlabel{fig:chapter23_sh_idea}{{23.72}{1517}{\textbf {Spherical–harmonics basis intuition.} Each panel shows a real spherical harmonic $Y_{\ell m}(\theta ,\phi )$ on the unit sphere. Red/blue indicate positive/negative values and white shows nodal sets (zeros). The degree $\ell $ controls the number of latitudinal (polar) bands, while $|m|$ controls the number of longitudinal (azimuthal) oscillations. Changing the sign of $m$ rotates the pattern around the vertical axis without altering its node counts. Small glyphs on the $\ell =1$ row mark whether variation is with $\phi $ (horizontal, around the equator) or with $\theta $ (vertical, pole to equator)}{figure.caption.3286}{}}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\@writefile{toc}{\contentsline {paragraph}{Appearance as a directional color field}{1518}{section*.3287}\protected@file@percent }
\abx@aux@backref{2096}{kerbl2023_3dgaussiansplatting}{0}{1518}{1518}
\@writefile{toc}{\contentsline {subsubsection}{Image formation and compositing}{1518}{section*.3288}\protected@file@percent }
\newlabel{enr:subsubsec_chapter23_image_formation}{{23.10.9}{1518}{Image formation and compositing}{section*.3288}{}}
\@writefile{toc}{\contentsline {paragraph}{What each splat provides}{1518}{section*.3289}\protected@file@percent }
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\@writefile{toc}{\contentsline {paragraph}{Depth ordering for visibility}{1519}{section*.3290}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Front–to–back transmittance (premultiplied form)}{1519}{section*.3291}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why \emph  {front–to–back} matters now}{1519}{section*.3292}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Practical footprint (compact support)}{1519}{section*.3293}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Adaptive densification}{1519}{section*.3294}\protected@file@percent }
\newlabel{enr:subsubsec_chapter23_densification}{{23.10.9}{1519}{Adaptive densification}{section*.3294}{}}
\@writefile{toc}{\contentsline {paragraph}{Goal and signal}{1519}{section*.3295}\protected@file@percent }
\abx@aux@backref{2097}{kerbl2023_3dgaussiansplatting}{0}{1519}{1519}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\@writefile{lof}{\contentsline {figure}{\numberline {23.73}{\ignorespaces \textbf  {Adaptive Gaussian densification} (illustration following \blx@tocontentsinit {0}\cite {kerbl2023_3dgaussiansplatting}). \emph  {Top:} under-reconstruction (black outline not well covered) $\Rightarrow $ \textbf  {clone} the Gaussian and nudge along the positional gradient to add coverage. \emph  {Bottom:} over-reconstruction by a single large splat $\Rightarrow $ \textbf  {split} into two smaller splats to increase spatial resolution.}}{1520}{figure.caption.3296}\protected@file@percent }
\abx@aux@backref{2099}{kerbl2023_3dgaussiansplatting}{0}{1520}{1520}
\newlabel{fig:chapter23_gaussian_densification}{{23.73}{1520}{\textbf {Adaptive Gaussian densification} (illustration following \cite {kerbl2023_3dgaussiansplatting}). \emph {Top:} under-reconstruction (black outline not well covered) $\Rightarrow $ \textbf {clone} the Gaussian and nudge along the positional gradient to add coverage. \emph {Bottom:} over-reconstruction by a single large splat $\Rightarrow $ \textbf {split} into two smaller splats to increase spatial resolution}{figure.caption.3296}{}}
\@writefile{toc}{\contentsline {paragraph}{Clone (add coverage)}{1520}{section*.3297}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Split (resolve detail)}{1520}{section*.3298}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Prune (stay compact)}{1520}{section*.3299}\protected@file@percent }
\abx@aux@cite{0}{nilsson2020_understanding_ssim}
\abx@aux@segm{0}{0}{nilsson2020_understanding_ssim}
\abx@aux@cite{0}{baker2022_ssim_floating_point}
\abx@aux@segm{0}{0}{baker2022_ssim_floating_point}
\abx@aux@cite{0}{zhang2018_lpips}
\abx@aux@segm{0}{0}{zhang2018_lpips}
\@writefile{toc}{\contentsline {paragraph}{When and how often}{1521}{section*.3300}\protected@file@percent }
\abx@aux@backref{2100}{kerbl2023_3dgaussiansplatting}{0}{1521}{1521}
\@writefile{toc}{\contentsline {paragraph}{Effect on optimization}{1521}{section*.3301}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Training objective and schedules}{1521}{section*.3302}\protected@file@percent }
\newlabel{enr:subsubsec_chapter23_objective}{{23.10.9}{1521}{Training objective and schedules}{section*.3302}{}}
\@writefile{toc}{\contentsline {paragraph}{Photometric objective}{1521}{section*.3303}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{SSIM, D-SSIM, and the notion of “structure”}{1521}{section*.3304}\protected@file@percent }
\abx@aux@backref{2101}{baker2022_ssim_floating_point}{0}{1521}{1521}
\abx@aux@backref{2102}{nilsson2020_understanding_ssim}{0}{1521}{1521}
\@writefile{toc}{\contentsline {paragraph}{Why not pure MSE/PSNR}{1521}{section*.3305}\protected@file@percent }
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\@writefile{toc}{\contentsline {paragraph}{LPIPS vs.\ SSIM in training}{1522}{section*.3306}\protected@file@percent }
\abx@aux@backref{2103}{zhang2018_lpips}{0}{1522}{1522}
\abx@aux@backref{2104}{kerbl2023_3dgaussiansplatting}{0}{1522}{1522}
\@writefile{toc}{\contentsline {paragraph}{Update schedule and stability}{1522}{section*.3307}\protected@file@percent }
\abx@aux@backref{2105}{kerbl2023_3dgaussiansplatting}{0}{1522}{1522}
\@writefile{toc}{\contentsline {subsubsection}{Differentiable tile–based rasterizer}{1522}{section*.3308}\protected@file@percent }
\newlabel{enr:subsubsec_chapter23_3dgs_rasterizer}{{23.10.9}{1522}{Differentiable tile–based rasterizer}{section*.3308}{}}
\@writefile{toc}{\contentsline {paragraph}{Goal and inputs}{1522}{section*.3309}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Stage A: cull and bound}{1522}{section*.3310}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Stage B: tile binning}{1523}{section*.3311}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Stage C: global sort by (tile, depth)}{1523}{section*.3312}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Stage D: per–tile blending (forward)}{1523}{section*.3313}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Stage E: per–tile gradients (backward)}{1523}{section*.3314}\protected@file@percent }
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{knapitsch2017_tanks}
\abx@aux@segm{0}{0}{knapitsch2017_tanks}
\abx@aux@cite{0}{hedman2018_deepblending}
\abx@aux@segm{0}{0}{hedman2018_deepblending}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{fridovichkeil2022_plenoxels}
\abx@aux@segm{0}{0}{fridovichkeil2022_plenoxels}
\abx@aux@cite{0}{mueller2022_instantngp}
\abx@aux@segm{0}{0}{mueller2022_instantngp}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{fridovichkeil2022_plenoxels}
\abx@aux@segm{0}{0}{fridovichkeil2022_plenoxels}
\abx@aux@cite{0}{mueller2022_instantngp}
\abx@aux@segm{0}{0}{mueller2022_instantngp}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\@writefile{toc}{\contentsline {paragraph}{Numerical and implementation notes}{1524}{section*.3315}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Experiments and ablations}{1524}{section*.3316}\protected@file@percent }
\newlabel{enr:subsubsec_chapter23_3dgs_experiments}{{23.10.9}{1524}{Experiments and ablations}{section*.3316}{}}
\@writefile{toc}{\contentsline {paragraph}{Datasets and evaluation protocol}{1524}{section*.3317}\protected@file@percent }
\abx@aux@backref{2106}{barron2022_mipnerf360}{0}{1524}{1524}
\abx@aux@backref{2107}{knapitsch2017_tanks}{0}{1524}{1524}
\abx@aux@backref{2108}{hedman2018_deepblending}{0}{1524}{1524}
\abx@aux@backref{2109}{mildenhall2020_nerf}{0}{1524}{1524}
\abx@aux@backref{2110}{kerbl2023_3dgaussiansplatting}{0}{1524}{1524}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\@writefile{lof}{\contentsline {figure}{\numberline {23.74}{\ignorespaces \textbf  {Real-time quality vs.\ training budget (Bike).} 3D Gaussian Splatting (3DGS) achieves real-time rendering with image quality competitive with the best prior method (Mip-NeRF360 \blx@tocontentsinit {0}\cite {barron2022_mipnerf360}) while requiring training times comparable to the fastest explicit methods (Plenoxels \blx@tocontentsinit {0}\cite {fridovichkeil2022_plenoxels}, Instant-NGP \blx@tocontentsinit {0}\cite {mueller2022_instantngp}). For a training budget similar to Instant-NGP, 3DGS matches its quality; with a longer budget (e.g., $\sim $51\,min), 3DGS reaches state-of-the-art PSNR, even slightly surpassing Mip-NeRF360 on this scene. Higher PSNR is better. Adapted from \blx@tocontentsinit {0}\cite {kerbl2023_3dgaussiansplatting}.}}{1525}{figure.caption.3318}\protected@file@percent }
\abx@aux@backref{2115}{barron2022_mipnerf360}{0}{1525}{1525}
\abx@aux@backref{2116}{fridovichkeil2022_plenoxels}{0}{1525}{1525}
\abx@aux@backref{2117}{mueller2022_instantngp}{0}{1525}{1525}
\abx@aux@backref{2118}{kerbl2023_3dgaussiansplatting}{0}{1525}{1525}
\newlabel{fig:chapter23_bike_visual}{{23.74}{1525}{\textbf {Real-time quality vs.\ training budget (Bike).} 3D Gaussian Splatting (3DGS) achieves real-time rendering with image quality competitive with the best prior method (Mip-NeRF360 \cite {barron2022_mipnerf360}) while requiring training times comparable to the fastest explicit methods (Plenoxels \cite {fridovichkeil2022_plenoxels}, Instant-NGP \cite {mueller2022_instantngp}). For a training budget similar to Instant-NGP, 3DGS matches its quality; with a longer budget (e.g., $\sim $51\,min), 3DGS reaches state-of-the-art PSNR, even slightly surpassing Mip-NeRF360 on this scene. Higher PSNR is better. Adapted from \cite {kerbl2023_3dgaussiansplatting}}{figure.caption.3318}{}}
\@writefile{toc}{\contentsline {paragraph}{Quantitative comparison (held-out views)}{1525}{section*.3319}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {23.15}{\ignorespaces \textbf  {Mip-NeRF360} (test views). Higher SSIM/PSNR, lower LPIPS are better. Training time and inference FPS reported as in \blx@tocontentsinit {0}\cite {kerbl2023_3dgaussiansplatting}.}}{1525}{table.caption.3320}\protected@file@percent }
\abx@aux@backref{2120}{kerbl2023_3dgaussiansplatting}{0}{1525}{1525}
\newlabel{tab:chapter23_mipnerf360}{{23.15}{1525}{\textbf {Mip-NeRF360} (test views). Higher SSIM/PSNR, lower LPIPS are better. Training time and inference FPS reported as in \cite {kerbl2023_3dgaussiansplatting}}{table.caption.3320}{}}
\@writefile{lot}{\contentsline {table}{\numberline {23.16}{\ignorespaces \textbf  {Tanks\&Temples} (test views). Reported as in \blx@tocontentsinit {0}\cite {kerbl2023_3dgaussiansplatting}.}}{1525}{table.caption.3321}\protected@file@percent }
\abx@aux@backref{2122}{kerbl2023_3dgaussiansplatting}{0}{1525}{1525}
\newlabel{tab:chapter23_tandt}{{23.16}{1525}{\textbf {Tanks\&Temples} (test views). Reported as in \cite {kerbl2023_3dgaussiansplatting}}{table.caption.3321}{}}
\@writefile{lot}{\contentsline {table}{\numberline {23.17}{\ignorespaces \textbf  {Deep Blending} (test views). Reported as in \blx@tocontentsinit {0}\cite {kerbl2023_3dgaussiansplatting}.}}{1525}{table.caption.3322}\protected@file@percent }
\abx@aux@backref{2124}{kerbl2023_3dgaussiansplatting}{0}{1525}{1525}
\newlabel{tab:chapter23_deepblending}{{23.17}{1525}{\textbf {Deep Blending} (test views). Reported as in \cite {kerbl2023_3dgaussiansplatting}}{table.caption.3322}{}}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\@writefile{toc}{\contentsline {paragraph}{Qualitative comparisons}{1526}{section*.3323}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.75}{\ignorespaces \textbf  {Visual comparisons on held-out views} across Mip-NeRF360 (Bicycle, Garden, Stump, Counter, Room), Deep Blending (Playroom, DrJohnson) and Tanks\&Temples (Truck, Train). Insets/arrows highlight non-obvious differences. 3DGS matches or exceeds prior methods detail and stability while rendering in real time. Adapted from \blx@tocontentsinit {0}\cite {kerbl2023_3dgaussiansplatting}.}}{1526}{figure.caption.3324}\protected@file@percent }
\abx@aux@backref{2126}{kerbl2023_3dgaussiansplatting}{0}{1526}{1526}
\newlabel{fig:chapter23_additional_comparisons}{{23.75}{1526}{\textbf {Visual comparisons on held-out views} across Mip-NeRF360 (Bicycle, Garden, Stump, Counter, Room), Deep Blending (Playroom, DrJohnson) and Tanks\&Temples (Truck, Train). Insets/arrows highlight non-obvious differences. 3DGS matches or exceeds prior methods detail and stability while rendering in real time. Adapted from \cite {kerbl2023_3dgaussiansplatting}}{figure.caption.3324}{}}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\@writefile{toc}{\contentsline {paragraph}{Training-time vs.\ quality}{1527}{section*.3325}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.76}{\ignorespaces \textbf  {Quality over training iterations.} Top: at $7$K iters ($\sim $5–8\,min), the Train scene is already well reconstructed; by $30$K iters ($\sim $35\,min) background artifacts fade. Bottom: in easier scenes, $7$K is nearly indistinguishable from $30$K. Adapted from \blx@tocontentsinit {0}\cite {kerbl2023_3dgaussiansplatting}.}}{1527}{figure.caption.3326}\protected@file@percent }
\abx@aux@backref{2128}{kerbl2023_3dgaussiansplatting}{0}{1527}{1527}
\newlabel{fig:chapter23_iters}{{23.76}{1527}{\textbf {Quality over training iterations.} Top: at $7$K iters ($\sim $5–8\,min), the Train scene is already well reconstructed; by $30$K iters ($\sim $35\,min) background artifacts fade. Bottom: in easier scenes, $7$K is nearly indistinguishable from $30$K. Adapted from \cite {kerbl2023_3dgaussiansplatting}}{figure.caption.3326}{}}
\@writefile{toc}{\contentsline {paragraph}{Synthetic NeRF (Blender) PSNR}{1527}{section*.3327}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {23.18}{\ignorespaces \textbf  {Synthetic NeRF} (PSNR, $\uparrow $). 3DGS uses $100$K randomly initialized points. Reported as in \blx@tocontentsinit {0}\cite {kerbl2023_3dgaussiansplatting}.}}{1527}{table.caption.3328}\protected@file@percent }
\abx@aux@backref{2130}{kerbl2023_3dgaussiansplatting}{0}{1527}{1527}
\newlabel{tab:chapter23_synthetic}{{23.18}{1527}{\textbf {Synthetic NeRF} (PSNR, $\uparrow $). 3DGS uses $100$K randomly initialized points. Reported as in \cite {kerbl2023_3dgaussiansplatting}}{table.caption.3328}{}}
\@writefile{toc}{\contentsline {paragraph}{Ablations}{1527}{section*.3329}\protected@file@percent }
\abx@aux@backref{2131}{kerbl2023_3dgaussiansplatting}{0}{1527}{1527}
\@writefile{lot}{\contentsline {table}{\numberline {23.19}{\ignorespaces \textbf  {Ablation PSNR} (downsampled inputs for stability). Average over Truck/Garden/Bicycle at 5K/30K iterations.}}{1527}{table.caption.3330}\protected@file@percent }
\newlabel{tab:chapter23_ablation}{{23.19}{1527}{\textbf {Ablation PSNR} (downsampled inputs for stability). Average over Truck/Garden/Bicycle at 5K/30K iterations}{table.caption.3330}{}}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\@writefile{lof}{\contentsline {figure}{\numberline {23.77}{\ignorespaces \textbf  {Densification ablation.} Without \textbf  {split}: background remains blurred. Without \textbf  {clone}: high-frequency structures (e.g., bike spokes/wheels) artifact. Both operations are needed (cf.\ Fig.~\ref {fig:chapter23_gaussian_densification}). Adapted from \blx@tocontentsinit {0}\cite {kerbl2023_3dgaussiansplatting}.}}{1528}{figure.caption.3331}\protected@file@percent }
\abx@aux@backref{2133}{kerbl2023_3dgaussiansplatting}{0}{1528}{1528}
\newlabel{fig:chapter23_densification_ablation}{{23.77}{1528}{\textbf {Densification ablation.} Without \textbf {split}: background remains blurred. Without \textbf {clone}: high-frequency structures (e.g., bike spokes/wheels) artifact. Both operations are needed (cf.\ Fig.~\ref {fig:chapter23_gaussian_densification}). Adapted from \cite {kerbl2023_3dgaussiansplatting}}{figure.caption.3331}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23.78}{\ignorespaces \textbf  {Gradient sparsity ablation.} Limiting the number of Gaussians that receive gradients per step severely degrades quality (left); full method (right) benefits from dense, overlapped gradients through the rasterizer. Adapted from \blx@tocontentsinit {0}\cite {kerbl2023_3dgaussiansplatting}.}}{1528}{figure.caption.3332}\protected@file@percent }
\abx@aux@backref{2135}{kerbl2023_3dgaussiansplatting}{0}{1528}{1528}
\newlabel{fig:chapter23_points_gradients}{{23.78}{1528}{\textbf {Gradient sparsity ablation.} Limiting the number of Gaussians that receive gradients per step severely degrades quality (left); full method (right) benefits from dense, overlapped gradients through the rasterizer. Adapted from \cite {kerbl2023_3dgaussiansplatting}}{figure.caption.3332}{}}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\@writefile{lof}{\contentsline {figure}{\numberline {23.79}{\ignorespaces \textbf  {Initialization matters.} Top: random point cloud; bottom: SfM seeding. SfM provides a much better starting geometry, accelerating and stabilizing training. Adapted from \blx@tocontentsinit {0}\cite {kerbl2023_3dgaussiansplatting}.}}{1529}{figure.caption.3333}\protected@file@percent }
\abx@aux@backref{2137}{kerbl2023_3dgaussiansplatting}{0}{1529}{1529}
\newlabel{fig:chapter23_random_vs_sfm}{{23.79}{1529}{\textbf {Initialization matters.} Top: random point cloud; bottom: SfM seeding. SfM provides a much better starting geometry, accelerating and stabilizing training. Adapted from \cite {kerbl2023_3dgaussiansplatting}}{figure.caption.3333}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23.80}{\ignorespaces \textbf  {Anisotropy ablation (Ficus, capped at 5k Gaussians).} Enabling \emph  {anisotropic} covariances is critical for fine/filamentary structure; isotropic splats require many more primitives and still blur edges. Adapted from \blx@tocontentsinit {0}\cite {kerbl2023_3dgaussiansplatting}.}}{1529}{figure.caption.3334}\protected@file@percent }
\abx@aux@backref{2139}{kerbl2023_3dgaussiansplatting}{0}{1529}{1529}
\newlabel{fig:chapter23_anisotropy}{{23.80}{1529}{\textbf {Anisotropy ablation (Ficus, capped at 5k Gaussians).} Enabling \emph {anisotropic} covariances is critical for fine/filamentary structure; isotropic splats require many more primitives and still blur edges. Adapted from \cite {kerbl2023_3dgaussiansplatting}}{figure.caption.3334}{}}
\@writefile{toc}{\contentsline {paragraph}{Takeaways}{1529}{section*.3335}\protected@file@percent }
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@cite{0}{kerbl2023_3dgaussiansplatting}
\abx@aux@segm{0}{0}{kerbl2023_3dgaussiansplatting}
\@writefile{toc}{\contentsline {subsubsection}{Limitations and future work}{1530}{section*.3336}\protected@file@percent }
\newlabel{enr:subsubsec_chapter23_limits}{{23.10.9}{1530}{Limitations and future work}{section*.3336}{}}
\@writefile{toc}{\contentsline {paragraph}{Observed failure modes}{1530}{section*.3337}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.81}{\ignorespaces \textbf  {Failure artifacts (Train).} Compared to Mip-NeRF360~\blx@tocontentsinit {0}\cite {barron2022_mipnerf360} (left), which can exhibit ``floaters'' and grain in poorly constrained regions, 3DGS~\blx@tocontentsinit {0}\cite {kerbl2023_3dgaussiansplatting} (right) may render coarse, anisotropic blobs where multi-view coverage is weak, limiting far-background detail. Adapted from \blx@tocontentsinit {0}\cite {kerbl2023_3dgaussiansplatting}.}}{1530}{figure.caption.3338}\protected@file@percent }
\abx@aux@backref{2143}{barron2022_mipnerf360}{0}{1530}{1530}
\abx@aux@backref{2144}{kerbl2023_3dgaussiansplatting}{0}{1530}{1530}
\abx@aux@backref{2145}{kerbl2023_3dgaussiansplatting}{0}{1530}{1530}
\newlabel{fig:chapter23_failure_cases}{{23.81}{1530}{\textbf {Failure artifacts (Train).} Compared to Mip-NeRF360~\cite {barron2022_mipnerf360} (left), which can exhibit ``floaters'' and grain in poorly constrained regions, 3DGS~\cite {kerbl2023_3dgaussiansplatting} (right) may render coarse, anisotropic blobs where multi-view coverage is weak, limiting far-background detail. Adapted from \cite {kerbl2023_3dgaussiansplatting}}{figure.caption.3338}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23.82}{\ignorespaces \textbf  {View extrapolation artifacts (DrJohnson).} When camera poses have little overlap with training views, 3DGS~\blx@tocontentsinit {0}\cite {kerbl2023_3dgaussiansplatting} can produce artifacts (right). Mip-NeRF360~\blx@tocontentsinit {0}\cite {barron2022_mipnerf360} also degrades under such extrapolation (left), albeit with different characteristics. Adapted from \blx@tocontentsinit {0}\cite {kerbl2023_3dgaussiansplatting}.}}{1530}{figure.caption.3339}\protected@file@percent }
\abx@aux@backref{2149}{kerbl2023_3dgaussiansplatting}{0}{1530}{1530}
\abx@aux@backref{2150}{barron2022_mipnerf360}{0}{1530}{1530}
\abx@aux@backref{2151}{kerbl2023_3dgaussiansplatting}{0}{1530}{1530}
\newlabel{fig:chapter23_artifacts}{{23.82}{1530}{\textbf {View extrapolation artifacts (DrJohnson).} When camera poses have little overlap with training views, 3DGS~\cite {kerbl2023_3dgaussiansplatting} can produce artifacts (right). Mip-NeRF360~\cite {barron2022_mipnerf360} also degrades under such extrapolation (left), albeit with different characteristics. Adapted from \cite {kerbl2023_3dgaussiansplatting}}{figure.caption.3339}{}}
\abx@aux@backref{2152}{barron2022_mipnerf360}{0}{1530}{1530}
\abx@aux@backref{2153}{kerbl2023_3dgaussiansplatting}{0}{1530}{1530}
\abx@aux@backref{2154}{kerbl2023_3dgaussiansplatting}{0}{1530}{1530}
\abx@aux@backref{2155}{kerbl2023_3dgaussiansplatting}{0}{1530}{1530}
\abx@aux@backref{2156}{kerbl2023_3dgaussiansplatting}{0}{1530}{1530}
\abx@aux@cite{0}{Wu_2023_4DGS}
\abx@aux@segm{0}{0}{Wu_2023_4DGS}
\abx@aux@cite{0}{Rosinol_2024_SplaTAM}
\abx@aux@segm{0}{0}{Rosinol_2024_SplaTAM}
\abx@aux@cite{0}{Wang_2024_GSIR}
\abx@aux@segm{0}{0}{Wang_2024_GSIR}
\abx@aux@cite{0}{Kerbl_2024_GaussianSurfels}
\abx@aux@segm{0}{0}{Kerbl_2024_GaussianSurfels}
\abx@aux@cite{0}{Fu_2024_COLMAPFreeGS}
\abx@aux@segm{0}{0}{Fu_2024_COLMAPFreeGS}
\abx@aux@cite{0}{Tang_2023_DreamGaussian}
\abx@aux@segm{0}{0}{Tang_2023_DreamGaussian}
\abx@aux@cite{0}{Zhang_2024_SuGaR}
\abx@aux@segm{0}{0}{Zhang_2024_SuGaR}
\@writefile{toc}{\contentsline {paragraph}{Future work}{1531}{section*.3340}\protected@file@percent }
\abx@aux@backref{2157}{Wu_2023_4DGS}{0}{1531}{1531}
\abx@aux@backref{2158}{Rosinol_2024_SplaTAM}{0}{1531}{1531}
\abx@aux@backref{2159}{Wang_2024_GSIR}{0}{1531}{1531}
\abx@aux@backref{2160}{Kerbl_2024_GaussianSurfels}{0}{1531}{1531}
\abx@aux@backref{2161}{Fu_2024_COLMAPFreeGS}{0}{1531}{1531}
\abx@aux@backref{2162}{Tang_2023_DreamGaussian}{0}{1531}{1531}
\abx@aux@backref{2163}{Zhang_2024_SuGaR}{0}{1531}{1531}
\BKM@entry{id=914,dest={73656374696F6E2A2E33333431},srcline={6066}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030335C3030302E5C303030315C303030315C3030303A5C3030305C3034305C3030304E5C303030655C303030525C303030465C3030303A5C3030305C3034305C303030525C303030655C303030615C3030306C5C3030302D5C303030575C3030306F5C303030725C3030306C5C303030645C3030305C3034305C303030525C3030306F5C303030625C303030755C303030735C303030745C3030306E5C303030655C303030735C303030735C3030305C3034305C3030305C3034365C3030305C3034305C303030535C303030705C303030615C303030725C303030735C303030655C3030305C3034305C303030535C303030755C303030705C303030655C303030725C303030765C303030695C303030735C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{lin2021_barf}
\abx@aux@segm{0}{0}{lin2021_barf}
\abx@aux@cite{0}{martinbrualla2021_nerfw}
\abx@aux@segm{0}{0}{martinbrualla2021_nerfw}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\abx@aux@cite{0}{yu2020_pixelnerf}
\abx@aux@segm{0}{0}{yu2020_pixelnerf}
\abx@aux@cite{0}{niemeyer2022_regnerf}
\abx@aux@segm{0}{0}{niemeyer2022_regnerf}
\abx@aux@cite{0}{turki2022_meganerf}
\abx@aux@segm{0}{0}{turki2022_meganerf}
\BKM@entry{id=915,dest={73656374696F6E2A2E33333432},srcline={6081}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030335C3030302E5C303030315C303030315C3030302E5C303030315C3030303A5C3030305C3034305C303030425C303030415C303030525C303030465C3030303A5C3030305C3034305C303030425C303030755C3030306E5C303030645C3030306C5C303030655C3030302D5C303030415C303030645C3030306A5C303030755C303030735C303030745C303030695C3030306E5C303030675C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C303030525C303030615C303030645C303030695C303030615C3030306E5C303030635C303030655C3030305C3034305C303030465C303030695C303030655C3030306C5C303030645C30303073}
\abx@aux@cite{0}{lin2021_barf}
\abx@aux@segm{0}{0}{lin2021_barf}
\@writefile{toc}{\contentsline {section}{Enrichment 23.11: NeRF: Real-World Robustness \& Sparse Supervision}{1532}{section*.3341}\protected@file@percent }
\abx@aux@backref{2164}{lin2021_barf}{0}{1532}{1532}
\abx@aux@backref{2165}{martinbrualla2021_nerfw}{0}{1532}{1532}
\abx@aux@backref{2166}{wang2021_ibrnet}{0}{1532}{1532}
\abx@aux@backref{2167}{yu2020_pixelnerf}{0}{1532}{1532}
\abx@aux@backref{2168}{niemeyer2022_regnerf}{0}{1532}{1532}
\abx@aux@backref{2169}{turki2022_meganerf}{0}{1532}{1532}
\@writefile{toc}{\contentsline {subsection}{Enrichment 23.11.1: BARF: Bundle-Adjusting Neural Radiance Fields}{1532}{section*.3342}\protected@file@percent }
\newlabel{enr:subsec_chapter23_barf}{{23.11.1}{1532}{\color {ocre}Enrichment \thesubsection : BARF: Bundle-Adjusting Neural Radiance Fields}{section*.3342}{}}
\@writefile{toc}{\contentsline {subsubsection}{Motivation and problem setting}{1532}{section*.3343}\protected@file@percent }
\newlabel{subsubsec:chapter23_barf_motivation}{{23.11.1}{1532}{Motivation and problem setting}{section*.3343}{}}
\@writefile{toc}{\contentsline {paragraph}{Why this problem matters}{1532}{section*.3344}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{What makes joint optimization hard}{1532}{section*.3345}\protected@file@percent }
\abx@aux@backref{2170}{lin2021_barf}{0}{1532}{1532}
\@writefile{toc}{\contentsline {paragraph}{A lesson from classical image alignment}{1532}{section*.3346}\protected@file@percent }
\abx@aux@cite{0}{lin2021_barf}
\abx@aux@segm{0}{0}{lin2021_barf}
\abx@aux@cite{0}{lin2021_barf}
\abx@aux@segm{0}{0}{lin2021_barf}
\abx@aux@cite{0}{lin2021_barf}
\abx@aux@segm{0}{0}{lin2021_barf}
\@writefile{toc}{\contentsline {paragraph}{The paper's idea and contribution}{1533}{section*.3347}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.83}{\ignorespaces \textbf  {Training NeRF requires accurate camera poses.} BARF jointly optimizes camera registration and neural 3D reconstruction, enabling learning from imperfect or unknown poses. Reproduced from \blx@tocontentsinit {0}\cite {lin2021_barf}.}}{1533}{figure.caption.3348}\protected@file@percent }
\abx@aux@backref{2172}{lin2021_barf}{0}{1533}{1533}
\newlabel{fig:chapter23_barf_overview}{{23.83}{1533}{\textbf {Training NeRF requires accurate camera poses.} BARF jointly optimizes camera registration and neural 3D reconstruction, enabling learning from imperfect or unknown poses. Reproduced from \cite {lin2021_barf}}{figure.caption.3348}{}}
\abx@aux@backref{2173}{lin2021_barf}{0}{1533}{1533}
\@writefile{toc}{\contentsline {subsubsection}{High level overview of BARF}{1534}{section*.3349}\protected@file@percent }
\newlabel{subsubsec:chapter23_barf_high_level}{{23.11.1}{1534}{High level overview of BARF}{section*.3349}{}}
\@writefile{toc}{\contentsline {paragraph}{Joint objective}{1534}{section*.3350}\protected@file@percent }
\newlabel{eq:barf_joint_objective}{{23.38}{1534}{Joint objective}{equation.23.38}{}}
\@writefile{toc}{\contentsline {paragraph}{Bandwidth scheduling via windowed positional encoding}{1534}{section*.3351}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Roadmap}{1535}{section*.3352}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Method and derivations}{1535}{section*.3353}\protected@file@percent }
\newlabel{subsubsec:chapter23_barf_method}{{23.11.1}{1535}{Method and derivations}{section*.3353}{}}
\@writefile{toc}{\contentsline {paragraph}{NeRF with differentiable volume rendering}{1535}{section*.3354}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Joint objective (reference)}{1535}{section*.3355}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{From rigid motions to minimal pose updates}{1535}{section*.3356}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Twist updates via the exponential map}{1536}{section*.3357}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How a pose increment moves 3D samples (and affects colors)}{1537}{section*.3358}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Ray-compositing gradients (decomposition)}{1538}{section*.3359}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why smooth inputs help pose gradients}{1538}{section*.3360}\protected@file@percent }
\abx@aux@cite{0}{lin2021_barf}
\abx@aux@segm{0}{0}{lin2021_barf}
\newlabel{eq:barf_gamma_jac_from_paper}{{23.39}{1539}{Why smooth inputs help pose gradients}{equation.23.39}{}}
\@writefile{toc}{\contentsline {subsubsection}{Coarse-to-fine positional encoding}{1539}{section*.3361}\protected@file@percent }
\newlabel{subsubsec:chapter23_barf_c2f_pe}{{23.11.1}{1539}{Coarse-to-fine positional encoding}{section*.3361}{}}
\@writefile{toc}{\contentsline {paragraph}{Windowed positional encoding}{1539}{section*.3362}\protected@file@percent }
\newlabel{eq:barf_gamma_weighted}{{23.40}{1539}{Windowed positional encoding}{equation.23.40}{}}
\newlabel{eq:barf_wk_exact}{{23.41}{1539}{Windowed positional encoding}{equation.23.41}{}}
\newlabel{eq:barf_gamma_jac_weighted}{{23.42}{1539}{Windowed positional encoding}{equation.23.42}{}}
\@writefile{toc}{\contentsline {paragraph}{Architecture and implementation details}{1539}{section*.3363}\protected@file@percent }
\newlabel{subsubsec:chapter23_barf_impl}{{23.11.1}{1539}{Architecture and implementation details}{section*.3363}{}}
\abx@aux@backref{2174}{lin2021_barf}{0}{1539}{1539}
\abx@aux@cite{0}{lin2021_barf}
\abx@aux@segm{0}{0}{lin2021_barf}
\abx@aux@cite{0}{lin2021_barf}
\abx@aux@segm{0}{0}{lin2021_barf}
\abx@aux@cite{0}{lin2021_barf}
\abx@aux@segm{0}{0}{lin2021_barf}
\abx@aux@cite{0}{lin2021_barf}
\abx@aux@segm{0}{0}{lin2021_barf}
\abx@aux@cite{0}{lin2021_barf}
\abx@aux@segm{0}{0}{lin2021_barf}
\@writefile{toc}{\contentsline {paragraph}{Network and sampling}{1540}{section*.3364}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Optimization}{1540}{section*.3365}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Experiments and ablations}{1540}{section*.3366}\protected@file@percent }
\newlabel{enr:subsubsec_chapter23_barf_experiments}{{23.11.1}{1540}{Experiments and ablations}{section*.3366}{}}
\@writefile{toc}{\contentsline {paragraph}{Datasets and evaluation protocol}{1540}{section*.3367}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Planar image alignment}{1540}{section*.3368}\protected@file@percent }
\abx@aux@backref{2175}{lin2021_barf}{0}{1540}{1540}
\@writefile{lof}{\contentsline {figure}{\numberline {23.84}{\ignorespaces Planar alignment setup and ground-truth warps. Reproduced from \blx@tocontentsinit {0}\cite {lin2021_barf}.}}{1540}{figure.caption.3369}\protected@file@percent }
\abx@aux@backref{2177}{lin2021_barf}{0}{1540}{1540}
\newlabel{fig:chapter23_barf_patches}{{23.84}{1540}{Planar alignment setup and ground-truth warps. Reproduced from \cite {lin2021_barf}}{figure.caption.3369}{}}
\abx@aux@cite{0}{lin2021_barf}
\abx@aux@segm{0}{0}{lin2021_barf}
\abx@aux@cite{0}{lin2021_barf}
\abx@aux@segm{0}{0}{lin2021_barf}
\@writefile{lof}{\contentsline {figure}{\numberline {23.85}{\ignorespaces Qualitative planar alignment: optimized warps (top), patch reconstructions (middle), and recovered image representation (bottom). BARF attains accurate warps and high-fidelity reconstruction; full PE misregisters and no-PE blurs. Reproduced from \blx@tocontentsinit {0}\cite {lin2021_barf}.}}{1541}{figure.caption.3370}\protected@file@percent }
\abx@aux@backref{2179}{lin2021_barf}{0}{1541}{1541}
\newlabel{fig:chapter23_barf_planar_qual}{{23.85}{1541}{Qualitative planar alignment: optimized warps (top), patch reconstructions (middle), and recovered image representation (bottom). BARF attains accurate warps and high-fidelity reconstruction; full PE misregisters and no-PE blurs. Reproduced from \cite {lin2021_barf}}{figure.caption.3370}{}}
\@writefile{lot}{\contentsline {table}{\numberline {23.20}{\ignorespaces Planar alignment ablation on positional encoding (values from \blx@tocontentsinit {0}\cite {lin2021_barf}). Homographies are estimated by minimizing photometric error and compared to ground truth via the geodesic/log metric on $\mathrm  {SL}(3)$, $d_{\mathrm  {SL}(3)}(H_\text  {est},H_\text  {gt})=\|\log (H_\text  {est}H_\text  {gt}^{-1})\|_F$ (lower is better). Reconstruction quality is measured by PSNR on a target patch after warping with $H_\text  {est}$ (higher is better). Coarse-to-fine PE yields both the most accurate registration and the best reconstruction.}}{1541}{table.caption.3371}\protected@file@percent }
\abx@aux@backref{2181}{lin2021_barf}{0}{1541}{1541}
\newlabel{tab:chapter23_barf_planar}{{23.20}{1541}{Planar alignment ablation on positional encoding (values from \cite {lin2021_barf}). Homographies are estimated by minimizing photometric error and compared to ground truth via the geodesic/log metric on $\mathrm {SL}(3)$, $d_{\mathrm {SL}(3)}(H_\text {est},H_\text {gt})=\|\log (H_\text {est}H_\text {gt}^{-1})\|_F$ (lower is better). Reconstruction quality is measured by PSNR on a target patch after warping with $H_\text {est}$ (higher is better). Coarse-to-fine PE yields both the most accurate registration and the best reconstruction}{table.caption.3371}{}}
\abx@aux@cite{0}{lin2021_barf}
\abx@aux@segm{0}{0}{lin2021_barf}
\abx@aux@cite{0}{lin2021_barf}
\abx@aux@segm{0}{0}{lin2021_barf}
\abx@aux@cite{0}{lin2021_barf}
\abx@aux@segm{0}{0}{lin2021_barf}
\abx@aux@cite{0}{lin2021_barf}
\abx@aux@segm{0}{0}{lin2021_barf}
\abx@aux@cite{0}{lin2021_barf}
\abx@aux@segm{0}{0}{lin2021_barf}
\abx@aux@cite{0}{lin2021_barf}
\abx@aux@segm{0}{0}{lin2021_barf}
\abx@aux@cite{0}{lin2021_barf}
\abx@aux@segm{0}{0}{lin2021_barf}
\@writefile{toc}{\contentsline {paragraph}{Synthetic NeRF scenes}{1542}{section*.3372}\protected@file@percent }
\abx@aux@backref{2182}{lin2021_barf}{0}{1542}{1542}
\@writefile{lof}{\contentsline {figure}{\numberline {23.86}{\ignorespaces Initial versus optimized poses on \texttt  {chair} (Procrustes aligned). BARF realigns the trajectory; full PE gets stuck. Reproduced from \blx@tocontentsinit {0}\cite {lin2021_barf}.}}{1542}{figure.caption.3373}\protected@file@percent }
\abx@aux@backref{2184}{lin2021_barf}{0}{1542}{1542}
\newlabel{fig:chapter23_barf_pose_chair}{{23.86}{1542}{Initial versus optimized poses on \texttt {chair} (Procrustes aligned). BARF realigns the trajectory; full PE gets stuck. Reproduced from \cite {lin2021_barf}}{figure.caption.3373}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23.87}{\ignorespaces Synthetic scenes: image synthesis (top row for each image) and expected depth (bottom row for each image). BARF achieves quality comparable to NeRF trained with perfect poses. Reproduced from \blx@tocontentsinit {0}\cite {lin2021_barf}.}}{1542}{figure.caption.3374}\protected@file@percent }
\abx@aux@backref{2186}{lin2021_barf}{0}{1542}{1542}
\newlabel{fig:chapter23_barf_synth_qual}{{23.87}{1542}{Synthetic scenes: image synthesis (top row for each image) and expected depth (bottom row for each image). BARF achieves quality comparable to NeRF trained with perfect poses. Reproduced from \cite {lin2021_barf}}{figure.caption.3374}{}}
\abx@aux@cite{0}{lin2021_barf}
\abx@aux@segm{0}{0}{lin2021_barf}
\abx@aux@cite{0}{lin2021_barf}
\abx@aux@segm{0}{0}{lin2021_barf}
\abx@aux@cite{0}{lin2021_barf}
\abx@aux@segm{0}{0}{lin2021_barf}
\abx@aux@cite{0}{lin2021_barf}
\abx@aux@segm{0}{0}{lin2021_barf}
\abx@aux@cite{0}{lin2021_barf}
\abx@aux@segm{0}{0}{lin2021_barf}
\abx@aux@cite{0}{lin2021_barf}
\abx@aux@segm{0}{0}{lin2021_barf}
\abx@aux@cite{0}{lin2021_barf}
\abx@aux@segm{0}{0}{lin2021_barf}
\@writefile{lot}{\contentsline {table}{\numberline {23.21}{\ignorespaces NeRF on synthetic scenes with perturbed poses. BARF optimizes registration while maintaining synthesis quality near the pose-supervised reference. Numbers from \blx@tocontentsinit {0}\cite {lin2021_barf}.}}{1543}{table.caption.3375}\protected@file@percent }
\abx@aux@backref{2188}{lin2021_barf}{0}{1543}{1543}
\newlabel{tab:chapter23_barf_blender}{{23.21}{1543}{NeRF on synthetic scenes with perturbed poses. BARF optimizes registration while maintaining synthesis quality near the pose-supervised reference. Numbers from \cite {lin2021_barf}}{table.caption.3375}{}}
\@writefile{toc}{\contentsline {paragraph}{Real LLFF scenes with unknown poses}{1543}{section*.3376}\protected@file@percent }
\abx@aux@backref{2189}{lin2021_barf}{0}{1543}{1543}
\@writefile{lof}{\contentsline {figure}{\numberline {23.88}{\ignorespaces Real scenes from unknown poses. BARF jointly recovers poses and scene; full PE diverges. Reproduced from \blx@tocontentsinit {0}\cite {lin2021_barf}.}}{1543}{figure.caption.3377}\protected@file@percent }
\abx@aux@backref{2191}{lin2021_barf}{0}{1543}{1543}
\newlabel{fig:chapter23_barf_real}{{23.88}{1543}{Real scenes from unknown poses. BARF jointly recovers poses and scene; full PE diverges. Reproduced from \cite {lin2021_barf}}{figure.caption.3377}{}}
\@writefile{lot}{\contentsline {table}{\numberline {23.22}{\ignorespaces LLFF forward-facing scenes from unknown poses. BARF localizes from scratch and attains high-fidelity synthesis. Numbers from \blx@tocontentsinit {0}\cite {lin2021_barf}.}}{1543}{table.caption.3378}\protected@file@percent }
\abx@aux@backref{2193}{lin2021_barf}{0}{1543}{1543}
\newlabel{tab:chapter23_barf_llff}{{23.22}{1543}{LLFF forward-facing scenes from unknown poses. BARF localizes from scratch and attains high-fidelity synthesis. Numbers from \cite {lin2021_barf}}{table.caption.3378}{}}
\abx@aux@cite{0}{truong2023_sparf}
\abx@aux@segm{0}{0}{truong2023_sparf}
\abx@aux@cite{0}{bian2023_nopenerf}
\abx@aux@segm{0}{0}{bian2023_nopenerf}
\abx@aux@cite{0}{wang2023_badnerf}
\abx@aux@segm{0}{0}{wang2023_badnerf}
\abx@aux@cite{0}{chen2023_dbarf}
\abx@aux@segm{0}{0}{chen2023_dbarf}
\BKM@entry{id=916,dest={73656374696F6E2A2E33333833},srcline={6713}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030335C3030302E5C303030315C303030315C3030302E5C303030325C3030303A5C3030305C3034305C3030304E5C303030655C303030525C303030465C3030302D5C303030575C3030303A5C3030305C3034305C3030304E5C303030655C303030525C303030465C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030555C3030306E5C303030635C3030306F5C3030306E5C303030735C303030745C303030725C303030615C303030695C3030306E5C303030655C303030645C3030305C3034305C303030505C303030685C3030306F5C303030745C3030306F5C3030305C3034305C303030435C3030306F5C3030306C5C3030306C5C303030655C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{snavely2008_photo}
\abx@aux@segm{0}{0}{snavely2008_photo}
\@writefile{lof}{\contentsline {figure}{\numberline {23.89}{\ignorespaces Optimized poses on \texttt  {fern} (Procrustes aligned). BARF closely agrees with SfM. Reproduced from \blx@tocontentsinit {0}\cite {lin2021_barf}.}}{1544}{figure.caption.3379}\protected@file@percent }
\abx@aux@backref{2195}{lin2021_barf}{0}{1544}{1544}
\newlabel{fig:chapter23_barf_fern_poses}{{23.89}{1544}{Optimized poses on \texttt {fern} (Procrustes aligned). BARF closely agrees with SfM. Reproduced from \cite {lin2021_barf}}{figure.caption.3379}{}}
\@writefile{toc}{\contentsline {subsubsection}{Limitations and future work}{1544}{section*.3380}\protected@file@percent }
\newlabel{subsubsec:chapter23_barf_limits}{{23.11.1}{1544}{Limitations and future work}{section*.3380}{}}
\@writefile{toc}{\contentsline {paragraph}{Limitations}{1544}{section*.3381}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Follow-ups addressing BARF’s limitations}{1544}{section*.3382}\protected@file@percent }
\abx@aux@backref{2196}{truong2023_sparf}{0}{1544}{1544}
\abx@aux@backref{2197}{bian2023_nopenerf}{0}{1544}{1544}
\abx@aux@backref{2198}{wang2023_badnerf}{0}{1544}{1544}
\abx@aux@backref{2199}{chen2023_dbarf}{0}{1544}{1544}
\abx@aux@cite{0}{martinbrualla2021_nerfw}
\abx@aux@segm{0}{0}{martinbrualla2021_nerfw}
\abx@aux@cite{0}{martinbrualla2021_nerfw}
\abx@aux@segm{0}{0}{martinbrualla2021_nerfw}
\abx@aux@cite{0}{martinbrualla2021_nerfw}
\abx@aux@segm{0}{0}{martinbrualla2021_nerfw}
\@writefile{toc}{\contentsline {subsection}{Enrichment 23.11.2: NeRF-W: NeRF for Unconstrained Photo Collections}{1545}{section*.3383}\protected@file@percent }
\newlabel{enr:subsec_chapter23_nerfw}{{23.11.2}{1545}{\color {ocre}Enrichment \thesubsection : NeRF-W: NeRF for Unconstrained Photo Collections}{section*.3383}{}}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{1545}{section*.3384}\protected@file@percent }
\newlabel{subsec:chapter23_nerfw_motivation}{{23.11.2}{1545}{Motivation}{section*.3384}{}}
\abx@aux@backref{2200}{mildenhall2020_nerf}{0}{1545}{1545}
\abx@aux@backref{2201}{snavely2008_photo}{0}{1545}{1545}
\@writefile{lof}{\contentsline {figure}{\numberline {23.90}{\ignorespaces \textbf  {Variable illumination control with NeRF-W.} (a) Given only an internet photo collection NeRF-W renders novel views with variable illumination (b) Slices from renderings driven by appearance embeddings associated with four training images (Phototourism). Photos by Flickr users dbowie78, vasnic64, punch / CC BY. \emph  {Credit:} \blx@tocontentsinit {0}\cite {martinbrualla2021_nerfw}.}}{1545}{figure.caption.3385}\protected@file@percent }
\abx@aux@backref{2203}{martinbrualla2021_nerfw}{0}{1545}{1545}
\newlabel{fig:chapter23_nerfw_rendering}{{23.90}{1545}{\textbf {Variable illumination control with NeRF-W.} (a) Given only an internet photo collection NeRF-W renders novel views with variable illumination (b) Slices from renderings driven by appearance embeddings associated with four training images (Phototourism). Photos by Flickr users dbowie78, vasnic64, punch / CC BY. \emph {Credit:} \cite {martinbrualla2021_nerfw}}{figure.caption.3385}{}}
\@writefile{toc}{\contentsline {paragraph}{NeRF-W at a glance}{1545}{section*.3386}\protected@file@percent }
\abx@aux@backref{2204}{martinbrualla2021_nerfw}{0}{1545}{1545}
\abx@aux@cite{0}{martinbrualla2021_nerfw}
\abx@aux@segm{0}{0}{martinbrualla2021_nerfw}
\abx@aux@cite{0}{martinbrualla2021_nerfw}
\abx@aux@segm{0}{0}{martinbrualla2021_nerfw}
\@writefile{lof}{\contentsline {figure}{\numberline {23.91}{\ignorespaces \textbf  {NeRF-W model architecture.} Given a 3D position \(\mathbf  {x}\), viewing direction \(\mathbf  {d}\), and learned per-image embeddings \(\ell ^{(a)}_i\) (appearance) and \(\ell ^{(\tau )}_i\) (transient), the network outputs static and transient colors and densities, along with a training-time uncertainty. Static opacity is produced before conditioning on appearance, enforcing a single shared geometry across images. \emph  {Credit:} \blx@tocontentsinit {0}\cite {martinbrualla2021_nerfw}.}}{1546}{figure.caption.3387}\protected@file@percent }
\abx@aux@backref{2206}{martinbrualla2021_nerfw}{0}{1546}{1546}
\newlabel{fig:chapter23_nerfw_architecture}{{23.91}{1546}{\textbf {NeRF-W model architecture.} Given a 3D position \(\mathbf {x}\), viewing direction \(\mathbf {d}\), and learned per-image embeddings \(\ell ^{(a)}_i\) (appearance) and \(\ell ^{(\tau )}_i\) (transient), the network outputs static and transient colors and densities, along with a training-time uncertainty. Static opacity is produced before conditioning on appearance, enforcing a single shared geometry across images. \emph {Credit:} \cite {martinbrualla2021_nerfw}}{figure.caption.3387}{}}
\@writefile{toc}{\contentsline {subsubsection}{Method: Formulation and Derivation}{1547}{section*.3388}\protected@file@percent }
\newlabel{subsec:chapter23_nerfw_method}{{23.11.2}{1547}{Method: Formulation and Derivation}{section*.3388}{}}
\@writefile{toc}{\contentsline {paragraph}{Rendering operator}{1547}{section*.3389}\protected@file@percent }
\newlabel{eq:nerfw_R}{{23.43}{1547}{Rendering operator}{equation.23.43}{}}
\@writefile{toc}{\contentsline {paragraph}{What \(f\) is in practice and how NeRF-W instantiates \(R\)}{1547}{section*.3390}\protected@file@percent }
\newlabel{eq:nerfw_composite_color_recap}{{23.44}{1547}{What \(f\) is in practice and how NeRF-W instantiates \(R\)}{equation.23.44}{}}
\newlabel{eq:nerfw_rendered_beta}{{23.45}{1547}{What \(f\) is in practice and how NeRF-W instantiates \(R\)}{equation.23.45}{}}
\abx@aux@cite{0}{martinbrualla2021_nerfw}
\abx@aux@segm{0}{0}{martinbrualla2021_nerfw}
\abx@aux@cite{0}{martinbrualla2021_nerfw}
\abx@aux@segm{0}{0}{martinbrualla2021_nerfw}
\newlabel{eq:nerfw_gauss_nll_recap}{{23.46}{1548}{What \(f\) is in practice and how NeRF-W instantiates \(R\)}{equation.23.46}{}}
\abx@aux@cite{0}{martinbrualla2021_nerfw}
\abx@aux@segm{0}{0}{martinbrualla2021_nerfw}
\abx@aux@cite{0}{martinbrualla2021_nerfw}
\abx@aux@segm{0}{0}{martinbrualla2021_nerfw}
\abx@aux@cite{0}{martinbrualla2021_nerfw}
\abx@aux@segm{0}{0}{martinbrualla2021_nerfw}
\abx@aux@cite{0}{hedman2018_nrwinwild}
\abx@aux@segm{0}{0}{hedman2018_nrwinwild}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\@writefile{lof}{\contentsline {figure}{\numberline {23.92}{\ignorespaces \textbf  {Training-time composition and uncertainty.} NeRF-W separately renders the static (a) and transient (b) elements of the scene, composites them (c), and compares to the image (d) with a loss weighted by a rendered uncertainty map (e) that discounts anomalous regions. Photo by Flickr user vasnic64 / CC BY. \emph  {Credit:} \blx@tocontentsinit {0}\cite {martinbrualla2021_nerfw}.}}{1549}{figure.caption.3391}\protected@file@percent }
\abx@aux@backref{2208}{martinbrualla2021_nerfw}{0}{1549}{1549}
\newlabel{fig:chapter23_nerfw_flow}{{23.92}{1549}{\textbf {Training-time composition and uncertainty.} NeRF-W separately renders the static (a) and transient (b) elements of the scene, composites them (c), and compares to the image (d) with a loss weighted by a rendered uncertainty map (e) that discounts anomalous regions. Photo by Flickr user vasnic64 / CC BY. \emph {Credit:} \cite {martinbrualla2021_nerfw}}{figure.caption.3391}{}}
\@writefile{toc}{\contentsline {subsubsection}{Architecture \& Implementation Details}{1549}{section*.3392}\protected@file@percent }
\newlabel{subsec:chapter23_nerfw_impl}{{23.11.2}{1549}{Architecture \& Implementation Details}{section*.3392}{}}
\abx@aux@backref{2209}{martinbrualla2021_nerfw}{0}{1549}{1549}
\@writefile{lof}{\contentsline {figure}{\numberline {23.93}{\ignorespaces \textbf  {Half-image optimization for test-time appearance.} During evaluation, $\ell ^{(a)}$ is optimized on the left half of the test image; metrics use the right half. Photo by Flickr user eadaoinflynn / CC BY. \emph  {Credit:} \blx@tocontentsinit {0}\cite {martinbrualla2021_nerfw}.}}{1549}{figure.caption.3393}\protected@file@percent }
\abx@aux@backref{2211}{martinbrualla2021_nerfw}{0}{1549}{1549}
\newlabel{fig:chapter23_nerfw_halfopt}{{23.93}{1549}{\textbf {Half-image optimization for test-time appearance.} During evaluation, $\ell ^{(a)}$ is optimized on the left half of the test image; metrics use the right half. Photo by Flickr user eadaoinflynn / CC BY. \emph {Credit:} \cite {martinbrualla2021_nerfw}}{figure.caption.3393}{}}
\abx@aux@cite{0}{martinbrualla2021_nerfw}
\abx@aux@segm{0}{0}{martinbrualla2021_nerfw}
\abx@aux@cite{0}{martinbrualla2021_nerfw}
\abx@aux@segm{0}{0}{martinbrualla2021_nerfw}
\abx@aux@cite{0}{martinbrualla2021_nerfw}
\abx@aux@segm{0}{0}{martinbrualla2021_nerfw}
\abx@aux@cite{0}{martinbrualla2021_nerfw}
\abx@aux@segm{0}{0}{martinbrualla2021_nerfw}
\abx@aux@cite{0}{martinbrualla2021_nerfw}
\abx@aux@segm{0}{0}{martinbrualla2021_nerfw}
\abx@aux@cite{0}{martinbrualla2021_nerfw}
\abx@aux@segm{0}{0}{martinbrualla2021_nerfw}
\@writefile{toc}{\contentsline {subsubsection}{Experiments and Ablations}{1550}{section*.3394}\protected@file@percent }
\newlabel{subsec:chapter23_nerfw_exps}{{23.11.2}{1550}{Experiments and Ablations}{section*.3394}{}}
\abx@aux@backref{2212}{hedman2018_nrwinwild}{0}{1550}{1550}
\abx@aux@backref{2213}{mildenhall2020_nerf}{0}{1550}{1550}
\@writefile{lof}{\contentsline {figure}{\numberline {23.94}{\ignorespaces \textbf  {Qualitative results on the Phototourism dataset.} Columns show methods (NRW, NeRF, NeRF-A, NeRF-U, NeRF-W) and the held-out ground-truth view; rows show scenes: \emph  {Prague Old Town} (appearance variation), \emph  {Sacre Coeur} (transient occluder: flag), and \emph  {Taj Mahal} (fine geometric/detail reconstruction). Red/blue insets zoom into regions that highlight the differences. NeRF-W simultaneously adapts to appearance changes (top), removes image-specific occluders (middle), and preserves fine details (bottom). More scenes are provided in Fig.~14 (supplementary). Photos by Flickr users \textit  {firewave}, \textit  {clintonjeff}, \textit  {leoglenn\_g} / CC BY. \emph  {Credit:} \blx@tocontentsinit {0}\cite {martinbrualla2021_nerfw}.}}{1550}{figure.caption.3395}\protected@file@percent }
\abx@aux@backref{2215}{martinbrualla2021_nerfw}{0}{1550}{1550}
\newlabel{fig:chapter23_nerfw_qual}{{23.94}{1550}{\textbf {Qualitative results on the Phototourism dataset.} Columns show methods (NRW, NeRF, NeRF-A, NeRF-U, NeRF-W) and the held-out ground-truth view; rows show scenes: \emph {Prague Old Town} (appearance variation), \emph {Sacre Coeur} (transient occluder: flag), and \emph {Taj Mahal} (fine geometric/detail reconstruction). Red/blue insets zoom into regions that highlight the differences. NeRF-W simultaneously adapts to appearance changes (top), removes image-specific occluders (middle), and preserves fine details (bottom). More scenes are provided in Fig.~14 (supplementary). Photos by Flickr users \textit {firewave}, \textit {clintonjeff}, \textit {leoglenn\_g} / CC BY. \emph {Credit:} \cite {martinbrualla2021_nerfw}}{figure.caption.3395}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23.95}{\ignorespaces \textbf  {Depth maps (expected termination).} NeRF’s geometry is corrupted by appearance variation and occluders; NeRF-W is robust and produces accurate reconstructions. Photos by Flickr users burkeandhare, photogreuhphies / CC BY. \emph  {Credit:} \blx@tocontentsinit {0}\cite {martinbrualla2021_nerfw}.}}{1550}{figure.caption.3396}\protected@file@percent }
\abx@aux@backref{2217}{martinbrualla2021_nerfw}{0}{1550}{1550}
\newlabel{fig:chapter23_nerfw_depth}{{23.95}{1550}{\textbf {Depth maps (expected termination).} NeRF’s geometry is corrupted by appearance variation and occluders; NeRF-W is robust and produces accurate reconstructions. Photos by Flickr users burkeandhare, photogreuhphies / CC BY. \emph {Credit:} \cite {martinbrualla2021_nerfw}}{figure.caption.3396}{}}
\abx@aux@cite{0}{martinbrualla2021_nerfw}
\abx@aux@segm{0}{0}{martinbrualla2021_nerfw}
\abx@aux@cite{0}{martinbrualla2021_nerfw}
\abx@aux@segm{0}{0}{martinbrualla2021_nerfw}
\abx@aux@cite{0}{martinbrualla2021_nerfw}
\abx@aux@segm{0}{0}{martinbrualla2021_nerfw}
\@writefile{lof}{\contentsline {figure}{\numberline {23.96}{\ignorespaces \textbf  {Appearance-space interpolation.} Interpolating between two $\ell ^{(a)}$ produces renderings where color/illumination vary smoothly while geometry remains fixed. Photos by Flickr users mightyohm, blatez / CC BY. \emph  {Credit:} \blx@tocontentsinit {0}\cite {martinbrualla2021_nerfw}.}}{1551}{figure.caption.3397}\protected@file@percent }
\abx@aux@backref{2219}{martinbrualla2021_nerfw}{0}{1551}{1551}
\newlabel{fig:chapter23_nerfw_interp}{{23.96}{1551}{\textbf {Appearance-space interpolation.} Interpolating between two $\ell ^{(a)}$ produces renderings where color/illumination vary smoothly while geometry remains fixed. Photos by Flickr users mightyohm, blatez / CC BY. \emph {Credit:} \cite {martinbrualla2021_nerfw}}{figure.caption.3397}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23.97}{\ignorespaces \textbf  {Temporal consistency via EPIs.} Epipolar plane images (EPI) synthesized from rendered videos on Brandenburg Gate. NRW exhibits flicker; NeRF shows ghosting; NeRF-W yields clean, smooth EPIs (high temporal consistency). \emph  {Credit:} \blx@tocontentsinit {0}\cite {martinbrualla2021_nerfw}.}}{1551}{figure.caption.3398}\protected@file@percent }
\abx@aux@backref{2221}{martinbrualla2021_nerfw}{0}{1551}{1551}
\newlabel{fig:chapter23_nerfw_epi}{{23.97}{1551}{\textbf {Temporal consistency via EPIs.} Epipolar plane images (EPI) synthesized from rendered videos on Brandenburg Gate. NRW exhibits flicker; NeRF shows ghosting; NeRF-W yields clean, smooth EPIs (high temporal consistency). \emph {Credit:} \cite {martinbrualla2021_nerfw}}{figure.caption.3398}{}}
\@writefile{toc}{\contentsline {subsubsection}{Limitations and Future Work}{1551}{section*.3400}\protected@file@percent }
\newlabel{subsec:chapter23_nerfw_limits}{{23.11.2}{1551}{Limitations and Future Work}{section*.3400}{}}
\abx@aux@backref{2222}{martinbrualla2021_nerfw}{0}{1551}{1551}
\abx@aux@cite{0}{martinbrualla2021_nerfw}
\abx@aux@segm{0}{0}{martinbrualla2021_nerfw}
\abx@aux@cite{0}{martinbrualla2021_nerfw}
\abx@aux@segm{0}{0}{martinbrualla2021_nerfw}
\@writefile{lof}{\contentsline {figure}{\numberline {23.98}{\ignorespaces \textbf  {Limitations on Phototourism.} Rarely-seen parts of the scene (ground, left) and incorrect camera poses (lamp post, right) can result in blur. \emph  {Credit:} \blx@tocontentsinit {0}\cite {martinbrualla2021_nerfw}.}}{1552}{figure.caption.3401}\protected@file@percent }
\abx@aux@backref{2224}{martinbrualla2021_nerfw}{0}{1552}{1552}
\newlabel{fig:chapter23_nerfw_limitations}{{23.98}{1552}{\textbf {Limitations on Phototourism.} Rarely-seen parts of the scene (ground, left) and incorrect camera poses (lamp post, right) can result in blur. \emph {Credit:} \cite {martinbrualla2021_nerfw}}{figure.caption.3401}{}}
\BKM@entry{id=917,dest={73656374696F6E2A2E33343032},srcline={7013}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030335C3030302E5C303030315C303030315C3030302E5C303030335C3030303A5C3030305C3034305C303030495C303030425C303030525C3030304E5C303030655C303030745C3030303A5C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C3030302D5C303030565C303030695C303030655C303030775C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3030302D5C303030425C303030615C303030735C303030655C303030645C3030305C3034305C303030525C303030655C3030306E5C303030645C303030655C303030725C303030695C3030306E5C30303067}
\abx@aux@cite{0}{mildenhall2019_llff}
\abx@aux@segm{0}{0}{mildenhall2019_llff}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\@writefile{toc}{\contentsline {subsection}{Enrichment 23.11.3: IBRNet: Learning Multi-View Image-Based Rendering}{1553}{section*.3402}\protected@file@percent }
\newlabel{enr:subsec_chapter23_ibrnet}{{23.11.3}{1553}{\color {ocre}Enrichment \thesubsection : IBRNet: Learning Multi-View Image-Based Rendering}{section*.3402}{}}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{1553}{section*.3403}\protected@file@percent }
\newlabel{subsec:chapter23_ibrnet_motivation}{{23.11.3}{1553}{Motivation}{section*.3403}{}}
\abx@aux@backref{2225}{mildenhall2019_llff}{0}{1553}{1553}
\abx@aux@backref{2226}{mildenhall2020_nerf}{0}{1553}{1553}
\abx@aux@backref{2227}{wang2021_ibrnet}{0}{1553}{1553}
\abx@aux@backref{2228}{mildenhall2020_nerf}{0}{1553}{1553}
\abx@aux@backref{2229}{wang2021_ibrnet}{0}{1553}{1553}
\@writefile{toc}{\contentsline {subsubsection}{Method: image-conditioned RGB--$\sigma $ prediction and NeRF-style rendering}{1553}{section*.3404}\protected@file@percent }
\newlabel{subsec:chapter23_ibrnet_method}{{23.11.3}{1553}{Method: image-conditioned RGB--\texorpdfstring {$\sigma $}{sigma} prediction and NeRF-style rendering}{section*.3404}{}}
\@writefile{toc}{\contentsline {paragraph}{Setup and notation.}{1553}{section*.3405}\protected@file@percent }
\abx@aux@backref{2230}{mildenhall2020_nerf}{0}{1553}{1553}
\abx@aux@backref{2231}{wang2021_ibrnet}{0}{1553}{1553}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\@writefile{toc}{\contentsline {paragraph}{Pipeline overview (stages).}{1554}{section*.3406}\protected@file@percent }
\abx@aux@backref{2232}{wang2021_ibrnet}{0}{1554}{1554}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\abx@aux@backref{2233}{wang2021_ibrnet}{0}{1555}{1555}
\@writefile{lof}{\contentsline {figure}{\numberline {23.99}{\ignorespaces \textbf  {Density and color prediction at a 5D location} $(\mathbf  {x},\mathbf  {d})$ in IBRNet~\blx@tocontentsinit {0}\cite {wang2021_ibrnet}. Per-view features $\{\mathbf  {f}_i\}$ are combined with global statistics (mean/variance) by a PointNet-like MLP to produce multi-view–aware features and \emph  {visibility-aware} weights; weighted pooling yields a compact density feature $\mathbf  {f}_\sigma $. A \emph  {ray transformer} consumes the sequence $\{\mathbf  {f}_\sigma (\mathbf  {x}_k)\}$ on a ray (with positional encodings) and outputs coherent densities $\{\sigma _k\}$. For color, a blending head uses $[\mathbf  {f}_i,\Delta \mathbf  {d}_i]$ to predict weights that form $\mathbf  {c}_k$ as a weighted average of source colors, preserving view-dependent effects.}}{1555}{figure.caption.3407}\protected@file@percent }
\abx@aux@backref{2235}{wang2021_ibrnet}{0}{1555}{1555}
\newlabel{fig:chapter23_ibrnet_point}{{23.99}{1555}{\textbf {Density and color prediction at a 5D location} $(\mathbf {x},\mathbf {d})$ in IBRNet~\cite {wang2021_ibrnet}. Per-view features $\{\mathbf {f}_i\}$ are combined with global statistics (mean/variance) by a PointNet-like MLP to produce multi-view–aware features and \emph {visibility-aware} weights; weighted pooling yields a compact density feature $\mathbf {f}_\sigma $. A \emph {ray transformer} consumes the sequence $\{\mathbf {f}_\sigma (\mathbf {x}_k)\}$ on a ray (with positional encodings) and outputs coherent densities $\{\sigma _k\}$. For color, a blending head uses $[\mathbf {f}_i,\Delta \mathbf {d}_i]$ to predict weights that form $\mathbf {c}_k$ as a weighted average of source colors, preserving view-dependent effects}{figure.caption.3407}{}}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\abx@aux@backref{2236}{mildenhall2020_nerf}{0}{1556}{1556}
\abx@aux@backref{2237}{wang2021_ibrnet}{0}{1556}{1556}
\@writefile{lof}{\contentsline {figure}{\numberline {23.100}{\ignorespaces \textbf  {IBRNet system overview for novel view synthesis}~\blx@tocontentsinit {0}\cite {wang2021_ibrnet}. To render a target view (red dashed frustum), the pipeline: (1) selects $N$ neighboring source images (by pose proximity, viewing-direction similarity, and frustum overlap) and computes a dense CNN feature map for each (cached and reused); (2) for each target ray sample $(\mathbf  {x}_k,\mathbf  {d})$, projects into all sources to read colors $\{\mathbf  {C}_i\}$ and features $\{\mathbf  {f}_i\}$, forms relative directions $\{\Delta \mathbf  {d}_i\}$, predicts a view-dependent color $\mathbf  {c}_k$ by learned blending of $\{\mathbf  {C}_i\}$, and aggregates $\{\mathbf  {f}_i\}$ with visibility-aware weights into a compact density feature $\mathbf  {f}_\sigma (\mathbf  {x}_k)$; (3) feeds the sequence $\{\mathbf  {f}_\sigma (\mathbf  {x}_k)\}_{k=1}^{M}$ to a lightweight ray transformer to obtain coherent per-sample densities $\{\sigma _k\}$, and composes $(\mathbf  {c}_k,\sigma _k)$ using the standard NeRF volume renderer with coarse/fine hierarchical sampling and an $\ell _2$ reconstruction loss.}}{1556}{figure.caption.3408}\protected@file@percent }
\abx@aux@backref{2239}{wang2021_ibrnet}{0}{1556}{1556}
\newlabel{fig:chapter23_ibrnet_overview}{{23.100}{1556}{\textbf {IBRNet system overview for novel view synthesis}~\cite {wang2021_ibrnet}. To render a target view (red dashed frustum), the pipeline: (1) selects $N$ neighboring source images (by pose proximity, viewing-direction similarity, and frustum overlap) and computes a dense CNN feature map for each (cached and reused); (2) for each target ray sample $(\mathbf {x}_k,\mathbf {d})$, projects into all sources to read colors $\{\mathbf {C}_i\}$ and features $\{\mathbf {f}_i\}$, forms relative directions $\{\Delta \mathbf {d}_i\}$, predicts a view-dependent color $\mathbf {c}_k$ by learned blending of $\{\mathbf {C}_i\}$, and aggregates $\{\mathbf {f}_i\}$ with visibility-aware weights into a compact density feature $\mathbf {f}_\sigma (\mathbf {x}_k)$; (3) feeds the sequence $\{\mathbf {f}_\sigma (\mathbf {x}_k)\}_{k=1}^{M}$ to a lightweight ray transformer to obtain coherent per-sample densities $\{\sigma _k\}$, and composes $(\mathbf {c}_k,\sigma _k)$ using the standard NeRF volume renderer with coarse/fine hierarchical sampling and an $\ell _2$ reconstruction loss}{figure.caption.3408}{}}
\@writefile{toc}{\contentsline {paragraph}{Why fast and zero-shot.}{1556}{section*.3409}\protected@file@percent }
\abx@aux@backref{2240}{wang2021_ibrnet}{0}{1556}{1556}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\@writefile{toc}{\contentsline {subsubsection}{Architecture \& Implementation Details}{1557}{section*.3410}\protected@file@percent }
\newlabel{subsec:chapter23_ibrnet_arch_impl}{{23.11.3}{1557}{Architecture \& Implementation Details}{section*.3410}{}}
\@writefile{toc}{\contentsline {paragraph}{High-level architecture.}{1557}{section*.3411}\protected@file@percent }
\abx@aux@backref{2241}{wang2021_ibrnet}{0}{1557}{1557}
\@writefile{lot}{\contentsline {table}{\numberline {23.24}{\ignorespaces \textbf  {Feature extraction network architecture}~\blx@tocontentsinit {0}\cite {wang2021_ibrnet}. ``Conv'' denotes conv + ReLU + InstanceNorm; ``Upconv'' is bilinear upsampling then a stride-1 conv. The $64$-D output map is split into two $32$-D maps for the coarse and fine branches, respectively.}}{1557}{table.caption.3412}\protected@file@percent }
\abx@aux@backref{2243}{wang2021_ibrnet}{0}{1557}{1557}
\newlabel{tab:chapter23_ibrnet_feat_extractor}{{23.24}{1557}{\textbf {Feature extraction network architecture}~\cite {wang2021_ibrnet}. ``Conv'' denotes conv + ReLU + InstanceNorm; ``Upconv'' is bilinear upsampling then a stride-1 conv. The $64$-D output map is split into two $32$-D maps for the coarse and fine branches, respectively}{table.caption.3412}{}}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\abx@aux@cite{0}{mildenhall2019_llff}
\abx@aux@segm{0}{0}{mildenhall2019_llff}
\abx@aux@cite{0}{sitzmann2019_srn}
\abx@aux@segm{0}{0}{sitzmann2019_srn}
\abx@aux@cite{0}{lombardi2019_neuralvolumes}
\abx@aux@segm{0}{0}{lombardi2019_neuralvolumes}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{mildenhall2019_llff}
\abx@aux@segm{0}{0}{mildenhall2019_llff}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\abx@aux@cite{0}{sitzmann2019_srn}
\abx@aux@segm{0}{0}{sitzmann2019_srn}
\abx@aux@cite{0}{lombardi2019_neuralvolumes}
\abx@aux@segm{0}{0}{lombardi2019_neuralvolumes}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\abx@aux@cite{0}{mildenhall2019_llff}
\abx@aux@segm{0}{0}{mildenhall2019_llff}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\abx@aux@cite{0}{sitzmann2019_srn}
\abx@aux@segm{0}{0}{sitzmann2019_srn}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\@writefile{toc}{\contentsline {paragraph}{Network size and compute.}{1558}{section*.3413}\protected@file@percent }
\abx@aux@backref{2244}{mildenhall2020_nerf}{0}{1558}{1558}
\@writefile{lot}{\contentsline {table}{\numberline {23.25}{\ignorespaces \textbf  {Complexity vs.\ quality (Real Forward-Facing)}~\blx@tocontentsinit {0}\cite {wang2021_ibrnet}. IBRNet's per-sample heads are tiny; FLOPs scale roughly linearly with the number of source views used at inference.}}{1558}{table.caption.3414}\protected@file@percent }
\abx@aux@backref{2246}{wang2021_ibrnet}{0}{1558}{1558}
\newlabel{tab:chapter23_ibrnet_complexity}{{23.25}{1558}{\textbf {Complexity vs.\ quality (Real Forward-Facing)}~\cite {wang2021_ibrnet}. IBRNet's per-sample heads are tiny; FLOPs scale roughly linearly with the number of source views used at inference}{table.caption.3414}{}}
\@writefile{toc}{\contentsline {subsubsection}{Experiments \& Ablations}{1558}{section*.3415}\protected@file@percent }
\newlabel{subsec:chapter23_ibrnet_experiments}{{23.11.3}{1558}{Experiments \& Ablations}{section*.3415}{}}
\@writefile{toc}{\contentsline {paragraph}{Datasets and evaluation protocol.}{1558}{section*.3416}\protected@file@percent }
\abx@aux@backref{2247}{wang2021_ibrnet}{0}{1558}{1558}
\@writefile{toc}{\contentsline {paragraph}{Baselines.}{1558}{section*.3417}\protected@file@percent }
\abx@aux@backref{2248}{mildenhall2019_llff}{0}{1558}{1558}
\abx@aux@backref{2249}{sitzmann2019_srn}{0}{1558}{1558}
\abx@aux@backref{2250}{lombardi2019_neuralvolumes}{0}{1558}{1558}
\abx@aux@backref{2251}{mildenhall2020_nerf}{0}{1558}{1558}
\@writefile{toc}{\contentsline {paragraph}{Quantitative comparison (synthetic datasets).}{1558}{section*.3418}\protected@file@percent }
\abx@aux@backref{2252}{mildenhall2019_llff}{0}{1558}{1558}
\abx@aux@backref{2253}{wang2021_ibrnet}{0}{1558}{1558}
\abx@aux@backref{2254}{sitzmann2019_srn}{0}{1558}{1558}
\abx@aux@backref{2255}{lombardi2019_neuralvolumes}{0}{1558}{1558}
\abx@aux@backref{2256}{mildenhall2020_nerf}{0}{1558}{1558}
\abx@aux@backref{2257}{wang2021_ibrnet}{0}{1558}{1558}
\@writefile{lot}{\contentsline {table}{\numberline {23.26}{\ignorespaces \textbf  {Synthetic datasets}~\blx@tocontentsinit {0}\cite {wang2021_ibrnet}. In the \emph  {no per-scene} regime, IBRNet outperforms LLFF on both synthetic benchmarks, indicating effective zero-shot generalization from learned multi-view priors. With \emph  {per-scene} tuning, IBRNet$_\text  {ft}$ becomes competitive with state-of-the-art per-scene methods on the Diffuse set and reduces the gap on the Realistic set.}}{1558}{table.caption.3419}\protected@file@percent }
\abx@aux@backref{2259}{wang2021_ibrnet}{0}{1558}{1558}
\newlabel{tab:chapter23_ibrnet_quant_synth}{{23.26}{1558}{\textbf {Synthetic datasets}~\cite {wang2021_ibrnet}. In the \emph {no per-scene} regime, IBRNet outperforms LLFF on both synthetic benchmarks, indicating effective zero-shot generalization from learned multi-view priors. With \emph {per-scene} tuning, IBRNet$_\text {ft}$ becomes competitive with state-of-the-art per-scene methods on the Diffuse set and reduces the gap on the Realistic set}{table.caption.3419}{}}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\@writefile{toc}{\contentsline {paragraph}{Quantitative comparison (Real Forward-Facing).}{1559}{section*.3420}\protected@file@percent }
\abx@aux@backref{2260}{mildenhall2019_llff}{0}{1559}{1559}
\abx@aux@backref{2261}{wang2021_ibrnet}{0}{1559}{1559}
\abx@aux@backref{2262}{sitzmann2019_srn}{0}{1559}{1559}
\abx@aux@backref{2263}{mildenhall2020_nerf}{0}{1559}{1559}
\abx@aux@backref{2264}{wang2021_ibrnet}{0}{1559}{1559}
\@writefile{lot}{\contentsline {table}{\numberline {23.27}{\ignorespaces \textbf  {Real Forward-Facing}~\blx@tocontentsinit {0}\cite {wang2021_ibrnet}. IBRNet improves over LLFF without per-scene optimization, supporting that image-conditioned blending and along-ray attention transfer across scenes. Optional fine-tuning (IBRNet$_\text  {ft}$) further sharpens thin structures and reflections, surpassing NeRF on SSIM/LPIPS and slightly on PSNR.}}{1559}{table.caption.3421}\protected@file@percent }
\abx@aux@backref{2266}{wang2021_ibrnet}{0}{1559}{1559}
\newlabel{tab:chapter23_ibrnet_quant_rff}{{23.27}{1559}{\textbf {Real Forward-Facing}~\cite {wang2021_ibrnet}. IBRNet improves over LLFF without per-scene optimization, supporting that image-conditioned blending and along-ray attention transfer across scenes. Optional fine-tuning (IBRNet$_\text {ft}$) further sharpens thin structures and reflections, surpassing NeRF on SSIM/LPIPS and slightly on PSNR}{table.caption.3421}{}}
\@writefile{toc}{\contentsline {paragraph}{Ablation studies.}{1559}{section*.3422}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {23.28}{\ignorespaces \textbf  {Ablations on Real Forward-Facing (pretrained, no per-scene tuning)}~\blx@tocontentsinit {0}\cite {wang2021_ibrnet}. Along-ray self-attention (ray transformer) is critical for resolving occlusions/depth; relative view directions improve view-dependent appearance; blending \emph  {observed} colors outperforms direct RGB regression.}}{1559}{table.caption.3423}\protected@file@percent }
\abx@aux@backref{2268}{wang2021_ibrnet}{0}{1559}{1559}
\newlabel{tab:chapter23_ibrnet_ablation}{{23.28}{1559}{\textbf {Ablations on Real Forward-Facing (pretrained, no per-scene tuning)}~\cite {wang2021_ibrnet}. Along-ray self-attention (ray transformer) is critical for resolving occlusions/depth; relative view directions improve view-dependent appearance; blending \emph {observed} colors outperforms direct RGB regression}{table.caption.3423}{}}
\@writefile{toc}{\contentsline {paragraph}{Sensitivity to source-view density.}{1559}{section*.3424}\protected@file@percent }
\newlabel{par:chapter23_ibrnet_sensitivity}{{23.11.3}{1559}{Sensitivity to source-view density}{section*.3424}{}}
\abx@aux@backref{2269}{wang2021_ibrnet}{0}{1559}{1559}
\@writefile{lof}{\contentsline {figure}{\numberline {23.101}{\ignorespaces \textbf  {Sensitivity to input view sparsity}~\blx@tocontentsinit {0}\cite {wang2021_ibrnet}. Source views are uniformly subsampled on the upper hemisphere by factors $\{2,4,6,8,10\}$ to create varying densities. Results are shown for the pretrained model (no per-scene tuning) and for a per-scene fine-tuned variant (IBRNet$_\text  {ft}$).}}{1559}{figure.caption.3425}\protected@file@percent }
\abx@aux@backref{2271}{wang2021_ibrnet}{0}{1559}{1559}
\newlabel{fig:chapter23_ibrnet_sensitivity}{{23.101}{1559}{\textbf {Sensitivity to input view sparsity}~\cite {wang2021_ibrnet}. Source views are uniformly subsampled on the upper hemisphere by factors $\{2,4,6,8,10\}$ to create varying densities. Results are shown for the pretrained model (no per-scene tuning) and for a per-scene fine-tuned variant (IBRNet$_\text {ft}$)}{figure.caption.3425}{}}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\abx@aux@backref{2272}{wang2021_ibrnet}{0}{1560}{1560}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\abx@aux@backref{2273}{wang2021_ibrnet}{0}{1561}{1561}
\@writefile{toc}{\contentsline {paragraph}{Qualitative comparisons.}{1561}{section*.3426}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.102}{\ignorespaces \textbf  {Qualitative comparison on Real Forward-Facing}~\blx@tocontentsinit {0}\cite {wang2021_ibrnet}. IBRNet reconstructs fine geometric and appearance details while avoiding ghosting near boundaries and thin structures where LLFF struggles; compared to NeRF, it reduces high-frequency artifacts and better preserves reflections in several scenes.}}{1561}{figure.caption.3427}\protected@file@percent }
\abx@aux@backref{2275}{wang2021_ibrnet}{0}{1561}{1561}
\newlabel{fig:chapter23_ibrnet_real_ff}{{23.102}{1561}{\textbf {Qualitative comparison on Real Forward-Facing}~\cite {wang2021_ibrnet}. IBRNet reconstructs fine geometric and appearance details while avoiding ghosting near boundaries and thin structures where LLFF struggles; compared to NeRF, it reduces high-frequency artifacts and better preserves reflections in several scenes}{figure.caption.3427}{}}
\@writefile{toc}{\contentsline {paragraph}{With/without ray transformer.}{1561}{section*.3428}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.103}{\ignorespaces \textbf  {With vs.\ without the ray transformer}~\blx@tocontentsinit {0}\cite {wang2021_ibrnet}. Each triplet shows (left) the pretrained model \emph  {without} the ray transformer, (middle) the model \emph  {with} the ray transformer, and (right) the ground truth. Without along-ray self-attention, densities are predicted from per-sample cues only, frequently yielding ``black-hole'' voids and boundary ghosting near occlusions. Adding a single along-ray self-attention layer (with depth-wise positional encodings) aggregates density features across all samples on the ray, enforcing coherent near--far ordering and markedly cleaner edges.}}{1561}{figure.caption.3429}\protected@file@percent }
\abx@aux@backref{2277}{wang2021_ibrnet}{0}{1561}{1561}
\newlabel{fig:chapter23_ibrnet_ray_tx}{{23.103}{1561}{\textbf {With vs.\ without the ray transformer}~\cite {wang2021_ibrnet}. Each triplet shows (left) the pretrained model \emph {without} the ray transformer, (middle) the model \emph {with} the ray transformer, and (right) the ground truth. Without along-ray self-attention, densities are predicted from per-sample cues only, frequently yielding ``black-hole'' voids and boundary ghosting near occlusions. Adding a single along-ray self-attention layer (with depth-wise positional encodings) aggregates density features across all samples on the ray, enforcing coherent near--far ordering and markedly cleaner edges}{figure.caption.3429}{}}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\@writefile{toc}{\contentsline {paragraph}{Geometry and additional results.}{1562}{section*.3430}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.104}{\ignorespaces \textbf  {Proxy geometry and rendering on two scenes (Leaves, Horns)}~\blx@tocontentsinit {0}\cite {wang2021_ibrnet}. For each scene, columns show: \emph  {left}—ground-truth image; \emph  {middle}—pretrained IBRNet (no per-scene tuning), with synthesized RGB (top) and estimated depth (bottom); \emph  {right}—IBRNet fine-tuned on the scene (IBRNet$_\text  {ft}$), again with synthesized RGB (top) and depth (bottom). Fine-tuning sharpens geometry and improves view-dependent appearance, yielding cleaner boundaries and more stable thin structures.}}{1562}{figure.caption.3431}\protected@file@percent }
\abx@aux@backref{2279}{wang2021_ibrnet}{0}{1562}{1562}
\newlabel{fig:chapter23_ibrnet_geometry}{{23.104}{1562}{\textbf {Proxy geometry and rendering on two scenes (Leaves, Horns)}~\cite {wang2021_ibrnet}. For each scene, columns show: \emph {left}—ground-truth image; \emph {middle}—pretrained IBRNet (no per-scene tuning), with synthesized RGB (top) and estimated depth (bottom); \emph {right}—IBRNet fine-tuned on the scene (IBRNet$_\text {ft}$), again with synthesized RGB (top) and depth (bottom). Fine-tuning sharpens geometry and improves view-dependent appearance, yielding cleaner boundaries and more stable thin structures}{figure.caption.3431}{}}
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\@writefile{lof}{\contentsline {figure}{\numberline {23.105}{\ignorespaces \textbf  {Realistic Synthetic 360$^\circ $ results}~\blx@tocontentsinit {0}\cite {wang2021_ibrnet}. High-fidelity renderings are achieved without per-scene optimization; remaining failures appear under very sparse views and complex geometry, where additional per-scene adaptation can help.}}{1563}{figure.caption.3432}\protected@file@percent }
\abx@aux@backref{2281}{wang2021_ibrnet}{0}{1563}{1563}
\newlabel{fig:chapter23_ibrnet_realistic_synth}{{23.105}{1563}{\textbf {Realistic Synthetic 360$^\circ $ results}~\cite {wang2021_ibrnet}. High-fidelity renderings are achieved without per-scene optimization; remaining failures appear under very sparse views and complex geometry, where additional per-scene adaptation can help}{figure.caption.3432}{}}
\@writefile{toc}{\contentsline {subsubsection}{Limitations and Future Directions}{1563}{section*.3433}\protected@file@percent }
\newlabel{subsec:chapter23_ibrnet_limitations_future}{{23.11.3}{1563}{Limitations and Future Directions}{section*.3433}{}}
\@writefile{toc}{\contentsline {paragraph}{Limitations.}{1563}{section*.3434}\protected@file@percent }
\abx@aux@backref{2282}{wang2021_ibrnet}{0}{1563}{1563}
\abx@aux@cite{0}{yu2020_pixelnerf}
\abx@aux@segm{0}{0}{yu2020_pixelnerf}
\abx@aux@cite{0}{trevithick2021_grf}
\abx@aux@segm{0}{0}{trevithick2021_grf}
\abx@aux@cite{0}{chen2021_mvsnerf}
\abx@aux@segm{0}{0}{chen2021_mvsnerf}
\abx@aux@cite{0}{liu2022_neuray}
\abx@aux@segm{0}{0}{liu2022_neuray}
\abx@aux@cite{0}{johari2022_geonerf}
\abx@aux@segm{0}{0}{johari2022_geonerf}
\abx@aux@cite{0}{xu2022_pointnerf}
\abx@aux@segm{0}{0}{xu2022_pointnerf}
\abx@aux@cite{0}{niemeyer2022_regnerf}
\abx@aux@segm{0}{0}{niemeyer2022_regnerf}
\BKM@entry{id=918,dest={73656374696F6E2A2E33343337},srcline={7386}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030335C3030302E5C303030315C303030315C3030302E5C303030345C3030303A5C3030305C3034305C303030705C303030695C303030785C303030655C3030306C5C3030304E5C303030655C303030525C303030465C3030303A5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C303030525C303030615C303030645C303030695C303030615C3030306E5C303030635C303030655C3030305C3034305C303030465C303030695C303030655C3030306C5C303030645C303030735C3030305C3034305C303030665C303030725C3030306F5C3030306D5C3030305C3034305C3030304F5C3030306E5C303030655C3030305C3034305C3030306F5C303030725C3030305C3034305C303030465C303030655C303030775C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C30303073}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{yu2020_pixelnerf}
\abx@aux@segm{0}{0}{yu2020_pixelnerf}
\@writefile{toc}{\contentsline {paragraph}{Concurrent and prior generalizable radiance-field methods.}{1564}{section*.3435}\protected@file@percent }
\abx@aux@backref{2283}{yu2020_pixelnerf}{0}{1564}{1564}
\abx@aux@backref{2284}{trevithick2021_grf}{0}{1564}{1564}
\abx@aux@backref{2285}{chen2021_mvsnerf}{0}{1564}{1564}
\@writefile{toc}{\contentsline {paragraph}{Subsequent follow-ups building on IBRNet’s goals.}{1564}{section*.3436}\protected@file@percent }
\abx@aux@backref{2286}{liu2022_neuray}{0}{1564}{1564}
\abx@aux@backref{2287}{johari2022_geonerf}{0}{1564}{1564}
\abx@aux@backref{2288}{xu2022_pointnerf}{0}{1564}{1564}
\abx@aux@backref{2289}{niemeyer2022_regnerf}{0}{1564}{1564}
\abx@aux@cite{0}{yu2020_pixelnerf}
\abx@aux@segm{0}{0}{yu2020_pixelnerf}
\abx@aux@cite{0}{yu2020_pixelnerf}
\abx@aux@segm{0}{0}{yu2020_pixelnerf}
\@writefile{toc}{\contentsline {subsection}{Enrichment 23.11.4: pixelNeRF: Neural Radiance Fields from One or Few Images}{1565}{section*.3437}\protected@file@percent }
\newlabel{enr:chapter23_pixelnerf}{{23.11.4}{1565}{\color {ocre}Enrichment \thesubsection : pixelNeRF: Neural Radiance Fields from One or Few Images}{section*.3437}{}}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{1565}{section*.3438}\protected@file@percent }
\newlabel{subsec:chapter23_pixelnerf_motivation}{{23.11.4}{1565}{Motivation}{section*.3438}{}}
\abx@aux@backref{2290}{mildenhall2020_nerf}{0}{1565}{1565}
\abx@aux@backref{2291}{yu2020_pixelnerf}{0}{1565}{1565}
\@writefile{lof}{\contentsline {figure}{\numberline {23.106}{\ignorespaces \textbf  {NeRF from one or few images.} PixelNeRF predicts neural radiance fields from a single (top) or few posed images (bottom). Unlike NeRF, which requires dense views to work, PixelNeRF generalizes across scenes and performs robustly even with sparse views~\blx@tocontentsinit {0}\cite {yu2020_pixelnerf}.}}{1565}{figure.caption.3439}\protected@file@percent }
\abx@aux@backref{2293}{yu2020_pixelnerf}{0}{1565}{1565}
\newlabel{fig:chapter23_pixelnerf_motivation}{{23.106}{1565}{\textbf {NeRF from one or few images.} PixelNeRF predicts neural radiance fields from a single (top) or few posed images (bottom). Unlike NeRF, which requires dense views to work, PixelNeRF generalizes across scenes and performs robustly even with sparse views~\cite {yu2020_pixelnerf}}{figure.caption.3439}{}}
\@writefile{toc}{\contentsline {subsubsection}{Method}{1565}{section*.3440}\protected@file@percent }
\newlabel{subsec:chapter23_pixelnerf_method}{{23.11.4}{1565}{Method}{section*.3440}{}}
\@writefile{toc}{\contentsline {paragraph}{Radiance field prediction}{1565}{section*.3441}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Feature encoding and alignment}{1565}{section*.3442}\protected@file@percent }
\abx@aux@cite{0}{yu2020_pixelnerf}
\abx@aux@segm{0}{0}{yu2020_pixelnerf}
\abx@aux@cite{0}{yu2020_pixelnerf}
\abx@aux@segm{0}{0}{yu2020_pixelnerf}
\@writefile{toc}{\contentsline {paragraph}{Feature-conditioned NeRF}{1566}{section*.3443}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Volume rendering loss}{1566}{section*.3444}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why view-space conditioning}{1566}{section*.3445}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Multi-view extension}{1566}{section*.3446}\protected@file@percent }
\abx@aux@cite{0}{yu2020_pixelnerf}
\abx@aux@segm{0}{0}{yu2020_pixelnerf}
\abx@aux@cite{0}{yu2020_pixelnerf}
\abx@aux@segm{0}{0}{yu2020_pixelnerf}
\abx@aux@cite{0}{sitzmann2019_srn}
\abx@aux@segm{0}{0}{sitzmann2019_srn}
\abx@aux@cite{0}{niemeyer2020_dvr}
\abx@aux@segm{0}{0}{niemeyer2020_dvr}
\@writefile{toc}{\contentsline {paragraph}{Intuition and significance}{1567}{section*.3447}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.107}{\ignorespaces \textbf  {PixelNeRF pipeline in the single-view case.} Query features are sampled from image-encoded feature volumes and combined with spatial coordinates before passing through the NeRF network~\blx@tocontentsinit {0}\cite {yu2020_pixelnerf}.}}{1567}{figure.caption.3448}\protected@file@percent }
\abx@aux@backref{2295}{yu2020_pixelnerf}{0}{1567}{1567}
\newlabel{fig:chapter23_pixelnerf_overview}{{23.107}{1567}{\textbf {PixelNeRF pipeline in the single-view case.} Query features are sampled from image-encoded feature volumes and combined with spatial coordinates before passing through the NeRF network~\cite {yu2020_pixelnerf}}{figure.caption.3448}{}}
\@writefile{toc}{\contentsline {subsubsection}{Architecture and Implementation}{1567}{section*.3449}\protected@file@percent }
\newlabel{subsec:chapter23_pixelnerf_architecture}{{23.11.4}{1567}{Architecture and Implementation}{section*.3449}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23.108}{\ignorespaces \textbf  {Multi-view PixelNeRF architecture.} Separate encoders produce feature grids per view, which are transformed, pooled, and aggregated through $f_1$ and $f_2$~\blx@tocontentsinit {0}\cite {yu2020_pixelnerf}.}}{1567}{figure.caption.3450}\protected@file@percent }
\abx@aux@backref{2297}{yu2020_pixelnerf}{0}{1567}{1567}
\newlabel{fig:chapter23_pixelnerf_architecture}{{23.108}{1567}{\textbf {Multi-view PixelNeRF architecture.} Separate encoders produce feature grids per view, which are transformed, pooled, and aggregated through $f_1$ and $f_2$~\cite {yu2020_pixelnerf}}{figure.caption.3450}{}}
\abx@aux@cite{0}{sitzmann2019_srn}
\abx@aux@segm{0}{0}{sitzmann2019_srn}
\abx@aux@cite{0}{yu2020_pixelnerf}
\abx@aux@segm{0}{0}{yu2020_pixelnerf}
\abx@aux@cite{0}{sitzmann2019_srn}
\abx@aux@segm{0}{0}{sitzmann2019_srn}
\abx@aux@cite{0}{yu2020_pixelnerf}
\abx@aux@segm{0}{0}{yu2020_pixelnerf}
\abx@aux@cite{0}{yu2020_pixelnerf}
\abx@aux@segm{0}{0}{yu2020_pixelnerf}
\abx@aux@cite{0}{yu2020_pixelnerf}
\abx@aux@segm{0}{0}{yu2020_pixelnerf}
\abx@aux@cite{0}{trevithick2021_grf}
\abx@aux@segm{0}{0}{trevithick2021_grf}
\abx@aux@cite{0}{tatarchenko2015_tco}
\abx@aux@segm{0}{0}{tatarchenko2015_tco}
\abx@aux@cite{0}{eslami2018_dgqn}
\abx@aux@segm{0}{0}{eslami2018_dgqn}
\abx@aux@cite{0}{dupont2020_enr}
\abx@aux@segm{0}{0}{dupont2020_enr}
\abx@aux@cite{0}{sitzmann2019_srn}
\abx@aux@segm{0}{0}{sitzmann2019_srn}
\abx@aux@cite{0}{yu2020_pixelnerf}
\abx@aux@segm{0}{0}{yu2020_pixelnerf}
\abx@aux@cite{0}{sitzmann2019_srn}
\abx@aux@segm{0}{0}{sitzmann2019_srn}
\abx@aux@cite{0}{dupont2020_enr}
\abx@aux@segm{0}{0}{dupont2020_enr}
\abx@aux@cite{0}{yu2020_pixelnerf}
\abx@aux@segm{0}{0}{yu2020_pixelnerf}
\@writefile{toc}{\contentsline {subsubsection}{Experiments and Ablations}{1568}{section*.3451}\protected@file@percent }
\newlabel{subsec:chapter23_pixelnerf_experiments}{{23.11.4}{1568}{Experiments and Ablations}{section*.3451}{}}
\abx@aux@backref{2298}{sitzmann2019_srn}{0}{1568}{1568}
\abx@aux@backref{2299}{niemeyer2020_dvr}{0}{1568}{1568}
\@writefile{toc}{\contentsline {paragraph}{Category-specific single-view reconstruction}{1568}{section*.3452}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.109}{\ignorespaces \textbf  {Category-specific single-view reconstruction benchmark.} Separate models for cars and chairs; qualitative comparison with SRN~\blx@tocontentsinit {0}\cite {sitzmann2019_srn}. \emph  {Credit:} \blx@tocontentsinit {0}\cite {yu2020_pixelnerf}.}}{1568}{figure.caption.3453}\protected@file@percent }
\abx@aux@backref{2302}{sitzmann2019_srn}{0}{1568}{1568}
\abx@aux@backref{2303}{yu2020_pixelnerf}{0}{1568}{1568}
\newlabel{fig:chapter23_pixelnerf_catspecific_single}{{23.109}{1568}{\textbf {Category-specific single-view reconstruction benchmark.} Separate models for cars and chairs; qualitative comparison with SRN~\cite {sitzmann2019_srn}. \emph {Credit:} \cite {yu2020_pixelnerf}}{figure.caption.3453}{}}
\@writefile{toc}{\contentsline {paragraph}{Category-specific two-view reconstruction}{1568}{section*.3454}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.110}{\ignorespaces \textbf  {Category-specific 2-view reconstruction benchmark.} \emph  {Credit:} \blx@tocontentsinit {0}\cite {yu2020_pixelnerf}.}}{1568}{figure.caption.3455}\protected@file@percent }
\abx@aux@backref{2305}{yu2020_pixelnerf}{0}{1568}{1568}
\newlabel{fig:chapter23_pixelnerf_catspecific_twoview}{{23.110}{1568}{\textbf {Category-specific 2-view reconstruction benchmark.} \emph {Credit:} \cite {yu2020_pixelnerf}}{figure.caption.3455}{}}
\abx@aux@cite{0}{yu2020_pixelnerf}
\abx@aux@segm{0}{0}{yu2020_pixelnerf}
\abx@aux@cite{0}{yu2020_pixelnerf}
\abx@aux@segm{0}{0}{yu2020_pixelnerf}
\abx@aux@cite{0}{yu2020_pixelnerf}
\abx@aux@segm{0}{0}{yu2020_pixelnerf}
\abx@aux@backref{2306}{trevithick2021_grf}{0}{1569}{1569}
\abx@aux@backref{2307}{tatarchenko2015_tco}{0}{1569}{1569}
\abx@aux@backref{2308}{eslami2018_dgqn}{0}{1569}{1569}
\abx@aux@backref{2309}{dupont2020_enr}{0}{1569}{1569}
\abx@aux@backref{2310}{sitzmann2019_srn}{0}{1569}{1569}
\abx@aux@backref{2311}{yu2020_pixelnerf}{0}{1569}{1569}
\abx@aux@backref{2312}{sitzmann2019_srn}{0}{1569}{1569}
\abx@aux@backref{2313}{dupont2020_enr}{0}{1569}{1569}
\abx@aux@backref{2314}{yu2020_pixelnerf}{0}{1569}{1569}
\@writefile{lot}{\contentsline {table}{\numberline {23.29}{\ignorespaces \textbf  {Category-specific 1- and 2-view reconstruction.} Methods marked $^\ast $ do not require canonical poses at test time. One model per category is evaluated in both settings. Values match the original paper’s Table~2.}}{1569}{table.caption.3456}\protected@file@percent }
\newlabel{tab:chapter23_pixelnerf_catspecific}{{23.29}{1569}{\textbf {Category-specific 1- and 2-view reconstruction.} Methods marked $^\ast $ do not require canonical poses at test time. One model per category is evaluated in both settings. Values match the original paper’s Table~2}{table.caption.3456}{}}
\@writefile{toc}{\contentsline {paragraph}{Ablation on local features and view directions}{1569}{section*.3457}\protected@file@percent }
\newlabel{par:chapter23_pixelnerf_ablation_local_dirs}{{23.11.4}{1569}{Ablation on local features and view directions}{section*.3457}{}}
\abx@aux@backref{2315}{yu2020_pixelnerf}{0}{1569}{1569}
\abx@aux@cite{0}{niemeyer2020_dvr}
\abx@aux@segm{0}{0}{niemeyer2020_dvr}
\abx@aux@cite{0}{sitzmann2019_srn}
\abx@aux@segm{0}{0}{sitzmann2019_srn}
\abx@aux@cite{0}{yu2020_pixelnerf}
\abx@aux@segm{0}{0}{yu2020_pixelnerf}
\abx@aux@cite{0}{niemeyer2020_dvr}
\abx@aux@segm{0}{0}{niemeyer2020_dvr}
\abx@aux@cite{0}{sitzmann2019_srn}
\abx@aux@segm{0}{0}{sitzmann2019_srn}
\abx@aux@cite{0}{yu2020_pixelnerf}
\abx@aux@segm{0}{0}{yu2020_pixelnerf}
\abx@aux@cite{0}{niemeyer2020_dvr}
\abx@aux@segm{0}{0}{niemeyer2020_dvr}
\abx@aux@cite{0}{sitzmann2019_srn}
\abx@aux@segm{0}{0}{sitzmann2019_srn}
\abx@aux@cite{0}{yu2020_pixelnerf}
\abx@aux@segm{0}{0}{yu2020_pixelnerf}
\abx@aux@cite{0}{yu2020_pixelnerf}
\abx@aux@segm{0}{0}{yu2020_pixelnerf}
\abx@aux@cite{0}{yu2020_pixelnerf}
\abx@aux@segm{0}{0}{yu2020_pixelnerf}
\abx@aux@cite{0}{yu2020_pixelnerf}
\abx@aux@segm{0}{0}{yu2020_pixelnerf}
\abx@aux@cite{0}{yu2020_pixelnerf}
\abx@aux@segm{0}{0}{yu2020_pixelnerf}
\abx@aux@cite{0}{yu2020_pixelnerf}
\abx@aux@segm{0}{0}{yu2020_pixelnerf}
\abx@aux@cite{0}{yu2020_pixelnerf}
\abx@aux@segm{0}{0}{yu2020_pixelnerf}
\@writefile{lot}{\contentsline {table}{\numberline {23.30}{\ignorespaces \textbf  {Ablation on \texttt  {ShapeNet} chairs.} Pixel-aligned local features and explicit view directions are both essential for few-shot quality; the \textbf  {Full} PixelNeRF model attains the best PSNR/SSIM and lowest LPIPS in 1-view and 2-view regimes (matches Table~3 in~\blx@tocontentsinit {0}\cite {yu2020_pixelnerf}).}}{1570}{table.caption.3458}\protected@file@percent }
\abx@aux@backref{2317}{yu2020_pixelnerf}{0}{1570}{1570}
\newlabel{tab:chapter23_pixelnerf_ablation}{{23.30}{1570}{\textbf {Ablation on \texttt {ShapeNet} chairs.} Pixel-aligned local features and explicit view directions are both essential for few-shot quality; the \textbf {Full} PixelNeRF model attains the best PSNR/SSIM and lowest LPIPS in 1-view and 2-view regimes (matches Table~3 in~\cite {yu2020_pixelnerf})}{table.caption.3458}{}}
\@writefile{toc}{\contentsline {paragraph}{Category-agnostic single-view reconstruction}{1570}{section*.3459}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.111}{\ignorespaces \textbf  {Category-agnostic single-view reconstruction.} PixelNeRF is trained as a single model across 13 ShapeNet categories, without category-specific specialization. The results show superior recovery of fine structures such as chair legs, monitors, and tabletop textures compared to methods that compress the scene into a single latent vector. Competing baselines such as SRN struggle in this setting, with degraded reconstructions and unreliable test-time latent inversion.}}{1570}{figure.caption.3460}\protected@file@percent }
\newlabel{fig:chapter23_pixelnerf_catagnostic}{{23.111}{1570}{\textbf {Category-agnostic single-view reconstruction.} PixelNeRF is trained as a single model across 13 ShapeNet categories, without category-specific specialization. The results show superior recovery of fine structures such as chair legs, monitors, and tabletop textures compared to methods that compress the scene into a single latent vector. Competing baselines such as SRN struggle in this setting, with degraded reconstructions and unreliable test-time latent inversion}{figure.caption.3460}{}}
\@writefile{toc}{\contentsline {paragraph}{Unseen categories and multi-object scenes}{1570}{section*.3462}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.112}{\ignorespaces \textbf  {Generalization to unseen categories.} A model trained only on \texttt  {plane}, \texttt  {car}, and \texttt  {chair} generalizes to 10 unseen ShapeNet categories. Despite not being exposed to these categories during training, PixelNeRF produces structurally reasonable and visually coherent reconstructions, demonstrating strong cross-category priors. \emph  {Credit:} \blx@tocontentsinit {0}\cite {yu2020_pixelnerf}.}}{1570}{figure.caption.3463}\protected@file@percent }
\abx@aux@backref{2330}{yu2020_pixelnerf}{0}{1570}{1570}
\newlabel{fig:chapter23_pixelnerf_unseen}{{23.112}{1570}{\textbf {Generalization to unseen categories.} A model trained only on \texttt {plane}, \texttt {car}, and \texttt {chair} generalizes to 10 unseen ShapeNet categories. Despite not being exposed to these categories during training, PixelNeRF produces structurally reasonable and visually coherent reconstructions, demonstrating strong cross-category priors. \emph {Credit:} \cite {yu2020_pixelnerf}}{figure.caption.3463}{}}
\abx@aux@cite{0}{niemeyer2020_dvr}
\abx@aux@segm{0}{0}{niemeyer2020_dvr}
\abx@aux@cite{0}{sitzmann2019_srn}
\abx@aux@segm{0}{0}{sitzmann2019_srn}
\abx@aux@cite{0}{yu2020_pixelnerf}
\abx@aux@segm{0}{0}{yu2020_pixelnerf}
\abx@aux@cite{0}{yu2020_pixelnerf}
\abx@aux@segm{0}{0}{yu2020_pixelnerf}
\abx@aux@cite{0}{yu2020_pixelnerf}
\abx@aux@segm{0}{0}{yu2020_pixelnerf}
\abx@aux@cite{0}{krause2013_stanfordcars}
\abx@aux@segm{0}{0}{krause2013_stanfordcars}
\abx@aux@cite{0}{kirillov2020_pointrend}
\abx@aux@segm{0}{0}{kirillov2020_pointrend}
\abx@aux@cite{0}{yu2020_pixelnerf}
\abx@aux@segm{0}{0}{yu2020_pixelnerf}
\abx@aux@cite{0}{krause2013_stanfordcars}
\abx@aux@segm{0}{0}{krause2013_stanfordcars}
\abx@aux@cite{0}{kirillov2020_pointrend}
\abx@aux@segm{0}{0}{kirillov2020_pointrend}
\abx@aux@cite{0}{yu2020_pixelnerf}
\abx@aux@segm{0}{0}{yu2020_pixelnerf}
\abx@aux@cite{0}{jensen2014_dtu}
\abx@aux@segm{0}{0}{jensen2014_dtu}
\abx@aux@cite{0}{yu2020_pixelnerf}
\abx@aux@segm{0}{0}{yu2020_pixelnerf}
\abx@aux@cite{0}{jensen2014_dtu}
\abx@aux@segm{0}{0}{jensen2014_dtu}
\abx@aux@cite{0}{yu2020_pixelnerf}
\abx@aux@segm{0}{0}{yu2020_pixelnerf}
\@writefile{lof}{\contentsline {figure}{\numberline {23.113}{\ignorespaces \textbf  {360$^\circ $ view prediction with multiple objects.} PixelNeRF naturally handles multi-object scenes, such as multiple ShapeNet chairs, because its prediction is conditioned in view space. In contrast, canonical-space models like SRN struggle with alignment when multiple objects are present. \emph  {Credit:} \blx@tocontentsinit {0}\cite {yu2020_pixelnerf}.}}{1571}{figure.caption.3464}\protected@file@percent }
\abx@aux@backref{2332}{yu2020_pixelnerf}{0}{1571}{1571}
\newlabel{fig:chapter23_pixelnerf_multiobject}{{23.113}{1571}{\textbf {360$^\circ $ view prediction with multiple objects.} PixelNeRF naturally handles multi-object scenes, such as multiple ShapeNet chairs, because its prediction is conditioned in view space. In contrast, canonical-space models like SRN struggle with alignment when multiple objects are present. \emph {Credit:} \cite {yu2020_pixelnerf}}{figure.caption.3464}{}}
\abx@aux@backref{2333}{niemeyer2020_dvr}{0}{1571}{1571}
\abx@aux@backref{2334}{sitzmann2019_srn}{0}{1571}{1571}
\abx@aux@backref{2335}{yu2020_pixelnerf}{0}{1571}{1571}
\@writefile{lot}{\contentsline {table}{\numberline {23.32}{\ignorespaces \textbf  {Challenging ShapeNet tasks.} Left: zero-shot generalization to 10 unseen categories using a model trained on only three classes (\texttt  {plane}, \texttt  {car}, \texttt  {chair}). Right: two-view reconstruction of scenes with multiple chairs. PixelNeRF clearly surpasses baselines in both settings, showcasing robustness to unseen object types and multi-object compositions. Matches Table~5 in~\blx@tocontentsinit {0}\cite {yu2020_pixelnerf}.}}{1571}{table.caption.3465}\protected@file@percent }
\abx@aux@backref{2337}{yu2020_pixelnerf}{0}{1571}{1571}
\newlabel{tab:chapter23_pixelnerf_unseen_multi}{{23.32}{1571}{\textbf {Challenging ShapeNet tasks.} Left: zero-shot generalization to 10 unseen categories using a model trained on only three classes (\texttt {plane}, \texttt {car}, \texttt {chair}). Right: two-view reconstruction of scenes with multiple chairs. PixelNeRF clearly surpasses baselines in both settings, showcasing robustness to unseen object types and multi-object compositions. Matches Table~5 in~\cite {yu2020_pixelnerf}}{table.caption.3465}{}}
\@writefile{toc}{\contentsline {paragraph}{Real images: Stanford Cars and DTU MVS}{1571}{section*.3466}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.114}{\ignorespaces \textbf  {Results on real car photos.} PixelNeRF trained on ShapeNet cars is directly applied to the Stanford Cars dataset~\blx@tocontentsinit {0}\cite {krause2013_stanfordcars}. Backgrounds are removed using PointRend~\blx@tocontentsinit {0}\cite {kirillov2020_pointrend}. The model generates plausible view rotations about the vertical axis without any fine-tuning, demonstrating cross-dataset transfer. \emph  {Credit:} \blx@tocontentsinit {0}\cite {yu2020_pixelnerf}.}}{1571}{figure.caption.3467}\protected@file@percent }
\abx@aux@backref{2341}{krause2013_stanfordcars}{0}{1571}{1571}
\abx@aux@backref{2342}{kirillov2020_pointrend}{0}{1571}{1571}
\abx@aux@backref{2343}{yu2020_pixelnerf}{0}{1571}{1571}
\newlabel{fig:chapter23_pixelnerf_realcars}{{23.114}{1571}{\textbf {Results on real car photos.} PixelNeRF trained on ShapeNet cars is directly applied to the Stanford Cars dataset~\cite {krause2013_stanfordcars}. Backgrounds are removed using PointRend~\cite {kirillov2020_pointrend}. The model generates plausible view rotations about the vertical axis without any fine-tuning, demonstrating cross-dataset transfer. \emph {Credit:} \cite {yu2020_pixelnerf}}{figure.caption.3467}{}}
\abx@aux@cite{0}{yu2020_pixelnerf}
\abx@aux@segm{0}{0}{yu2020_pixelnerf}
\abx@aux@cite{0}{yu2020_pixelnerf}
\abx@aux@segm{0}{0}{yu2020_pixelnerf}
\@writefile{lof}{\contentsline {figure}{\numberline {23.115}{\ignorespaces \textbf  {Wide-baseline novel view synthesis on DTU.} On the DTU MVS dataset~\blx@tocontentsinit {0}\cite {jensen2014_dtu}, PixelNeRF synthesizes novel views from as few as three posed input images. Notably, the training and test sets share no scenes, yet reconstructions remain consistent, highlighting the generalization ability of learned priors under wide-baseline, real-scene conditions. \emph  {Credit:} \blx@tocontentsinit {0}\cite {yu2020_pixelnerf}.}}{1572}{figure.caption.3468}\protected@file@percent }
\abx@aux@backref{2346}{jensen2014_dtu}{0}{1572}{1572}
\abx@aux@backref{2347}{yu2020_pixelnerf}{0}{1572}{1572}
\newlabel{fig:chapter23_pixelnerf_dtu_qual}{{23.115}{1572}{\textbf {Wide-baseline novel view synthesis on DTU.} On the DTU MVS dataset~\cite {jensen2014_dtu}, PixelNeRF synthesizes novel views from as few as three posed input images. Notably, the training and test sets share no scenes, yet reconstructions remain consistent, highlighting the generalization ability of learned priors under wide-baseline, real-scene conditions. \emph {Credit:} \cite {yu2020_pixelnerf}}{figure.caption.3468}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23.116}{\ignorespaces \textbf  {Few-shot reconstruction performance on DTU.} PSNR quantiles across scenes with 1, 3, 6, or 9 input views. PixelNeRF uses a single trained model with 3-view conditioning, while NeRF is retrained per scene and per view count. PixelNeRF maintains competitive or superior performance without test-time optimization. Matches Figure~9 in~\blx@tocontentsinit {0}\cite {yu2020_pixelnerf}.}}{1572}{figure.caption.3469}\protected@file@percent }
\abx@aux@backref{2349}{yu2020_pixelnerf}{0}{1572}{1572}
\newlabel{fig:chapter23_pixelnerf_dtu_psnr}{{23.116}{1572}{\textbf {Few-shot reconstruction performance on DTU.} PSNR quantiles across scenes with 1, 3, 6, or 9 input views. PixelNeRF uses a single trained model with 3-view conditioning, while NeRF is retrained per scene and per view count. PixelNeRF maintains competitive or superior performance without test-time optimization. Matches Figure~9 in~\cite {yu2020_pixelnerf}}{figure.caption.3469}{}}
\@writefile{toc}{\contentsline {subsubsection}{Limitations and Future Work}{1572}{section*.3470}\protected@file@percent }
\newlabel{subsec:chapter23_pixelnerf_limitations}{{23.11.4}{1572}{Limitations and Future Work}{section*.3470}{}}
\@writefile{toc}{\contentsline {paragraph}{Limitations}{1572}{section*.3471}\protected@file@percent }
\abx@aux@cite{0}{wang2021_ibrnet}
\abx@aux@segm{0}{0}{wang2021_ibrnet}
\abx@aux@cite{0}{chen2021_mvsnerf}
\abx@aux@segm{0}{0}{chen2021_mvsnerf}
\abx@aux@cite{0}{liu2022_neuray}
\abx@aux@segm{0}{0}{liu2022_neuray}
\abx@aux@cite{0}{johari2022_geonerf}
\abx@aux@segm{0}{0}{johari2022_geonerf}
\@writefile{toc}{\contentsline {paragraph}{Future work and influence}{1573}{section*.3472}\protected@file@percent }
\abx@aux@backref{2350}{wang2021_ibrnet}{0}{1573}{1573}
\abx@aux@backref{2351}{chen2021_mvsnerf}{0}{1573}{1573}
\abx@aux@backref{2352}{johari2022_geonerf}{0}{1573}{1573}
\abx@aux@backref{2353}{liu2022_neuray}{0}{1573}{1573}
\BKM@entry{id=919,dest={73656374696F6E2A2E33343733},srcline={7699}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030335C3030302E5C303030315C303030325C3030303A5C3030305C3034305C3030304E5C303030655C303030525C303030465C3030303A5C3030305C3034305C303030555C3030306E5C303030625C3030306F5C303030755C3030306E5C303030645C303030655C303030645C3030302C5C3030305C3034305C303030445C303030795C3030306E5C303030615C3030306D5C303030695C303030635C3030302C5C3030305C3034305C3030304C5C303030615C303030725C303030675C303030655C3030302D5C303030535C303030635C303030615C3030306C5C303030655C3030305C3034305C303030535C303030635C303030655C3030306E5C303030655C30303073}
\abx@aux@cite{0}{tancik2022_blocknerf}
\abx@aux@segm{0}{0}{tancik2022_blocknerf}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{park2021_nerfies}
\abx@aux@segm{0}{0}{park2021_nerfies}
\abx@aux@cite{0}{pumarola2021_dnerf}
\abx@aux@segm{0}{0}{pumarola2021_dnerf}
\abx@aux@cite{0}{park2021_hypernerf}
\abx@aux@segm{0}{0}{park2021_hypernerf}
\abx@aux@cite{0}{turki2022_meganerf}
\abx@aux@segm{0}{0}{turki2022_meganerf}
\BKM@entry{id=920,dest={73656374696F6E2A2E33343734},srcline={7711}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030335C3030302E5C303030315C303030325C3030302E5C303030315C3030303A5C3030305C3034305C303030425C3030306C5C3030306F5C303030635C3030306B5C3030302D5C3030304E5C303030655C303030525C303030465C3030303A5C3030305C3034305C303030535C303030635C303030615C3030306C5C303030615C303030625C3030306C5C303030655C3030305C3034305C3030304C5C303030615C303030725C303030675C303030655C3030305C3034305C303030535C303030635C303030655C3030306E5C303030655C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C303030565C303030695C303030655C303030775C3030305C3034305C303030535C303030795C3030306E5C303030745C303030685C303030655C303030735C303030695C30303073}
\abx@aux@cite{0}{tancik2022_blocknerf}
\abx@aux@segm{0}{0}{tancik2022_blocknerf}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\@writefile{toc}{\contentsline {section}{Enrichment 23.12: NeRF: Unbounded, Dynamic, Large-Scale Scenes}{1574}{section*.3473}\protected@file@percent }
\abx@aux@backref{2354}{tancik2022_blocknerf}{0}{1574}{1574}
\abx@aux@backref{2355}{barron2022_mipnerf360}{0}{1574}{1574}
\abx@aux@backref{2356}{park2021_nerfies}{0}{1574}{1574}
\abx@aux@backref{2357}{pumarola2021_dnerf}{0}{1574}{1574}
\abx@aux@backref{2358}{park2021_hypernerf}{0}{1574}{1574}
\abx@aux@backref{2359}{turki2022_meganerf}{0}{1574}{1574}
\@writefile{toc}{\contentsline {subsection}{Enrichment 23.12.1: Block-NeRF: Scalable Large Scene Neural View Synthesis}{1574}{section*.3474}\protected@file@percent }
\newlabel{enr:chapter23_blocknerf}{{23.12.1}{1574}{\color {ocre}Enrichment \thesubsection : Block-NeRF: Scalable Large Scene Neural View Synthesis}{section*.3474}{}}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{1574}{section*.3475}\protected@file@percent }
\newlabel{subsec:chapter23_blocknerf_motivation}{{23.12.1}{1574}{Motivation}{section*.3475}{}}
\abx@aux@backref{2360}{tancik2022_blocknerf}{0}{1574}{1574}
\@writefile{lof}{\contentsline {figure}{\numberline {23.117}{\ignorespaces \textbf  {City-scale reconstruction with Block-NeRF.} The Alamo Square neighborhood in San Francisco reconstructed using multiple Block-NeRFs trained on data from three months. Updates can be applied locally (e.g., construction area on the right) without retraining the entire model. \emph  {Credit:} \blx@tocontentsinit {0}\cite {barron2022_mipnerf360}.}}{1574}{figure.caption.3476}\protected@file@percent }
\abx@aux@backref{2362}{barron2022_mipnerf360}{0}{1574}{1574}
\newlabel{fig:chapter23_blocknerf_example}{{23.117}{1574}{\textbf {City-scale reconstruction with Block-NeRF.} The Alamo Square neighborhood in San Francisco reconstructed using multiple Block-NeRFs trained on data from three months. Updates can be applied locally (e.g., construction area on the right) without retraining the entire model. \emph {Credit:} \cite {barron2022_mipnerf360}}{figure.caption.3476}{}}
\abx@aux@cite{0}{tancik2022_blocknerf}
\abx@aux@segm{0}{0}{tancik2022_blocknerf}
\@writefile{toc}{\contentsline {subsubsection}{Method}{1575}{section*.3477}\protected@file@percent }
\newlabel{subsec:chapter23_blocknerf_method}{{23.12.1}{1575}{Method}{section*.3477}{}}
\@writefile{toc}{\contentsline {paragraph}{High-level overview}{1575}{section*.3478}\protected@file@percent }
\abx@aux@backref{2363}{tancik2022_blocknerf}{0}{1575}{1575}
\@writefile{toc}{\contentsline {paragraph}{Block partitioning and structure}{1575}{section*.3479}\protected@file@percent }
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\@writefile{toc}{\contentsline {paragraph}{Architectural design choices}{1576}{section*.3480}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.118}{\ignorespaces \textbf  {Block-NeRF architecture.} Built on mip-NeRF (see \ref {enr:subsec_chapter23_mipnerf}). The density MLP $f_\sigma $ outputs $\sigma $ and features; the color MLP $f_c$ consumes features, view direction, exposure encoding, and appearance embedding to predict RGB; the visibility MLP $f_v$ regresses training-time transmittance, supporting block selection and overlap-based appearance alignment. \emph  {Credit:} \blx@tocontentsinit {0}\cite {barron2022_mipnerf360}.}}{1576}{figure.caption.3481}\protected@file@percent }
\abx@aux@backref{2365}{barron2022_mipnerf360}{0}{1576}{1576}
\newlabel{fig:chapter23_blocknerf_pipeline}{{23.118}{1576}{\textbf {Block-NeRF architecture.} Built on mip-NeRF (see \ref {enr:subsec_chapter23_mipnerf}). The density MLP $f_\sigma $ outputs $\sigma $ and features; the color MLP $f_c$ consumes features, view direction, exposure encoding, and appearance embedding to predict RGB; the visibility MLP $f_v$ regresses training-time transmittance, supporting block selection and overlap-based appearance alignment. \emph {Credit:} \cite {barron2022_mipnerf360}}{figure.caption.3481}{}}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\@writefile{toc}{\contentsline {paragraph}{How $f_v$ integrates into the pipeline}{1577}{section*.3482}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.119}{\ignorespaces \textbf  {Visibility-guided compositing.} Candidate blocks near the camera are scored by $f_v$. Blocks with low predicted visibility (bottom) are culled. The remaining per-block renderings are blended in image space with distance-based weights, producing seamless transitions across block boundaries while avoiding seams from irrelevant blocks. \emph  {Credit:} \blx@tocontentsinit {0}\cite {barron2022_mipnerf360}.}}{1577}{figure.caption.3483}\protected@file@percent }
\abx@aux@backref{2367}{barron2022_mipnerf360}{0}{1577}{1577}
\newlabel{fig:chapter23_blocknerf_rendering}{{23.119}{1577}{\textbf {Visibility-guided compositing.} Candidate blocks near the camera are scored by $f_v$. Blocks with low predicted visibility (bottom) are culled. The remaining per-block renderings are blended in image space with distance-based weights, producing seamless transitions across block boundaries while avoiding seams from irrelevant blocks. \emph {Credit:} \cite {barron2022_mipnerf360}}{figure.caption.3483}{}}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\@writefile{toc}{\contentsline {paragraph}{Why $f_v$ is essential}{1578}{section*.3484}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Compositing across blocks}{1578}{section*.3485}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Appearance control and cross-block alignment}{1578}{section*.3486}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.120}{\ignorespaces \textbf  {Appearance embeddings} Per-image latents represent weather/illumination diversity (day/night, clear/cloudy), preventing geometry corruption and enabling controllable appearance during inference. \emph  {Credit:} \blx@tocontentsinit {0}\cite {barron2022_mipnerf360}.}}{1578}{figure.caption.3487}\protected@file@percent }
\abx@aux@backref{2369}{barron2022_mipnerf360}{0}{1578}{1578}
\newlabel{fig:chapter23_blocknerf_appearance}{{23.120}{1578}{\textbf {Appearance embeddings} Per-image latents represent weather/illumination diversity (day/night, clear/cloudy), preventing geometry corruption and enabling controllable appearance during inference. \emph {Credit:} \cite {barron2022_mipnerf360}}{figure.caption.3487}{}}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{barron2021_mipnerf}
\abx@aux@segm{0}{0}{barron2021_mipnerf}
\@writefile{lof}{\contentsline {figure}{\numberline {23.121}{\ignorespaces \textbf  {Exposure conditioning} Conditioning on exposure stabilizes training across brightness variation and provides an interpretable control knob at render time (e.g., brighten/darken without altering geometry). \emph  {Credit:} \blx@tocontentsinit {0}\cite {barron2022_mipnerf360}.}}{1579}{figure.caption.3488}\protected@file@percent }
\abx@aux@backref{2371}{barron2022_mipnerf360}{0}{1579}{1579}
\newlabel{fig:chapter23_blocknerf_exposure}{{23.121}{1579}{\textbf {Exposure conditioning} Conditioning on exposure stabilizes training across brightness variation and provides an interpretable control knob at render time (e.g., brighten/darken without altering geometry). \emph {Credit:} \cite {barron2022_mipnerf360}}{figure.caption.3488}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23.122}{\ignorespaces \textbf  {Cross-block appearance matching} A fixed target appearance (left) is propagated to neighbors by optimizing only their appearance codes on overlapping, high-visibility regions, yielding a consistent global style (e.g., coherent night appearance) across blocks. \emph  {Credit:} \blx@tocontentsinit {0}\cite {barron2022_mipnerf360}.}}{1579}{figure.caption.3489}\protected@file@percent }
\abx@aux@backref{2373}{barron2022_mipnerf360}{0}{1579}{1579}
\newlabel{fig:chapter23_blocknerf_match}{{23.122}{1579}{\textbf {Cross-block appearance matching} A fixed target appearance (left) is propagated to neighbors by optimizing only their appearance codes on overlapping, high-visibility regions, yielding a consistent global style (e.g., coherent night appearance) across blocks. \emph {Credit:} \cite {barron2022_mipnerf360}}{figure.caption.3489}{}}
\@writefile{toc}{\contentsline {paragraph}{Why this design works}{1579}{section*.3490}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Experiments and Ablations}{1579}{section*.3491}\protected@file@percent }
\newlabel{subsec:chapter23_blocknerf_experiments}{{23.12.1}{1579}{Experiments and Ablations}{section*.3491}{}}
\@writefile{toc}{\contentsline {paragraph}{Ablations on Alamo Square}{1579}{section*.3492}\protected@file@percent }
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@backref{2374}{barron2021_mipnerf}{0}{1580}{1580}
\@writefile{lot}{\contentsline {table}{\numberline {23.33}{\ignorespaces \textbf  {Ablation study on Alamo Square.} Each architectural component contributes: appearance embeddings mitigate geometry hallucinations; pose refinement sharpens alignment; exposure conditioning improves stability and control.}}{1580}{table.caption.3493}\protected@file@percent }
\newlabel{tab:chapter23_blocknerf_ablation}{{23.33}{1580}{\textbf {Ablation study on Alamo Square.} Each architectural component contributes: appearance embeddings mitigate geometry hallucinations; pose refinement sharpens alignment; exposure conditioning improves stability and control}{table.caption.3493}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23.123}{\ignorespaces \textbf  {Qualitative ablations.} Without appearance embeddings, cloudy geometry is introduced. Without pose optimization, ghosting occurs (e.g., duplicated telephone pole in the first row). Exposure conditioning provides modest improvements in fidelity and crucial control at inference. \emph  {Credit:} \blx@tocontentsinit {0}\cite {barron2022_mipnerf360}.}}{1580}{figure.caption.3494}\protected@file@percent }
\abx@aux@backref{2376}{barron2022_mipnerf360}{0}{1580}{1580}
\newlabel{fig:chapter23_blocknerf_multi}{{23.123}{1580}{\textbf {Qualitative ablations.} Without appearance embeddings, cloudy geometry is introduced. Without pose optimization, ghosting occurs (e.g., duplicated telephone pole in the first row). Exposure conditioning provides modest improvements in fidelity and crucial control at inference. \emph {Credit:} \cite {barron2022_mipnerf360}}{figure.caption.3494}{}}
\@writefile{toc}{\contentsline {paragraph}{Block granularity on Mission Bay}{1580}{section*.3495}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {23.34}{\ignorespaces \textbf  {Effect of block granularity on Mission Bay.} More blocks yield higher reconstruction fidelity. Even with fixed total parameters (bottom), splitting capacity into multiple small blocks improves accuracy and reduces per-frame compute since only a subset of blocks is active. \emph  {Credit:} \blx@tocontentsinit {0}\cite {barron2022_mipnerf360}.}}{1580}{table.caption.3496}\protected@file@percent }
\abx@aux@backref{2378}{barron2022_mipnerf360}{0}{1580}{1580}
\newlabel{tab:chapter23_blocknerf_blocks}{{23.34}{1580}{\textbf {Effect of block granularity on Mission Bay.} More blocks yield higher reconstruction fidelity. Even with fixed total parameters (bottom), splitting capacity into multiple small blocks improves accuracy and reduces per-frame compute since only a subset of blocks is active. \emph {Credit:} \cite {barron2022_mipnerf360}}{table.caption.3496}{}}
\abx@aux@cite{0}{zhang2020_nerfplusplus}
\abx@aux@segm{0}{0}{zhang2020_nerfplusplus}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{liu2020_nsvf}
\abx@aux@segm{0}{0}{liu2020_nsvf}
\abx@aux@cite{0}{yu2021_plenoctrees}
\abx@aux@segm{0}{0}{yu2021_plenoctrees}
\abx@aux@cite{0}{mueller2022_instantngp}
\abx@aux@segm{0}{0}{mueller2022_instantngp}
\BKM@entry{id=921,dest={73656374696F6E2A2E33343938},srcline={7921}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030335C3030302E5C303030315C303030325C3030302E5C303030325C3030303A5C3030305C3034305C3030304D5C303030695C303030705C3030302D5C3030304E5C303030655C303030525C303030465C3030305C3034305C303030335C303030365C303030305C3030303A5C3030305C3034305C303030555C3030306E5C303030625C3030306F5C303030755C3030306E5C303030645C303030655C303030645C3030305C3034305C303030415C3030306E5C303030745C303030695C3030302D5C303030415C3030306C5C303030695C303030615C303030735C303030655C303030645C3030305C3034305C3030304E5C303030655C303030525C30303046}
\abx@aux@cite{0}{barron2021_mipnerf}
\abx@aux@segm{0}{0}{barron2021_mipnerf}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{barron2021_mipnerf}
\abx@aux@segm{0}{0}{barron2021_mipnerf}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{barron2021_mipnerf}
\abx@aux@segm{0}{0}{barron2021_mipnerf}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{barron2021_mipnerf}
\abx@aux@segm{0}{0}{barron2021_mipnerf}
\@writefile{toc}{\contentsline {subsubsection}{Limitations and Future Work}{1581}{section*.3497}\protected@file@percent }
\newlabel{subsec:chapter23_blocknerf_limits}{{23.12.1}{1581}{Limitations and Future Work}{section*.3497}{}}
\abx@aux@backref{2379}{zhang2020_nerfplusplus}{0}{1581}{1581}
\abx@aux@backref{2380}{barron2022_mipnerf360}{0}{1581}{1581}
\abx@aux@backref{2381}{liu2020_nsvf}{0}{1581}{1581}
\abx@aux@backref{2382}{yu2021_plenoctrees}{0}{1581}{1581}
\abx@aux@backref{2383}{mueller2022_instantngp}{0}{1581}{1581}
\@writefile{toc}{\contentsline {subsection}{Enrichment 23.12.2: Mip-NeRF 360: Unbounded Anti-Aliased NeRF}{1581}{section*.3498}\protected@file@percent }
\newlabel{enr:chapter23_mipnerf360}{{23.12.2}{1581}{\color {ocre}Enrichment \thesubsection : Mip-NeRF 360: Unbounded Anti-Aliased NeRF}{section*.3498}{}}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{1581}{section*.3499}\protected@file@percent }
\newlabel{subsec:chapter23_mipnerf360_motivation}{{23.12.2}{1581}{Motivation}{section*.3499}{}}
\abx@aux@backref{2384}{barron2021_mipnerf}{0}{1581}{1581}
\abx@aux@backref{2385}{barron2022_mipnerf360}{0}{1581}{1581}
\@writefile{toc}{\contentsline {paragraph}{Challenges in unbounded 360° scenes}{1581}{section*.3500}\protected@file@percent }
\abx@aux@backref{2386}{barron2022_mipnerf360}{0}{1581}{1581}
\abx@aux@backref{2387}{barron2021_mipnerf}{0}{1581}{1581}
\abx@aux@backref{2388}{barron2022_mipnerf360}{0}{1581}{1581}
\abx@aux@backref{2389}{barron2021_mipnerf}{0}{1581}{1581}
\abx@aux@backref{2390}{barron2021_mipnerf}{0}{1581}{1581}
\abx@aux@backref{2391}{mildenhall2020_nerf}{0}{1581}{1581}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\@writefile{toc}{\contentsline {paragraph}{MipNeRF360 solutions to unbounded scenes challenges}{1582}{section*.3501}\protected@file@percent }
\abx@aux@backref{2392}{barron2022_mipnerf360}{0}{1582}{1582}
\abx@aux@backref{2393}{barron2022_mipnerf360}{0}{1582}{1582}
\abx@aux@backref{2394}{barron2022_mipnerf360}{0}{1582}{1582}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@backref{2395}{barron2022_mipnerf360}{0}{1583}{1583}
\@writefile{lof}{\contentsline {figure}{\numberline {23.124}{\ignorespaces \textbf  {Comparison to mip-NeRF.} (a) Though mip-NeRF is able to produce accurate renderings of objects, for unbounded scenes it often generates blurry backgrounds and low-detail foregrounds. (b) MipNeRF360 produces detailed realistic renderings of these unbounded scenes, as evidenced by the renderings (top) and depth maps (bottom) from both models. See the supplemental video for additional results. \emph  {Credit:} \blx@tocontentsinit {0}\cite {barron2022_mipnerf360}.}}{1583}{figure.caption.3502}\protected@file@percent }
\abx@aux@backref{2397}{barron2022_mipnerf360}{0}{1583}{1583}
\newlabel{fig:chapter23_mipnerf360_comp_mipnerf}{{23.124}{1583}{\textbf {Comparison to mip-NeRF.} (a) Though mip-NeRF is able to produce accurate renderings of objects, for unbounded scenes it often generates blurry backgrounds and low-detail foregrounds. (b) MipNeRF360 produces detailed realistic renderings of these unbounded scenes, as evidenced by the renderings (top) and depth maps (bottom) from both models. See the supplemental video for additional results. \emph {Credit:} \cite {barron2022_mipnerf360}}{figure.caption.3502}{}}
\@writefile{toc}{\contentsline {subsubsection}{Method}{1583}{section*.3503}\protected@file@percent }
\newlabel{subsec:chapter23_mipnerf360_method}{{23.12.2}{1583}{Method}{section*.3503}{}}
\abx@aux@backref{2398}{barron2022_mipnerf360}{0}{1583}{1583}
\@writefile{toc}{\contentsline {paragraph}{Preliminaries: mip-NeRF}{1583}{section*.3504}\protected@file@percent }
\newlabel{par:chapter23_mipnerf360_prelims}{{23.12.2}{1583}{Preliminaries: mip-NeRF}{section*.3504}{}}
\newlabel{eq:mipnerf_ipe}{{23.47}{1583}{Preliminaries: mip-NeRF}{equation.23.47}{}}
\newlabel{eq:mipnerf_mlp}{{23.48}{1583}{Preliminaries: mip-NeRF}{equation.23.48}{}}
\abx@aux@cite{0}{barron2021_mipnerf}
\abx@aux@segm{0}{0}{barron2021_mipnerf}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\newlabel{eq:mipnerf_color}{{23.49}{1584}{Preliminaries: mip-NeRF}{equation.23.49}{}}
\newlabel{eq:mipnerf_weights}{{23.50}{1584}{Preliminaries: mip-NeRF}{equation.23.50}{}}
\newlabel{eq:mipnerf_tc}{{23.51}{1584}{Preliminaries: mip-NeRF}{equation.23.51}{}}
\newlabel{eq:mipnerf_tf}{{23.52}{1584}{Preliminaries: mip-NeRF}{equation.23.52}{}}
\newlabel{eq:mipnerf_loss}{{23.53}{1584}{Preliminaries: mip-NeRF}{equation.23.53}{}}
\abx@aux@backref{2399}{barron2021_mipnerf}{0}{1584}{1584}
\@writefile{toc}{\contentsline {paragraph}{Scene and ray parameterization}{1584}{section*.3505}\protected@file@percent }
\newlabel{par:chapter23_mipnerf360_param}{{23.12.2}{1584}{Scene and ray parameterization}{section*.3505}{}}
\abx@aux@backref{2400}{barron2022_mipnerf360}{0}{1584}{1584}
\newlabel{eq:linearize-push}{{23.54}{1584}{Scene and ray parameterization}{equation.23.54}{}}
\newlabel{eq:contract}{{23.55}{1584}{Scene and ray parameterization}{equation.23.55}{}}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@backref{2401}{barron2022_mipnerf360}{0}{1585}{1585}
\abx@aux@backref{2402}{barron2022_mipnerf360}{0}{1585}{1585}
\abx@aux@backref{2403}{barron2022_mipnerf360}{0}{1585}{1585}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\newlabel{eq:s-mapping}{{23.56}{1586}{Scene and ray parameterization}{equation.23.56}{}}
\abx@aux@backref{2404}{barron2022_mipnerf360}{0}{1586}{1586}
\@writefile{lof}{\contentsline {figure}{\numberline {23.125}{\ignorespaces \textbf  {Scene reparameterization visualization.} A 2D visualization of the scene parameterization. The operator $\operatorname  {contract}(\cdot )$ (Eq.\,(10), arrows) maps coordinates onto a ball of radius 2 (orange), leaving points within radius 1 (blue) unchanged. This contraction is applied to mip-NeRF Gaussians in Euclidean 3D (gray ellipses) similarly to a Kalman filter to produce contracted Gaussians (red ellipses), whose centers lie within radius 2. The design of $\operatorname  {contract}(\cdot )$, combined with linear-in-disparity ray spacing, yields equidistant intervals in the orange region for rays cast from the origin. \emph  {Credit:} \blx@tocontentsinit {0}\cite {barron2022_mipnerf360}.}}{1586}{figure.caption.3506}\protected@file@percent }
\abx@aux@backref{2406}{barron2022_mipnerf360}{0}{1586}{1586}
\newlabel{fig:chapter23_mipnerf360_contract_vis}{{23.125}{1586}{\textbf {Scene reparameterization visualization.} A 2D visualization of the scene parameterization. The operator $\operatorname {contract}(\cdot )$ (Eq.\,(10), arrows) maps coordinates onto a ball of radius 2 (orange), leaving points within radius 1 (blue) unchanged. This contraction is applied to mip-NeRF Gaussians in Euclidean 3D (gray ellipses) similarly to a Kalman filter to produce contracted Gaussians (red ellipses), whose centers lie within radius 2. The design of $\operatorname {contract}(\cdot )$, combined with linear-in-disparity ray spacing, yields equidistant intervals in the orange region for rays cast from the origin. \emph {Credit:} \cite {barron2022_mipnerf360}}{figure.caption.3506}{}}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\@writefile{toc}{\contentsline {paragraph}{Coarse-to-fine online distillation}{1587}{section*.3507}\protected@file@percent }
\newlabel{par:chapter23_mipnerf360_distill}{{23.12.2}{1587}{Coarse-to-fine online distillation}{section*.3507}{}}
\abx@aux@backref{2407}{barron2022_mipnerf360}{0}{1587}{1587}
\@writefile{lof}{\contentsline {figure}{\numberline {23.126}{\ignorespaces \textbf  {Architecture vs.\ mip-NeRF.} mip-NeRF reuses one MLP across scales and supervises all scales. MipNeRF360 replaces early image-supervised passes with a \emph  {proposal MLP} that emits weights (no color) to guide resampling, and a single final \emph  {NeRF MLP} that outputs weights and colors for supervision. The proposal MLP is trained so its weights $\hat  {w}$ are consistent with the NeRF MLP’s final weights $w$. A small proposal MLP plus a large NeRF MLP yields high capacity while remaining tractable. \emph  {Credit:} \blx@tocontentsinit {0}\cite {barron2022_mipnerf360}.}}{1587}{figure.caption.3508}\protected@file@percent }
\abx@aux@backref{2409}{barron2022_mipnerf360}{0}{1587}{1587}
\newlabel{fig:chapter23_mipnerf360_architecture}{{23.126}{1587}{\textbf {Architecture vs.\ mip-NeRF.} mip-NeRF reuses one MLP across scales and supervises all scales. MipNeRF360 replaces early image-supervised passes with a \emph {proposal MLP} that emits weights (no color) to guide resampling, and a single final \emph {NeRF MLP} that outputs weights and colors for supervision. The proposal MLP is trained so its weights $\hat {w}$ are consistent with the NeRF MLP’s final weights $w$. A small proposal MLP plus a large NeRF MLP yields high capacity while remaining tractable. \emph {Credit:} \cite {barron2022_mipnerf360}}{figure.caption.3508}{}}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@backref{2410}{barron2022_mipnerf360}{0}{1588}{1588}
\newlabel{eq:bound}{{23.57}{1588}{Coarse-to-fine online distillation}{equation.23.57}{}}
\newlabel{eq:lprop}{{23.58}{1588}{Coarse-to-fine online distillation}{equation.23.58}{}}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\@writefile{lof}{\contentsline {figure}{\numberline {23.127}{\ignorespaces \textbf  {Histogram evolution over training.} For a single ray in \emph  {bicycle}, the NeRF histogram $(t,w)$ (black) and two proposal histograms $(\hat  {t},\hat  {w})$ (yellow, orange) across training. Early weights are near-uniform; later, NeRF concentrates at a surface while proposals adapt to cover it, enabling robust resampling. \emph  {Credit:} \blx@tocontentsinit {0}\cite {barron2022_mipnerf360}.}}{1589}{figure.caption.3509}\protected@file@percent }
\abx@aux@backref{2412}{barron2022_mipnerf360}{0}{1589}{1589}
\newlabel{fig:chapter23_mipnerf360_hist_viz}{{23.127}{1589}{\textbf {Histogram evolution over training.} For a single ray in \emph {bicycle}, the NeRF histogram $(t,w)$ (black) and two proposal histograms $(\hat {t},\hat {w})$ (yellow, orange) across training. Early weights are near-uniform; later, NeRF concentrates at a surface while proposals adapt to cover it, enabling robust resampling. \emph {Credit:} \cite {barron2022_mipnerf360}}{figure.caption.3509}{}}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\@writefile{lof}{\contentsline {figure}{\numberline {23.128}{\ignorespaces \textbf  {Motivation behind $\mathcal  {L}_{\mathrm  {prop}}$.} If two histograms could arise from the same underlying distribution, the bound induced by $(\hat  {t},\hat  {w})$ upper-bounds $(t,w)$ and the loss is zero; otherwise, any surplus final mass (red) is penalized, teaching the proposal to cover regions NeRF actually uses. \emph  {Credit:} \blx@tocontentsinit {0}\cite {barron2022_mipnerf360}.}}{1590}{figure.caption.3510}\protected@file@percent }
\abx@aux@backref{2414}{barron2022_mipnerf360}{0}{1590}{1590}
\newlabel{fig:chapter23_mipnerf360_lprop}{{23.128}{1590}{\textbf {Motivation behind $\mathcal {L}_{\mathrm {prop}}$.} If two histograms could arise from the same underlying distribution, the bound induced by $(\hat {t},\hat {w})$ upper-bounds $(t,w)$ and the loss is zero; otherwise, any surplus final mass (red) is penalized, teaching the proposal to cover regions NeRF actually uses. \emph {Credit:} \cite {barron2022_mipnerf360}}{figure.caption.3510}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23.129}{\ignorespaces \textbf  {Midpoint resampling.} Using sampled points as endpoints (blue) erodes coarse modes and spans gaps asymmetrically; midpoints between sorted samples (red) yield more regular refinements and reduce aliasing. \emph  {Credit:} \blx@tocontentsinit {0}\cite {barron2022_mipnerf360}.}}{1590}{figure.caption.3511}\protected@file@percent }
\abx@aux@backref{2416}{barron2022_mipnerf360}{0}{1590}{1590}
\newlabel{fig:chapter23_mipnerf360_resampling}{{23.129}{1590}{\textbf {Midpoint resampling.} Using sampled points as endpoints (blue) erodes coarse modes and spans gaps asymmetrically; midpoints between sorted samples (red) yield more regular refinements and reduce aliasing. \emph {Credit:} \cite {barron2022_mipnerf360}}{figure.caption.3511}{}}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\@writefile{toc}{\contentsline {paragraph}{Regularization for interval-based models}{1591}{section*.3512}\protected@file@percent }
\newlabel{par:chapter23_mipnerf360_distortion}{{23.12.2}{1591}{Regularization for interval-based models}{section*.3512}{}}
\newlabel{eq:ldist_integral}{{23.59}{1591}{Regularization for interval-based models}{equation.23.59}{}}
\newlabel{eq:ldist_discrete}{{23.60}{1591}{Regularization for interval-based models}{equation.23.60}{}}
\abx@aux@backref{2417}{barron2022_mipnerf360}{0}{1591}{1591}
\@writefile{lof}{\contentsline {figure}{\numberline {23.130}{\ignorespaces \textbf  {Effect of $\mathcal  {L}_{\mathrm  {dist}}$.} The regularizer suppresses floaters and prevents background collapse more effectively than density-noise injection \blx@tocontentsinit {0}\cite {mildenhall2020_nerf}, which can also reduce reconstruction detail. \emph  {Credit:} \blx@tocontentsinit {0}\cite {barron2022_mipnerf360}.}}{1591}{figure.caption.3513}\protected@file@percent }
\abx@aux@backref{2420}{mildenhall2020_nerf}{0}{1591}{1591}
\abx@aux@backref{2421}{barron2022_mipnerf360}{0}{1591}{1591}
\newlabel{fig:chapter23_mipnerf360_ldist_regularizer}{{23.130}{1591}{\textbf {Effect of $\mathcal {L}_{\mathrm {dist}}$.} The regularizer suppresses floaters and prevents background collapse more effectively than density-noise injection \cite {mildenhall2020_nerf}, which can also reduce reconstruction detail. \emph {Credit:} \cite {barron2022_mipnerf360}}{figure.caption.3513}{}}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\@writefile{lof}{\contentsline {figure}{\numberline {23.131}{\ignorespaces \textbf  {Gradients of $\mathcal  {L}_{\mathrm  {dist}}$.} Visualization of $\nabla \mathcal  {L}_{\mathrm  {dist}}$ on a toy step function: it shrinks interval widths, pulls distant intervals together, consolidates mass into a small number of nearby intervals, and drives all weights to zero when the ray is empty. \emph  {Credit:} \blx@tocontentsinit {0}\cite {barron2022_mipnerf360}.}}{1592}{figure.caption.3514}\protected@file@percent }
\abx@aux@backref{2423}{barron2022_mipnerf360}{0}{1592}{1592}
\newlabel{fig:chapter23_mipnerf360_ldist_grads}{{23.131}{1592}{\textbf {Gradients of $\mathcal {L}_{\mathrm {dist}}$.} Visualization of $\nabla \mathcal {L}_{\mathrm {dist}}$ on a toy step function: it shrinks interval widths, pulls distant intervals together, consolidates mass into a small number of nearby intervals, and drives all weights to zero when the ray is empty. \emph {Credit:} \cite {barron2022_mipnerf360}}{figure.caption.3514}{}}
\@writefile{toc}{\contentsline {paragraph}{Optimization and training recipe}{1592}{section*.3515}\protected@file@percent }
\newlabel{par:chapter23_mipnerf360_opt}{{23.12.2}{1592}{Optimization and training recipe}{section*.3515}{}}
\abx@aux@backref{2424}{barron2022_mipnerf360}{0}{1592}{1592}
\newlabel{eq:total_loss}{{23.61}{1592}{Optimization and training recipe}{equation.23.61}{}}
\abx@aux@backref{2425}{barron2022_mipnerf360}{0}{1592}{1592}
\abx@aux@cite{0}{barron2022_mipnerf360}
\abx@aux@segm{0}{0}{barron2022_mipnerf360}
\@writefile{toc}{\contentsline {subsubsection}{Results and Ablations}{1593}{section*.3516}\protected@file@percent }
\newlabel{subsubsec:chapter23_mipnerf360_results}{{23.12.2}{1593}{Results and Ablations}{section*.3516}{}}
\@writefile{toc}{\contentsline {paragraph}{Quantitative evaluation}{1593}{section*.3517}\protected@file@percent }
\abx@aux@backref{2426}{barron2022_mipnerf360}{0}{1593}{1593}
\@writefile{toc}{\contentsline {paragraph}{Qualitative comparison}{1593}{section*.3518}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Ablations}{1593}{section*.3519}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Generalization across datasets}{1593}{section*.3520}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Limitations}{1593}{section*.3521}\protected@file@percent }
\newlabel{subsubsec:chapter23_mipnerf360_limitations}{{23.12.2}{1593}{Limitations}{section*.3521}{}}
\@writefile{toc}{\contentsline {paragraph}{Outlook}{1593}{section*.3522}\protected@file@percent }
\BKM@entry{id=922,dest={73656374696F6E2A2E33353233},srcline={8241}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030335C3030302E5C303030315C303030325C3030302E5C303030335C3030303A5C3030305C3034305C303030445C3030302D5C3030304E5C303030655C303030525C303030465C3030303A5C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C303030525C303030615C303030645C303030695C303030615C3030306E5C303030635C303030655C3030305C3034305C303030465C303030695C303030655C3030306C5C303030645C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030445C303030795C3030306E5C303030615C3030306D5C303030695C303030635C3030305C3034305C303030535C303030635C303030655C3030306E5C303030655C30303073}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{pumarola2021_dnerf}
\abx@aux@segm{0}{0}{pumarola2021_dnerf}
\abx@aux@cite{0}{pumarola2021_dnerf}
\abx@aux@segm{0}{0}{pumarola2021_dnerf}
\abx@aux@cite{0}{pumarola2021_dnerf}
\abx@aux@segm{0}{0}{pumarola2021_dnerf}
\abx@aux@cite{0}{pumarola2021_dnerf}
\abx@aux@segm{0}{0}{pumarola2021_dnerf}
\abx@aux@cite{0}{pumarola2021_dnerf}
\abx@aux@segm{0}{0}{pumarola2021_dnerf}
\@writefile{toc}{\contentsline {subsection}{Enrichment 23.12.3: D-NeRF: Neural Radiance Fields for Dynamic Scenes}{1594}{section*.3523}\protected@file@percent }
\newlabel{enr:subsec_chapter23_dnerf}{{23.12.3}{1594}{\color {ocre}Enrichment \thesubsection : D-NeRF: Neural Radiance Fields for Dynamic Scenes}{section*.3523}{}}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{1594}{section*.3524}\protected@file@percent }
\newlabel{subsec:chapter23_dnerf_motivation}{{23.12.3}{1594}{Motivation}{section*.3524}{}}
\abx@aux@backref{2427}{mildenhall2020_nerf}{0}{1594}{1594}
\abx@aux@backref{2428}{pumarola2021_dnerf}{0}{1594}{1594}
\@writefile{lof}{\contentsline {figure}{\numberline {23.132}{\ignorespaces \textbf  {Dynamic scene synthesis with D-NeRF.} The authors propose a method to render novel views at arbitrary time instants for dynamic scenes with complex non-rigid geometry. Results include a dinosaur skeleton moving over time (top) and a construction worker changing poses (bottom). Each frame is synthesized from sparse monocular input without requiring ground-truth geometry or multi-view capture~\blx@tocontentsinit {0}\cite {pumarola2021_dnerf}.}}{1594}{figure.caption.3525}\protected@file@percent }
\abx@aux@backref{2430}{pumarola2021_dnerf}{0}{1594}{1594}
\newlabel{fig:chapter23_dnerf_examples}{{23.132}{1594}{\textbf {Dynamic scene synthesis with D-NeRF.} The authors propose a method to render novel views at arbitrary time instants for dynamic scenes with complex non-rigid geometry. Results include a dinosaur skeleton moving over time (top) and a construction worker changing poses (bottom). Each frame is synthesized from sparse monocular input without requiring ground-truth geometry or multi-view capture~\cite {pumarola2021_dnerf}}{figure.caption.3525}{}}
\@writefile{toc}{\contentsline {subsubsection}{Problem Setup}{1594}{section*.3526}\protected@file@percent }
\newlabel{subsec:chapter23_dnerf_problemsetup}{{23.12.3}{1594}{Problem Setup}{section*.3526}{}}
\abx@aux@cite{0}{pumarola2021_dnerf}
\abx@aux@segm{0}{0}{pumarola2021_dnerf}
\abx@aux@cite{0}{pumarola2021_dnerf}
\abx@aux@segm{0}{0}{pumarola2021_dnerf}
\@writefile{lof}{\contentsline {figure}{\numberline {23.133}{\ignorespaces \textbf  {Problem setup of D-NeRF.} From a sparse set of monocular frames of a non-rigid dynamic scene, paired with camera parameters, D-NeRF learns an implicit scene representation. The model synthesizes novel views at arbitrary time instants, as shown on the right~\blx@tocontentsinit {0}\cite {pumarola2021_dnerf}.}}{1595}{figure.caption.3527}\protected@file@percent }
\abx@aux@backref{2432}{pumarola2021_dnerf}{0}{1595}{1595}
\newlabel{fig:chapter23_dnerf_problemsetting}{{23.133}{1595}{\textbf {Problem setup of D-NeRF.} From a sparse set of monocular frames of a non-rigid dynamic scene, paired with camera parameters, D-NeRF learns an implicit scene representation. The model synthesizes novel views at arbitrary time instants, as shown on the right~\cite {pumarola2021_dnerf}}{figure.caption.3527}{}}
\@writefile{toc}{\contentsline {paragraph}{Challenges of direct spatio-temporal regression.}{1595}{section*.3528}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Method}{1595}{section*.3529}\protected@file@percent }
\newlabel{subsec:chapter23_dnerf_method}{{23.12.3}{1595}{Method}{section*.3529}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23.134}{\ignorespaces \textbf  {Model architecture of D-NeRF.} The deformation network $\Psi _t$ maps points observed at time $t$ to a canonical space. The canonical network $\Psi _x$ assigns volume density and view-dependent radiance in this canonical configuration~\blx@tocontentsinit {0}\cite {pumarola2021_dnerf}.}}{1596}{figure.caption.3530}\protected@file@percent }
\abx@aux@backref{2434}{pumarola2021_dnerf}{0}{1596}{1596}
\newlabel{fig:chapter23_dnerf_model}{{23.134}{1596}{\textbf {Model architecture of D-NeRF.} The deformation network $\Psi _t$ maps points observed at time $t$ to a canonical space. The canonical network $\Psi _x$ assigns volume density and view-dependent radiance in this canonical configuration~\cite {pumarola2021_dnerf}}{figure.caption.3530}{}}
\@writefile{toc}{\contentsline {paragraph}{Canonical network}{1596}{section*.3531}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Deformation network}{1596}{section*.3532}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Volume rendering with deformations}{1596}{section*.3533}\protected@file@percent }
\newlabel{eq:chapter23_dnerf_volint}{{23.62}{1596}{Volume rendering with deformations}{equation.23.62}{}}
\newlabel{eq:chapter23_dnerf_warp}{{23.63}{1597}{Volume rendering with deformations}{equation.23.63}{}}
\newlabel{eq:chapter23_dnerf_canonical}{{23.64}{1597}{Volume rendering with deformations}{equation.23.64}{}}
\newlabel{eq:chapter23_dnerf_transmittance}{{23.65}{1597}{Volume rendering with deformations}{equation.23.65}{}}
\newlabel{eq:chapter23_dnerf_discrete}{{23.66}{1597}{Volume rendering with deformations}{equation.23.66}{}}
\newlabel{eq:chapter23_dnerf_alpha}{{23.67}{1597}{Volume rendering with deformations}{equation.23.67}{}}
\newlabel{eq:chapter23_dnerf_Tprime}{{23.68}{1597}{Volume rendering with deformations}{equation.23.68}{}}
\@writefile{toc}{\contentsline {paragraph}{Learning objective}{1597}{section*.3534}\protected@file@percent }
\newlabel{eq:chapter23_dnerf_loss}{{23.69}{1597}{Learning objective}{equation.23.69}{}}
\@writefile{toc}{\contentsline {subsubsection}{Architecture and Implementation Details}{1597}{section*.3535}\protected@file@percent }
\newlabel{subsec:chapter23_dnerf_architecture}{{23.12.3}{1597}{Architecture and Implementation Details}{section*.3535}{}}
\@writefile{toc}{\contentsline {paragraph}{Network design}{1597}{section*.3536}\protected@file@percent }
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{kingma2017_adam}
\abx@aux@segm{0}{0}{kingma2017_adam}
\@writefile{toc}{\contentsline {paragraph}{Positional encoding}{1598}{section*.3537}\protected@file@percent }
\abx@aux@backref{2435}{mildenhall2020_nerf}{0}{1598}{1598}
\@writefile{toc}{\contentsline {paragraph}{Canonical reference frame}{1598}{section*.3538}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Curriculum strategy}{1598}{section*.3539}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Optimization details}{1598}{section*.3540}\protected@file@percent }
\abx@aux@backref{2436}{kingma2017_adam}{0}{1598}{1598}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{pumarola2021_dnerf}
\abx@aux@segm{0}{0}{pumarola2021_dnerf}
\abx@aux@cite{0}{pumarola2021_dnerf}
\abx@aux@segm{0}{0}{pumarola2021_dnerf}
\@writefile{toc}{\contentsline {subsubsection}{Experiments and Ablations}{1599}{section*.3541}\protected@file@percent }
\newlabel{subsec:chapter23_dnerf_experiments}{{23.12.3}{1599}{Experiments and Ablations}{section*.3541}{}}
\abx@aux@backref{2437}{mildenhall2020_nerf}{0}{1599}{1599}
\@writefile{toc}{\contentsline {paragraph}{Learned canonical scene and displacement fields}{1599}{section*.3542}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.135}{\ignorespaces \textbf  {Visualization of the learned canonical scene.} A dynamic scene at time $t$ is mapped into a canonical configuration via the learned displacement field $\Delta \mathbf  {x}$. From left to right: rendered radiance, density mesh, depth map, and color-coded correspondences between canonical and deformed meshes. Consistent colors indicate correct alignment across time~\blx@tocontentsinit {0}\cite {pumarola2021_dnerf}.}}{1599}{figure.caption.3543}\protected@file@percent }
\abx@aux@backref{2439}{pumarola2021_dnerf}{0}{1599}{1599}
\newlabel{fig:chapter23_dnerf_learnedscene}{{23.135}{1599}{\textbf {Visualization of the learned canonical scene.} A dynamic scene at time $t$ is mapped into a canonical configuration via the learned displacement field $\Delta \mathbf {x}$. From left to right: rendered radiance, density mesh, depth map, and color-coded correspondences between canonical and deformed meshes. Consistent colors indicate correct alignment across time~\cite {pumarola2021_dnerf}}{figure.caption.3543}{}}
\abx@aux@cite{0}{pumarola2021_dnerf}
\abx@aux@segm{0}{0}{pumarola2021_dnerf}
\abx@aux@cite{0}{pumarola2021_dnerf}
\abx@aux@segm{0}{0}{pumarola2021_dnerf}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{pumarola2021_dnerf}
\abx@aux@segm{0}{0}{pumarola2021_dnerf}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{pumarola2021_dnerf}
\abx@aux@segm{0}{0}{pumarola2021_dnerf}
\abx@aux@cite{0}{pumarola2021_dnerf}
\abx@aux@segm{0}{0}{pumarola2021_dnerf}
\abx@aux@cite{0}{pumarola2021_dnerf}
\abx@aux@segm{0}{0}{pumarola2021_dnerf}
\@writefile{toc}{\contentsline {paragraph}{Shading and appearance consistency}{1600}{section*.3544}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.136}{\ignorespaces \textbf  {Analyzing shading effects.} Correspondences between canonical space and observed scenes at $t=0.5$ and $t=1$ for three balls of different materials. Shading changes, such as floor shadows, are synthesized by warping the canonical configuration, preserving temporal coherence~\blx@tocontentsinit {0}\cite {pumarola2021_dnerf}.}}{1600}{figure.caption.3545}\protected@file@percent }
\abx@aux@backref{2441}{pumarola2021_dnerf}{0}{1600}{1600}
\newlabel{fig:chapter23_dnerf_shading}{{23.136}{1600}{\textbf {Analyzing shading effects.} Correspondences between canonical space and observed scenes at $t=0.5$ and $t=1$ for three balls of different materials. Shading changes, such as floor shadows, are synthesized by warping the canonical configuration, preserving temporal coherence~\cite {pumarola2021_dnerf}}{figure.caption.3545}{}}
\@writefile{toc}{\contentsline {paragraph}{Quantitative and qualitative comparisons}{1600}{section*.3546}\protected@file@percent }
\abx@aux@backref{2442}{mildenhall2020_nerf}{0}{1600}{1600}
\abx@aux@backref{2443}{mildenhall2020_nerf}{0}{1600}{1600}
\abx@aux@backref{2444}{pumarola2021_dnerf}{0}{1600}{1600}
\abx@aux@backref{2445}{mildenhall2020_nerf}{0}{1600}{1600}
\abx@aux@backref{2446}{pumarola2021_dnerf}{0}{1600}{1600}
\@writefile{lot}{\contentsline {table}{\numberline {23.35}{\ignorespaces \textbf  {Quantitative comparison} MSE/LPIPS (lower is better) and PSNR/SSIM (higher is better) across eight dynamic scenes.}}{1600}{table.caption.3547}\protected@file@percent }
\newlabel{tab:dnerf_quant_full}{{23.35}{1600}{\textbf {Quantitative comparison} MSE/LPIPS (lower is better) and PSNR/SSIM (higher is better) across eight dynamic scenes}{table.caption.3547}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23.137}{\ignorespaces \textbf  {Qualitative comparisons.} Novel view synthesis at arbitrary time instants for dynamic scenes. Close-ups show ground truth, NeRF, T-NeRF, and D-NeRF. NeRF fails to represent motion, T-NeRF captures dynamics but loses high-frequency detail, while D-NeRF produces sharp reconstructions~\blx@tocontentsinit {0}\cite {pumarola2021_dnerf}.}}{1601}{figure.caption.3548}\protected@file@percent }
\abx@aux@backref{2448}{pumarola2021_dnerf}{0}{1601}{1601}
\newlabel{fig:chapter23_dnerf_qualitative}{{23.137}{1601}{\textbf {Qualitative comparisons.} Novel view synthesis at arbitrary time instants for dynamic scenes. Close-ups show ground truth, NeRF, T-NeRF, and D-NeRF. NeRF fails to represent motion, T-NeRF captures dynamics but loses high-frequency detail, while D-NeRF produces sharp reconstructions~\cite {pumarola2021_dnerf}}{figure.caption.3548}{}}
\abx@aux@cite{0}{pumarola2021_dnerf}
\abx@aux@segm{0}{0}{pumarola2021_dnerf}
\abx@aux@cite{0}{pumarola2021_dnerf}
\abx@aux@segm{0}{0}{pumarola2021_dnerf}
\@writefile{toc}{\contentsline {paragraph}{Time and view conditioning}{1602}{section*.3549}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.138}{\ignorespaces \textbf  {Time and view conditioning.} Novel renderings from two unseen viewpoints across time. Scenes include articulated motion (Tractor), human motion (Jumping Jacks, Warrior), and asynchronous dynamics (Bouncing Balls). Canonical spaces (first column) serve as anchors for consistent geometry~\blx@tocontentsinit {0}\cite {pumarola2021_dnerf}.}}{1602}{figure.caption.3550}\protected@file@percent }
\abx@aux@backref{2450}{pumarola2021_dnerf}{0}{1602}{1602}
\newlabel{fig:chapter23_dnerf_timeview}{{23.138}{1602}{\textbf {Time and view conditioning.} Novel renderings from two unseen viewpoints across time. Scenes include articulated motion (Tractor), human motion (Jumping Jacks, Warrior), and asynchronous dynamics (Bouncing Balls). Canonical spaces (first column) serve as anchors for consistent geometry~\cite {pumarola2021_dnerf}}{figure.caption.3550}{}}
\@writefile{toc}{\contentsline {paragraph}{Limitations}{1603}{section*.3551}\protected@file@percent }
\newlabel{par:chapter23_dnerf_limitations}{{23.12.3}{1603}{Limitations}{section*.3551}{}}
\@writefile{toc}{\contentsline {paragraph}{Future directions}{1603}{section*.3552}\protected@file@percent }
\newlabel{par:chapter23_dnerf_future}{{23.12.3}{1603}{Future directions}{section*.3552}{}}
\BKM@entry{id=923,dest={73656374696F6E2A2E33353533},srcline={8613}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030335C3030302E5C303030315C303030325C3030302E5C303030345C3030303A5C3030305C3034305C3030304E5C303030655C303030725C303030665C303030695C303030655C303030735C3030303A5C3030305C3034305C303030445C303030655C303030665C3030306F5C303030725C3030306D5C303030615C303030625C3030306C5C303030655C3030305C3034305C3030304E5C303030655C303030755C303030725C303030615C3030306C5C3030305C3034305C303030525C303030615C303030645C303030695C303030615C3030306E5C303030635C303030655C3030305C3034305C303030465C303030695C303030655C3030306C5C303030645C30303073}
\abx@aux@cite{0}{pumarola2021_dnerf}
\abx@aux@segm{0}{0}{pumarola2021_dnerf}
\abx@aux@cite{0}{park2021_nerfies}
\abx@aux@segm{0}{0}{park2021_nerfies}
\abx@aux@cite{0}{park2021_nerfies}
\abx@aux@segm{0}{0}{park2021_nerfies}
\abx@aux@cite{0}{park2021_nerfies}
\abx@aux@segm{0}{0}{park2021_nerfies}
\@writefile{toc}{\contentsline {subsection}{Enrichment 23.12.4: Nerfies: Deformable Neural Radiance Fields}{1604}{section*.3553}\protected@file@percent }
\newlabel{enr:subsec_chapter23_nerfies}{{23.12.4}{1604}{\color {ocre}Enrichment \thesubsection : Nerfies: Deformable Neural Radiance Fields}{section*.3553}{}}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{1604}{section*.3554}\protected@file@percent }
\newlabel{subsec:chapter23_nerfies_motivation}{{23.12.4}{1604}{Motivation}{section*.3554}{}}
\abx@aux@backref{2451}{pumarola2021_dnerf}{0}{1604}{1604}
\abx@aux@backref{2452}{park2021_nerfies}{0}{1604}{1604}
\@writefile{lof}{\contentsline {figure}{\numberline {23.139}{\ignorespaces \textbf  {Results from Nerfies.} Photo-realistic reconstructions from handheld mobile captures: casual waving sequences (a) and selfie photos/videos (b) are turned into free-viewpoint renderings (c) with accurate geometry (d). Source:~\blx@tocontentsinit {0}\cite {park2021_nerfies}.}}{1604}{figure.caption.3555}\protected@file@percent }
\abx@aux@backref{2454}{park2021_nerfies}{0}{1604}{1604}
\newlabel{fig:chapter23_nerfies_results}{{23.139}{1604}{\textbf {Results from Nerfies.} Photo-realistic reconstructions from handheld mobile captures: casual waving sequences (a) and selfie photos/videos (b) are turned into free-viewpoint renderings (c) with accurate geometry (d). Source:~\cite {park2021_nerfies}}{figure.caption.3555}{}}
\@writefile{toc}{\contentsline {subsubsection}{Method}{1604}{section*.3556}\protected@file@percent }
\newlabel{subsec:chapter23_nerfies_method}{{23.12.4}{1604}{Method}{section*.3556}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation relative to D-NeRF}{1604}{section*.3557}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Canonical radiance field}{1605}{section*.3558}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Observation-to-canonical deformation}{1605}{section*.3559}\protected@file@percent }
\abx@aux@cite{0}{park2021_nerfies}
\abx@aux@segm{0}{0}{park2021_nerfies}
\abx@aux@cite{0}{park2021_nerfies}
\abx@aux@segm{0}{0}{park2021_nerfies}
\abx@aux@cite{0}{park2021_nerfies}
\abx@aux@segm{0}{0}{park2021_nerfies}
\abx@aux@cite{0}{park2021_nerfies}
\abx@aux@segm{0}{0}{park2021_nerfies}
\@writefile{lof}{\contentsline {figure}{\numberline {23.140}{\ignorespaces \textbf  {Architecture overview.} Each image has a deformation code \(\omega \) and an appearance code \(\psi \). Samples are traced in the observation frame, mapped to the canonical frame by a deformation field (an MLP conditioned on \(\omega \)), then the canonical NeRF is queried and integrated. Source:~\blx@tocontentsinit {0}\cite {park2021_nerfies}.}}{1606}{figure.caption.3560}\protected@file@percent }
\abx@aux@backref{2456}{park2021_nerfies}{0}{1606}{1606}
\newlabel{fig:chapter23_nerfies_idea}{{23.140}{1606}{\textbf {Architecture overview.} Each image has a deformation code \(\omega \) and an appearance code \(\psi \). Samples are traced in the observation frame, mapped to the canonical frame by a deformation field (an MLP conditioned on \(\omega \)), then the canonical NeRF is queried and integrated. Source:~\cite {park2021_nerfies}}{figure.caption.3560}{}}
\@writefile{toc}{\contentsline {paragraph}{Why dense \(\mathrm  {SE}(3)\) fields}{1606}{section*.3561}\protected@file@percent }
\abx@aux@cite{0}{park2021_nerfies}
\abx@aux@segm{0}{0}{park2021_nerfies}
\abx@aux@cite{0}{park2021_nerfies}
\abx@aux@segm{0}{0}{park2021_nerfies}
\@writefile{lof}{\contentsline {figure}{\numberline {23.141}{\ignorespaces \textbf  {SE(3) vs.\ translation fields.} To represent a simple rigid rotation of the star, a translation field must assign \emph  {different} displacement vectors depending on location: a faraway point requires a long translation $t_1$, while a point closer to the center requires a shorter one $t_2$. This spatially varying pattern complicates learning, since the network must coordinate many different magnitudes and directions just to encode one global rotation. In contrast, an $\mathrm  {SE}(3)$ field expresses the same motion with a single rotation angle $\theta $ applied consistently across space. The network only needs to learn one compact parameterization of the rigid transform, making optimization easier and the resulting deformations more coherent. Source:~\blx@tocontentsinit {0}\cite {park2021_nerfies}.}}{1607}{figure.caption.3562}\protected@file@percent }
\abx@aux@backref{2458}{park2021_nerfies}{0}{1607}{1607}
\newlabel{fig:chapter23_nerfies_se3_vs_trans}{{23.141}{1607}{\textbf {SE(3) vs.\ translation fields.} To represent a simple rigid rotation of the star, a translation field must assign \emph {different} displacement vectors depending on location: a faraway point requires a long translation $t_1$, while a point closer to the center requires a shorter one $t_2$. This spatially varying pattern complicates learning, since the network must coordinate many different magnitudes and directions just to encode one global rotation. In contrast, an $\mathrm {SE}(3)$ field expresses the same motion with a single rotation angle $\theta $ applied consistently across space. The network only needs to learn one compact parameterization of the rigid transform, making optimization easier and the resulting deformations more coherent. Source:~\cite {park2021_nerfies}}{figure.caption.3562}{}}
\@writefile{toc}{\contentsline {paragraph}{Observation vs.\ canonical frames}{1607}{section*.3563}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.142}{\ignorespaces \textbf  {Observation vs.\ canonical frames.} The \emph  {observation frame} (left) shows the raw geometry as captured in a specific image, here with the head turned and displaced. The \emph  {canonical frame} (right) shows the learned static template in a standardized pose. Insets highlight displacements (sideways or front–back shifts) that the learned deformation field applies to map observed points into the canonical configuration. Source:~\blx@tocontentsinit {0}\cite {park2021_nerfies}.}}{1607}{figure.caption.3564}\protected@file@percent }
\abx@aux@backref{2460}{park2021_nerfies}{0}{1607}{1607}
\newlabel{fig:chapter23_nerfies_obs_canon}{{23.142}{1607}{\textbf {Observation vs.\ canonical frames.} The \emph {observation frame} (left) shows the raw geometry as captured in a specific image, here with the head turned and displaced. The \emph {canonical frame} (right) shows the learned static template in a standardized pose. Insets highlight displacements (sideways or front–back shifts) that the learned deformation field applies to map observed points into the canonical configuration. Source:~\cite {park2021_nerfies}}{figure.caption.3564}{}}
\abx@aux@cite{0}{park2021_nerfies}
\abx@aux@segm{0}{0}{park2021_nerfies}
\abx@aux@cite{0}{park2021_nerfies}
\abx@aux@segm{0}{0}{park2021_nerfies}
\@writefile{toc}{\contentsline {paragraph}{Elastic regularization (why, what, how)}{1608}{section*.3565}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.143}{\ignorespaces \textbf  {Elastic regularization effect.} Under-constrained captures (few, biased views) are prone to distortion; the elastic prior substantially reduces such artifacts. Source:~\blx@tocontentsinit {0}\cite {park2021_nerfies}.}}{1608}{figure.caption.3566}\protected@file@percent }
\abx@aux@backref{2462}{park2021_nerfies}{0}{1608}{1608}
\newlabel{fig:chapter23_nerfies_elastic}{{23.143}{1608}{\textbf {Elastic regularization effect.} Under-constrained captures (few, biased views) are prone to distortion; the elastic prior substantially reduces such artifacts. Source:~\cite {park2021_nerfies}}{figure.caption.3566}{}}
\@writefile{toc}{\contentsline {paragraph}{Background regularization}{1608}{section*.3567}\protected@file@percent }
\abx@aux@cite{0}{park2021_nerfies}
\abx@aux@segm{0}{0}{park2021_nerfies}
\abx@aux@cite{0}{park2021_nerfies}
\abx@aux@segm{0}{0}{park2021_nerfies}
\@writefile{toc}{\contentsline {paragraph}{Coarse-to-fine optimization (why, what, how)}{1609}{section*.3568}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.144}{\ignorespaces \textbf  {Effect of coarse-to-fine optimization.} Comparison of three training strategies for dynamic scenes (\emph  {head turn}, \emph  {smile}). \emph  {gt:} Ground-truth reference frame. \emph  {$m{=}4$:} Training with only a few low-frequency positional-encoding bands produces overly smooth results—large motions are captured but fine details (e.g., cheek deformation in a smile) are blurred. \emph  {$m{=}8$:} Allowing all frequency bands from the start destabilizes training: the network overfits local details before learning the global motion, leading to severe artifacts (e.g., head turn collapse). \emph  {c2f:} The proposed coarse-to-fine schedule gradually introduces higher frequencies. This curriculum lets the model first align global motion and later refine fine-scale details, yielding sharp and accurate reconstructions closely matching the ground truth. Source:~\blx@tocontentsinit {0}\cite {park2021_nerfies}.}}{1609}{figure.caption.3569}\protected@file@percent }
\abx@aux@backref{2464}{park2021_nerfies}{0}{1609}{1609}
\newlabel{fig:chapter23_nerfies_c2f}{{23.144}{1609}{\textbf {Effect of coarse-to-fine optimization.} Comparison of three training strategies for dynamic scenes (\emph {head turn}, \emph {smile}). \emph {gt:} Ground-truth reference frame. \emph {$m{=}4$:} Training with only a few low-frequency positional-encoding bands produces overly smooth results—large motions are captured but fine details (e.g., cheek deformation in a smile) are blurred. \emph {$m{=}8$:} Allowing all frequency bands from the start destabilizes training: the network overfits local details before learning the global motion, leading to severe artifacts (e.g., head turn collapse). \emph {c2f:} The proposed coarse-to-fine schedule gradually introduces higher frequencies. This curriculum lets the model first align global motion and later refine fine-scale details, yielding sharp and accurate reconstructions closely matching the ground truth. Source:~\cite {park2021_nerfies}}{figure.caption.3569}{}}
\abx@aux@cite{0}{park2021_nerfies}
\abx@aux@segm{0}{0}{park2021_nerfies}
\abx@aux@cite{0}{park2021_nerfies}
\abx@aux@segm{0}{0}{park2021_nerfies}
\@writefile{toc}{\contentsline {paragraph}{Latent-code interpolation}{1610}{section*.3570}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.145}{\ignorespaces \textbf  {Latent-code interpolation from a novel viewpoint.} Start/end frames (cyan/pink borders) from \emph  {BADMINTON} define two observed states. The middle columns are \emph  {synthetic} frames rendered from a \emph  {novel camera} by linearly interpolating the corresponding deformation codes \(\omega \) and evaluating \(F(T(\cdot ,\omega (\alpha )),\!\cdot ,\psi )\). Top row: RGB; bottom row: depth. The racquet sweeps smoothly and depth varies coherently, illustrating that per-image deformation latents form a state space that supports continuous interpolation. Source:~\blx@tocontentsinit {0}\cite {park2021_nerfies}.}}{1610}{figure.caption.3571}\protected@file@percent }
\abx@aux@backref{2466}{park2021_nerfies}{0}{1610}{1610}
\newlabel{fig:chapter23_nerfies_latent}{{23.145}{1610}{\textbf {Latent-code interpolation from a novel viewpoint.} Start/end frames (cyan/pink borders) from \emph {BADMINTON} define two observed states. The middle columns are \emph {synthetic} frames rendered from a \emph {novel camera} by linearly interpolating the corresponding deformation codes \(\omega \) and evaluating \(F(T(\cdot ,\omega (\alpha )),\!\cdot ,\psi )\). Top row: RGB; bottom row: depth. The racquet sweeps smoothly and depth varies coherently, illustrating that per-image deformation latents form a state space that supports continuous interpolation. Source:~\cite {park2021_nerfies}}{figure.caption.3571}{}}
\@writefile{toc}{\contentsline {subsubsection}{Architecture and implementation details}{1610}{section*.3572}\protected@file@percent }
\newlabel{subsec:chapter23_nerfies_architecture}{{23.12.4}{1610}{Architecture and implementation details}{section*.3572}{}}
\abx@aux@cite{0}{park2021_nerfies}
\abx@aux@segm{0}{0}{park2021_nerfies}
\abx@aux@cite{0}{park2021_nerfies}
\abx@aux@segm{0}{0}{park2021_nerfies}
\abx@aux@cite{0}{park2021_nerfies}
\abx@aux@segm{0}{0}{park2021_nerfies}
\abx@aux@cite{0}{park2021_nerfies}
\abx@aux@segm{0}{0}{park2021_nerfies}
\abx@aux@cite{0}{park2021_nerfies}
\abx@aux@segm{0}{0}{park2021_nerfies}
\abx@aux@cite{0}{park2021_nerfies}
\abx@aux@segm{0}{0}{park2021_nerfies}
\@writefile{toc}{\contentsline {subsubsection}{Experiments and Ablations}{1611}{section*.3573}\protected@file@percent }
\newlabel{subsec:chapter23_nerfies_experiments}{{23.12.4}{1611}{Experiments and Ablations}{section*.3573}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23.146}{\ignorespaces \textbf  {Thin hair strands.} Adjusting the far plane allows rendering against a flat background, highlighting fine geometry. Source:~\blx@tocontentsinit {0}\cite {park2021_nerfies}.}}{1611}{figure.caption.3574}\protected@file@percent }
\abx@aux@backref{2468}{park2021_nerfies}{0}{1611}{1611}
\newlabel{fig:chapter23_nerfies_thin_hair}{{23.146}{1611}{\textbf {Thin hair strands.} Adjusting the far plane allows rendering against a flat background, highlighting fine geometry. Source:~\cite {park2021_nerfies}}{figure.caption.3574}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23.147}{\ignorespaces \textbf  {Full-body reconstructions.} High-quality details such as fabric wrinkles and eyeglasses are captured from casual recordings. Source:~\blx@tocontentsinit {0}\cite {park2021_nerfies}.}}{1611}{figure.caption.3575}\protected@file@percent }
\abx@aux@backref{2470}{park2021_nerfies}{0}{1611}{1611}
\newlabel{fig:chapter23_nerfies_full_body}{{23.147}{1611}{\textbf {Full-body reconstructions.} High-quality details such as fabric wrinkles and eyeglasses are captured from casual recordings. Source:~\cite {park2021_nerfies}}{figure.caption.3575}{}}
\abx@aux@cite{0}{park2021_nerfies}
\abx@aux@segm{0}{0}{park2021_nerfies}
\abx@aux@cite{0}{park2021_nerfies}
\abx@aux@segm{0}{0}{park2021_nerfies}
\@writefile{lof}{\contentsline {figure}{\numberline {23.148}{\ignorespaces \textbf  {Dynamic scenes comparison.} Side-by-side baselines with PSNR/LPIPS (best highlighted in red) illustrate that numerical gains do not always reflect perceptual quality. Source:~\blx@tocontentsinit {0}\cite {park2021_nerfies}.}}{1612}{figure.caption.3576}\protected@file@percent }
\abx@aux@backref{2472}{park2021_nerfies}{0}{1612}{1612}
\newlabel{fig:chapter23_nerfies_dynamic_comparisons}{{23.148}{1612}{\textbf {Dynamic scenes comparison.} Side-by-side baselines with PSNR/LPIPS (best highlighted in red) illustrate that numerical gains do not always reflect perceptual quality. Source:~\cite {park2021_nerfies}}{figure.caption.3576}{}}
\abx@aux@cite{0}{park2021_nerfies}
\abx@aux@segm{0}{0}{park2021_nerfies}
\abx@aux@cite{0}{park2021_nerfies}
\abx@aux@segm{0}{0}{park2021_nerfies}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{lombardi2019_neuralvolumes}
\abx@aux@segm{0}{0}{lombardi2019_neuralvolumes}
\abx@aux@cite{0}{li2021_neural}
\abx@aux@segm{0}{0}{li2021_neural}
\abx@aux@cite{0}{pumarola2021_dnerf}
\abx@aux@segm{0}{0}{pumarola2021_dnerf}
\@writefile{lof}{\contentsline {figure}{\numberline {23.149}{\ignorespaces \textbf  {Quasi-static scenes comparison.} Similar trends appear on mostly static captures; perceptual quality correlates imperfectly with PSNR. Source:~\blx@tocontentsinit {0}\cite {park2021_nerfies}.}}{1613}{figure.caption.3577}\protected@file@percent }
\abx@aux@backref{2474}{park2021_nerfies}{0}{1613}{1613}
\newlabel{fig:chapter23_nerfies_quasistatic_comparisons}{{23.149}{1613}{\textbf {Quasi-static scenes comparison.} Similar trends appear on mostly static captures; perceptual quality correlates imperfectly with PSNR. Source:~\cite {park2021_nerfies}}{figure.caption.3577}{}}
\abx@aux@cite{0}{park2021_nerfies}
\abx@aux@segm{0}{0}{park2021_nerfies}
\abx@aux@cite{0}{park2021_nerfies}
\abx@aux@segm{0}{0}{park2021_nerfies}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{lombardi2019_neuralvolumes}
\abx@aux@segm{0}{0}{lombardi2019_neuralvolumes}
\abx@aux@cite{0}{li2021_neural}
\abx@aux@segm{0}{0}{li2021_neural}
\abx@aux@cite{0}{pumarola2021_dnerf}
\abx@aux@segm{0}{0}{pumarola2021_dnerf}
\abx@aux@cite{0}{park2021_nerfies}
\abx@aux@segm{0}{0}{park2021_nerfies}
\abx@aux@cite{0}{park2021_nerfies}
\abx@aux@segm{0}{0}{park2021_nerfies}
\@writefile{lot}{\contentsline {table}{\numberline {23.36}{\ignorespaces \textbf  {Quasi-static captures} from~\blx@tocontentsinit {0}\cite {park2021_nerfies}. Each entry is \emph  {PSNR/LPIPS} (dB/unitless). \textbf  {Bold} marks the best value per column for each metric (higher PSNR, lower LPIPS).}}{1614}{table.caption.3578}\protected@file@percent }
\abx@aux@backref{2476}{park2021_nerfies}{0}{1614}{1614}
\newlabel{tab:nerfies_quasi_static_clean}{{23.36}{1614}{\textbf {Quasi-static captures} from~\cite {park2021_nerfies}. Each entry is \emph {PSNR/LPIPS} (dB/unitless). \textbf {Bold} marks the best value per column for each metric (higher PSNR, lower LPIPS)}{table.caption.3578}{}}
\abx@aux@backref{2477}{mildenhall2020_nerf}{0}{1614}{1614}
\abx@aux@backref{2478}{lombardi2019_neuralvolumes}{0}{1614}{1614}
\abx@aux@backref{2479}{li2021_neural}{0}{1614}{1614}
\abx@aux@backref{2480}{pumarola2021_dnerf}{0}{1614}{1614}
\@writefile{lot}{\contentsline {table}{\numberline {23.37}{\ignorespaces \textbf  {Dynamic captures} from~\blx@tocontentsinit {0}\cite {park2021_nerfies}. Each entry is \emph  {PSNR/LPIPS} (dB/unitless). \textbf  {Bold} marks the best value per column for each metric (higher PSNR, lower LPIPS).}}{1614}{table.caption.3579}\protected@file@percent }
\abx@aux@backref{2482}{park2021_nerfies}{0}{1614}{1614}
\newlabel{tab:nerfies_dynamic_clean}{{23.37}{1614}{\textbf {Dynamic captures} from~\cite {park2021_nerfies}. Each entry is \emph {PSNR/LPIPS} (dB/unitless). \textbf {Bold} marks the best value per column for each metric (higher PSNR, lower LPIPS)}{table.caption.3579}{}}
\abx@aux@backref{2483}{mildenhall2020_nerf}{0}{1614}{1614}
\abx@aux@backref{2484}{lombardi2019_neuralvolumes}{0}{1614}{1614}
\abx@aux@backref{2485}{li2021_neural}{0}{1614}{1614}
\abx@aux@backref{2486}{pumarola2021_dnerf}{0}{1614}{1614}
\@writefile{toc}{\contentsline {subsubsection}{Limitations and Future Work}{1614}{section*.3580}\protected@file@percent }
\newlabel{subsec:chapter23_nerfies_limitations}{{23.12.4}{1614}{Limitations and Future Work}{section*.3580}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23.150}{\ignorespaces \textbf  {Topological limitations.} Color renderings may remain plausible while geometry degrades under topology changes or rapid motion. Source:~\blx@tocontentsinit {0}\cite {park2021_nerfies}.}}{1614}{figure.caption.3581}\protected@file@percent }
\abx@aux@backref{2488}{park2021_nerfies}{0}{1614}{1614}
\newlabel{fig:chapter23_nerfies_limits}{{23.150}{1614}{\textbf {Topological limitations.} Color renderings may remain plausible while geometry degrades under topology changes or rapid motion. Source:~\cite {park2021_nerfies}}{figure.caption.3581}{}}
\BKM@entry{id=924,dest={73656374696F6E2A2E33353832},srcline={8956}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030335C3030302E5C303030315C303030335C3030303A5C3030305C3034305C3030304E5C303030655C303030525C303030465C3030303A5C3030305C3034305C303030455C303030645C303030695C303030745C303030695C3030306E5C303030675C3030302C5C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030725C3030306F5C3030306C5C3030306C5C303030615C303030625C303030695C3030306C5C303030695C303030745C303030795C3030305C3034305C3030305C3034365C3030305C3034305C303030535C303030655C3030306D5C303030615C3030306E5C303030745C303030695C303030635C3030305C3034305C3030304D5C303030615C3030306E5C303030695C303030705C303030755C3030306C5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{kerr2023_lerf}
\abx@aux@segm{0}{0}{kerr2023_lerf}
\abx@aux@cite{0}{haque2023_instructnerf2nerf}
\abx@aux@segm{0}{0}{haque2023_instructnerf2nerf}
\abx@aux@cite{0}{gao2022_nerfdiff}
\abx@aux@segm{0}{0}{gao2022_nerfdiff}
\BKM@entry{id=925,dest={73656374696F6E2A2E33353833},srcline={8966}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030335C3030302E5C303030315C303030335C3030302E5C303030315C3030303A5C3030305C3034305C3030304C5C303030615C3030306E5C303030675C303030755C303030615C303030675C303030655C3030305C3034305C303030455C3030306D5C303030625C303030655C303030645C303030645C303030655C303030645C3030305C3034305C303030525C303030615C303030645C303030695C303030615C3030306E5C303030635C303030655C3030305C3034305C303030465C303030695C303030655C3030306C5C303030645C303030735C3030305C3034305C3030305C3035305C3030304C5C303030455C303030525C303030465C3030305C303531}
\abx@aux@cite{0}{mildenhall2020_nerf}
\abx@aux@segm{0}{0}{mildenhall2020_nerf}
\abx@aux@cite{0}{li2022_lseg}
\abx@aux@segm{0}{0}{li2022_lseg}
\abx@aux@cite{0}{minderer2022_owlvit}
\abx@aux@segm{0}{0}{minderer2022_owlvit}
\abx@aux@cite{0}{kerr2023_lerf}
\abx@aux@segm{0}{0}{kerr2023_lerf}
\abx@aux@cite{0}{kerr2023_lerf}
\abx@aux@segm{0}{0}{kerr2023_lerf}
\abx@aux@cite{0}{kerr2023_lerf}
\abx@aux@segm{0}{0}{kerr2023_lerf}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\@writefile{toc}{\contentsline {section}{Enrichment 23.13: NeRF: Editing, Controllability \& Semantic Manipulation}{1615}{section*.3582}\protected@file@percent }
\abx@aux@backref{2489}{kerr2023_lerf}{0}{1615}{1615}
\abx@aux@backref{2490}{haque2023_instructnerf2nerf}{0}{1615}{1615}
\abx@aux@backref{2491}{gao2022_nerfdiff}{0}{1615}{1615}
\@writefile{toc}{\contentsline {subsection}{Enrichment 23.13.1: Language Embedded Radiance Fields (LERF)}{1615}{section*.3583}\protected@file@percent }
\newlabel{enr:subsec_chapter23_lerf}{{23.13.1}{1615}{\color {ocre}Enrichment \thesubsection : Language Embedded Radiance Fields (LERF)}{section*.3583}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation}{1615}{section*.3584}\protected@file@percent }
\abx@aux@backref{2492}{mildenhall2020_nerf}{0}{1615}{1615}
\abx@aux@backref{2493}{li2022_lseg}{0}{1615}{1615}
\abx@aux@backref{2494}{minderer2022_owlvit}{0}{1615}{1615}
\@writefile{lof}{\contentsline {figure}{\numberline {23.151}{\ignorespaces \textbf  {Language Embedded Radiance Fields (LERF).} CLIP representations are distilled into a dense, multi-scale 3D field that can be reconstructed from a hand-held phone capture in under 45 minutes. Once trained, LERF supports real-time natural-language queries, producing relevancy maps for prompts ranging from abstract concepts to fine-grained attributes and even scene text. Source:~\blx@tocontentsinit {0}\cite {kerr2023_lerf}.}}{1615}{figure.caption.3585}\protected@file@percent }
\abx@aux@backref{2496}{kerr2023_lerf}{0}{1615}{1615}
\newlabel{fig:chapter23_lerf_mainidea}{{23.151}{1615}{\textbf {Language Embedded Radiance Fields (LERF).} CLIP representations are distilled into a dense, multi-scale 3D field that can be reconstructed from a hand-held phone capture in under 45 minutes. Once trained, LERF supports real-time natural-language queries, producing relevancy maps for prompts ranging from abstract concepts to fine-grained attributes and even scene text. Source:~\cite {kerr2023_lerf}}{figure.caption.3585}{}}
\abx@aux@cite{0}{kerr2023_lerf}
\abx@aux@segm{0}{0}{kerr2023_lerf}
\@writefile{toc}{\contentsline {paragraph}{High-level overview}{1616}{section*.3586}\protected@file@percent }
\abx@aux@backref{2497}{kerr2023_lerf}{0}{1616}{1616}
\abx@aux@backref{2498}{radford2021_clip}{0}{1616}{1616}
\@writefile{toc}{\contentsline {paragraph}{How it works at a glance}{1616}{section*.3587}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why this suits open-vocabulary 3D queries}{1616}{section*.3588}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Method}{1617}{section*.3589}\protected@file@percent }
\newlabel{subsec:chapter23_lerf_method}{{23.13.1}{1617}{Method}{section*.3589}{}}
\abx@aux@backref{2499}{kerr2023_lerf}{0}{1617}{1617}
\@writefile{toc}{\contentsline {paragraph}{Language field definition}{1617}{section*.3590}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Supervision via CLIP pyramid}{1617}{section*.3591}\protected@file@percent }
\abx@aux@cite{0}{kerr2023_lerf}
\abx@aux@segm{0}{0}{kerr2023_lerf}
\abx@aux@cite{0}{kerr2023_lerf}
\abx@aux@segm{0}{0}{kerr2023_lerf}
\@writefile{toc}{\contentsline {paragraph}{Volumetric language rendering}{1619}{section*.3592}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.152}{\ignorespaces \textbf  {LERF optimization} Left: a language field over 3D volumes $(\mathbf  {x},s)$ is sampled along rays and aggregated with NeRF transmittance weights. Right: supervision comes from a precomputed multi-scale CLIP pyramid; features are interpolated at the projected location and scale. Source:~\blx@tocontentsinit {0}\cite {kerr2023_lerf}.}}{1619}{figure.caption.3593}\protected@file@percent }
\abx@aux@backref{2501}{kerr2023_lerf}{0}{1619}{1619}
\newlabel{fig:chapter23_lerf_optimization}{{23.152}{1619}{\textbf {LERF optimization} Left: a language field over 3D volumes $(\mathbf {x},s)$ is sampled along rays and aggregated with NeRF transmittance weights. Right: supervision comes from a precomputed multi-scale CLIP pyramid; features are interpolated at the projected location and scale. Source:~\cite {kerr2023_lerf}}{figure.caption.3593}{}}
\@writefile{toc}{\contentsline {paragraph}{Regularization with DINO}{1619}{section*.3594}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Inference: scale selection and heatmap rendering}{1620}{section*.3595}\protected@file@percent }
\abx@aux@cite{0}{kerr2023_lerf}
\abx@aux@segm{0}{0}{kerr2023_lerf}
\abx@aux@cite{0}{kerr2023_lerf}
\abx@aux@segm{0}{0}{kerr2023_lerf}
\abx@aux@cite{0}{kerr2023_lerf}
\abx@aux@segm{0}{0}{kerr2023_lerf}
\@writefile{toc}{\contentsline {subsubsection}{Results and Ablations}{1623}{section*.3596}\protected@file@percent }
\newlabel{subsec:chapter23_lerf_results}{{23.13.1}{1623}{Results and Ablations}{section*.3596}{}}
\@writefile{toc}{\contentsline {paragraph}{Qualitative results}{1623}{section*.3597}\protected@file@percent }
\abx@aux@backref{2502}{kerr2023_lerf}{0}{1623}{1623}
\@writefile{lof}{\contentsline {figure}{\numberline {23.153}{\ignorespaces \textbf  {Qualitative results across diverse scenes.} LERF grounds free-form queries in 3D for in-the-wild captures (ramen, hand object, figurines, bookstore, shoe rack). Queries span categories (``cartoon''), attributes (``glass of water''), parts (``fingers''), long-tail entities (``waldo'', ``jake from adventure time''), brands (``vans''), and book titles (``the cookie bible''). Heatmaps are view-consistent and multi-scale. Source:~\blx@tocontentsinit {0}\cite {kerr2023_lerf}.}}{1623}{figure.caption.3598}\protected@file@percent }
\abx@aux@backref{2504}{kerr2023_lerf}{0}{1623}{1623}
\newlabel{fig:chapter23_lerf_results_qualitative}{{23.153}{1623}{\textbf {Qualitative results across diverse scenes.} LERF grounds free-form queries in 3D for in-the-wild captures (ramen, hand object, figurines, bookstore, shoe rack). Queries span categories (``cartoon''), attributes (``glass of water''), parts (``fingers''), long-tail entities (``waldo'', ``jake from adventure time''), brands (``vans''), and book titles (``the cookie bible''). Heatmaps are view-consistent and multi-scale. Source:~\cite {kerr2023_lerf}}{figure.caption.3598}{}}
\abx@aux@cite{0}{kerr2023_lerf}
\abx@aux@segm{0}{0}{kerr2023_lerf}
\abx@aux@cite{0}{kerr2023_lerf}
\abx@aux@segm{0}{0}{kerr2023_lerf}
\@writefile{toc}{\contentsline {paragraph}{2D CLIP vs.\ volumetric LERF}{1624}{section*.3599}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.154}{\ignorespaces \textbf  {2D CLIP interpolation vs.\ LERF.} Per-image 2D CLIP heatmaps (middle) are blob-like and inconsistent, while LERF (right) produces crisp, geometry-aligned activations for prompts such as ``lily'', ``vase'', and ``wooden spoon''. Source:~\blx@tocontentsinit {0}\cite {kerr2023_lerf}.}}{1624}{figure.caption.3600}\protected@file@percent }
\abx@aux@backref{2506}{kerr2023_lerf}{0}{1624}{1624}
\newlabel{fig:chapter23_lerf_vs_2dclip}{{23.154}{1624}{\textbf {2D CLIP interpolation vs.\ LERF.} Per-image 2D CLIP heatmaps (middle) are blob-like and inconsistent, while LERF (right) produces crisp, geometry-aligned activations for prompts such as ``lily'', ``vase'', and ``wooden spoon''. Source:~\cite {kerr2023_lerf}}{figure.caption.3600}{}}
\abx@aux@cite{0}{kerr2023_lerf}
\abx@aux@segm{0}{0}{kerr2023_lerf}
\abx@aux@cite{0}{kerr2023_lerf}
\abx@aux@segm{0}{0}{kerr2023_lerf}
\abx@aux@cite{0}{kerr2023_lerf}
\abx@aux@segm{0}{0}{kerr2023_lerf}
\abx@aux@cite{0}{kerr2023_lerf}
\abx@aux@segm{0}{0}{kerr2023_lerf}
\abx@aux@cite{0}{kerr2023_lerf}
\abx@aux@segm{0}{0}{kerr2023_lerf}
\abx@aux@cite{0}{kerr2023_lerf}
\abx@aux@segm{0}{0}{kerr2023_lerf}
\@writefile{toc}{\contentsline {paragraph}{Localization against LSeg (3D) and OWL-ViT}{1625}{section*.3601}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.155}{\ignorespaces \textbf  {Localization comparison in novel views.} LERF correctly localizes long-tail targets such as ``cookbooks'' and ``olive oil'' where LSeg(3D) and OWL-ViT often fail. Source:~\blx@tocontentsinit {0}\cite {kerr2023_lerf}.}}{1625}{figure.caption.3602}\protected@file@percent }
\abx@aux@backref{2508}{kerr2023_lerf}{0}{1625}{1625}
\newlabel{fig:chapter23_lerf_localization}{{23.155}{1625}{\textbf {Localization comparison in novel views.} LERF correctly localizes long-tail targets such as ``cookbooks'' and ``olive oil'' where LSeg(3D) and OWL-ViT often fail. Source:~\cite {kerr2023_lerf}}{figure.caption.3602}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23.156}{\ignorespaces \textbf  {Comparison to LSeg(3D).} LSeg succeeds on in-distribution labels (e.g., ``glass of water'') but struggles with out-of-distribution queries (e.g., ``egg''); LERF handles both via open-vocabulary volumetric grounding. Source:~\blx@tocontentsinit {0}\cite {kerr2023_lerf}.}}{1625}{figure.caption.3603}\protected@file@percent }
\abx@aux@backref{2510}{kerr2023_lerf}{0}{1625}{1625}
\newlabel{fig:chapter23_lerf_lseg_comp}{{23.156}{1625}{\textbf {Comparison to LSeg(3D).} LSeg succeeds on in-distribution labels (e.g., ``glass of water'') but struggles with out-of-distribution queries (e.g., ``egg''); LERF handles both via open-vocabulary volumetric grounding. Source:~\cite {kerr2023_lerf}}{figure.caption.3603}{}}
\@writefile{lot}{\contentsline {table}{\numberline {23.38}{\ignorespaces \textbf  {Localization accuracy.} Comparison between LSeg(3D), OWL-ViT, and LERF across scenes. LERF substantially improves performance on long-tail queries. Source:~\blx@tocontentsinit {0}\cite {kerr2023_lerf}.}}{1625}{table.caption.3604}\protected@file@percent }
\abx@aux@backref{2512}{kerr2023_lerf}{0}{1625}{1625}
\newlabel{tab:chapter23_lerf_localization}{{23.38}{1625}{\textbf {Localization accuracy.} Comparison between LSeg(3D), OWL-ViT, and LERF across scenes. LERF substantially improves performance on long-tail queries. Source:~\cite {kerr2023_lerf}}{table.caption.3604}{}}
\abx@aux@cite{0}{kerr2023_lerf}
\abx@aux@segm{0}{0}{kerr2023_lerf}
\abx@aux@cite{0}{kerr2023_lerf}
\abx@aux@segm{0}{0}{kerr2023_lerf}
\@writefile{toc}{\contentsline {paragraph}{3D existence: precision--recall}{1626}{section*.3605}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.157}{\ignorespaces \textbf  {Precision--recall on 3D existence queries.} LERF (orange) dominates 3D LSeg (purple) across operating points. On \emph  {COCO-like} labels (left), both methods reach high precision at low recall, but LERF sustains better recall as thresholds relax. On the \emph  {long-tail} set (right), LSeg collapses while LERF maintains a wide, high-precision regime, reflecting the advantages of volumetric rendering, multi-scale supervision, and open-vocabulary text alignment. Source:~\blx@tocontentsinit {0}\cite {kerr2023_lerf}.}}{1626}{figure.caption.3606}\protected@file@percent }
\abx@aux@backref{2514}{kerr2023_lerf}{0}{1626}{1626}
\newlabel{fig:chapter23_lerf_pr}{{23.157}{1626}{\textbf {Precision--recall on 3D existence queries.} LERF (orange) dominates 3D LSeg (purple) across operating points. On \emph {COCO-like} labels (left), both methods reach high precision at low recall, but LERF sustains better recall as thresholds relax. On the \emph {long-tail} set (right), LSeg collapses while LERF maintains a wide, high-precision regime, reflecting the advantages of volumetric rendering, multi-scale supervision, and open-vocabulary text alignment. Source:~\cite {kerr2023_lerf}}{figure.caption.3606}{}}
\abx@aux@cite{0}{kerr2023_lerf}
\abx@aux@segm{0}{0}{kerr2023_lerf}
\abx@aux@cite{0}{kerr2023_lerf}
\abx@aux@segm{0}{0}{kerr2023_lerf}
\@writefile{toc}{\contentsline {paragraph}{Ablation studies}{1627}{section*.3607}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.158}{\ignorespaces \textbf  {Ablations.} \emph  {Top:} Without DINO, activations become diffuse and off-surface, bleeding across object boundaries; with DINO, crisp object-aligned maps are recovered (e.g., hand/fingers, blue dish soap). \emph  {Bottom:} Without multi-scale supervision, both large concepts (``espresso machine'') and tiny details (``creamer pods'') are missed; full multi-scale training restores correct localization at the appropriate scale. DINO acts only as a training-time regularizer; inference uses the CLIP head. Source:~\blx@tocontentsinit {0}\cite {kerr2023_lerf}.}}{1627}{figure.caption.3608}\protected@file@percent }
\abx@aux@backref{2516}{kerr2023_lerf}{0}{1627}{1627}
\newlabel{fig:chapter23_lerf_ablation}{{23.158}{1627}{\textbf {Ablations.} \emph {Top:} Without DINO, activations become diffuse and off-surface, bleeding across object boundaries; with DINO, crisp object-aligned maps are recovered (e.g., hand/fingers, blue dish soap). \emph {Bottom:} Without multi-scale supervision, both large concepts (``espresso machine'') and tiny details (``creamer pods'') are missed; full multi-scale training restores correct localization at the appropriate scale. DINO acts only as a training-time regularizer; inference uses the CLIP head. Source:~\cite {kerr2023_lerf}}{figure.caption.3608}{}}
\abx@aux@cite{0}{kerr2023_lerf}
\abx@aux@segm{0}{0}{kerr2023_lerf}
\abx@aux@cite{0}{kerr2023_lerf}
\abx@aux@segm{0}{0}{kerr2023_lerf}
\abx@aux@cite{0}{kerr2023_lerf}
\abx@aux@segm{0}{0}{kerr2023_lerf}
\abx@aux@cite{0}{kerr2023_lerf}
\abx@aux@segm{0}{0}{kerr2023_lerf}
\@writefile{toc}{\contentsline {paragraph}{Failure cases and ambiguities}{1628}{section*.3609}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.159}{\ignorespaces \textbf  {Common failure modes.} (Top) Long-tail mix-ups under similar appearance (``zucchini'' activating on cucumbers/corn). (Bottom) Texture and shape confusions (``leaf'' on green plastics) and weak global reasoning (``table'' edges dominate). Source:~\blx@tocontentsinit {0}\cite {kerr2023_lerf}.}}{1628}{figure.caption.3610}\protected@file@percent }
\abx@aux@backref{2518}{kerr2023_lerf}{0}{1628}{1628}
\newlabel{fig:chapter23_lerf_failures}{{23.159}{1628}{\textbf {Common failure modes.} (Top) Long-tail mix-ups under similar appearance (``zucchini'' activating on cucumbers/corn). (Bottom) Texture and shape confusions (``leaf'' on green plastics) and weak global reasoning (``table'' edges dominate). Source:~\cite {kerr2023_lerf}}{figure.caption.3610}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23.160}{\ignorespaces \textbf  {CLIP-driven ambiguities.} Errors often trace to the frozen teacher: (left) grocery queries where visually similar produce cluster in feature space (``bell peppers'' vs.\ jalape\~nos); (right) metallic context around ``portafilter''; and absent queries (``kiwis'') that nevertheless elicit responses. Source:~\blx@tocontentsinit {0}\cite {kerr2023_lerf}.}}{1628}{figure.caption.3611}\protected@file@percent }
\abx@aux@backref{2520}{kerr2023_lerf}{0}{1628}{1628}
\newlabel{fig:chapter23_lerf_ambiguities}{{23.160}{1628}{\textbf {CLIP-driven ambiguities.} Errors often trace to the frozen teacher: (left) grocery queries where visually similar produce cluster in feature space (``bell peppers'' vs.\ jalape\~nos); (right) metallic context around ``portafilter''; and absent queries (``kiwis'') that nevertheless elicit responses. Source:~\cite {kerr2023_lerf}}{figure.caption.3611}{}}
\abx@aux@cite{0}{kerr2023_lerf}
\abx@aux@segm{0}{0}{kerr2023_lerf}
\abx@aux@cite{0}{kerr2023_lerf}
\abx@aux@segm{0}{0}{kerr2023_lerf}
\abx@aux@cite{0}{kerr2023_lerf}
\abx@aux@segm{0}{0}{kerr2023_lerf}
\abx@aux@cite{0}{kerr2023_lerf}
\abx@aux@segm{0}{0}{kerr2023_lerf}
\@writefile{toc}{\contentsline {paragraph}{Prompt sensitivity (prompt tuning)}{1629}{section*.3612}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.161}{\ignorespaces \textbf  {Prompt specificity matters.} Refined queries (``blue dish soap'' vs.\ ``dish soap''; ``tea in a transparent glass'' vs.\ ``tea'') suppress distractors and sharpen activations, highlighting the sensitivity of the system to linguistic detail. Source:~\blx@tocontentsinit {0}\cite {kerr2023_lerf}.}}{1629}{figure.caption.3613}\protected@file@percent }
\abx@aux@backref{2522}{kerr2023_lerf}{0}{1629}{1629}
\newlabel{fig:chapter23_lerf_prompt_tuning}{{23.161}{1629}{\textbf {Prompt specificity matters.} Refined queries (``blue dish soap'' vs.\ ``dish soap''; ``tea in a transparent glass'' vs.\ ``tea'') suppress distractors and sharpen activations, highlighting the sensitivity of the system to linguistic detail. Source:~\cite {kerr2023_lerf}}{figure.caption.3613}{}}
\@writefile{toc}{\contentsline {paragraph}{CLIP bag-of-words behavior}{1629}{section*.3614}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.162}{\ignorespaces \textbf  {Bag-of-words effects in CLIP.} Query phrasing reveals CLIP’s tendency toward bag-of-words behavior. A single-word query like ``blueberry'' correctly isolates the blueberry donuts, but the seemingly more specific phrase ``blueberry donuts'' instead highlights \emph  {all} donuts, dominated by the noun. Similarly, part-level queries such as ``handle'' produce diffuse activations, while ``mug handle'' strengthens the focus near the handle yet still covers most of the mug. These effects illustrate the limits of CLIP’s compositional reasoning. Source:~\blx@tocontentsinit {0}\cite {kerr2023_lerf}.}}{1629}{figure.caption.3615}\protected@file@percent }
\abx@aux@backref{2524}{kerr2023_lerf}{0}{1629}{1629}
\newlabel{fig:chapter23_lerf_bow}{{23.162}{1629}{\textbf {Bag-of-words effects in CLIP.} Query phrasing reveals CLIP’s tendency toward bag-of-words behavior. A single-word query like ``blueberry'' correctly isolates the blueberry donuts, but the seemingly more specific phrase ``blueberry donuts'' instead highlights \emph {all} donuts, dominated by the noun. Similarly, part-level queries such as ``handle'' produce diffuse activations, while ``mug handle'' strengthens the focus near the handle yet still covers most of the mug. These effects illustrate the limits of CLIP’s compositional reasoning. Source:~\cite {kerr2023_lerf}}{figure.caption.3615}{}}
\@writefile{toc}{\contentsline {paragraph}{Efficiency analysis}{1630}{section*.3616}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary of findings}{1630}{section*.3617}\protected@file@percent }
\BKM@entry{id=926,dest={73656374696F6E2A2E33363138},srcline={9468}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030335C3030302E5C303030315C303030335C3030302E5C303030325C3030303A5C3030305C3034305C303030495C3030306E5C303030735C303030745C303030725C303030755C303030635C303030745C3030304E5C303030655C303030525C303030465C303030325C3030304E5C303030655C303030525C303030465C3030303A5C3030305C3034305C303030455C303030645C303030695C303030745C303030695C3030306E5C303030675C3030305C3034305C303030335C303030445C3030305C3034305C303030535C303030635C303030655C3030306E5C303030655C303030735C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030495C3030306E5C303030735C303030745C303030725C303030755C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\abx@aux@cite{0}{haque2023_instructnerf2nerf}
\abx@aux@segm{0}{0}{haque2023_instructnerf2nerf}
\abx@aux@cite{0}{haque2023_instructnerf2nerf}
\abx@aux@segm{0}{0}{haque2023_instructnerf2nerf}
\abx@aux@cite{0}{haque2023_instructnerf2nerf}
\abx@aux@segm{0}{0}{haque2023_instructnerf2nerf}
\abx@aux@cite{0}{brooks2023_instructpix2pix}
\abx@aux@segm{0}{0}{brooks2023_instructpix2pix}
\@writefile{toc}{\contentsline {subsection}{Enrichment 23.13.2: InstructNeRF2NeRF: Editing 3D Scenes with Instructions}{1631}{section*.3618}\protected@file@percent }
\newlabel{enr:subsec_chapter23_instructnerf2nerf}{{23.13.2}{1631}{\color {ocre}Enrichment \thesubsection : InstructNeRF2NeRF: Editing 3D Scenes with Instructions}{section*.3618}{}}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{1631}{section*.3619}\protected@file@percent }
\newlabel{subsec:chapter23_instructnerf2nerf_motivation}{{23.13.2}{1631}{Motivation}{section*.3619}{}}
\abx@aux@backref{2525}{haque2023_instructnerf2nerf}{0}{1631}{1631}
\@writefile{lof}{\contentsline {figure}{\numberline {23.163}{\ignorespaces \textbf  {Editing 3D scenes with instructions.} InstructNeRF2NeRF performs diverse global and local edits on a reconstructed NeRF using only natural language. Shown prompts include: ``Give him a cowboy hat'', ``Give him a mustache'', ``Make him bald'', ``Turn him into a clown'', ``As a bronze bust'', etc., demonstrating the abilities of this work. Source:~\blx@tocontentsinit {0}\cite {haque2023_instructnerf2nerf}.}}{1631}{figure.caption.3620}\protected@file@percent }
\abx@aux@backref{2527}{haque2023_instructnerf2nerf}{0}{1631}{1631}
\newlabel{fig:chapter23_instructnerf2nerf_examples}{{23.163}{1631}{\textbf {Editing 3D scenes with instructions.} InstructNeRF2NeRF performs diverse global and local edits on a reconstructed NeRF using only natural language. Shown prompts include: ``Give him a cowboy hat'', ``Give him a mustache'', ``Make him bald'', ``Turn him into a clown'', ``As a bronze bust'', etc., demonstrating the abilities of this work. Source:~\cite {haque2023_instructnerf2nerf}}{figure.caption.3620}{}}
\@writefile{toc}{\contentsline {subsubsection}{Background on InstructPix2Pix}{1631}{section*.3621}\protected@file@percent }
\newlabel{subsec:chapter23_instructnerf2nerf_instructpix2pix}{{23.13.2}{1631}{Background on InstructPix2Pix}{section*.3621}{}}
\abx@aux@backref{2528}{brooks2023_instructpix2pix}{0}{1631}{1631}
\@writefile{toc}{\contentsline {paragraph}{Core idea of InstructPix2Pix}{1631}{section*.3622}\protected@file@percent }
\abx@aux@cite{0}{brooks2023_instructpix2pix}
\abx@aux@segm{0}{0}{brooks2023_instructpix2pix}
\abx@aux@cite{0}{brooks2023_instructpix2pix}
\abx@aux@segm{0}{0}{brooks2023_instructpix2pix}
\abx@aux@cite{0}{brooks2023_instructpix2pix}
\abx@aux@segm{0}{0}{brooks2023_instructpix2pix}
\abx@aux@cite{0}{brooks2023_instructpix2pix}
\abx@aux@segm{0}{0}{brooks2023_instructpix2pix}
\abx@aux@cite{0}{brooks2023_instructpix2pix}
\abx@aux@segm{0}{0}{brooks2023_instructpix2pix}
\@writefile{toc}{\contentsline {paragraph}{Crucial controls: dual guidance scales}{1632}{section*.3623}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.164}{\ignorespaces \textbf  {InstructPix2Pix examples.} Given an image and a text instruction, IP2P applies the appropriate edit in a single forward pass without per-example inversion or fine-tuning. For instance: ``Swap sunflowers with roses'' (top left), ``Add fireworks to the sky'' (top row, middle), and ``Make his jacket out of leather'' (bottom right). Source:~\blx@tocontentsinit {0}\cite {brooks2023_instructpix2pix}.}}{1632}{figure.caption.3624}\protected@file@percent }
\abx@aux@backref{2530}{brooks2023_instructpix2pix}{0}{1632}{1632}
\newlabel{fig:chapter23_instructnerf2nerf_instructpix2pix_examples}{{23.164}{1632}{\textbf {InstructPix2Pix examples.} Given an image and a text instruction, IP2P applies the appropriate edit in a single forward pass without per-example inversion or fine-tuning. For instance: ``Swap sunflowers with roses'' (top left), ``Add fireworks to the sky'' (top row, middle), and ``Make his jacket out of leather'' (bottom right). Source:~\cite {brooks2023_instructpix2pix}}{figure.caption.3624}{}}
\@writefile{toc}{\contentsline {paragraph}{How InstructPix2Pix is trained and why Prompt-to-Prompt alone is insufficient}{1632}{section*.3625}\protected@file@percent }
\abx@aux@backref{2531}{brooks2023_instructpix2pix}{0}{1632}{1632}
\@writefile{lof}{\contentsline {figure}{\numberline {23.165}{\ignorespaces \textbf  {Training pipeline of InstructPix2Pix.} (a) Finetuned GPT-3 generates instructions and edited captions; (b) Stable Diffusion with Prompt-to-Prompt produces aligned original/edited image pairs; (c) over 450k training triplets are assembled; (d) a diffusion model is trained to follow instructions for image editing. At inference, the model edits real images using natural instructions. Source:~\blx@tocontentsinit {0}\cite {brooks2023_instructpix2pix}.}}{1633}{figure.caption.3626}\protected@file@percent }
\abx@aux@backref{2533}{brooks2023_instructpix2pix}{0}{1633}{1633}
\newlabel{fig:chapter23_instructnerf2nerf_instructpix2pix_idea}{{23.165}{1633}{\textbf {Training pipeline of InstructPix2Pix.} (a) Finetuned GPT-3 generates instructions and edited captions; (b) Stable Diffusion with Prompt-to-Prompt produces aligned original/edited image pairs; (c) over 450k training triplets are assembled; (d) a diffusion model is trained to follow instructions for image editing. At inference, the model edits real images using natural instructions. Source:~\cite {brooks2023_instructpix2pix}}{figure.caption.3626}{}}
\@writefile{toc}{\contentsline {paragraph}{What IP2P brings beyond text-to-image diffusion and Prompt-to-Prompt}{1633}{section*.3627}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Connection to InstructNeRF2NeRF}{1633}{section*.3628}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Method}{1634}{section*.3629}\protected@file@percent }
\newlabel{subsec:chapter23_instructnerf2nerf_method}{{23.13.2}{1634}{Method}{section*.3629}{}}
\@writefile{toc}{\contentsline {paragraph}{Editing a single dataset image}{1634}{section*.3630}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Iterative Dataset Update}{1634}{section*.3631}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Training objective and relation to SDS}{1634}{section*.3632}\protected@file@percent }
\abx@aux@cite{0}{haque2023_instructnerf2nerf}
\abx@aux@segm{0}{0}{haque2023_instructnerf2nerf}
\abx@aux@cite{0}{haque2023_instructnerf2nerf}
\abx@aux@segm{0}{0}{haque2023_instructnerf2nerf}
\abx@aux@cite{0}{haque2023_instructnerf2nerf}
\abx@aux@segm{0}{0}{haque2023_instructnerf2nerf}
\abx@aux@cite{0}{haque2023_instructnerf2nerf}
\abx@aux@segm{0}{0}{haque2023_instructnerf2nerf}
\abx@aux@cite{0}{haque2023_instructnerf2nerf}
\abx@aux@segm{0}{0}{haque2023_instructnerf2nerf}
\@writefile{lof}{\contentsline {figure}{\numberline {23.166}{\ignorespaces \textbf  {Overview.} InstructNeRF2NeRF alternates between rendering, instruction-based 2D editing (InstructPix2Pix), dataset replacement, and continued NeRF training to gradually realize the edit in 3D. Source:~\blx@tocontentsinit {0}\cite {haque2023_instructnerf2nerf}.}}{1635}{figure.caption.3633}\protected@file@percent }
\abx@aux@backref{2535}{haque2023_instructnerf2nerf}{0}{1635}{1635}
\newlabel{fig:chapter23_instructnerf2nerf_overview}{{23.166}{1635}{\textbf {Overview.} InstructNeRF2NeRF alternates between rendering, instruction-based 2D editing (InstructPix2Pix), dataset replacement, and continued NeRF training to gradually realize the edit in 3D. Source:~\cite {haque2023_instructnerf2nerf}}{figure.caption.3633}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23.167}{\ignorespaces \textbf  {Dataset evolution.} Early edited views can be inconsistent; alternating IP2P updates with NeRF optimization consolidates them into a 3D-consistent scene. Source:~\blx@tocontentsinit {0}\cite {haque2023_instructnerf2nerf}.}}{1635}{figure.caption.3634}\protected@file@percent }
\abx@aux@backref{2537}{haque2023_instructnerf2nerf}{0}{1635}{1635}
\newlabel{fig:chapter23_instructnerf2nerf_dataset_update}{{23.167}{1635}{\textbf {Dataset evolution.} Early edited views can be inconsistent; alternating IP2P updates with NeRF optimization consolidates them into a 3D-consistent scene. Source:~\cite {haque2023_instructnerf2nerf}}{figure.caption.3634}{}}
\abx@aux@cite{0}{haque2023_instructnerf2nerf}
\abx@aux@segm{0}{0}{haque2023_instructnerf2nerf}
\abx@aux@cite{0}{haque2023_instructnerf2nerf}
\abx@aux@segm{0}{0}{haque2023_instructnerf2nerf}
\abx@aux@cite{0}{haque2023_instructnerf2nerf}
\abx@aux@segm{0}{0}{haque2023_instructnerf2nerf}
\abx@aux@cite{0}{haque2023_instructnerf2nerf}
\abx@aux@segm{0}{0}{haque2023_instructnerf2nerf}
\abx@aux@cite{0}{haque2023_instructnerf2nerf}
\abx@aux@segm{0}{0}{haque2023_instructnerf2nerf}
\abx@aux@cite{0}{haque2023_instructnerf2nerf}
\abx@aux@segm{0}{0}{haque2023_instructnerf2nerf}
\@writefile{toc}{\contentsline {subsubsection}{Architecture and Implementation Details}{1636}{section*.3635}\protected@file@percent }
\newlabel{subsec:chapter23_instructnerf2nerf_architecture}{{23.13.2}{1636}{Architecture and Implementation Details}{section*.3635}{}}
\abx@aux@backref{2538}{haque2023_instructnerf2nerf}{0}{1636}{1636}
\@writefile{lof}{\contentsline {figure}{\numberline {23.168}{\ignorespaces \textbf  {Guidance scale.} Varying the image guidance controls resemblance to the original scene; text guidance controls adherence to the instruction. Renderings are from the edited 3D scenes. Source:~\blx@tocontentsinit {0}\cite {haque2023_instructnerf2nerf}.}}{1636}{figure.caption.3636}\protected@file@percent }
\abx@aux@backref{2540}{haque2023_instructnerf2nerf}{0}{1636}{1636}
\newlabel{fig:chapter23_instructnerf2nerf_guidance}{{23.168}{1636}{\textbf {Guidance scale.} Varying the image guidance controls resemblance to the original scene; text guidance controls adherence to the instruction. Renderings are from the edited 3D scenes. Source:~\cite {haque2023_instructnerf2nerf}}{figure.caption.3636}{}}
\@writefile{toc}{\contentsline {subsubsection}{Experiments and Ablation}{1636}{section*.3637}\protected@file@percent }
\newlabel{subsec:chapter23_instructnerf2nerf_experiments}{{23.13.2}{1636}{Experiments and Ablation}{section*.3637}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23.169}{\ignorespaces \textbf  {Qualitative results.} Diverse contextual and localized edits on real scenes, including environmental changes (e.g., time of day, weather) and object-specific modifications. These are renderings from the edited 3D scenes produced by InstructNeRF2NeRF. Source:~\blx@tocontentsinit {0}\cite {haque2023_instructnerf2nerf}.}}{1636}{figure.caption.3638}\protected@file@percent }
\abx@aux@backref{2542}{haque2023_instructnerf2nerf}{0}{1636}{1636}
\newlabel{fig:chapter23_instructnerf2nerf_qual}{{23.169}{1636}{\textbf {Qualitative results.} Diverse contextual and localized edits on real scenes, including environmental changes (e.g., time of day, weather) and object-specific modifications. These are renderings from the edited 3D scenes produced by InstructNeRF2NeRF. Source:~\cite {haque2023_instructnerf2nerf}}{figure.caption.3638}{}}
\abx@aux@cite{0}{haque2023_instructnerf2nerf}
\abx@aux@segm{0}{0}{haque2023_instructnerf2nerf}
\abx@aux@cite{0}{haque2023_instructnerf2nerf}
\abx@aux@segm{0}{0}{haque2023_instructnerf2nerf}
\@writefile{lof}{\contentsline {figure}{\numberline {23.170}{\ignorespaces \textbf  {Viewpoint consistency.} Vertical slice montage along a camera path. Top: original NeRF. Middle: InstructNeRF2NeRF with the instruction ``turn him into a clown'' produces consistent appearance across viewpoints. Bottom: per-frame InstructPix2Pix edits on renderings lead to inconsistencies such as changing hair and shirt colors. Source:~\blx@tocontentsinit {0}\cite {haque2023_instructnerf2nerf}.}}{1637}{figure.caption.3639}\protected@file@percent }
\abx@aux@backref{2544}{haque2023_instructnerf2nerf}{0}{1637}{1637}
\newlabel{fig:chapter23_instructnerf2nerf_consistency}{{23.170}{1637}{\textbf {Viewpoint consistency.} Vertical slice montage along a camera path. Top: original NeRF. Middle: InstructNeRF2NeRF with the instruction ``turn him into a clown'' produces consistent appearance across viewpoints. Bottom: per-frame InstructPix2Pix edits on renderings lead to inconsistencies such as changing hair and shirt colors. Source:~\cite {haque2023_instructnerf2nerf}}{figure.caption.3639}{}}
\@writefile{toc}{\contentsline {paragraph}{Baselines and iterative update importance}{1637}{section*.3640}\protected@file@percent }
\abx@aux@cite{0}{brooks2023_instructpix2pix}
\abx@aux@segm{0}{0}{brooks2023_instructpix2pix}
\abx@aux@cite{0}{brooks2023_instructpix2pix}
\abx@aux@segm{0}{0}{brooks2023_instructpix2pix}
\abx@aux@cite{0}{haque2023_instructnerf2nerf}
\abx@aux@segm{0}{0}{haque2023_instructnerf2nerf}
\abx@aux@cite{0}{haque2023_instructnerf2nerf}
\abx@aux@segm{0}{0}{haque2023_instructnerf2nerf}
\@writefile{lof}{\contentsline {figure}{\numberline {23.171}{\ignorespaces \textbf  {Baselines and ablations.} Left to right in each block: Original NeRF, IN2N + Stable Diffusion (no image conditioning), SDS + IP2P, One-time dataset update, and InstructNeRF2NeRF (full method with Iterative DU and IP2P). The full method best preserves geometry while producing coherent semantic edits across views. Source:~\blx@tocontentsinit {0}\cite {haque2023_instructnerf2nerf}.}}{1638}{figure.caption.3641}\protected@file@percent }
\abx@aux@backref{2546}{haque2023_instructnerf2nerf}{0}{1638}{1638}
\newlabel{fig:chapter23_instructnerf2nerf_baselines}{{23.171}{1638}{\textbf {Baselines and ablations.} Left to right in each block: Original NeRF, IN2N + Stable Diffusion (no image conditioning), SDS + IP2P, One-time dataset update, and InstructNeRF2NeRF (full method with Iterative DU and IP2P). The full method best preserves geometry while producing coherent semantic edits across views. Source:~\cite {haque2023_instructnerf2nerf}}{figure.caption.3641}{}}
\@writefile{toc}{\contentsline {paragraph}{Quantitative evaluation}{1638}{section*.3642}\protected@file@percent }
\abx@aux@backref{2547}{brooks2023_instructpix2pix}{0}{1638}{1638}
\abx@aux@backref{2548}{brooks2023_instructpix2pix}{0}{1638}{1638}
\@writefile{lot}{\contentsline {table}{\numberline {23.39}{\ignorespaces \textbf  {Quantitative metrics.} Alignment with the textual edit and inter-frame consistency in CLIP space. InstructNeRF2NeRF preserves edit strength comparable to per-frame IP2P while achieving the best consistency. Source:~\blx@tocontentsinit {0}\cite {haque2023_instructnerf2nerf}.}}{1638}{table.caption.3643}\protected@file@percent }
\abx@aux@backref{2550}{haque2023_instructnerf2nerf}{0}{1638}{1638}
\newlabel{tab:chapter23_instructnerf2nerf_quant}{{23.39}{1638}{\textbf {Quantitative metrics.} Alignment with the textual edit and inter-frame consistency in CLIP space. InstructNeRF2NeRF preserves edit strength comparable to per-frame IP2P while achieving the best consistency. Source:~\cite {haque2023_instructnerf2nerf}}{table.caption.3643}{}}
\abx@aux@cite{0}{haque2023_instructnerf2nerf}
\abx@aux@segm{0}{0}{haque2023_instructnerf2nerf}
\abx@aux@cite{0}{haque2023_instructnerf2nerf}
\abx@aux@segm{0}{0}{haque2023_instructnerf2nerf}
\@writefile{toc}{\contentsline {subsubsection}{Limitations and Future Work}{1639}{section*.3644}\protected@file@percent }
\newlabel{subsec:chapter23_instructnerf2nerf_limitations}{{23.13.2}{1639}{Limitations and Future Work}{section*.3644}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23.172}{\ignorespaces \textbf  {Limitations.} Top: instruction ``Delete the bear statue'' results in weak or inconsistent inpainting from the 2D editor, limiting 3D removal. Bottom: instruction ``Give him a checkered jacket'' is applied weakly and inconsistently in 2D, and the effect washes out after NeRF training. Source:~\blx@tocontentsinit {0}\cite {haque2023_instructnerf2nerf}.}}{1639}{figure.caption.3645}\protected@file@percent }
\abx@aux@backref{2552}{haque2023_instructnerf2nerf}{0}{1639}{1639}
\newlabel{fig:chapter23_instructnerf2nerf_limitations}{{23.172}{1639}{\textbf {Limitations.} Top: instruction ``Delete the bear statue'' results in weak or inconsistent inpainting from the 2D editor, limiting 3D removal. Bottom: instruction ``Give him a checkered jacket'' is applied weakly and inconsistently in 2D, and the effect washes out after NeRF training. Source:~\cite {haque2023_instructnerf2nerf}}{figure.caption.3645}{}}
\@writefile{toc}{\contentsline {paragraph}{Observed failure modes}{1639}{section*.3646}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Future directions}{1639}{section*.3647}\protected@file@percent }
\BKM@entry{id=927,dest={73656374696F6E2A2E33363438},srcline={9707}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030335C3030302E5C303030315C303030345C3030303A5C3030305C3034305C3030304E5C303030655C303030525C303030465C3030303A5C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C303030765C303030655C3030305C3034305C3030305C3034365C3030305C3034305C303030435C303030725C3030306F5C303030735C303030735C3030302D5C3030304D5C3030306F5C303030645C303030615C3030306C5C3030305C3034305C303030465C3030306F5C303030755C3030306E5C303030645C303030615C303030745C303030695C3030306F5C3030306E5C30303073}
\abx@aux@cite{0}{poole2022_dreamfusion}
\abx@aux@segm{0}{0}{poole2022_dreamfusion}
\abx@aux@cite{0}{metzer2022_latentnerf}
\abx@aux@segm{0}{0}{metzer2022_latentnerf}
\abx@aux@cite{0}{tang2023_dreamfields}
\abx@aux@segm{0}{0}{tang2023_dreamfields}
\abx@aux@cite{0}{lin2023_magic3d}
\abx@aux@segm{0}{0}{lin2023_magic3d}
\abx@aux@cite{0}{chen2023_fantasia3d}
\abx@aux@segm{0}{0}{chen2023_fantasia3d}
\BKM@entry{id=928,dest={73656374696F6E2A2E33363439},srcline={9717}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030335C3030302E5C303030315C303030345C3030302E5C303030315C3030303A5C3030305C3034305C303030445C303030725C303030655C303030615C3030306D5C303030465C303030755C303030735C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030545C303030655C303030785C303030745C3030302D5C303030745C3030306F5C3030302D5C303030335C303030445C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030535C303030635C3030306F5C303030725C303030655C3030305C3034305C303030445C303030695C303030735C303030745C303030695C3030306C5C3030306C5C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030535C303030615C3030306D5C303030705C3030306C5C303030695C3030306E5C30303067}
\abx@aux@cite{0}{poole2022_dreamfusion}
\abx@aux@segm{0}{0}{poole2022_dreamfusion}
\@writefile{toc}{\contentsline {section}{Enrichment 23.14: NeRF: Generative \& Cross-Modal Foundations}{1640}{section*.3648}\protected@file@percent }
\abx@aux@backref{2553}{poole2022_dreamfusion}{0}{1640}{1640}
\abx@aux@backref{2554}{metzer2022_latentnerf}{0}{1640}{1640}
\abx@aux@backref{2555}{tang2023_dreamfields}{0}{1640}{1640}
\abx@aux@backref{2556}{lin2023_magic3d}{0}{1640}{1640}
\abx@aux@backref{2557}{chen2023_fantasia3d}{0}{1640}{1640}
\@writefile{toc}{\contentsline {subsection}{Enrichment 23.14.1: DreamFusion: Text-to-3D with Score Distillation Sampling}{1640}{section*.3649}\protected@file@percent }
\newlabel{enr:chapter23_dreamfusion}{{23.14.1}{1640}{\color {ocre}Enrichment \thesubsection : DreamFusion: Text-to-3D with Score Distillation Sampling}{section*.3649}{}}
\abx@aux@backref{2558}{poole2022_dreamfusion}{0}{1640}{1640}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{1640}{section*.3650}\protected@file@percent }
\newlabel{subsec:chapter23_dreamfusion_motivation}{{23.14.1}{1640}{Motivation}{section*.3650}{}}
\@writefile{toc}{\contentsline {paragraph}{Why “many valid 2D views” need not imply valid 3D}{1640}{section*.3651}\protected@file@percent }
\abx@aux@cite{0}{poole2022_dreamfusion}
\abx@aux@segm{0}{0}{poole2022_dreamfusion}
\abx@aux@cite{0}{poole2022_dreamfusion}
\abx@aux@segm{0}{0}{poole2022_dreamfusion}
\@writefile{toc}{\contentsline {paragraph}{How DreamFusion closes the loophole}{1641}{section*.3652}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.173}{\ignorespaces \textbf  {DreamFusion training loop overview.} A NeRF parameterized by weights \(\theta \) predicts two intrinsic fields: \emph  {density} \(\tau (\mathbf  {x})\), which encodes geometry, and \emph  {albedo} \(\rho (\mathbf  {x})\), the view-independent base color. Surface normals from \(-\nabla _\mu \tau \) combined with randomized point lighting yield shaded renders, while volume rendering integrates along rays from randomly sampled cameras to produce 2D images. These images are noised to form \(\mathbf  {z}_t\), then passed with the text prompt \(y\) into a frozen text-to-image diffusion prior (Imagen). The diffusion model predicts the added noise \(\hat  {\varepsilon }_\phi (\mathbf  {z}_t|y,t)\); comparing against the true noise \(\varepsilon \) defines the Score Distillation Sampling (SDS) loss. The residual \(w(t)(\hat  {\varepsilon }_\phi -\varepsilon )\) provides a low-variance update direction that is backpropagated through the differentiable renderer, adjusting NeRF parameters \(\theta \). Iterating this loop with randomized cameras and lighting gradually sculpts the fog-like NeRF volume into a coherent, view-consistent 3D object faithful to the caption. Credit: DreamFusion~\blx@tocontentsinit {0}\cite {poole2022_dreamfusion}.}}{1641}{figure.caption.3653}\protected@file@percent }
\abx@aux@backref{2560}{poole2022_dreamfusion}{0}{1641}{1641}
\newlabel{fig:chapter23_dreamfusion_overview}{{23.173}{1641}{\textbf {DreamFusion training loop overview.} A NeRF parameterized by weights \(\theta \) predicts two intrinsic fields: \emph {density} \(\tau (\mathbf {x})\), which encodes geometry, and \emph {albedo} \(\rho (\mathbf {x})\), the view-independent base color. Surface normals from \(-\nabla _\mu \tau \) combined with randomized point lighting yield shaded renders, while volume rendering integrates along rays from randomly sampled cameras to produce 2D images. These images are noised to form \(\mathbf {z}_t\), then passed with the text prompt \(y\) into a frozen text-to-image diffusion prior (Imagen). The diffusion model predicts the added noise \(\hat {\varepsilon }_\phi (\mathbf {z}_t|y,t)\); comparing against the true noise \(\varepsilon \) defines the Score Distillation Sampling (SDS) loss. The residual \(w(t)(\hat {\varepsilon }_\phi -\varepsilon )\) provides a low-variance update direction that is backpropagated through the differentiable renderer, adjusting NeRF parameters \(\theta \). Iterating this loop with randomized cameras and lighting gradually sculpts the fog-like NeRF volume into a coherent, view-consistent 3D object faithful to the caption. Credit: DreamFusion~\cite {poole2022_dreamfusion}}{figure.caption.3653}{}}
\@writefile{toc}{\contentsline {subsubsection}{Method}{1642}{section*.3654}\protected@file@percent }
\newlabel{subsec:chapter23_dreamfusion_method}{{23.14.1}{1642}{Method}{section*.3654}{}}
\@writefile{toc}{\contentsline {paragraph}{High-level optimization loop}{1642}{section*.3655}\protected@file@percent }
\newlabel{par:chapter23_dreamfusion_loop_overview}{{23.14.1}{1642}{High-level optimization loop}{section*.3655}{}}
\@writefile{toc}{\contentsline {paragraph}{Foreground–background separation}{1642}{section*.3656}\protected@file@percent }
\newlabel{par:chapter23_dreamfusion_heads}{{23.14.1}{1642}{Foreground–background separation}{section*.3656}{}}
\@writefile{toc}{\contentsline {paragraph}{From density to orientation: making shape visible}{1642}{section*.3657}\protected@file@percent }
\newlabel{par:chapter23_dreamfusion_normals}{{23.14.1}{1642}{From density to orientation: making shape visible}{section*.3657}{}}
\@writefile{toc}{\contentsline {paragraph}{Render modes: complementary recipes for supervision}{1643}{section*.3658}\protected@file@percent }
\newlabel{par:chapter23_dreamfusion_modes}{{23.14.1}{1643}{Render modes: complementary recipes for supervision}{section*.3658}{}}
\@writefile{toc}{\contentsline {paragraph}{Score Distillation Sampling: turning a 2D prior into 3D updates}{1644}{section*.3659}\protected@file@percent }
\newlabel{par:chapter23_dreamfusion_sds}{{23.14.1}{1644}{Score Distillation Sampling: turning a 2D prior into 3D updates}{section*.3659}{}}
\@writefile{toc}{\contentsline {paragraph}{Albedo (unlit): appearance-centric updates}{1645}{section*.3660}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Shaded color (lit): coupled appearance\(\to \)geometry updates}{1645}{section*.3661}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Textureless shaded (lit, no texture): pure geometry updates}{1645}{section*.3662}\protected@file@percent }
\abx@aux@cite{0}{poole2022_dreamfusion}
\abx@aux@segm{0}{0}{poole2022_dreamfusion}
\abx@aux@cite{0}{poole2022_dreamfusion}
\abx@aux@segm{0}{0}{poole2022_dreamfusion}
\abx@aux@cite{0}{poole2022_dreamfusion}
\abx@aux@segm{0}{0}{poole2022_dreamfusion}
\abx@aux@cite{0}{poole2022_dreamfusion}
\abx@aux@segm{0}{0}{poole2022_dreamfusion}
\@writefile{toc}{\contentsline {paragraph}{View sampling and view-aware prompting}{1646}{section*.3663}\protected@file@percent }
\newlabel{par:chapter23_dreamfusion_view_prompt}{{23.14.1}{1646}{View sampling and view-aware prompting}{section*.3663}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23.174}{\ignorespaces \textbf  {Sampling an image vs.\ sculpting a 3D field.} \emph  {Left (ancestral diffusion):} a standard diffusion model synthesizes a 2D image by denoising \(z_T\!\to z_0\) directly in pixel space. \emph  {Right (SDS in DreamFusion):} the diffusion model is used as a \emph  {critic}. A NeRF render \(x=g(\theta )\) is noised to \(z_t=\alpha _t x+\sigma _t\varepsilon \); the model predicts \(\hat  {\varepsilon }_\phi \), and the residual \((\hat  {\varepsilon }_\phi -\varepsilon )\) becomes an image-space update that is backpropagated through the differentiable renderer to adjust the NeRF parameters \(\theta \). Thus, SDS does not generate pixels; it provides gradients that \emph  {sculpt} the 3D field. Credit: DreamFusion~\blx@tocontentsinit {0}\cite {poole2022_dreamfusion}.}}{1646}{figure.caption.3664}\protected@file@percent }
\abx@aux@backref{2562}{poole2022_dreamfusion}{0}{1646}{1646}
\newlabel{fig:chapter23_dreamfusion_sampling}{{23.174}{1646}{\textbf {Sampling an image vs.\ sculpting a 3D field.} \emph {Left (ancestral diffusion):} a standard diffusion model synthesizes a 2D image by denoising \(z_T\!\to z_0\) directly in pixel space. \emph {Right (SDS in DreamFusion):} the diffusion model is used as a \emph {critic}. A NeRF render \(x=g(\theta )\) is noised to \(z_t=\alpha _t x+\sigma _t\varepsilon \); the model predicts \(\hat {\varepsilon }_\phi \), and the residual \((\hat {\varepsilon }_\phi -\varepsilon )\) becomes an image-space update that is backpropagated through the differentiable renderer to adjust the NeRF parameters \(\theta \). Thus, SDS does not generate pixels; it provides gradients that \emph {sculpt} the 3D field. Credit: DreamFusion~\cite {poole2022_dreamfusion}}{figure.caption.3664}{}}
\@writefile{toc}{\contentsline {paragraph}{Putting the loop together}{1646}{section*.3665}\protected@file@percent }
\newlabel{par:chapter23_dreamfusion_schedule}{{23.14.1}{1646}{Putting the loop together}{section*.3665}{}}
\@writefile{toc}{\contentsline {subsubsection}{Implementation Details}{1646}{section*.3666}\protected@file@percent }
\newlabel{subsec:chapter23_dreamfusion_arch}{{23.14.1}{1646}{Implementation Details}{section*.3666}{}}
\@writefile{toc}{\contentsline {paragraph}{Frozen diffusion prior}{1646}{section*.3667}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Foreground–background composition}{1646}{section*.3668}\protected@file@percent }
\abx@aux@cite{0}{jain2022_dreamfields}
\abx@aux@segm{0}{0}{jain2022_dreamfields}
\abx@aux@cite{0}{sanghi2022_clipmesh}
\abx@aux@segm{0}{0}{sanghi2022_clipmesh}
\abx@aux@cite{0}{poole2022_dreamfusion}
\abx@aux@segm{0}{0}{poole2022_dreamfusion}
\abx@aux@cite{0}{jain2022_dreamfields}
\abx@aux@segm{0}{0}{jain2022_dreamfields}
\abx@aux@cite{0}{sanghi2022_clipmesh}
\abx@aux@segm{0}{0}{sanghi2022_clipmesh}
\abx@aux@cite{0}{poole2022_dreamfusion}
\abx@aux@segm{0}{0}{poole2022_dreamfusion}
\@writefile{toc}{\contentsline {subsubsection}{Experiments and Ablation}{1647}{section*.3669}\protected@file@percent }
\newlabel{subsec:chapter23_dreamfusion_experiments}{{23.14.1}{1647}{Experiments and Ablation}{section*.3669}{}}
\@writefile{toc}{\contentsline {paragraph}{Qualitative gallery and comparisons}{1647}{section*.3670}\protected@file@percent }
\newlabel{par:chapter23_dreamfusion_gallery}{{23.14.1}{1647}{Qualitative gallery and comparisons}{section*.3670}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23.175}{\ignorespaces \textbf  {DreamFusion gallery of text-to-3D assets.} Each cell shows results for one text prompt (examples include \emph  {``a raccoon astronaut holding his helmet''}, \emph  {``a baby bunny sitting on top of a stack of pancakes''}, \emph  {``a sliced loaf of fresh bread''}, and \emph  {``Sydney Opera House, aerial view''}). For every prompt, two novel viewpoints demonstrate \emph  {multi-view consistency} of the learned 3D NeRF. Insets provide disentangled visualizations: \emph  {textureless shading} reveals the learned geometry independent of albedo, while \emph  {normal maps} expose smooth surfaces and curvature. The collection highlights DreamFusion’s ability to (i) synthesize creative and compositional scenes (e.g., \emph  {a robot and dinosaur playing chess}), (ii) generate faithful geometry and detailed textures across diverse categories (animals, food, vehicles, architecture), and (iii) disentangle shape from appearance by supervising both albedo and geometry. Prompt modifiers (\(*\), \(\dagger \), \(\ddagger \)) correspond to stylistic cues improving realism. Videos and interactive results available at \url  {dreamfusion3d.github.io}. \emph  {Credit:} DreamFusion~\blx@tocontentsinit {0}\cite {poole2022_dreamfusion}.}}{1647}{figure.caption.3671}\protected@file@percent }
\abx@aux@backref{2564}{poole2022_dreamfusion}{0}{1647}{1647}
\newlabel{fig:chapter23_dreamfusion_examples}{{23.175}{1647}{\textbf {DreamFusion gallery of text-to-3D assets.} Each cell shows results for one text prompt (examples include \emph {``a raccoon astronaut holding his helmet''}, \emph {``a baby bunny sitting on top of a stack of pancakes''}, \emph {``a sliced loaf of fresh bread''}, and \emph {``Sydney Opera House, aerial view''}). For every prompt, two novel viewpoints demonstrate \emph {multi-view consistency} of the learned 3D NeRF. Insets provide disentangled visualizations: \emph {textureless shading} reveals the learned geometry independent of albedo, while \emph {normal maps} expose smooth surfaces and curvature. The collection highlights DreamFusion’s ability to (i) synthesize creative and compositional scenes (e.g., \emph {a robot and dinosaur playing chess}), (ii) generate faithful geometry and detailed textures across diverse categories (animals, food, vehicles, architecture), and (iii) disentangle shape from appearance by supervising both albedo and geometry. Prompt modifiers (\(*\), \(\dagger \), \(\ddagger \)) correspond to stylistic cues improving realism. Videos and interactive results available at \url {dreamfusion3d.github.io}. \emph {Credit:} DreamFusion~\cite {poole2022_dreamfusion}}{figure.caption.3671}{}}
\abx@aux@cite{0}{poole2022_dreamfusion}
\abx@aux@segm{0}{0}{poole2022_dreamfusion}
\@writefile{lof}{\contentsline {figure}{\numberline {23.176}{\ignorespaces \textbf  {Qualitative comparison of text-to-3D methods.} Each column corresponds to the same text prompt (e.g., \emph  {``a matte painting of a castle made of cheesecake surrounded by a moat made of ice cream''}, \emph  {``a vase with pink flowers''}, \emph  {``a hamburger''}); The figure compares between Dream Fields (original), Dream Fields (reimplementation)~\blx@tocontentsinit {0}\cite {jain2022_dreamfields}, CLIP-Mesh~\blx@tocontentsinit {0}\cite {sanghi2022_clipmesh}, and DreamFusion. \emph  {Dream Fields} often produces amorphous, low-detail shapes with color patterns loosely matching the text. \emph  {CLIP-Mesh} improves geometric definition (e.g., a recognizable vase or castle) but introduces noisy, unrealistic textures typical of CLIP-guided optimization. \emph  {DreamFusion}, guided by a diffusion prior via SDS, produces coherent 3D structures with clean silhouettes, semantically faithful details, and plausible textures across views. \emph  {Credit:} DreamFusion~\blx@tocontentsinit {0}\cite {poole2022_dreamfusion}.}}{1648}{figure.caption.3672}\protected@file@percent }
\abx@aux@backref{2568}{jain2022_dreamfields}{0}{1648}{1648}
\abx@aux@backref{2569}{sanghi2022_clipmesh}{0}{1648}{1648}
\abx@aux@backref{2570}{poole2022_dreamfusion}{0}{1648}{1648}
\newlabel{fig:chapter23_dreamfusion_qual}{{23.176}{1648}{\textbf {Qualitative comparison of text-to-3D methods.} Each column corresponds to the same text prompt (e.g., \emph {``a matte painting of a castle made of cheesecake surrounded by a moat made of ice cream''}, \emph {``a vase with pink flowers''}, \emph {``a hamburger''}); The figure compares between Dream Fields (original), Dream Fields (reimplementation)~\cite {jain2022_dreamfields}, CLIP-Mesh~\cite {sanghi2022_clipmesh}, and DreamFusion. \emph {Dream Fields} often produces amorphous, low-detail shapes with color patterns loosely matching the text. \emph {CLIP-Mesh} improves geometric definition (e.g., a recognizable vase or castle) but introduces noisy, unrealistic textures typical of CLIP-guided optimization. \emph {DreamFusion}, guided by a diffusion prior via SDS, produces coherent 3D structures with clean silhouettes, semantically faithful details, and plausible textures across views. \emph {Credit:} DreamFusion~\cite {poole2022_dreamfusion}}{figure.caption.3672}{}}
\@writefile{toc}{\contentsline {paragraph}{Caption–image coherence via CLIP retrieval}{1648}{section*.3673}\protected@file@percent }
\newlabel{par:chapter23_dreamfusion_clipretrieval}{{23.14.1}{1648}{Caption–image coherence via CLIP retrieval}{section*.3673}{}}
\abx@aux@backref{2571}{poole2022_dreamfusion}{0}{1648}{1648}
\abx@aux@cite{0}{jain2022_dreamfields}
\abx@aux@segm{0}{0}{jain2022_dreamfields}
\abx@aux@cite{0}{sanghi2022_clipmesh}
\abx@aux@segm{0}{0}{sanghi2022_clipmesh}
\@writefile{lot}{\contentsline {table}{\numberline {23.40}{\ignorespaces \textbf  {CLIP R-Precision (\%) on object-centric COCO.} ``Geo'' uses \emph  {textureless} shaded renders (albedo removed) to test geometry–text alignment. $\dagger $ evaluated with one seed. Baseline numbers in parentheses may be inflated when training and evaluation share the same CLIP model.}}{1649}{table.caption.3674}\protected@file@percent }
\abx@aux@backref{2572}{jain2022_dreamfields}{0}{1649}{1649}
\abx@aux@backref{2573}{sanghi2022_clipmesh}{0}{1649}{1649}
\newlabel{tab:chapter23_dreamfusion_rprec}{{23.40}{1649}{\textbf {CLIP R-Precision (\%) on object-centric COCO.} ``Geo'' uses \emph {textureless} shaded renders (albedo removed) to test geometry–text alignment. $\dagger $ evaluated with one seed. Baseline numbers in parentheses may be inflated when training and evaluation share the same CLIP model}{table.caption.3674}{}}
\abx@aux@cite{0}{poole2022_dreamfusion}
\abx@aux@segm{0}{0}{poole2022_dreamfusion}
\abx@aux@cite{0}{poole2022_dreamfusion}
\abx@aux@segm{0}{0}{poole2022_dreamfusion}
\@writefile{toc}{\contentsline {paragraph}{Ablations: what unlocks geometry?}{1650}{section*.3675}\protected@file@percent }
\newlabel{par:chapter23_dreamfusion_ablations}{{23.14.1}{1650}{Ablations: what unlocks geometry?}{section*.3675}{}}
\abx@aux@cite{0}{poole2022_dreamfusion}
\abx@aux@segm{0}{0}{poole2022_dreamfusion}
\abx@aux@cite{0}{poole2022_dreamfusion}
\abx@aux@segm{0}{0}{poole2022_dreamfusion}
\@writefile{lof}{\contentsline {figure}{\numberline {23.177}{\ignorespaces \textbf  {Ablation: components that improve geometry.} \emph  {Left:} CLIP L/14 R-Precision measured on three evaluation renders (Albedo, Shaded, Textureless) as components are added. Gains are largest for geometry-sensitive evaluations (Shaded/Textureless). \emph  {Right:} Prompt ``A bulldog is wearing a black pirate hat''. Progression shows: base $\rightarrow $ +view tokens $\rightarrow $ +lighting $\rightarrow $ +textureless shading. Tokens stabilize semantics; lighting exposes curvature; textureless passes remove the billboard shortcut and yield more volumetric geometry. \emph  {Credit:} DreamFusion~\blx@tocontentsinit {0}\cite {poole2022_dreamfusion}.}}{1651}{figure.caption.3676}\protected@file@percent }
\abx@aux@backref{2575}{poole2022_dreamfusion}{0}{1651}{1651}
\newlabel{fig:chapter23_dreamfusion_ablation}{{23.177}{1651}{\textbf {Ablation: components that improve geometry.} \emph {Left:} CLIP L/14 R-Precision measured on three evaluation renders (Albedo, Shaded, Textureless) as components are added. Gains are largest for geometry-sensitive evaluations (Shaded/Textureless). \emph {Right:} Prompt ``A bulldog is wearing a black pirate hat''. Progression shows: base $\rightarrow $ +view tokens $\rightarrow $ +lighting $\rightarrow $ +textureless shading. Tokens stabilize semantics; lighting exposes curvature; textureless passes remove the billboard shortcut and yield more volumetric geometry. \emph {Credit:} DreamFusion~\cite {poole2022_dreamfusion}}{figure.caption.3676}{}}
\@writefile{toc}{\contentsline {paragraph}{Iterative refinement and compositional editing}{1651}{section*.3677}\protected@file@percent }
\newlabel{par:chapter23_dreamfusion_iterative_part}{{23.14.1}{1651}{Iterative refinement and compositional editing}{section*.3677}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23.178}{\ignorespaces \textbf  {Iterative refinement with compositional editing.} From a base model, optimization continues as the text is edited (attributes, style, background). Top rows show two novel views per edit; strips give additional viewpoints. Because a \emph  {single} NeRF is optimized throughout, new attributes are layered onto the same geometry rather than regenerated from scratch, maintaining view consistency and identity while enabling interactive scene building. \emph  {Credit:} DreamFusion~\blx@tocontentsinit {0}\cite {poole2022_dreamfusion}.}}{1651}{figure.caption.3678}\protected@file@percent }
\abx@aux@backref{2577}{poole2022_dreamfusion}{0}{1651}{1651}
\newlabel{fig:chapter23_dreamfusion_iterative}{{23.178}{1651}{\textbf {Iterative refinement with compositional editing.} From a base model, optimization continues as the text is edited (attributes, style, background). Top rows show two novel views per edit; strips give additional viewpoints. Because a \emph {single} NeRF is optimized throughout, new attributes are layered onto the same geometry rather than regenerated from scratch, maintaining view consistency and identity while enabling interactive scene building. \emph {Credit:} DreamFusion~\cite {poole2022_dreamfusion}}{figure.caption.3678}{}}
\abx@aux@cite{0}{metzer2022_latentnerf}
\abx@aux@segm{0}{0}{metzer2022_latentnerf}
\abx@aux@cite{0}{chen2023_fantasia3d}
\abx@aux@segm{0}{0}{chen2023_fantasia3d}
\abx@aux@cite{0}{lin2023_magic3d}
\abx@aux@segm{0}{0}{lin2023_magic3d}
\@writefile{toc}{\contentsline {subsubsection}{Limitations and Future Work}{1652}{section*.3679}\protected@file@percent }
\newlabel{subsec:chapter23_dreamfusion_limitations}{{23.14.1}{1652}{Limitations and Future Work}{section*.3679}{}}
\abx@aux@backref{2578}{metzer2022_latentnerf}{0}{1652}{1652}
\abx@aux@backref{2579}{chen2023_fantasia3d}{0}{1652}{1652}
\abx@aux@backref{2580}{lin2023_magic3d}{0}{1652}{1652}
\BKM@entry{id=929,dest={73656374696F6E2A2E33363830},srcline={10188}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030335C3030302E5C303030315C303030345C3030302E5C303030325C3030303A5C3030305C3034305C3030304C5C303030615C303030745C303030655C3030306E5C303030745C3030302D5C3030304E5C303030655C303030525C303030465C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030535C303030685C303030615C303030705C303030655C3030302D5C303030475C303030755C303030695C303030645C303030655C303030645C3030305C3034305C303030335C303030445C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{metzer2022_latentnerf}
\abx@aux@segm{0}{0}{metzer2022_latentnerf}
\abx@aux@cite{0}{metzer2022_latentnerf}
\abx@aux@segm{0}{0}{metzer2022_latentnerf}
\@writefile{toc}{\contentsline {subsection}{Enrichment 23.14.2: Latent-NeRF for Shape-Guided 3D Generation}{1653}{section*.3680}\protected@file@percent }
\newlabel{enr:chapter23_latentnerf}{{23.14.2}{1653}{\color {ocre}Enrichment \thesubsection : Latent-NeRF for Shape-Guided 3D Generation}{section*.3680}{}}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{1653}{section*.3681}\protected@file@percent }
\newlabel{subsec:chapter23_latentnerf_motivation}{{23.14.2}{1653}{Motivation}{section*.3681}{}}
\@writefile{toc}{\contentsline {paragraph}{From DreamFusion to Latent-NeRF}{1653}{section*.3682}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.179}{\ignorespaces \textbf  {Latent-NeRF's three text-guided modes.} Left: \emph  {Latent-NeRF} (text-only text-to-3D). Middle: \emph  {Sketch-Shape} for coarse shape control. Right: \emph  {Latent-Paint} for text-guided texture on explicit shapes. The top row shows inputs. Examples include: ``A stack of pancakes covered in maple syrup'', ``A highly detailed sandcastle'', ``A German Shepherd'', and ``A fish with leopard spots''. Source: \blx@tocontentsinit {0}\cite {metzer2022_latentnerf}.}}{1653}{figure.caption.3683}\protected@file@percent }
\abx@aux@backref{2582}{metzer2022_latentnerf}{0}{1653}{1653}
\newlabel{fig:chapter23_latentnerf_resultant_models}{{23.179}{1653}{\textbf {Latent-NeRF's three text-guided modes.} Left: \emph {Latent-NeRF} (text-only text-to-3D). Middle: \emph {Sketch-Shape} for coarse shape control. Right: \emph {Latent-Paint} for text-guided texture on explicit shapes. The top row shows inputs. Examples include: ``A stack of pancakes covered in maple syrup'', ``A highly detailed sandcastle'', ``A German Shepherd'', and ``A fish with leopard spots''. Source: \cite {metzer2022_latentnerf}}{figure.caption.3683}{}}
\@writefile{toc}{\contentsline {paragraph}{Why latent supervision}{1653}{section*.3684}\protected@file@percent }
\abx@aux@cite{0}{metzer2022_latentnerf}
\abx@aux@segm{0}{0}{metzer2022_latentnerf}
\abx@aux@cite{0}{metzer2022_latentnerf}
\abx@aux@segm{0}{0}{metzer2022_latentnerf}
\@writefile{toc}{\contentsline {subsubsection}{Method}{1654}{section*.3685}\protected@file@percent }
\newlabel{subsec:chapter23_latentnerf_method}{{23.14.2}{1654}{Method}{section*.3685}{}}
\@writefile{toc}{\contentsline {paragraph}{Overview and connection to DreamFusion}{1654}{section*.3686}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.180}{\ignorespaces \textbf  {Overview of latent-space SDS for Latent-NeRF.} A rendered latent map $\mathbf  {z}$ is noised at time $t$ and denoised by Stable Diffusion; the difference between predicted and injected noise yields gradients that update the NeRF in latent space. Inference decodes $\mathbf  {z}$ to RGB via the VAE decoder. Source: \blx@tocontentsinit {0}\cite {metzer2022_latentnerf}.}}{1654}{figure.caption.3687}\protected@file@percent }
\abx@aux@backref{2584}{metzer2022_latentnerf}{0}{1654}{1654}
\newlabel{fig:chapter23_latentnerf_overview}{{23.180}{1654}{\textbf {Overview of latent-space SDS for Latent-NeRF.} A rendered latent map $\mathbf {z}$ is noised at time $t$ and denoised by Stable Diffusion; the difference between predicted and injected noise yields gradients that update the NeRF in latent space. Inference decodes $\mathbf {z}$ to RGB via the VAE decoder. Source: \cite {metzer2022_latentnerf}}{figure.caption.3687}{}}
\@writefile{toc}{\contentsline {paragraph}{NeRF in Stable Diffusion latent space}{1655}{section*.3688}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{SDS in latent space}{1655}{section*.3689}\protected@file@percent }
\newlabel{eq:chapter23_latentnerf_textonly_loss}{{23.72}{1655}{SDS in latent space}{equation.23.72}{}}
\@writefile{toc}{\contentsline {paragraph}{Step-by-step training loop}{1655}{section*.3690}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Rendering and latent image formation}{1656}{section*.3691}\protected@file@percent }
\newlabel{eq:chapter23_latentnerf_alphaTi}{{23.73}{1656}{Rendering and latent image formation}{equation.23.73}{}}
\newlabel{eq:chapter23_latentnerf_z}{{23.74}{1656}{Rendering and latent image formation}{equation.23.74}{}}
\@writefile{toc}{\contentsline {paragraph}{Diffusion guidance in latent space (SDS)}{1656}{section*.3692}\protected@file@percent }
\newlabel{eq:chapter23_latentnerf_eq1}{{23.75}{1656}{Diffusion guidance in latent space (SDS)}{equation.23.75}{}}
\newlabel{eq:chapter23_latentnerf_eq2}{{23.76}{1656}{Diffusion guidance in latent space (SDS)}{equation.23.76}{}}
\@writefile{toc}{\contentsline {paragraph}{Classifier-free guidance (CFG) in latent SDS}{1656}{section*.3693}\protected@file@percent }
\newlabel{eq:chapter23_latentnerf_cfg}{{23.77}{1656}{Classifier-free guidance (CFG) in latent SDS}{equation.23.77}{}}
\@writefile{toc}{\contentsline {paragraph}{Sparsity / anti-fog regularization}{1656}{section*.3694}\protected@file@percent }
\newlabel{eq:chapter23_latentnerf_sparse}{{23.78}{1656}{Sparsity / anti-fog regularization}{equation.23.78}{}}
\@writefile{toc}{\contentsline {paragraph}{Total objective (normal/text-only mode)}{1657}{section*.3695}\protected@file@percent }
\newlabel{eq:chapter23_latentnerf_total_objective}{{23.79}{1657}{Total objective (normal/text-only mode)}{equation.23.79}{}}
\abx@aux@cite{0}{metzer2022_latentnerf}
\abx@aux@segm{0}{0}{metzer2022_latentnerf}
\abx@aux@cite{0}{metzer2022_latentnerf}
\abx@aux@segm{0}{0}{metzer2022_latentnerf}
\@writefile{toc}{\contentsline {paragraph}{Sketch-Shape guidance: Rationale and Mechanism}{1658}{section*.3696}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.181}{\ignorespaces \textbf  {Sketch-Shape results under different prompts.} One simple animal-like mesh guides distinct objects (\emph  {deer}, \emph  {German Shepherd}, \emph  {pig}); four views per result demonstrate 3D consistency. Source: \blx@tocontentsinit {0}\cite {metzer2022_latentnerf}.}}{1658}{figure.caption.3697}\protected@file@percent }
\abx@aux@backref{2586}{metzer2022_latentnerf}{0}{1658}{1658}
\newlabel{fig:chapter23_latentnerf_sketchshape1}{{23.181}{1658}{\textbf {Sketch-Shape results under different prompts.} One simple animal-like mesh guides distinct objects (\emph {deer}, \emph {German Shepherd}, \emph {pig}); four views per result demonstrate 3D consistency. Source: \cite {metzer2022_latentnerf}}{figure.caption.3697}{}}
\newlabel{eq:chapter23_latentnerf_eq4}{{23.80}{1658}{Sketch-Shape guidance: Rationale and Mechanism}{equation.23.80}{}}
\abx@aux@cite{0}{metzer2022_latentnerf}
\abx@aux@segm{0}{0}{metzer2022_latentnerf}
\abx@aux@cite{0}{metzer2022_latentnerf}
\abx@aux@segm{0}{0}{metzer2022_latentnerf}
\@writefile{toc}{\contentsline {paragraph}{Effect of the leniency parameter $\sigma _S$}{1659}{section*.3698}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.182}{\ignorespaces \textbf  {Ablation over $\sigma _S$ in Eq.~\ref {eq:chapter23_latentnerf_eq4}.} Larger $\sigma _S$ yields more lenient alignment and greater geometric evolution. Source: \blx@tocontentsinit {0}\cite {metzer2022_latentnerf}.}}{1659}{figure.caption.3699}\protected@file@percent }
\abx@aux@backref{2588}{metzer2022_latentnerf}{0}{1659}{1659}
\newlabel{fig:chapter23_latentnerf_sigma_ablation}{{23.182}{1659}{\textbf {Ablation over $\sigma _S$ in Eq.~\ref {eq:chapter23_latentnerf_eq4}.} Larger $\sigma _S$ yields more lenient alignment and greater geometric evolution. Source: \cite {metzer2022_latentnerf}}{figure.caption.3699}{}}
\abx@aux@cite{0}{metzer2022_latentnerf}
\abx@aux@segm{0}{0}{metzer2022_latentnerf}
\abx@aux@cite{0}{metzer2022_latentnerf}
\abx@aux@segm{0}{0}{metzer2022_latentnerf}
\@writefile{toc}{\contentsline {paragraph}{Latent-Paint for explicit meshes}{1660}{section*.3700}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.183}{\ignorespaces \textbf  {Latent-Paint pipeline.} A $128{\times }128{\times }4$ latent texture is optimized by latent SDS through a differentiable renderer; a single VAE decode yields the final RGB texture. Source: \blx@tocontentsinit {0}\cite {metzer2022_latentnerf}.}}{1660}{figure.caption.3701}\protected@file@percent }
\abx@aux@backref{2590}{metzer2022_latentnerf}{0}{1660}{1660}
\newlabel{fig:chapter23_latentnerf_latent_paint}{{23.183}{1660}{\textbf {Latent-Paint pipeline.} A $128{\times }128{\times }4$ latent texture is optimized by latent SDS through a differentiable renderer; a single VAE decode yields the final RGB texture. Source: \cite {metzer2022_latentnerf}}{figure.caption.3701}{}}
\abx@aux@cite{0}{metzer2022_latentnerf}
\abx@aux@segm{0}{0}{metzer2022_latentnerf}
\abx@aux@cite{0}{metzer2022_latentnerf}
\abx@aux@segm{0}{0}{metzer2022_latentnerf}
\@writefile{toc}{\contentsline {paragraph}{RGB refinement with a learnable linear adapter}{1661}{section*.3702}\protected@file@percent }
\newlabel{eq:chapter23_latentnerf_eq3}{{23.81}{1661}{RGB refinement with a learnable linear adapter}{equation.23.81}{}}
\abx@aux@cite{0}{metzer2022_latentnerf}
\abx@aux@segm{0}{0}{metzer2022_latentnerf}
\abx@aux@cite{0}{metzer2022_latentnerf}
\abx@aux@segm{0}{0}{metzer2022_latentnerf}
\abx@aux@cite{0}{metzer2022_latentnerf}
\abx@aux@segm{0}{0}{metzer2022_latentnerf}
\abx@aux@cite{0}{metzer2022_latentnerf}
\abx@aux@segm{0}{0}{metzer2022_latentnerf}
\@writefile{lof}{\contentsline {figure}{\numberline {23.184}{\ignorespaces \textbf  {RGB fine-tuning strategy.} Starting from a Latent-NeRF trained in \emph  {latent space}, a trainable matrix adapter maps the four latent channels to RGB to obtain an RGB preview. The system then continues optimization with \emph  {supervision in RGB}: render an RGB view, \emph  {re-encode} it with the VAE encoder to $Z$, and apply the same SDS guidance. Gradients update both the NeRF MLP and the adapter, improving high-frequency color/detail while retaining the robustness of latent-space supervision. Source: \blx@tocontentsinit {0}\cite {metzer2022_latentnerf}.}}{1662}{figure.caption.3703}\protected@file@percent }
\abx@aux@backref{2592}{metzer2022_latentnerf}{0}{1662}{1662}
\newlabel{fig:chapter23_latentnerf_rgb_finetune}{{23.184}{1662}{\textbf {RGB fine-tuning strategy.} Starting from a Latent-NeRF trained in \emph {latent space}, a trainable matrix adapter maps the four latent channels to RGB to obtain an RGB preview. The system then continues optimization with \emph {supervision in RGB}: render an RGB view, \emph {re-encode} it with the VAE encoder to $Z$, and apply the same SDS guidance. Gradients update both the NeRF MLP and the adapter, improving high-frequency color/detail while retaining the robustness of latent-space supervision. Source: \cite {metzer2022_latentnerf}}{figure.caption.3703}{}}
\@writefile{toc}{\contentsline {subsubsection}{Architecture and Implementation Details}{1662}{section*.3704}\protected@file@percent }
\newlabel{subsec:chapter23_latentnerf_arch}{{23.14.2}{1662}{Architecture and Implementation Details}{section*.3704}{}}
\@writefile{toc}{\contentsline {paragraph}{Backbones}{1662}{section*.3705}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Schedules and regularizers}{1662}{section*.3706}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Experiments and Ablations}{1662}{section*.3707}\protected@file@percent }
\newlabel{subsec:chapter23_latentnerf_experiments}{{23.14.2}{1662}{Experiments and Ablations}{section*.3707}{}}
\@writefile{toc}{\contentsline {paragraph}{Text-only generation and multi-view consistency}{1662}{section*.3708}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.185}{\ignorespaces \textbf  {Results from different viewpoints.} Examples include ``A photo of a giraffe'', ``A photo of a vase with sunflowers'', and ``A photo of a basket with fruits'', rendered from multiple views to demonstrate 3D consistency. Source: \blx@tocontentsinit {0}\cite {metzer2022_latentnerf}.}}{1662}{figure.caption.3709}\protected@file@percent }
\abx@aux@backref{2594}{metzer2022_latentnerf}{0}{1662}{1662}
\newlabel{fig:chapter23_latentnerf_views}{{23.185}{1662}{\textbf {Results from different viewpoints.} Examples include ``A photo of a giraffe'', ``A photo of a vase with sunflowers'', and ``A photo of a basket with fruits'', rendered from multiple views to demonstrate 3D consistency. Source: \cite {metzer2022_latentnerf}}{figure.caption.3709}{}}
\abx@aux@cite{0}{metzer2022_latentnerf}
\abx@aux@segm{0}{0}{metzer2022_latentnerf}
\abx@aux@cite{0}{metzer2022_latentnerf}
\abx@aux@segm{0}{0}{metzer2022_latentnerf}
\@writefile{toc}{\contentsline {paragraph}{Qualitative comparison}{1663}{section*.3710}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.186}{\ignorespaces \textbf  {Qualitative comparison.} Rows: DreamFields/reimpl., CLIPMesh, DreamFusion, Latent-NeRF. Columns: prompts such as \emph  {castle}, \emph  {vase}, \emph  {hamburger}. Latent-NeRF yields detailed and prompt-faithful geometry and materials. Source: \blx@tocontentsinit {0}\cite {metzer2022_latentnerf}.}}{1663}{figure.caption.3711}\protected@file@percent }
\abx@aux@backref{2596}{metzer2022_latentnerf}{0}{1663}{1663}
\newlabel{fig:chapter23_latentnerf_comparison}{{23.186}{1663}{\textbf {Qualitative comparison.} Rows: DreamFields/reimpl., CLIPMesh, DreamFusion, Latent-NeRF. Columns: prompts such as \emph {castle}, \emph {vase}, \emph {hamburger}. Latent-NeRF yields detailed and prompt-faithful geometry and materials. Source: \cite {metzer2022_latentnerf}}{figure.caption.3711}{}}
\@writefile{toc}{\contentsline {paragraph}{RGB refinement improvements}{1664}{section*.3712}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.187}{\ignorespaces \textbf  {RGB refinement results.} Improvements are shown for latent text-to-3D (ice cream, temple) and Sketch-Shape (lego, car); per-pixel normals visualize geometry. Source: \blx@tocontentsinit {0}\cite {metzer2022_latentnerf}.}}{1664}{figure.caption.3713}\protected@file@percent }
\abx@aux@backref{2598}{metzer2022_latentnerf}{0}{1664}{1664}
\newlabel{fig:chapter23_latentnerf_rgb_results}{{23.187}{1664}{\textbf {RGB refinement results.} Improvements are shown for latent text-to-3D (ice cream, temple) and Sketch-Shape (lego, car); per-pixel normals visualize geometry. Source: \cite {metzer2022_latentnerf}}{figure.caption.3713}{}}
\abx@aux@cite{0}{metzer2022_latentnerf}
\abx@aux@segm{0}{0}{metzer2022_latentnerf}
\abx@aux@cite{0}{metzer2022_latentnerf}
\abx@aux@segm{0}{0}{metzer2022_latentnerf}
\abx@aux@cite{0}{metzer2022_latentnerf}
\abx@aux@segm{0}{0}{metzer2022_latentnerf}
\abx@aux@cite{0}{metzer2022_latentnerf}
\abx@aux@segm{0}{0}{metzer2022_latentnerf}
\abx@aux@cite{0}{metzer2022_latentnerf}
\abx@aux@segm{0}{0}{metzer2022_latentnerf}
\abx@aux@cite{0}{metzer2022_latentnerf}
\abx@aux@segm{0}{0}{metzer2022_latentnerf}
\@writefile{toc}{\contentsline {paragraph}{Controllability via Sketch-Shape}{1665}{section*.3714}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.188}{\ignorespaces \textbf  {Ablation on shape guidance.} With vs.\ without Sketch-Shape under identical prompts (\emph  {robot hand}, \emph  {lego man}) shows the role of geometric priors in eliminating wispy, incoherent structures. Source: \blx@tocontentsinit {0}\cite {metzer2022_latentnerf}.}}{1665}{figure.caption.3715}\protected@file@percent }
\abx@aux@backref{2600}{metzer2022_latentnerf}{0}{1665}{1665}
\newlabel{fig:chapter23_latentnerf_ablate_shape_guidance}{{23.188}{1665}{\textbf {Ablation on shape guidance.} With vs.\ without Sketch-Shape under identical prompts (\emph {robot hand}, \emph {lego man}) shows the role of geometric priors in eliminating wispy, incoherent structures. Source: \cite {metzer2022_latentnerf}}{figure.caption.3715}{}}
\@writefile{toc}{\contentsline {paragraph}{More Sketch-Shape results}{1665}{section*.3716}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.189}{\ignorespaces \textbf  {House prior under multiple styles.} \emph  {Lego}, \emph  {gingerbread}, \emph  {gothic}, and \emph  {candy}; RGB refinement is applied for detail. Source: \blx@tocontentsinit {0}\cite {metzer2022_latentnerf}.}}{1665}{figure.caption.3717}\protected@file@percent }
\abx@aux@backref{2602}{metzer2022_latentnerf}{0}{1665}{1665}
\newlabel{fig:chapter23_latentnerf_sketchshape2}{{23.189}{1665}{\textbf {House prior under multiple styles.} \emph {Lego}, \emph {gingerbread}, \emph {gothic}, and \emph {candy}; RGB refinement is applied for detail. Source: \cite {metzer2022_latentnerf}}{figure.caption.3717}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23.190}{\ignorespaces \textbf  {Additional Sketch-Shape examples.} \emph  {Robot hand}, \emph  {teddy bear in a tuxedo}, \emph  {lego man} demonstrate cross-category controllability. Source: \blx@tocontentsinit {0}\cite {metzer2022_latentnerf}.}}{1666}{figure.caption.3718}\protected@file@percent }
\abx@aux@backref{2604}{metzer2022_latentnerf}{0}{1666}{1666}
\newlabel{fig:chapter23_latentnerf_sketchshape3}{{23.190}{1666}{\textbf {Additional Sketch-Shape examples.} \emph {Robot hand}, \emph {teddy bear in a tuxedo}, \emph {lego man} demonstrate cross-category controllability. Source: \cite {metzer2022_latentnerf}}{figure.caption.3718}{}}
\abx@aux@cite{0}{metzer2022_latentnerf}
\abx@aux@segm{0}{0}{metzer2022_latentnerf}
\abx@aux@cite{0}{metzer2022_latentnerf}
\abx@aux@segm{0}{0}{metzer2022_latentnerf}
\abx@aux@cite{0}{metzer2022_latentnerf}
\abx@aux@segm{0}{0}{metzer2022_latentnerf}
\abx@aux@cite{0}{metzer2022_latentnerf}
\abx@aux@segm{0}{0}{metzer2022_latentnerf}
\abx@aux@cite{0}{metzer2022_latentnerf}
\abx@aux@segm{0}{0}{metzer2022_latentnerf}
\abx@aux@cite{0}{metzer2022_latentnerf}
\abx@aux@segm{0}{0}{metzer2022_latentnerf}
\@writefile{toc}{\contentsline {paragraph}{Latent-Paint on generic meshes}{1667}{section*.3719}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.191}{\ignorespaces \textbf  {Latent-Paint on ModelNet40 meshes.} UVs absent in inputs; XAtlas is used. Variation across seeds is shown on NASCAR; material/style shifts illustrated on cabinet, piano, and gnome. Source: \blx@tocontentsinit {0}\cite {metzer2022_latentnerf}.}}{1667}{figure.caption.3720}\protected@file@percent }
\abx@aux@backref{2606}{metzer2022_latentnerf}{0}{1667}{1667}
\newlabel{fig:chapter23_latentnerf_latentpaint_modelnet}{{23.191}{1667}{\textbf {Latent-Paint on ModelNet40 meshes.} UVs absent in inputs; XAtlas is used. Variation across seeds is shown on NASCAR; material/style shifts illustrated on cabinet, piano, and gnome. Source: \cite {metzer2022_latentnerf}}{figure.caption.3720}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23.192}{\ignorespaces \textbf  {Latent-Paint with precomputed UVs.} A single fish mesh textured as \emph  {piranha}, \emph  {leopard-spotted fish}, and \emph  {goldfish}. Source: \blx@tocontentsinit {0}\cite {metzer2022_latentnerf}.}}{1667}{figure.caption.3721}\protected@file@percent }
\abx@aux@backref{2608}{metzer2022_latentnerf}{0}{1667}{1667}
\newlabel{fig:chapter23_latentnerf_latentpaint_fish}{{23.192}{1667}{\textbf {Latent-Paint with precomputed UVs.} A single fish mesh textured as \emph {piranha}, \emph {leopard-spotted fish}, and \emph {goldfish}. Source: \cite {metzer2022_latentnerf}}{figure.caption.3721}{}}
\@writefile{toc}{\contentsline {paragraph}{Texturing comparison on a common mesh}{1668}{section*.3722}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.193}{\ignorespaces \textbf  {Boot texturing comparison.} Rows: Tango, CLIPMesh, Latent-Paint. Columns: \emph  {black boot}, \emph  {blue Converse All-Star}, \emph  {UGG boot}. Latent-Paint captures correct materials and iconic details. Source: \blx@tocontentsinit {0}\cite {metzer2022_latentnerf}.}}{1668}{figure.caption.3723}\protected@file@percent }
\abx@aux@backref{2610}{metzer2022_latentnerf}{0}{1668}{1668}
\newlabel{fig:chapter23_latentnerf_latentpaint_boot}{{23.193}{1668}{\textbf {Boot texturing comparison.} Rows: Tango, CLIPMesh, Latent-Paint. Columns: \emph {black boot}, \emph {blue Converse All-Star}, \emph {UGG boot}. Latent-Paint captures correct materials and iconic details. Source: \cite {metzer2022_latentnerf}}{figure.caption.3723}{}}
\abx@aux@cite{0}{metzer2022_latentnerf}
\abx@aux@segm{0}{0}{metzer2022_latentnerf}
\abx@aux@cite{0}{metzer2022_latentnerf}
\abx@aux@segm{0}{0}{metzer2022_latentnerf}
\abx@aux@cite{0}{metzer2022_latentnerf}
\abx@aux@segm{0}{0}{metzer2022_latentnerf}
\abx@aux@cite{0}{metzer2022_latentnerf}
\abx@aux@segm{0}{0}{metzer2022_latentnerf}
\@writefile{toc}{\contentsline {paragraph}{Personalization via Textual Inversion}{1669}{section*.3724}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.194}{\ignorespaces \textbf  {Textual Inversion.} A token learned from few images enables generating ``a * sculpture'' and composing ``a backpack that looks like *''. Source: \blx@tocontentsinit {0}\cite {metzer2022_latentnerf}.}}{1669}{figure.caption.3725}\protected@file@percent }
\abx@aux@backref{2612}{metzer2022_latentnerf}{0}{1669}{1669}
\newlabel{fig:chapter23_latentnerf_textinv}{{23.194}{1669}{\textbf {Textual Inversion.} A token learned from few images enables generating ``a * sculpture'' and composing ``a backpack that looks like *''. Source: \cite {metzer2022_latentnerf}}{figure.caption.3725}{}}
\@writefile{toc}{\contentsline {subsubsection}{Limitations and Future Work}{1669}{section*.3726}\protected@file@percent }
\newlabel{subsec:chapter23_latentnerf_limitations}{{23.14.2}{1669}{Limitations and Future Work}{section*.3726}{}}
\@writefile{toc}{\contentsline {paragraph}{View ambiguity and Janus artifacts}{1669}{section*.3727}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23.195}{\ignorespaces \textbf  {Janus artifact.} A ``squirrel'' generated with Latent-NeRF shows two faces from different views, caused by the 2D diffusion prior failing on unseen backs. Source: \blx@tocontentsinit {0}\cite {metzer2022_latentnerf}.}}{1669}{figure.caption.3728}\protected@file@percent }
\abx@aux@backref{2614}{metzer2022_latentnerf}{0}{1669}{1669}
\newlabel{fig:chapter23_latentnerf_limitations}{{23.195}{1669}{\textbf {Janus artifact.} A ``squirrel'' generated with Latent-NeRF shows two faces from different views, caused by the 2D diffusion prior failing on unseen backs. Source: \cite {metzer2022_latentnerf}}{figure.caption.3728}{}}
\@writefile{toc}{\contentsline {paragraph}{Controllability and future directions}{1669}{section*.3729}\protected@file@percent }
\BKM@entry{id=930,dest={636861707465722E3234},srcline={4}}{5C3337365C3337375C3030304C5C303030655C303030635C303030745C303030755C303030725C303030655C3030305C3034305C303030325C303030345C3030303A5C3030305C3034305C303030565C303030695C303030645C303030655C3030306F5C303030735C3030305C3034305C3030305C3035305C303030565C303030695C303030645C303030655C3030306F5C3030305C3034305C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C303030675C3030305C303531}
\BKM@entry{id=931,dest={73656374696F6E2E32342E31},srcline={10}}{5C3337365C3337375C303030495C3030306E5C303030745C303030725C3030306F5C303030645C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030745C3030306F5C3030305C3034305C303030565C303030695C303030645C303030655C3030306F5C3030305C3034305C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C30303067}
\BKM@entry{id=932,dest={73756273656374696F6E2E32342E312E31},srcline={22}}{5C3337365C3337375C303030465C303030725C3030306F5C3030306D5C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C303030565C303030695C303030645C303030655C3030306F5C30303073}
\@writefile{toc}{\contentsline {chapter}{\numberline {24}Lecture 24: Videos (Video Understanding)}{1670}{chapter.24}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\ttl@writefile{ptc}{\ttl@stoptoc{default@23}}
\ttl@writefile{ptc}{\ttl@starttoc{default@24}}
\pgfsyspdfmark {pgfid136}{0}{52099153}
\pgfsyspdfmark {pgfid135}{5966969}{45620378}
\@writefile{toc}{\contentsline {section}{\numberline {24.1}Introduction to Video Understanding}{1670}{section.24.1}\protected@file@percent }
\newlabel{sec:chapter24_video_intro}{{24.1}{1670}{Introduction to Video Understanding}{section.24.1}{}}
\newlabel{eq:chapter24_video_tensor}{{24.1}{1670}{Introduction to Video Understanding}{equation.24.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {24.1.1}From Images to Videos}{1670}{subsection.24.1.1}\protected@file@percent }
\newlabel{subsec:chapter24_from_images_to_videos}{{24.1.1}{1670}{From Images to Videos}{subsection.24.1.1}{}}
\BKM@entry{id=933,dest={73756273656374696F6E2E32342E312E32},srcline={34}}{5C3337365C3337375C303030435C303030685C303030615C3030306C5C3030306C5C303030655C3030306E5C303030675C303030655C303030735C3030305C3034305C3030306F5C303030665C3030305C3034305C303030565C303030695C303030645C303030655C3030306F5C3030305C3034305C303030445C303030615C303030745C303030615C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030435C3030306C5C303030695C303030705C3030302D5C303030425C303030615C303030735C303030655C303030645C3030305C3034305C303030545C303030725C303030615C303030695C3030306E5C303030695C3030306E5C30303067}
\@writefile{lof}{\contentsline {figure}{\numberline {24.1}{\ignorespaces Contrasting image and video classification. While images are labeled with objects such as ``cat'' or ``truck'', video clips are typically labeled with actions such as ``running'' or ``swimming''.}}{1671}{figure.caption.3730}\protected@file@percent }
\newlabel{fig:chapter24_image_vs_video_classification}{{24.1}{1671}{Contrasting image and video classification. While images are labeled with objects such as ``cat'' or ``truck'', video clips are typically labeled with actions such as ``running'' or ``swimming''}{figure.caption.3730}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {24.1.2}Challenges of Video Data and Clip-Based Training}{1671}{subsection.24.1.2}\protected@file@percent }
\newlabel{subsec:chapter24_video_clips}{{24.1.2}{1671}{Challenges of Video Data and Clip-Based Training}{subsection.24.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24.2}{\ignorespaces Illustration of video storage cost. Uncompressed video scales rapidly with resolution and frame rate, motivating the need for short clips and reduced sampling during training.}}{1671}{figure.caption.3731}\protected@file@percent }
\newlabel{fig:chapter24_video_size}{{24.2}{1671}{Illustration of video storage cost. Uncompressed video scales rapidly with resolution and frame rate, motivating the need for short clips and reduced sampling during training}{figure.caption.3731}{}}
\BKM@entry{id=934,dest={73656374696F6E2E32342E32},srcline={64}}{5C3337365C3337375C303030565C303030695C303030645C303030655C3030306F5C3030305C3034305C303030435C3030306C5C303030615C303030735C303030735C303030695C303030665C303030695C303030635C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C303030735C3030305C3034305C303030615C3030305C3034305C303030435C303030615C3030306E5C3030306F5C3030306E5C303030695C303030635C303030615C3030306C5C3030305C3034305C303030545C303030615C303030735C3030306B}
\@writefile{lof}{\contentsline {figure}{\numberline {24.3}{\ignorespaces Training and testing with clips. During training, models are trained on short subsampled clips. At test time, the model is applied to multiple subclips, and predictions are averaged to yield a video-level decision.}}{1672}{figure.caption.3732}\protected@file@percent }
\newlabel{fig:chapter24_clips_training_testing}{{24.3}{1672}{Training and testing with clips. During training, models are trained on short subsampled clips. At test time, the model is applied to multiple subclips, and predictions are averaged to yield a video-level decision}{figure.caption.3732}{}}
\@writefile{toc}{\contentsline {section}{\numberline {24.2}Video Classification as a Canonical Task}{1672}{section.24.2}\protected@file@percent }
\newlabel{sec:chapter24_video_classification_intro}{{24.2}{1672}{Video Classification as a Canonical Task}{section.24.2}{}}
\newlabel{eq:chapter24_video_classification}{{24.2}{1672}{Video Classification as a Canonical Task}{equation.24.2}{}}
\BKM@entry{id=935,dest={73756273656374696F6E2E32342E322E31},srcline={78}}{5C3337365C3337375C303030535C303030695C3030306E5C303030675C3030306C5C303030655C3030302D5C303030465C303030725C303030615C3030306D5C303030655C3030305C3034305C303030425C303030615C303030735C303030655C3030306C5C303030695C3030306E5C30303065}
\BKM@entry{id=936,dest={73756273656374696F6E2E32342E322E32},srcline={90}}{5C3337365C3337375C3030304C5C303030615C303030745C303030655C3030305C3034305C303030465C303030755C303030735C303030695C3030306F5C3030306E}
\@writefile{toc}{\contentsline {subsection}{\numberline {24.2.1}Single-Frame Baseline}{1673}{subsection.24.2.1}\protected@file@percent }
\newlabel{subsec:chapter24_single_frame}{{24.2.1}{1673}{Single-Frame Baseline}{subsection.24.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24.4}{\ignorespaces Single-frame CNN baseline. Each frame is classified independently, and predictions are aggregated at test time. Despite ignoring temporal structure, this baseline is surprisingly strong.}}{1673}{figure.caption.3733}\protected@file@percent }
\newlabel{fig:chapter24_single_frame}{{24.4}{1673}{Single-frame CNN baseline. Each frame is classified independently, and predictions are aggregated at test time. Despite ignoring temporal structure, this baseline is surprisingly strong}{figure.caption.3733}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {24.2.2}Late Fusion}{1673}{subsection.24.2.2}\protected@file@percent }
\newlabel{subsec:chapter24_late_fusion}{{24.2.2}{1673}{Late Fusion}{subsection.24.2.2}{}}
\newlabel{eq:chapter24_late_fusion}{{24.3}{1673}{Late Fusion}{equation.24.3}{}}
\BKM@entry{id=937,dest={73756273656374696F6E2E32342E322E33},srcline={118}}{5C3337365C3337375C303030455C303030615C303030725C3030306C5C303030795C3030305C3034305C303030465C303030755C303030735C303030695C3030306F5C3030306E}
\@writefile{lof}{\contentsline {figure}{\numberline {24.5}{\ignorespaces Late fusion with fully connected layers. Frame-level features are concatenated, flattened, and passed to an MLP for classification.}}{1674}{figure.caption.3734}\protected@file@percent }
\newlabel{fig:chapter24_late_fusion_fc}{{24.5}{1674}{Late fusion with fully connected layers. Frame-level features are concatenated, flattened, and passed to an MLP for classification}{figure.caption.3734}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24.6}{\ignorespaces Late fusion with global average pooling. Although parameter-efficient, this approach struggles to capture low-level motion cues such as periodic leg movement in running.}}{1674}{figure.caption.3735}\protected@file@percent }
\newlabel{fig:chapter24_late_fusion_gap}{{24.6}{1674}{Late fusion with global average pooling. Although parameter-efficient, this approach struggles to capture low-level motion cues such as periodic leg movement in running}{figure.caption.3735}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {24.2.3}Early Fusion}{1675}{subsection.24.2.3}\protected@file@percent }
\newlabel{subsec:chapter24_early_fusion}{{24.2.3}{1675}{Early Fusion}{subsection.24.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24.7}{\ignorespaces Early fusion approach. The temporal dimension is stacked as channels, enabling the first 2D convolution to compare frames directly.}}{1675}{figure.caption.3736}\protected@file@percent }
\newlabel{fig:chapter24_early_fusion}{{24.7}{1675}{Early fusion approach. The temporal dimension is stacked as channels, enabling the first 2D convolution to compare frames directly}{figure.caption.3736}{}}
\BKM@entry{id=938,dest={73756273656374696F6E2E32342E322E34},srcline={134}}{5C3337365C3337375C303030335C303030445C3030305C3034305C303030435C3030304E5C3030304E5C303030735C3030303A5C3030305C3034305C303030535C3030306C5C3030306F5C303030775C3030305C3034305C303030465C303030755C303030735C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{ji2010_3dcnn}
\abx@aux@segm{0}{0}{ji2010_3dcnn}
\abx@aux@cite{0}{karpathy2014_videocnn}
\abx@aux@segm{0}{0}{karpathy2014_videocnn}
\BKM@entry{id=939,dest={73756273656374696F6E2E32342E322E35},srcline={159}}{5C3337365C3337375C303030325C303030445C3030305C3034305C303030765C303030735C3030305C3034305C303030335C303030445C3030305C3034305C303030435C3030306F5C3030306E5C303030765C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C30303073}
\@writefile{toc}{\contentsline {subsection}{\numberline {24.2.4}3D CNNs: Slow Fusion}{1676}{subsection.24.2.4}\protected@file@percent }
\newlabel{subsec:chapter24_3dcnn}{{24.2.4}{1676}{3D CNNs: Slow Fusion}{subsection.24.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24.8}{\ignorespaces 3D convolution over video clips. Filters extend across both spatial dimensions and time, producing feature maps that jointly capture motion and appearance.}}{1676}{figure.caption.3737}\protected@file@percent }
\newlabel{fig:chapter24_3dconv}{{24.8}{1676}{3D convolution over video clips. Filters extend across both spatial dimensions and time, producing feature maps that jointly capture motion and appearance}{figure.caption.3737}{}}
\abx@aux@backref{2615}{ji2010_3dcnn}{0}{1676}{1676}
\abx@aux@backref{2616}{karpathy2014_videocnn}{0}{1676}{1676}
\@writefile{lof}{\contentsline {figure}{\numberline {24.9}{\ignorespaces Comparison of fusion strategies. Late fusion: spatial receptive field grows gradually, temporal fusion only at the end. Early fusion: temporal fusion at the start, spatial receptive field grows gradually. 3D CNN (slow fusion): both spatial and temporal receptive fields expand gradually.}}{1676}{figure.caption.3738}\protected@file@percent }
\newlabel{fig:chapter24_fusion_comparison}{{24.9}{1676}{Comparison of fusion strategies. Late fusion: spatial receptive field grows gradually, temporal fusion only at the end. Early fusion: temporal fusion at the start, spatial receptive field grows gradually. 3D CNN (slow fusion): both spatial and temporal receptive fields expand gradually}{figure.caption.3738}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {24.2.5}2D vs 3D Convolutions}{1677}{subsection.24.2.5}\protected@file@percent }
\newlabel{subsec:chapter24_2d_vs_3d}{{24.2.5}{1677}{2D vs 3D Convolutions}{subsection.24.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24.10}{\ignorespaces Early fusion setup. Filters span the entire temporal dimension, tying responses to absolute time positions.}}{1677}{figure.caption.3739}\protected@file@percent }
\newlabel{fig:chapter24_early_fusion_limit_setup}{{24.10}{1677}{Early fusion setup. Filters span the entire temporal dimension, tying responses to absolute time positions}{figure.caption.3739}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24.11}{\ignorespaces Limitation illustrated. To detect an orange$\!\to \!$blue transition early vs late, early fusion needs \emph  {two different} filters aligned to different temporal offsets.}}{1678}{figure.caption.3740}\protected@file@percent }
\newlabel{fig:chapter24_early_fusion_limit_colors}{{24.11}{1678}{Limitation illustrated. To detect an orange$\!\to \!$blue transition early vs late, early fusion needs \emph {two different} filters aligned to different temporal offsets}{figure.caption.3740}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24.12}{\ignorespaces 3D convolution: filters slide in time, providing temporal shift invariance. A single kernel that detects the orange$\!\to \!$blue transition generalizes to any temporal position in the sequence.}}{1678}{figure.caption.3741}\protected@file@percent }
\newlabel{fig:chapter24_3dconv_invariance}{{24.12}{1678}{3D convolution: filters slide in time, providing temporal shift invariance. A single kernel that detects the orange$\!\to \!$blue transition generalizes to any temporal position in the sequence}{figure.caption.3741}{}}
\BKM@entry{id=940,dest={73756273656374696F6E2E32342E322E36},srcline={229}}{5C3337365C3337375C303030535C303030705C3030306F5C303030725C303030745C303030735C3030302D5C303030315C3030304D5C3030305C3034305C303030445C303030615C303030745C303030615C303030735C303030655C303030745C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030425C303030615C303030735C303030655C3030306C5C303030695C3030306E5C303030655C3030305C3034305C303030435C3030306F5C3030306D5C303030705C303030615C303030725C303030695C303030735C3030306F5C3030306E5C30303073}
\abx@aux@cite{0}{karpathy2014_videocnn}
\abx@aux@segm{0}{0}{karpathy2014_videocnn}
\BKM@entry{id=941,dest={73756273656374696F6E2E32342E322E37},srcline={243}}{5C3337365C3337375C303030425C303030615C303030735C303030655C3030306C5C303030695C3030306E5C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C3030305C3034305C303030505C303030655C303030725C303030665C3030306F5C303030725C3030306D5C303030615C3030306E5C303030635C30303065}
\@writefile{toc}{\contentsline {paragraph}{Clarifying Input Channels vs Temporal Dimension}{1679}{section*.3742}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {24.2.6}Sports-1M Dataset and Baseline Comparisons}{1679}{subsection.24.2.6}\protected@file@percent }
\newlabel{subsec:chapter24_sports1m}{{24.2.6}{1679}{Sports-1M Dataset and Baseline Comparisons}{subsection.24.2.6}{}}
\abx@aux@backref{2617}{karpathy2014_videocnn}{0}{1679}{1679}
\@writefile{lof}{\contentsline {figure}{\numberline {24.13}{\ignorespaces Examples from the Sports-1M dataset. For each video, the ground truth label is shown in blue, with the model’s top-5 predictions listed below. Fine-grained distinctions are particularly difficult: for instance, \emph  {track cycling} is sometimes misclassified as the broader \emph  {cycling}, while in another example the model successfully distinguishes an \emph  {ultramarathon} from related classes like \emph  {half marathon} and regular \emph  {running}.}}{1679}{figure.caption.3743}\protected@file@percent }
\newlabel{fig:chapter24_sports1m_examples}{{24.13}{1679}{Examples from the Sports-1M dataset. For each video, the ground truth label is shown in blue, with the model’s top-5 predictions listed below. Fine-grained distinctions are particularly difficult: for instance, \emph {track cycling} is sometimes misclassified as the broader \emph {cycling}, while in another example the model successfully distinguishes an \emph {ultramarathon} from related classes like \emph {half marathon} and regular \emph {running}}{figure.caption.3743}{}}
\abx@aux@cite{0}{karpathy2014_videocnn}
\abx@aux@segm{0}{0}{karpathy2014_videocnn}
\BKM@entry{id=942,dest={73756273656374696F6E2E32342E322E38},srcline={263}}{5C3337365C3337375C303030435C303030335C303030445C3030303A5C3030305C3034305C303030545C303030685C303030655C3030305C3034305C303030565C303030475C303030475C3030305C3034305C3030306F5C303030665C3030305C3034305C303030335C303030445C3030305C3034305C303030435C3030304E5C3030304E5C30303073}
\abx@aux@cite{0}{tran2015_c3d}
\abx@aux@segm{0}{0}{tran2015_c3d}
\@writefile{toc}{\contentsline {subsection}{\numberline {24.2.7}Baseline Model Performance}{1680}{subsection.24.2.7}\protected@file@percent }
\newlabel{subsec:chapter24_baselines}{{24.2.7}{1680}{Baseline Model Performance}{subsection.24.2.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24.14}{\ignorespaces Sports-1M performance comparison. The single-frame baseline outperforms early fusion, while late fusion and 3D CNNs yield further improvements. Source: Johnson lecture slides}}{1680}{figure.caption.3744}\protected@file@percent }
\newlabel{fig:chapter24_sports1m_baselines}{{24.14}{1680}{Sports-1M performance comparison. The single-frame baseline outperforms early fusion, while late fusion and 3D CNNs yield further improvements. Source: Johnson lecture slides}{figure.caption.3744}{}}
\abx@aux@backref{2618}{karpathy2014_videocnn}{0}{1680}{1680}
\@writefile{toc}{\contentsline {subsection}{\numberline {24.2.8}C3D: The VGG of 3D CNNs}{1680}{subsection.24.2.8}\protected@file@percent }
\newlabel{subsec:chapter24_c3d}{{24.2.8}{1680}{C3D: The VGG of 3D CNNs}{subsection.24.2.8}{}}
\abx@aux@backref{2619}{tran2015_c3d}{0}{1680}{1680}
\@writefile{lof}{\contentsline {figure}{\numberline {24.15}{\ignorespaces C3D architecture. Built entirely on $3 \times 3 \times 3$ convolutions and $2 \times 2 \times 2$ poolings (except Pool1). While effective, it is computationally expensive due to volumetric filtering across space and time.}}{1681}{figure.caption.3745}\protected@file@percent }
\newlabel{fig:chapter24_c3d_architecture}{{24.15}{1681}{C3D architecture. Built entirely on $3 \times 3 \times 3$ convolutions and $2 \times 2 \times 2$ poolings (except Pool1). While effective, it is computationally expensive due to volumetric filtering across space and time}{figure.caption.3745}{}}
\@writefile{toc}{\contentsline {paragraph}{Computation cost}{1681}{section*.3746}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24.16}{\ignorespaces Performance comparison. On Sports-1M, C3D improves accuracy from $80.2\%$ (earlier 3D CNNs) to $84.4\%$, at the cost of significantly higher computation.}}{1681}{figure.caption.3747}\protected@file@percent }
\newlabel{fig:chapter24_c3d_results}{{24.16}{1681}{Performance comparison. On Sports-1M, C3D improves accuracy from $80.2\%$ (earlier 3D CNNs) to $84.4\%$, at the cost of significantly higher computation}{figure.caption.3747}{}}
\BKM@entry{id=943,dest={73656374696F6E2E32342E33},srcline={294}}{5C3337365C3337375C303030535C303030655C303030705C303030615C303030725C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030545C303030695C3030306D5C303030655C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030535C303030705C303030615C303030635C303030655C3030305C3034305C303030695C3030306E5C3030305C3034305C303030335C303030445C3030305C3034305C303030505C303030725C3030306F5C303030635C303030655C303030735C303030735C303030695C3030306E5C30303067}
\BKM@entry{id=944,dest={73756273656374696F6E2E32342E332E31},srcline={299}}{5C3337365C3337375C3030304D5C303030655C303030615C303030735C303030755C303030725C303030695C3030306E5C303030675C3030305C3034305C3030304D5C3030306F5C303030745C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C3030304F5C303030705C303030745C303030695C303030635C303030615C3030306C5C3030305C3034305C303030465C3030306C5C3030306F5C30303077}
\@writefile{toc}{\contentsline {paragraph}{Summary}{1682}{section*.3748}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {24.3}Separating Time and Space in 3D Processing}{1682}{section.24.3}\protected@file@percent }
\newlabel{sec:chapter24_sep_time_space}{{24.3}{1682}{Separating Time and Space in 3D Processing}{section.24.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {24.3.1}Measuring Motion: Optical Flow}{1682}{subsection.24.3.1}\protected@file@percent }
\newlabel{subsec:chapter24_optical_flow}{{24.3.1}{1682}{Measuring Motion: Optical Flow}{subsection.24.3.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Dense vs.\ sparse flow}{1682}{section*.3749}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why this helps}{1682}{section*.3750}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24.17}{\ignorespaces Optical flow visualization. Horizontal (top) and vertical (bottom) components for a woman shooting a crossbow. Motion of the arm and bow is clearly highlighted.}}{1682}{figure.caption.3751}\protected@file@percent }
\newlabel{fig:chapter24_optical_flow}{{24.17}{1682}{Optical flow visualization. Horizontal (top) and vertical (bottom) components for a woman shooting a crossbow. Motion of the arm and bow is clearly highlighted}{figure.caption.3751}{}}
\BKM@entry{id=945,dest={73756273656374696F6E2E32342E332E32},srcline={323}}{5C3337365C3337375C303030545C303030775C3030306F5C3030302D5C303030535C303030745C303030725C303030655C303030615C3030306D5C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\abx@aux@cite{0}{simonyan2014_twostream}
\abx@aux@segm{0}{0}{simonyan2014_twostream}
\abx@aux@cite{0}{simonyan2014_twostream}
\abx@aux@segm{0}{0}{simonyan2014_twostream}
\abx@aux@cite{0}{simonyan2014_twostream}
\abx@aux@segm{0}{0}{simonyan2014_twostream}
\@writefile{toc}{\contentsline {subsection}{\numberline {24.3.2}Two-Stream Networks}{1683}{subsection.24.3.2}\protected@file@percent }
\newlabel{subsec:chapter24_two_stream}{{24.3.2}{1683}{Two-Stream Networks}{subsection.24.3.2}{}}
\abx@aux@backref{2620}{simonyan2014_twostream}{0}{1683}{1683}
\@writefile{lof}{\contentsline {figure}{\numberline {24.18}{\ignorespaces Two-stream architecture \blx@tocontentsinit {0}\cite {simonyan2014_twostream}. The spatial stream processes RGB frames, while the temporal stream processes stacked optical flows. Predictions are fused at test time.}}{1683}{figure.caption.3752}\protected@file@percent }
\abx@aux@backref{2622}{simonyan2014_twostream}{0}{1683}{1683}
\newlabel{fig:chapter24_two_stream}{{24.18}{1683}{Two-stream architecture \cite {simonyan2014_twostream}. The spatial stream processes RGB frames, while the temporal stream processes stacked optical flows. Predictions are fused at test time}{figure.caption.3752}{}}
\@writefile{toc}{\contentsline {subsubsection}{Evaluation on UCF-101}{1683}{section*.3753}\protected@file@percent }
\newlabel{subsubsec:chapter24_two_stream_eval}{{24.3.2}{1683}{Evaluation on UCF-101}{section*.3753}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24.19}{\ignorespaces Comparison on UCF-101. Motion information (temporal stream) is crucial. Fusing spatial and temporal streams significantly outperforms either stream alone.}}{1683}{figure.caption.3754}\protected@file@percent }
\newlabel{fig:chapter24_two_stream_eval}{{24.19}{1683}{Comparison on UCF-101. Motion information (temporal stream) is crucial. Fusing spatial and temporal streams significantly outperforms either stream alone}{figure.caption.3754}{}}
\BKM@entry{id=946,dest={73656374696F6E2E32342E34},srcline={364}}{5C3337365C3337375C3030304D5C3030306F5C303030645C303030655C3030306C5C303030695C3030306E5C303030675C3030305C3034305C3030304C5C3030306F5C3030306E5C303030675C3030302D5C303030545C303030655C303030725C3030306D5C3030305C3034305C303030545C303030655C3030306D5C303030705C3030306F5C303030725C303030615C3030306C5C3030305C3034305C303030535C303030745C303030725C303030755C303030635C303030745C303030755C303030725C30303065}
\BKM@entry{id=947,dest={73756273656374696F6E2E32342E342E31},srcline={369}}{5C3337365C3337375C303030435C3030304E5C3030304E5C3030305C3034305C303030465C303030655C303030615C303030745C303030755C303030725C303030655C303030735C3030305C3034305C3030302B5C3030305C3034305C303030525C303030655C303030635C303030755C303030725C303030725C303030655C3030306E5C303030745C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C30303073}
\abx@aux@cite{0}{baccouche2011_seqdl}
\abx@aux@segm{0}{0}{baccouche2011_seqdl}
\abx@aux@cite{0}{donahue2015_ltrcnn}
\abx@aux@segm{0}{0}{donahue2015_ltrcnn}
\abx@aux@cite{0}{ballas2016_delving}
\abx@aux@segm{0}{0}{ballas2016_delving}
\@writefile{toc}{\contentsline {section}{\numberline {24.4}Modeling Long-Term Temporal Structure}{1684}{section.24.4}\protected@file@percent }
\newlabel{sec:chapter24_long_term}{{24.4}{1684}{Modeling Long-Term Temporal Structure}{section.24.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {24.4.1}CNN Features + Recurrent Networks}{1684}{subsection.24.4.1}\protected@file@percent }
\newlabel{subsec:chapter24_cnn_rnn}{{24.4.1}{1684}{CNN Features + Recurrent Networks}{subsection.24.4.1}{}}
\abx@aux@backref{2623}{baccouche2011_seqdl}{0}{1684}{1684}
\abx@aux@backref{2624}{donahue2015_ltrcnn}{0}{1684}{1684}
\@writefile{lof}{\contentsline {figure}{\numberline {24.20}{\ignorespaces Hybrid CNN+RNN pipeline. A frozen C3D-like network produces per-step features which an LSTM aggregates; the final hidden state yields a video-level prediction.}}{1684}{figure.caption.3755}\protected@file@percent }
\newlabel{fig:chapter24_cnn_rnn}{{24.20}{1684}{Hybrid CNN+RNN pipeline. A frozen C3D-like network produces per-step features which an LSTM aggregates; the final hidden state yields a video-level prediction}{figure.caption.3755}{}}
\BKM@entry{id=948,dest={73756273656374696F6E2E32342E342E32},srcline={406}}{5C3337365C3337375C303030535C303030705C303030615C303030745C303030695C3030306F5C3030302D5C303030545C303030655C3030306D5C303030705C3030306F5C303030725C303030615C3030306C5C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030745C303030685C303030655C3030305C3034305C3030304E5C3030306F5C3030306E5C3030306C5C3030306F5C303030635C303030615C3030306C5C3030305C3034305C303030425C3030306C5C3030306F5C303030635C3030306B}
\abx@aux@cite{0}{wang2018_nonlocal_nn}
\abx@aux@segm{0}{0}{wang2018_nonlocal_nn}
\@writefile{toc}{\contentsline {paragraph}{From vector RNNs to recurrent convs}{1685}{section*.3756}\protected@file@percent }
\abx@aux@backref{2625}{ballas2016_delving}{0}{1685}{1685}
\@writefile{lof}{\contentsline {figure}{\numberline {24.21}{\ignorespaces Recurrent convolutional network schematic. Each feature map $\mathbf  {F}_t^{(l)}$ depends on the previous time at the same layer and the previous layer at the same time; weights are shared across time.}}{1685}{figure.caption.3757}\protected@file@percent }
\newlabel{fig:chapter24_recurrent_conv}{{24.21}{1685}{Recurrent convolutional network schematic. Each feature map $\mathbf {F}_t^{(l)}$ depends on the previous time at the same layer and the previous layer at the same time; weights are shared across time}{figure.caption.3757}{}}
\@writefile{toc}{\contentsline {paragraph}{Gated variants and practicality}{1685}{section*.3758}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24.22}{\ignorespaces Ways to process sequences. CNNs capture local context; RNNs aggregate sequentially; self-attention relates all positions directly.}}{1685}{figure.caption.3759}\protected@file@percent }
\newlabel{fig:chapter24_sequence_options}{{24.22}{1685}{Ways to process sequences. CNNs capture local context; RNNs aggregate sequentially; self-attention relates all positions directly}{figure.caption.3759}{}}
\abx@aux@cite{0}{wang2018_nonlocal_nn}
\abx@aux@segm{0}{0}{wang2018_nonlocal_nn}
\abx@aux@cite{0}{wang2018_nonlocal_nn}
\abx@aux@segm{0}{0}{wang2018_nonlocal_nn}
\@writefile{toc}{\contentsline {subsection}{\numberline {24.4.2}Spatio-Temporal Self-Attention and the Nonlocal Block}{1686}{subsection.24.4.2}\protected@file@percent }
\newlabel{subsec:chapter24_nonlocal_block}{{24.4.2}{1686}{Spatio-Temporal Self-Attention and the Nonlocal Block}{subsection.24.4.2}{}}
\abx@aux@backref{2626}{wang2018_nonlocal_nn}{0}{1686}{1686}
\@writefile{toc}{\contentsline {paragraph}{Definition}{1686}{section*.3760}\protected@file@percent }
\newlabel{eq:chapter24_nonlocal_input}{{24.4}{1686}{Definition}{equation.24.4}{}}
\newlabel{eq:chapter24_nonlocal_qkv}{{24.5}{1686}{Definition}{equation.24.5}{}}
\newlabel{eq:chapter24_nonlocal_attn}{{24.6}{1686}{Definition}{equation.24.6}{}}
\newlabel{eq:chapter24_nonlocal_agg}{{24.7}{1686}{Definition}{equation.24.7}{}}
\newlabel{eq:chapter24_nonlocal_residual}{{24.8}{1686}{Definition}{equation.24.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24.23}{\ignorespaces Nonlocal block \blx@tocontentsinit {0}\cite {wang2018_nonlocal_nn}. Each output location attends to and aggregates information from all spatio-temporal positions, enabling direct long-range reasoning.}}{1686}{figure.caption.3761}\protected@file@percent }
\abx@aux@backref{2628}{wang2018_nonlocal_nn}{0}{1686}{1686}
\newlabel{fig:chapter24_nonlocal_block}{{24.23}{1686}{Nonlocal block \cite {wang2018_nonlocal_nn}. Each output location attends to and aggregates information from all spatio-temporal positions, enabling direct long-range reasoning}{figure.caption.3761}{}}
\BKM@entry{id=949,dest={73756273656374696F6E2E32342E342E33},srcline={455}}{5C3337365C3337375C303030495C3030306E5C303030665C3030306C5C303030615C303030745C303030695C3030306E5C303030675C3030305C3034305C303030325C303030445C3030305C3034305C3030304E5C303030655C303030745C303030775C3030306F5C303030725C3030306B5C303030735C3030305C3034305C303030745C3030306F5C3030305C3034305C303030335C303030445C3030305C3034305C3030305C3035305C303030495C303030335C303030445C3030305C303531}
\abx@aux@cite{0}{carreira2017_i3d}
\abx@aux@segm{0}{0}{carreira2017_i3d}
\abx@aux@cite{0}{carreira2017_i3d}
\abx@aux@segm{0}{0}{carreira2017_i3d}
\abx@aux@cite{0}{carreira2017_i3d}
\abx@aux@segm{0}{0}{carreira2017_i3d}
\@writefile{toc}{\contentsline {paragraph}{Initialization and integration}{1687}{section*.3762}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24.24}{\ignorespaces 3D CNN augmented with nonlocal blocks. Local slow fusion is complemented with global all-to-all fusion across space and time.}}{1687}{figure.caption.3763}\protected@file@percent }
\newlabel{fig:chapter24_nonlocal_integration}{{24.24}{1687}{3D CNN augmented with nonlocal blocks. Local slow fusion is complemented with global all-to-all fusion across space and time}{figure.caption.3763}{}}
\@writefile{toc}{\contentsline {paragraph}{Takeaway}{1687}{section*.3764}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {24.4.3}Inflating 2D Networks to 3D (I3D)}{1687}{subsection.24.4.3}\protected@file@percent }
\newlabel{subsec:chapter24_i3d}{{24.4.3}{1687}{Inflating 2D Networks to 3D (I3D)}{subsection.24.4.3}{}}
\abx@aux@backref{2629}{carreira2017_i3d}{0}{1687}{1687}
\@writefile{toc}{\contentsline {paragraph}{Inflating the architecture}{1687}{section*.3765}\protected@file@percent }
\abx@aux@cite{0}{carreira2017_i3d}
\abx@aux@segm{0}{0}{carreira2017_i3d}
\abx@aux@cite{0}{carreira2017_i3d}
\abx@aux@segm{0}{0}{carreira2017_i3d}
\@writefile{lof}{\contentsline {figure}{\numberline {24.25}{\ignorespaces Inflating an Inception block to 3D \blx@tocontentsinit {0}\cite {carreira2017_i3d}. Spatial operators acquire a temporal extent (bolded), e.g., $3{\times }3$ pooling becomes $3{\times }3{\times }3$}}{1688}{figure.caption.3766}\protected@file@percent }
\abx@aux@backref{2631}{carreira2017_i3d}{0}{1688}{1688}
\newlabel{fig:chapter24_i3d_block}{{24.25}{1688}{Inflating an Inception block to 3D \cite {carreira2017_i3d}. Spatial operators acquire a temporal extent (bolded), e.g., $3{\times }3$ pooling becomes $3{\times }3{\times }3$}{figure.caption.3766}{}}
\@writefile{toc}{\contentsline {paragraph}{Inflating the weights: replication and normalization}{1688}{section*.3767}\protected@file@percent }
\newlabel{eq:chapter24_i3d_weight_inflate}{{24.9}{1688}{Inflating the weights: replication and normalization}{equation.24.9}{}}
\@writefile{toc}{\contentsline {paragraph}{Why divide by $K_t$}{1688}{section*.3768}\protected@file@percent }
\abx@aux@cite{0}{kay2017_kinetics}
\abx@aux@segm{0}{0}{kay2017_kinetics}
\abx@aux@cite{0}{carreira2017_i3d}
\abx@aux@segm{0}{0}{carreira2017_i3d}
\abx@aux@cite{0}{kay2017_kinetics}
\abx@aux@segm{0}{0}{kay2017_kinetics}
\abx@aux@cite{0}{carreira2017_i3d}
\abx@aux@segm{0}{0}{carreira2017_i3d}
\abx@aux@cite{0}{kay2017_kinetics}
\abx@aux@segm{0}{0}{kay2017_kinetics}
\@writefile{lof}{\contentsline {figure}{\numberline {24.26}{\ignorespaces Weight inflation \blx@tocontentsinit {0}\cite {carreira2017_i3d}. 2D kernels are replicated across the temporal axis and scaled by $1/K_t$ so that responses on static videos match the 2D parent network}}{1689}{figure.caption.3769}\protected@file@percent }
\abx@aux@backref{2633}{carreira2017_i3d}{0}{1689}{1689}
\newlabel{fig:chapter24_i3d_weights}{{24.26}{1689}{Weight inflation \cite {carreira2017_i3d}. 2D kernels are replicated across the temporal axis and scaled by $1/K_t$ so that responses on static videos match the 2D parent network}{figure.caption.3769}{}}
\@writefile{toc}{\contentsline {paragraph}{Why inflation is a natural fit}{1689}{section*.3770}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Evidence on Kinetics-400}{1689}{section*.3771}\protected@file@percent }
\abx@aux@backref{2634}{kay2017_kinetics}{0}{1689}{1689}
\@writefile{lof}{\contentsline {figure}{\numberline {24.27}{\ignorespaces Pretraining and inflation on Kinetics-400 \blx@tocontentsinit {0}\cite {carreira2017_i3d,kay2017_kinetics}. For identical Inception-v1 backbones, Inflated CNN trained from scratch achieves 68.4\% top-1, whereas inflation from ImageNet-pretrained weights reaches 71.1\%; two-stream I3D attains 74.2\%}}{1689}{figure.caption.3772}\protected@file@percent }
\abx@aux@backref{2637}{carreira2017_i3d}{0}{1689}{1689}
\abx@aux@backref{2638}{kay2017_kinetics}{0}{1689}{1689}
\newlabel{fig:chapter24_i3d_kinetics}{{24.27}{1689}{Pretraining and inflation on Kinetics-400 \cite {carreira2017_i3d,kay2017_kinetics}. For identical Inception-v1 backbones, Inflated CNN trained from scratch achieves 68.4\% top-1, whereas inflation from ImageNet-pretrained weights reaches 71.1\%; two-stream I3D attains 74.2\%}{figure.caption.3772}{}}
\BKM@entry{id=950,dest={73756273656374696F6E2E32342E342E34},srcline={527}}{5C3337365C3337375C303030545C303030725C303030615C3030306E5C303030735C303030665C3030306F5C303030725C3030306D5C303030655C303030725C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030565C303030695C303030645C303030655C3030306F5C3030305C3034305C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C30303067}
\abx@aux@cite{0}{bertasius2021_timesformer}
\abx@aux@segm{0}{0}{bertasius2021_timesformer}
\abx@aux@cite{0}{arnab2021_vivit}
\abx@aux@segm{0}{0}{arnab2021_vivit}
\abx@aux@cite{0}{neimark2021_vtn}
\abx@aux@segm{0}{0}{neimark2021_vtn}
\abx@aux@cite{0}{fan2021_mvit}
\abx@aux@segm{0}{0}{fan2021_mvit}
\abx@aux@cite{0}{li2021_improved_mvit}
\abx@aux@segm{0}{0}{li2021_improved_mvit}
\abx@aux@cite{0}{arnab2021_vivit}
\abx@aux@segm{0}{0}{arnab2021_vivit}
\abx@aux@cite{0}{arnab2021_vivit}
\abx@aux@segm{0}{0}{arnab2021_vivit}
\abx@aux@cite{0}{bertasius2021_timesformer}
\abx@aux@segm{0}{0}{bertasius2021_timesformer}
\abx@aux@cite{0}{neimark2021_vtn}
\abx@aux@segm{0}{0}{neimark2021_vtn}
\abx@aux@cite{0}{bertasius2021_timesformer}
\abx@aux@segm{0}{0}{bertasius2021_timesformer}
\abx@aux@cite{0}{fan2021_mvit}
\abx@aux@segm{0}{0}{fan2021_mvit}
\abx@aux@cite{0}{li2021_improved_mvit}
\abx@aux@segm{0}{0}{li2021_improved_mvit}
\abx@aux@cite{0}{neimark2021_vtn}
\abx@aux@segm{0}{0}{neimark2021_vtn}
\abx@aux@cite{0}{arnab2021_vivit}
\abx@aux@segm{0}{0}{arnab2021_vivit}
\@writefile{toc}{\contentsline {paragraph}{Takeaway}{1690}{section*.3773}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {24.4.4}Transformers for Video Understanding}{1690}{subsection.24.4.4}\protected@file@percent }
\newlabel{subsec:chapter24_video_transformers}{{24.4.4}{1690}{Transformers for Video Understanding}{subsection.24.4.4}{}}
\abx@aux@backref{2639}{arnab2021_vivit}{0}{1690}{1690}
\abx@aux@backref{2640}{bertasius2021_timesformer}{0}{1690}{1690}
\abx@aux@backref{2641}{fan2021_mvit}{0}{1690}{1690}
\abx@aux@backref{2642}{li2021_improved_mvit}{0}{1690}{1690}
\abx@aux@backref{2643}{neimark2021_vtn}{0}{1690}{1690}
\@writefile{toc}{\contentsline {paragraph}{What is a token in video}{1690}{section*.3774}\protected@file@percent }
\abx@aux@backref{2644}{arnab2021_vivit}{0}{1690}{1690}
\abx@aux@backref{2645}{arnab2021_vivit}{0}{1690}{1690}
\abx@aux@backref{2646}{bertasius2021_timesformer}{0}{1690}{1690}
\abx@aux@backref{2647}{neimark2021_vtn}{0}{1690}{1690}
\@writefile{toc}{\contentsline {paragraph}{Attention over space and time}{1690}{section*.3775}\protected@file@percent }
\abx@aux@backref{2648}{bertasius2021_timesformer}{0}{1690}{1690}
\abx@aux@backref{2649}{fan2021_mvit}{0}{1690}{1690}
\abx@aux@backref{2650}{li2021_improved_mvit}{0}{1690}{1690}
\abx@aux@backref{2651}{neimark2021_vtn}{0}{1690}{1690}
\abx@aux@cite{0}{bertasius2021_timesformer}
\abx@aux@segm{0}{0}{bertasius2021_timesformer}
\abx@aux@cite{0}{fan2021_mvit}
\abx@aux@segm{0}{0}{fan2021_mvit}
\abx@aux@cite{0}{li2021_improved_mvit}
\abx@aux@segm{0}{0}{li2021_improved_mvit}
\abx@aux@cite{0}{neimark2021_vtn}
\abx@aux@segm{0}{0}{neimark2021_vtn}
\@writefile{toc}{\contentsline {paragraph}{ViViT in depth: tokenization, factorization, computation, and findings}{1691}{section*.3776}\protected@file@percent }
\abx@aux@backref{2652}{arnab2021_vivit}{0}{1691}{1691}
\@writefile{toc}{\contentsline {subparagraph}{Tokenization}{1691}{subparagraph*.3777}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{What ``spatial'' and ``temporal'' transformers mean}{1691}{subparagraph*.3778}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Architectural variants and compute}{1691}{subparagraph*.3779}\protected@file@percent }
\@writefile{toc}{\contentsline {subparagraph}{Positioning relative to contemporaries}{1691}{subparagraph*.3780}\protected@file@percent }
\abx@aux@backref{2653}{bertasius2021_timesformer}{0}{1691}{1691}
\abx@aux@backref{2654}{fan2021_mvit}{0}{1691}{1691}
\abx@aux@backref{2655}{li2021_improved_mvit}{0}{1691}{1691}
\abx@aux@cite{0}{arnab2021_vivit}
\abx@aux@segm{0}{0}{arnab2021_vivit}
\abx@aux@cite{0}{arnab2021_vivit}
\abx@aux@segm{0}{0}{arnab2021_vivit}
\abx@aux@cite{0}{bertasius2021_timesformer}
\abx@aux@segm{0}{0}{bertasius2021_timesformer}
\abx@aux@cite{0}{arnab2021_vivit}
\abx@aux@segm{0}{0}{arnab2021_vivit}
\abx@aux@cite{0}{fan2021_mvit}
\abx@aux@segm{0}{0}{fan2021_mvit}
\abx@aux@cite{0}{li2021_improved_mvit}
\abx@aux@segm{0}{0}{li2021_improved_mvit}
\abx@aux@cite{0}{neimark2021_vtn}
\abx@aux@segm{0}{0}{neimark2021_vtn}
\BKM@entry{id=951,dest={73756273656374696F6E2E32342E342E35},srcline={600}}{5C3337365C3337375C303030565C303030695C303030735C303030755C303030615C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C3030306F5C303030635C303030615C3030306C5C303030695C3030307A5C303030695C3030306E5C303030675C3030305C3034305C303030415C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\abx@aux@cite{0}{feichtenhofer2018_deepreps}
\abx@aux@segm{0}{0}{feichtenhofer2018_deepreps}
\abx@aux@cite{0}{feichtenhofer2019_deepinsights}
\abx@aux@segm{0}{0}{feichtenhofer2019_deepinsights}
\abx@aux@backref{2656}{neimark2021_vtn}{0}{1692}{1692}
\@writefile{toc}{\contentsline {subparagraph}{Practical guidance and empirical takeaways from ViViT}{1692}{subparagraph*.3781}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24.28}{\ignorespaces ViViT overview \blx@tocontentsinit {0}\cite {arnab2021_vivit}. Videos are tokenized by per-frame patches or tubelets, enriched with space–time positions, and processed by joint or factorized attention. Factorized designs reduce attention from $O((n_t n_h n_w)^2)$ to $O((n_h n_w)^2 + n_t^2)$ while retaining strong accuracy.}}{1692}{figure.caption.3782}\protected@file@percent }
\abx@aux@backref{2658}{arnab2021_vivit}{0}{1692}{1692}
\newlabel{fig:chapter24_vivit_overview}{{24.28}{1692}{ViViT overview \cite {arnab2021_vivit}. Videos are tokenized by per-frame patches or tubelets, enriched with space–time positions, and processed by joint or factorized attention. Factorized designs reduce attention from $O((n_t n_h n_w)^2)$ to $O((n_h n_w)^2 + n_t^2)$ while retaining strong accuracy}{figure.caption.3782}{}}
\@writefile{toc}{\contentsline {paragraph}{Why transformers for video}{1692}{section*.3783}\protected@file@percent }
\abx@aux@backref{2659}{arnab2021_vivit}{0}{1692}{1692}
\abx@aux@backref{2660}{bertasius2021_timesformer}{0}{1692}{1692}
\abx@aux@backref{2661}{fan2021_mvit}{0}{1692}{1692}
\abx@aux@backref{2662}{li2021_improved_mvit}{0}{1692}{1692}
\abx@aux@backref{2663}{neimark2021_vtn}{0}{1692}{1692}
\abx@aux@cite{0}{feichtenhofer2018_deepreps}
\abx@aux@segm{0}{0}{feichtenhofer2018_deepreps}
\abx@aux@cite{0}{feichtenhofer2019_deepinsights}
\abx@aux@segm{0}{0}{feichtenhofer2019_deepinsights}
\abx@aux@cite{0}{feichtenhofer2018_deepreps}
\abx@aux@segm{0}{0}{feichtenhofer2018_deepreps}
\abx@aux@cite{0}{feichtenhofer2019_deepinsights}
\abx@aux@segm{0}{0}{feichtenhofer2019_deepinsights}
\abx@aux@cite{0}{feichtenhofer2018_deepreps}
\abx@aux@segm{0}{0}{feichtenhofer2018_deepreps}
\abx@aux@cite{0}{feichtenhofer2019_deepinsights}
\abx@aux@segm{0}{0}{feichtenhofer2019_deepinsights}
\abx@aux@cite{0}{feichtenhofer2018_deepreps}
\abx@aux@segm{0}{0}{feichtenhofer2018_deepreps}
\abx@aux@cite{0}{feichtenhofer2019_deepinsights}
\abx@aux@segm{0}{0}{feichtenhofer2019_deepinsights}
\@writefile{toc}{\contentsline {subsection}{\numberline {24.4.5}Visualizing and Localizing Actions}{1693}{subsection.24.4.5}\protected@file@percent }
\newlabel{subsec:chapter24_visualization_localization}{{24.4.5}{1693}{Visualizing and Localizing Actions}{subsection.24.4.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{Visualizing Video Models}{1693}{section*.3784}\protected@file@percent }
\newlabel{subsec:chapter24_visualizing_models}{{24.4.5}{1693}{Visualizing Video Models}{section*.3784}{}}
\abx@aux@backref{2664}{feichtenhofer2018_deepreps}{0}{1693}{1693}
\abx@aux@backref{2665}{feichtenhofer2019_deepinsights}{0}{1693}{1693}
\newlabel{eq:chapter24_vis_objective}{{24.10}{1693}{Visualizing Video Models}{equation.24.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24.29}{\ignorespaces Visualizing video models with spatiotemporal regularization \blx@tocontentsinit {0}\cite {feichtenhofer2018_deepreps,feichtenhofer2019_deepinsights}. Increasing the temporal smoothness weight highlights slow components; decreasing it exposes fast components.}}{1693}{figure.caption.3785}\protected@file@percent }
\abx@aux@backref{2668}{feichtenhofer2018_deepreps}{0}{1693}{1693}
\abx@aux@backref{2669}{feichtenhofer2019_deepinsights}{0}{1693}{1693}
\newlabel{fig:chapter24_visualize_overview}{{24.29}{1693}{Visualizing video models with spatiotemporal regularization \cite {feichtenhofer2018_deepreps,feichtenhofer2019_deepinsights}. Increasing the temporal smoothness weight highlights slow components; decreasing it exposes fast components}{figure.caption.3785}{}}
\@writefile{toc}{\contentsline {paragraph}{Qualitative examples}{1693}{section*.3786}\protected@file@percent }
\abx@aux@cite{0}{feichtenhofer2018_deepreps}
\abx@aux@segm{0}{0}{feichtenhofer2018_deepreps}
\abx@aux@cite{0}{feichtenhofer2019_deepinsights}
\abx@aux@segm{0}{0}{feichtenhofer2019_deepinsights}
\abx@aux@cite{0}{feichtenhofer2018_deepreps}
\abx@aux@segm{0}{0}{feichtenhofer2018_deepreps}
\abx@aux@cite{0}{feichtenhofer2019_deepinsights}
\abx@aux@segm{0}{0}{feichtenhofer2019_deepinsights}
\abx@aux@cite{0}{chao2018_tal}
\abx@aux@segm{0}{0}{chao2018_tal}
\abx@aux@cite{0}{chao2018_tal}
\abx@aux@segm{0}{0}{chao2018_tal}
\abx@aux@cite{0}{chao2018_tal}
\abx@aux@segm{0}{0}{chao2018_tal}
\@writefile{lof}{\contentsline {figure}{\numberline {24.30}{\ignorespaces Visualization by class score optimization \blx@tocontentsinit {0}\cite {feichtenhofer2018_deepreps,feichtenhofer2019_deepinsights}. Appearance, slow, and fast components for a weightlifting clip emphasize barbell, bar shaking, and push overhead respectively.}}{1694}{figure.caption.3787}\protected@file@percent }
\abx@aux@backref{2672}{feichtenhofer2018_deepreps}{0}{1694}{1694}
\abx@aux@backref{2673}{feichtenhofer2019_deepinsights}{0}{1694}{1694}
\newlabel{fig:chapter24_visualize_weightlifting}{{24.30}{1694}{Visualization by class score optimization \cite {feichtenhofer2018_deepreps,feichtenhofer2019_deepinsights}. Appearance, slow, and fast components for a weightlifting clip emphasize barbell, bar shaking, and push overhead respectively}{figure.caption.3787}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24.31}{\ignorespaces Visualization by class score optimization \blx@tocontentsinit {0}\cite {feichtenhofer2018_deepreps,feichtenhofer2019_deepinsights}. For \emph  {apply eye makeup}, appearance surfaces faces, slow motion emphasizes hand placement, and fast motion highlights brushing strokes.}}{1694}{figure.caption.3788}\protected@file@percent }
\abx@aux@backref{2676}{feichtenhofer2018_deepreps}{0}{1694}{1694}
\abx@aux@backref{2677}{feichtenhofer2019_deepinsights}{0}{1694}{1694}
\newlabel{fig:chapter24_visualize_makeup}{{24.31}{1694}{Visualization by class score optimization \cite {feichtenhofer2018_deepreps,feichtenhofer2019_deepinsights}. For \emph {apply eye makeup}, appearance surfaces faces, slow motion emphasizes hand placement, and fast motion highlights brushing strokes}{figure.caption.3788}{}}
\@writefile{toc}{\contentsline {subsubsection}{Temporal Action Localization}{1694}{section*.3789}\protected@file@percent }
\newlabel{subsec:chapter24_temporal_localization}{{24.4.5}{1694}{Temporal Action Localization}{section*.3789}{}}
\abx@aux@backref{2678}{chao2018_tal}{0}{1694}{1694}
\abx@aux@cite{0}{gu2018_ava}
\abx@aux@segm{0}{0}{gu2018_ava}
\abx@aux@cite{0}{gu2018_ava}
\abx@aux@segm{0}{0}{gu2018_ava}
\abx@aux@cite{0}{gu2018_ava}
\abx@aux@segm{0}{0}{gu2018_ava}
\abx@aux@cite{0}{grauman2022_ego4d}
\abx@aux@segm{0}{0}{grauman2022_ego4d}
\@writefile{lof}{\contentsline {figure}{\numberline {24.32}{\ignorespaces Temporal action localization. Proposal generation followed by classification and boundary refinement identifies action segments in long untrimmed videos \blx@tocontentsinit {0}\cite {chao2018_tal}.}}{1695}{figure.caption.3790}\protected@file@percent }
\abx@aux@backref{2680}{chao2018_tal}{0}{1695}{1695}
\newlabel{fig:chapter24_temporal_localization}{{24.32}{1695}{Temporal action localization. Proposal generation followed by classification and boundary refinement identifies action segments in long untrimmed videos \cite {chao2018_tal}}{figure.caption.3790}{}}
\@writefile{toc}{\contentsline {subsubsection}{Spatio-Temporal Action Detection}{1695}{section*.3791}\protected@file@percent }
\newlabel{subsec:chapter24_spatiotemporal_detection}{{24.4.5}{1695}{Spatio-Temporal Action Detection}{section*.3791}{}}
\abx@aux@backref{2681}{gu2018_ava}{0}{1695}{1695}
\@writefile{lof}{\contentsline {figure}{\numberline {24.33}{\ignorespaces Spatio-temporal detection examples from AVA \blx@tocontentsinit {0}\cite {gu2018_ava}. Activities such as clinking glass, drinking, looking at phone, or answering phone are localized in space and time for each person.}}{1695}{figure.caption.3792}\protected@file@percent }
\abx@aux@backref{2683}{gu2018_ava}{0}{1695}{1695}
\newlabel{fig:chapter24_ava_examples}{{24.33}{1695}{Spatio-temporal detection examples from AVA \cite {gu2018_ava}. Activities such as clinking glass, drinking, looking at phone, or answering phone are localized in space and time for each person}{figure.caption.3792}{}}
\abx@aux@cite{0}{grauman2022_ego4d}
\abx@aux@segm{0}{0}{grauman2022_ego4d}
\abx@aux@cite{0}{grauman2022_ego4d}
\abx@aux@segm{0}{0}{grauman2022_ego4d}
\@writefile{toc}{\contentsline {subsubsection}{Ego4D: Large-Scale Egocentric Video}{1696}{section*.3793}\protected@file@percent }
\newlabel{subsec:chapter24_ego4d}{{24.4.5}{1696}{Ego4D: Large-Scale Egocentric Video}{section*.3793}{}}
\abx@aux@backref{2684}{grauman2022_ego4d}{0}{1696}{1696}
\@writefile{lof}{\contentsline {figure}{\numberline {24.34}{\ignorespaces Ego4D overview \blx@tocontentsinit {0}\cite {grauman2022_ego4d}. A global, long-form egocentric video corpus with narrations and benchmarks spanning episodic memory, hands and objects, audio–video diarization, social interactions, and forecasting.}}{1696}{figure.caption.3794}\protected@file@percent }
\abx@aux@backref{2686}{grauman2022_ego4d}{0}{1696}{1696}
\newlabel{fig:chapter24_ego4d}{{24.34}{1696}{Ego4D overview \cite {grauman2022_ego4d}. A global, long-form egocentric video corpus with narrations and benchmarks spanning episodic memory, hands and objects, audio–video diarization, social interactions, and forecasting}{figure.caption.3794}{}}
\BKM@entry{id=952,dest={73656374696F6E2A2E33373935},srcline={686}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030345C3030302E5C303030355C3030303A5C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3034305C3032335C3030304C5C303030615C3030306E5C303030675C303030755C303030615C303030675C303030655C3030305C3034305C303030415C3030306C5C303030695C303030675C3030306E5C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030505C303030725C303030655C303030635C303030755C303030725C303030735C3030306F5C303030725C30303073}
\BKM@entry{id=953,dest={73656374696F6E2A2E33373936},srcline={690}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030345C3030302E5C303030355C3030302E5C303030315C3030303A5C3030305C3034305C303030535C303030695C303030675C3030304C5C303030495C303030505C3030303A5C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030725C303030615C303030735C303030745C303030695C303030765C303030655C3030305C3034305C303030415C3030306C5C303030695C303030675C3030306E5C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030775C303030695C303030745C303030685C3030305C3034305C303030535C303030695C303030675C3030306D5C3030306F5C303030695C303030645C3030305C3034305C3030304C5C3030306F5C303030735C30303073}
\abx@aux@cite{0}{zhai2023_siglip}
\abx@aux@segm{0}{0}{zhai2023_siglip}
\@writefile{toc}{\contentsline {section}{Enrichment 24.5: Vision--Language Alignment Precursors}{1697}{section*.3795}\protected@file@percent }
\newlabel{enr:sec_chapter24_vlprecursors}{{24.5}{1697}{\color {ocre}Enrichment \thesection : Vision--Language Alignment Precursors}{section*.3795}{}}
\@writefile{toc}{\contentsline {subsection}{Enrichment 24.5.1: SigLIP: Contrastive Alignment with Sigmoid Loss}{1697}{section*.3796}\protected@file@percent }
\newlabel{enr:subsec_chapter24_siglip}{{24.5.1}{1697}{\color {ocre}Enrichment \thesubsection : SigLIP: Contrastive Alignment with Sigmoid Loss}{section*.3796}{}}
\@writefile{toc}{\contentsline {paragraph}{From CLIP to SigLIP (Intuition First)}{1697}{section*.3797}\protected@file@percent }
\abx@aux@backref{2687}{zhai2023_siglip}{0}{1697}{1697}
\@writefile{toc}{\contentsline {paragraph}{Algorithmic Formulation and Intuition}{1697}{section*.3798}\protected@file@percent }
\newlabel{eq:chapter24_siglip_loss}{{24.11}{1697}{Algorithmic Formulation and Intuition}{equation.24.11}{}}
\abx@aux@cite{0}{zhai2023_siglip}
\abx@aux@segm{0}{0}{zhai2023_siglip}
\abx@aux@cite{0}{zhai2023_siglip}
\abx@aux@segm{0}{0}{zhai2023_siglip}
\@writefile{toc}{\contentsline {paragraph}{CLIP vs.\ SigLIP—why it matters}{1698}{section*.3799}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Efficient Implementation}{1698}{section*.3800}\protected@file@percent }
\abx@aux@cite{0}{tschannen2025_siglip2}
\abx@aux@segm{0}{0}{tschannen2025_siglip2}
\@writefile{lof}{\contentsline {figure}{\numberline {24.35}{\ignorespaces SigLIP computes the sigmoid loss over device-local blocks, avoiding global all-gathers required by CLIP’s batch–softmax. Source: \blx@tocontentsinit {0}\cite {zhai2023_siglip}.}}{1699}{figure.caption.3801}\protected@file@percent }
\abx@aux@backref{2689}{zhai2023_siglip}{0}{1699}{1699}
\newlabel{fig:chpapter24_chapter24_siglip_lossimpl}{{24.35}{1699}{SigLIP computes the sigmoid loss over device-local blocks, avoiding global all-gathers required by CLIP’s batch–softmax. Source: \cite {zhai2023_siglip}}{figure.caption.3801}{}}
\@writefile{toc}{\contentsline {paragraph}{Empirical Comparison to CLIP: What Improves in Practice}{1699}{section*.3802}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Impact, Limitations, and Legacy}{1699}{section*.3803}\protected@file@percent }
\abx@aux@backref{2690}{tschannen2025_siglip2}{0}{1699}{1699}
\BKM@entry{id=954,dest={73656374696F6E2A2E33383034},srcline={798}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030345C3030302E5C303030355C3030302E5C303030325C3030303A5C3030305C3034305C303030425C3030304C5C303030495C303030505C3030303A5C3030305C3034305C303030425C3030306F5C3030306F5C303030745C303030735C303030745C303030725C303030615C303030705C303030705C303030695C3030306E5C303030675C3030305C3034305C3030304C5C303030615C3030306E5C303030675C303030755C303030615C303030675C303030655C3034305C3032335C303030495C3030306D5C303030615C303030675C303030655C3030305C3034305C303030505C303030725C303030655C303030745C303030725C303030615C303030695C3030306E5C303030695C3030306E5C30303067}
\@writefile{toc}{\contentsline {subsection}{Enrichment 24.5.2: BLIP: Bootstrapping Language--Image Pretraining}{1700}{section*.3804}\protected@file@percent }
\newlabel{enr:subsec_chapter24_blip}{{24.5.2}{1700}{\color {ocre}Enrichment \thesubsection : BLIP: Bootstrapping Language--Image Pretraining}{section*.3804}{}}
\@writefile{toc}{\contentsline {paragraph}{High-Level Idea}{1700}{section*.3805}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{BLIP’s Two-Part Strategy}{1700}{section*.3806}\protected@file@percent }
\abx@aux@cite{0}{li2022_blip}
\abx@aux@segm{0}{0}{li2022_blip}
\abx@aux@cite{0}{li2022_blip}
\abx@aux@segm{0}{0}{li2022_blip}
\@writefile{toc}{\contentsline {subsubsection}{Method}{1701}{section*.3807}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Unified Architecture with Three Functional Modes}{1701}{section*.3808}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24.36}{\ignorespaces  \textbf  {BLIP's unified MED architecture and objectives.} The Vision Transformer image encoder is initialized from a pre-trained ViT (e.g., ImageNet) but remains \emph  {trainable} during pre-training, alongside the text transformer blocks. All components are optimized end-to-end under three objectives, reusing the same backbone with minimal changes: (i) \emph  {ITC} runs the image and text encoders unimodally (no cross-attention) to produce global embeddings for contrastive retrieval; (ii) \emph  {ITM} augments the text encoder with cross-attention to image tokens, using bidirectional self-attention for fine-grained matching; (iii) \emph  {LM} reuses the same cross-attention and feed-forward blocks but applies a causal self-attention mask to decode text autoregressively. Most parameters (vision encoder, cross-attention, FFN) are shared; the functional differences stem only from routing (cross-attention on/off) and attention masking (bidirectional vs.\ causal), not from freezing or separating modules. \emph  {Source:} \blx@tocontentsinit {0}\cite {li2022_blip}. }}{1701}{figure.caption.3809}\protected@file@percent }
\abx@aux@backref{2692}{li2022_blip}{0}{1701}{1701}
\newlabel{fig:chpapter24_blip_overview}{{24.36}{1701}{\textbf {BLIP's unified MED architecture and objectives.} The Vision Transformer image encoder is initialized from a pre-trained ViT (e.g., ImageNet) but remains \emph {trainable} during pre-training, alongside the text transformer blocks. All components are optimized end-to-end under three objectives, reusing the same backbone with minimal changes: (i) \emph {ITC} runs the image and text encoders unimodally (no cross-attention) to produce global embeddings for contrastive retrieval; (ii) \emph {ITM} augments the text encoder with cross-attention to image tokens, using bidirectional self-attention for fine-grained matching; (iii) \emph {LM} reuses the same cross-attention and feed-forward blocks but applies a causal self-attention mask to decode text autoregressively. Most parameters (vision encoder, cross-attention, FFN) are shared; the functional differences stem only from routing (cross-attention on/off) and attention masking (bidirectional vs.\ causal), not from freezing or separating modules. \emph {Source:} \cite {li2022_blip}}{figure.caption.3809}{}}
\@writefile{toc}{\contentsline {paragraph}{Why Causal vs.\ Bidirectional Attention?}{1702}{section*.3810}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Objectives in Mathematical Form.}{1702}{section*.3811}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Training Framework: End-to-End Chronology (CapFilt $\rightarrow $ Final BLIP)}{1703}{section*.3812}\protected@file@percent }
\abx@aux@cite{0}{li2022_blip}
\abx@aux@segm{0}{0}{li2022_blip}
\abx@aux@cite{0}{li2022_blip}
\abx@aux@segm{0}{0}{li2022_blip}
\abx@aux@cite{0}{li2022_blip}
\abx@aux@segm{0}{0}{li2022_blip}
\abx@aux@cite{0}{li2022_blip}
\abx@aux@segm{0}{0}{li2022_blip}
\@writefile{lof}{\contentsline {figure}{\numberline {24.37}{\ignorespaces \textbf  {Learning framework.} Captioner and filter, both BLIP-initialized, bootstrap a cleaner dataset from noisy web supervision. \emph  {Source:} \blx@tocontentsinit {0}\cite {li2022_blip}.}}{1704}{figure.caption.3813}\protected@file@percent }
\abx@aux@backref{2694}{li2022_blip}{0}{1704}{1704}
\newlabel{fig:chpapter24_blip_training_framework}{{24.37}{1704}{\textbf {Learning framework.} Captioner and filter, both BLIP-initialized, bootstrap a cleaner dataset from noisy web supervision. \emph {Source:} \cite {li2022_blip}}{figure.caption.3813}{}}
\@writefile{toc}{\contentsline {paragraph}{Downstream Usage}{1704}{section*.3814}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24.38}{\ignorespaces \textbf  {Downstream heads.} BLIP routes through ITC, ITM, or LM heads depending on the task. \emph  {Source:} \blx@tocontentsinit {0}\cite {li2022_blip}.}}{1704}{figure.caption.3815}\protected@file@percent }
\abx@aux@backref{2696}{li2022_blip}{0}{1704}{1704}
\newlabel{fig:chpapter24_blip_downstream}{{24.38}{1704}{\textbf {Downstream heads.} BLIP routes through ITC, ITM, or LM heads depending on the task. \emph {Source:} \cite {li2022_blip}}{figure.caption.3815}{}}
\@writefile{toc}{\contentsline {subsubsection}{Experiments and Ablations}{1705}{section*.3816}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{CapFilt Effectiveness}{1705}{section*.3817}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Ablations}{1705}{section*.3818}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Limitations and Future Work}{1705}{section*.3819}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Observed Constraints}{1705}{section*.3820}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Toward BLIP-2}{1705}{section*.3821}\protected@file@percent }
\BKM@entry{id=955,dest={73656374696F6E2A2E33383232},srcline={1009}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030345C3030302E5C303030355C3030302E5C303030335C3030303A5C3030305C3034305C303030425C3030304C5C303030495C303030505C3030302D5C303030325C3030303A5C3030305C3034305C303030425C303030725C303030695C303030645C303030675C303030695C3030306E5C303030675C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030455C3030306E5C303030635C3030306F5C303030645C303030655C303030725C303030735C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C3030304C5C3030304C5C3030304D5C303030735C3030305C3034305C303030765C303030695C303030615C3030305C3034305C303030515C3030302D5C303030465C3030306F5C303030725C3030306D5C303030655C30303072}
\abx@aux@cite{0}{li2023_blip2}
\abx@aux@segm{0}{0}{li2023_blip2}
\abx@aux@cite{0}{li2023_blip2}
\abx@aux@segm{0}{0}{li2023_blip2}
\abx@aux@cite{0}{li2023_blip2}
\abx@aux@segm{0}{0}{li2023_blip2}
\@writefile{toc}{\contentsline {subsection}{Enrichment 24.5.3: BLIP-2: Bridging Vision Encoders and LLMs via Q-Former}{1706}{section*.3822}\protected@file@percent }
\newlabel{enr:subsec_chapter24_blip2}{{24.5.3}{1706}{\color {ocre}Enrichment \thesubsection : BLIP-2: Bridging Vision Encoders and LLMs via Q-Former}{section*.3822}{}}
\@writefile{toc}{\contentsline {paragraph}{High-Level Idea}{1706}{section*.3823}\protected@file@percent }
\abx@aux@backref{2697}{li2023_blip2}{0}{1706}{1706}
\@writefile{lof}{\contentsline {figure}{\numberline {24.39}{\ignorespaces  \textbf  {BLIP-2 framework (frozen experts + lightweight bridge).} A \emph  {frozen} image encoder outputs dense visual tokens. A \textbf  {Q-Former} (trainable) with $K$ learnable query tokens attends to these tokens and produces $K$ query features. A linear adapter maps them to the LLM’s embedding space and feeds a \emph  {frozen} LLM for image-grounded generation. Training proceeds in two stages: (1) representation learning with a frozen vision encoder; (2) vision-to-language generation with a frozen LLM. Source: ~\blx@tocontentsinit {0}\cite {li2023_blip2}. }}{1706}{figure.caption.3824}\protected@file@percent }
\abx@aux@backref{2699}{li2023_blip2}{0}{1706}{1706}
\newlabel{fig:chpapter24_blip2_overview}{{24.39}{1706}{\textbf {BLIP-2 framework (frozen experts + lightweight bridge).} A \emph {frozen} image encoder outputs dense visual tokens. A \textbf {Q-Former} (trainable) with $K$ learnable query tokens attends to these tokens and produces $K$ query features. A linear adapter maps them to the LLM’s embedding space and feeds a \emph {frozen} LLM for image-grounded generation. Training proceeds in two stages: (1) representation learning with a frozen vision encoder; (2) vision-to-language generation with a frozen LLM. Source: ~\cite {li2023_blip2}}{figure.caption.3824}{}}
\abx@aux@cite{0}{li2023_blip2}
\abx@aux@segm{0}{0}{li2023_blip2}
\abx@aux@cite{0}{li2023_blip2}
\abx@aux@segm{0}{0}{li2023_blip2}
\abx@aux@cite{0}{li2023_blip2}
\abx@aux@segm{0}{0}{li2023_blip2}
\abx@aux@cite{0}{li2023_blip2}
\abx@aux@segm{0}{0}{li2023_blip2}
\@writefile{toc}{\contentsline {subsubsection}{Method: A Small Q-Former Bridging Two Frozen Experts}{1707}{section*.3825}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Stage~1: Vision--Language representation with a frozen image encoder}{1707}{section*.3826}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Stage~2: Vision-to-language generation with a frozen LLM}{1707}{section*.3827}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24.40}{\ignorespaces  \textbf  {Q-Former and Stage~1 objectives.} A small Transformer holds $K$ \emph  {learnable} queries (Q) which \emph  {self-attend} and \emph  {cross-attend} to frozen image features. Joint optimization: (i) \textbf  {ITC} for global alignment (comparable Q/text embeddings), (ii) \textbf  {ITM} for pair-level grounding (match vs.\ non-match), (iii) \textbf  {Image-grounded LM pretraining} to condition text on Q under causal constraints. These losses teach Q to extract visual information most relevant to the text. Source:\blx@tocontentsinit {0}\cite {li2023_blip2}. }}{1707}{figure.caption.3828}\protected@file@percent }
\abx@aux@backref{2701}{li2023_blip2}{0}{1707}{1707}
\newlabel{fig:chpapter24_blip2_qformer}{{24.40}{1707}{\textbf {Q-Former and Stage~1 objectives.} A small Transformer holds $K$ \emph {learnable} queries (Q) which \emph {self-attend} and \emph {cross-attend} to frozen image features. Joint optimization: (i) \textbf {ITC} for global alignment (comparable Q/text embeddings), (ii) \textbf {ITM} for pair-level grounding (match vs.\ non-match), (iii) \textbf {Image-grounded LM pretraining} to condition text on Q under causal constraints. These losses teach Q to extract visual information most relevant to the text. Source:\cite {li2023_blip2}}{figure.caption.3828}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24.41}{\ignorespaces  \textbf  {How attention masks steer Q--Text interaction in BLIP-2’s Q-Former (Stage~1).} A fixed set of learnable \emph  {query} tokens (Q) reads frozen ViT features and interacts with \emph  {text} tokens (T) under three masks: (i) \emph  {Uni-modal (ITC):} Q attends only to Q and T only to T, producing independent visual/text embeddings for contrastive alignment. (ii) \emph  {Bi-directional (ITM):} Q and T fully attend to each other to form a fused representation for fine-grained match classification. (iii) \emph  {Multimodal causal (image-grounded generation):} T attends to all Q and only past T (causal), while Q remains fully visible to itself, forcing the visual evidence to pass through the Q bottleneck and enabling autoregressive text generation. Source:\blx@tocontentsinit {0}\cite {li2023_blip2}.}}{1708}{figure.caption.3829}\protected@file@percent }
\abx@aux@backref{2703}{li2023_blip2}{0}{1708}{1708}
\newlabel{fig:chpapter24_blip2_masks}{{24.41}{1708}{\textbf {How attention masks steer Q--Text interaction in BLIP-2’s Q-Former (Stage~1).} A fixed set of learnable \emph {query} tokens (Q) reads frozen ViT features and interacts with \emph {text} tokens (T) under three masks: (i) \emph {Uni-modal (ITC):} Q attends only to Q and T only to T, producing independent visual/text embeddings for contrastive alignment. (ii) \emph {Bi-directional (ITM):} Q and T fully attend to each other to form a fused representation for fine-grained match classification. (iii) \emph {Multimodal causal (image-grounded generation):} T attends to all Q and only past T (causal), while Q remains fully visible to itself, forcing the visual evidence to pass through the Q bottleneck and enabling autoregressive text generation. Source:\cite {li2023_blip2}}{figure.caption.3829}{}}
\@writefile{toc}{\contentsline {paragraph}{Two-Stage Curriculum: What Trains When and Why}{1708}{section*.3830}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Objectives (concise math + intuition)}{1708}{section*.3831}\protected@file@percent }
\abx@aux@cite{0}{li2023_blip2}
\abx@aux@segm{0}{0}{li2023_blip2}
\abx@aux@cite{0}{li2023_blip2}
\abx@aux@segm{0}{0}{li2023_blip2}
\abx@aux@cite{0}{li2023_blip2}
\abx@aux@segm{0}{0}{li2023_blip2}
\abx@aux@cite{0}{li2023_blip2}
\abx@aux@segm{0}{0}{li2023_blip2}
\@writefile{toc}{\contentsline {paragraph}{How the pieces fit during training}{1709}{section*.3832}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24.42}{\ignorespaces  \textbf  {Stage~2: vision-to-language bootstrapping with frozen LLMs.} Top: decoder-only LLM (e.g., OPT). Bottom: encoder--decoder LLM (e.g., FlanT5). A linear adapter maps Q-Former outputs to the LLM’s embedding space. Only the \emph  {Q-Former and the adapter} are trained; both the vision encoder and LLM remain \emph  {frozen}. Source:\blx@tocontentsinit {0}\cite {li2023_blip2}. }}{1709}{figure.caption.3833}\protected@file@percent }
\abx@aux@backref{2705}{li2023_blip2}{0}{1709}{1709}
\newlabel{fig:chpapter24_blip2_stage2}{{24.42}{1709}{\textbf {Stage~2: vision-to-language bootstrapping with frozen LLMs.} Top: decoder-only LLM (e.g., OPT). Bottom: encoder--decoder LLM (e.g., FlanT5). A linear adapter maps Q-Former outputs to the LLM’s embedding space. Only the \emph {Q-Former and the adapter} are trained; both the vision encoder and LLM remain \emph {frozen}. Source:\cite {li2023_blip2}}{figure.caption.3833}{}}
\abx@aux@cite{0}{li2023_blip2}
\abx@aux@segm{0}{0}{li2023_blip2}
\abx@aux@cite{0}{li2023_blip2}
\abx@aux@segm{0}{0}{li2023_blip2}
\abx@aux@cite{0}{li2022_blip}
\abx@aux@segm{0}{0}{li2022_blip}
\abx@aux@cite{0}{wang2021b_simvlm}
\abx@aux@segm{0}{0}{wang2021b_simvlm}
\abx@aux@cite{0}{wang2022_beit3}
\abx@aux@segm{0}{0}{wang2022_beit3}
\abx@aux@cite{0}{flamingo2022_fewshot}
\abx@aux@segm{0}{0}{flamingo2022_fewshot}
\@writefile{lof}{\contentsline {figure}{\numberline {24.43}{\ignorespaces  \textbf  {Zero-shot instructed image-to-text.} With a frozen LLM and a trained Q-Former bridge, BLIP-2 exhibits visual dialogue, knowledge/commonsense grounded by images, storytelling, and personalization without full LLM fine-tuning. Source:\blx@tocontentsinit {0}\cite {li2023_blip2}. }}{1710}{figure.caption.3834}\protected@file@percent }
\abx@aux@backref{2707}{li2023_blip2}{0}{1710}{1710}
\newlabel{fig:chpapter24_blip2_examples}{{24.43}{1710}{\textbf {Zero-shot instructed image-to-text.} With a frozen LLM and a trained Q-Former bridge, BLIP-2 exhibits visual dialogue, knowledge/commonsense grounded by images, storytelling, and personalization without full LLM fine-tuning. Source:\cite {li2023_blip2}}{figure.caption.3834}{}}
\abx@aux@cite{0}{li2023_blip2}
\abx@aux@segm{0}{0}{li2023_blip2}
\abx@aux@cite{0}{li2023_blip2}
\abx@aux@segm{0}{0}{li2023_blip2}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\abx@aux@cite{0}{jia2021_align}
\abx@aux@segm{0}{0}{jia2021_align}
\abx@aux@cite{0}{yao2022_filip}
\abx@aux@segm{0}{0}{yao2022_filip}
\abx@aux@cite{0}{yuan2021_florence}
\abx@aux@segm{0}{0}{yuan2021_florence}
\abx@aux@cite{0}{wang2022_beit3}
\abx@aux@segm{0}{0}{wang2022_beit3}
\abx@aux@cite{0}{chen2020_uniter}
\abx@aux@segm{0}{0}{chen2020_uniter}
\abx@aux@cite{0}{li2020_oscar}
\abx@aux@segm{0}{0}{li2020_oscar}
\abx@aux@cite{0}{zhang2021_vinvl}
\abx@aux@segm{0}{0}{zhang2021_vinvl}
\abx@aux@cite{0}{li2021_albef}
\abx@aux@segm{0}{0}{li2021_albef}
\abx@aux@cite{0}{li2022_blip}
\abx@aux@segm{0}{0}{li2022_blip}
\@writefile{lot}{\contentsline {table}{\numberline {24.1}{\ignorespaces Overview of BLIP-2 results on various zero-shot vision--language tasks, compared with prior SOTA. Higher is better. Source:\blx@tocontentsinit {0}\cite {li2023_blip2}.}}{1711}{table.caption.3835}\protected@file@percent }
\abx@aux@backref{2709}{li2023_blip2}{0}{1711}{1711}
\newlabel{tab:blip2_overview}{{24.1}{1711}{Overview of BLIP-2 results on various zero-shot vision--language tasks, compared with prior SOTA. Higher is better. Source:\cite {li2023_blip2}}{table.caption.3835}{}}
\abx@aux@backref{2710}{li2022_blip}{0}{1711}{1711}
\abx@aux@backref{2711}{wang2021b_simvlm}{0}{1711}{1711}
\abx@aux@backref{2712}{wang2022_beit3}{0}{1711}{1711}
\abx@aux@backref{2713}{flamingo2022_fewshot}{0}{1711}{1711}
\@writefile{lot}{\contentsline {table}{\numberline {24.2}{\ignorespaces Comparison with SOTA image--text retrieval methods. Left: Flickr30K zero-shot (1K test). Right: COCO finetuned (5K test). R@K reported (\%). Source:\blx@tocontentsinit {0}\cite {li2023_blip2}.}}{1711}{table.caption.3836}\protected@file@percent }
\abx@aux@backref{2715}{li2023_blip2}{0}{1711}{1711}
\newlabel{tab:blip2_retrieval}{{24.2}{1711}{Comparison with SOTA image--text retrieval methods. Left: Flickr30K zero-shot (1K test). Right: COCO finetuned (5K test). R@K reported (\%). Source:\cite {li2023_blip2}}{table.caption.3836}{}}
\abx@aux@backref{2716}{radford2021_clip}{0}{1711}{1711}
\abx@aux@backref{2717}{jia2021_align}{0}{1711}{1711}
\abx@aux@backref{2718}{yao2022_filip}{0}{1711}{1711}
\abx@aux@backref{2719}{yuan2021_florence}{0}{1711}{1711}
\abx@aux@backref{2720}{wang2022_beit3}{0}{1711}{1711}
\abx@aux@backref{2721}{chen2020_uniter}{0}{1711}{1711}
\abx@aux@backref{2722}{li2020_oscar}{0}{1711}{1711}
\abx@aux@backref{2723}{zhang2021_vinvl}{0}{1711}{1711}
\abx@aux@backref{2724}{li2021_albef}{0}{1711}{1711}
\abx@aux@backref{2725}{li2022_blip}{0}{1711}{1711}
\@writefile{toc}{\contentsline {subsubsection}{Experiments \& Ablations (Concise)}{1711}{section*.3837}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Limitations \& Future Work}{1712}{section*.3838}\protected@file@percent }
\BKM@entry{id=956,dest={73656374696F6E2A2E33383339},srcline={1247}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030345C3030302E5C303030355C3030302E5C303030345C3030303A5C3030305C3034305C303030535C303030695C303030675C3030304C5C303030495C303030505C3030305C3034305C303030325C3030303A5C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C3030306C5C303030695C3030306E5C303030675C303030755C303030615C3030306C5C3030305C3034305C3030305C3034365C3030305C3034305C303030445C303030655C3030306E5C303030735C303030655C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3034305C3032335C3030304C5C303030615C3030306E5C303030675C303030755C303030615C303030675C303030655C3030305C3034305C303030455C3030306E5C303030635C3030306F5C303030645C303030695C3030306E5C30303067}
\abx@aux@cite{0}{zhai2023_siglip}
\abx@aux@segm{0}{0}{zhai2023_siglip}
\abx@aux@cite{0}{wan2024_locca}
\abx@aux@segm{0}{0}{wan2024_locca}
\abx@aux@cite{0}{naeem2023_silc}
\abx@aux@segm{0}{0}{naeem2023_silc}
\abx@aux@cite{0}{maninis2025_tips}
\abx@aux@segm{0}{0}{maninis2025_tips}
\abx@aux@cite{0}{dehghani2023_navit}
\abx@aux@segm{0}{0}{dehghani2023_navit}
\abx@aux@cite{0}{beyer2023_flexivit}
\abx@aux@segm{0}{0}{beyer2023_flexivit}
\abx@aux@cite{0}{zhai2023_siglip}
\abx@aux@segm{0}{0}{zhai2023_siglip}
\abx@aux@cite{0}{wan2024_locca}
\abx@aux@segm{0}{0}{wan2024_locca}
\abx@aux@cite{0}{naeem2023_silc}
\abx@aux@segm{0}{0}{naeem2023_silc}
\abx@aux@cite{0}{maninis2025_tips}
\abx@aux@segm{0}{0}{maninis2025_tips}
\abx@aux@cite{0}{dehghani2023_navit}
\abx@aux@segm{0}{0}{dehghani2023_navit}
\abx@aux@cite{0}{beyer2023_flexivit}
\abx@aux@segm{0}{0}{beyer2023_flexivit}
\abx@aux@cite{0}{udandarao2025_acid}
\abx@aux@segm{0}{0}{udandarao2025_acid}
\abx@aux@cite{0}{zhai2023_siglip}
\abx@aux@segm{0}{0}{zhai2023_siglip}
\abx@aux@cite{0}{wan2024_locca}
\abx@aux@segm{0}{0}{wan2024_locca}
\abx@aux@cite{0}{naeem2023_silc}
\abx@aux@segm{0}{0}{naeem2023_silc}
\abx@aux@cite{0}{maninis2025_tips}
\abx@aux@segm{0}{0}{maninis2025_tips}
\abx@aux@cite{0}{dehghani2023_navit}
\abx@aux@segm{0}{0}{dehghani2023_navit}
\abx@aux@cite{0}{beyer2023_flexivit}
\abx@aux@segm{0}{0}{beyer2023_flexivit}
\abx@aux@cite{0}{udandarao2025_acid}
\abx@aux@segm{0}{0}{udandarao2025_acid}
\@writefile{toc}{\contentsline {subsection}{Enrichment 24.5.4: SigLIP~2: Multilingual \& Dense Vision--Language Encoding}{1713}{section*.3839}\protected@file@percent }
\newlabel{enr:subsec_chapter24_siglip2}{{24.5.4}{1713}{\color {ocre}Enrichment \thesubsection : SigLIP~2: Multilingual \& Dense Vision--Language Encoding}{section*.3839}{}}
\@writefile{toc}{\contentsline {paragraph}{High-Level Overview}{1713}{section*.3840}\protected@file@percent }
\abx@aux@backref{2726}{zhai2023_siglip}{0}{1713}{1713}
\abx@aux@backref{2727}{wan2024_locca}{0}{1713}{1713}
\abx@aux@backref{2728}{maninis2025_tips}{0}{1713}{1713}
\abx@aux@backref{2729}{naeem2023_silc}{0}{1713}{1713}
\abx@aux@backref{2730}{beyer2023_flexivit}{0}{1713}{1713}
\abx@aux@backref{2731}{dehghani2023_navit}{0}{1713}{1713}
\@writefile{lof}{\contentsline {figure}{\numberline {24.44}{\ignorespaces \textbf  {SigLIP~2 training recipe (conceptual).} Starting from SigLIP’s pairwise sigmoid alignment~\blx@tocontentsinit {0}\cite {zhai2023_siglip}, pretraining adds (i) a lightweight decoder to inject localization/grounding supervision (captioning, grounded captioning, referring expressions) that shapes patch features but is \emph  {dropped at test time}~\blx@tocontentsinit {0}\cite {wan2024_locca}; (ii) a late \emph  {consistency tail} where an EMA teacher provides full-image targets for student crops and masked patches, improving global--local agreement and contextual completion~\blx@tocontentsinit {0}\cite {naeem2023_silc,maninis2025_tips}; and (iii) resolution/aspect adaptations, either via size-specific continuations or a single \emph  {NaFlex} checkpoint that supports multiple grids and native aspect ratios~\blx@tocontentsinit {0}\cite {dehghani2023_navit,beyer2023_flexivit}. Optional curated fine-tunes further boost compact models~\blx@tocontentsinit {0}\cite {udandarao2025_acid}. \emph  {Courtesy: SigLIP~2 authors.}}}{1713}{figure.caption.3841}\protected@file@percent }
\abx@aux@backref{2739}{zhai2023_siglip}{0}{1713}{1713}
\abx@aux@backref{2740}{wan2024_locca}{0}{1713}{1713}
\abx@aux@backref{2741}{maninis2025_tips}{0}{1713}{1713}
\abx@aux@backref{2742}{naeem2023_silc}{0}{1713}{1713}
\abx@aux@backref{2743}{beyer2023_flexivit}{0}{1713}{1713}
\abx@aux@backref{2744}{dehghani2023_navit}{0}{1713}{1713}
\abx@aux@backref{2745}{udandarao2025_acid}{0}{1713}{1713}
\newlabel{fig:chpapter24_siglip2_overview}{{24.44}{1713}{\textbf {SigLIP~2 training recipe (conceptual).} Starting from SigLIP’s pairwise sigmoid alignment~\cite {zhai2023_siglip}, pretraining adds (i) a lightweight decoder to inject localization/grounding supervision (captioning, grounded captioning, referring expressions) that shapes patch features but is \emph {dropped at test time}~\cite {wan2024_locca}; (ii) a late \emph {consistency tail} where an EMA teacher provides full-image targets for student crops and masked patches, improving global--local agreement and contextual completion~\cite {naeem2023_silc,maninis2025_tips}; and (iii) resolution/aspect adaptations, either via size-specific continuations or a single \emph {NaFlex} checkpoint that supports multiple grids and native aspect ratios~\cite {dehghani2023_navit,beyer2023_flexivit}. Optional curated fine-tunes further boost compact models~\cite {udandarao2025_acid}. \emph {Courtesy: SigLIP~2 authors.}}{figure.caption.3841}{}}
\abx@aux@cite{0}{zhai2023_siglip}
\abx@aux@segm{0}{0}{zhai2023_siglip}
\@writefile{toc}{\contentsline {subsubsection}{Foundational Reminder: How Sigmoid Loss (SigLIP) Works}{1714}{section*.3842}\protected@file@percent }
\newlabel{subsubsec:siglip2_foundation}{{24.5.4}{1714}{Foundational Reminder: How Sigmoid Loss (SigLIP) Works}{section*.3842}{}}
\abx@aux@backref{2746}{zhai2023_siglip}{0}{1714}{1714}
\@writefile{toc}{\contentsline {paragraph}{Compact formulation (per step)}{1714}{section*.3843}\protected@file@percent }
\abx@aux@cite{0}{wan2024_locca}
\abx@aux@segm{0}{0}{wan2024_locca}
\abx@aux@cite{0}{naeem2023_silc}
\abx@aux@segm{0}{0}{naeem2023_silc}
\abx@aux@cite{0}{maninis2025_tips}
\abx@aux@segm{0}{0}{maninis2025_tips}
\@writefile{toc}{\contentsline {subsubsection}{Method: A Staged Curriculum that Teaches \emph  {Where}, \emph  {Detail}, and \emph  {Robustness}}{1715}{section*.3844}\protected@file@percent }
\newlabel{subsubsec:siglip2_method}{{24.5.4}{1715}{Method: A Staged Curriculum that Teaches \emph {Where}, \emph {Detail}, and \emph {Robustness}}{section*.3844}{}}
\@writefile{toc}{\contentsline {paragraph}{Stage layout (flow first).}{1715}{section*.3845}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Decoder for captioning and grounding (LocCa-style)}{1715}{section*.3846}\protected@file@percent }
\abx@aux@backref{2747}{wan2024_locca}{0}{1715}{1715}
\@writefile{toc}{\contentsline {paragraph}{Late self-distillation and masked prediction (SILC/TIPS-style)}{1715}{section*.3847}\protected@file@percent }
\abx@aux@backref{2748}{maninis2025_tips}{0}{1715}{1715}
\abx@aux@backref{2749}{naeem2023_silc}{0}{1715}{1715}
\abx@aux@cite{0}{dehghani2023_navit}
\abx@aux@segm{0}{0}{dehghani2023_navit}
\abx@aux@cite{0}{beyer2023_flexivit}
\abx@aux@segm{0}{0}{beyer2023_flexivit}
\abx@aux@cite{0}{udandarao2025_acid}
\abx@aux@segm{0}{0}{udandarao2025_acid}
\@writefile{toc}{\contentsline {paragraph}{Resolution and aspect-ratio adaptation}{1716}{section*.3848}\protected@file@percent }
\abx@aux@backref{2750}{beyer2023_flexivit}{0}{1716}{1716}
\abx@aux@backref{2751}{dehghani2023_navit}{0}{1716}{1716}
\@writefile{toc}{\contentsline {paragraph}{Curation-focused fine-tuning for small models}{1716}{section*.3849}\protected@file@percent }
\abx@aux@backref{2752}{udandarao2025_acid}{0}{1716}{1716}
\@writefile{toc}{\contentsline {paragraph}{Multilingual training mix}{1716}{section*.3850}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why these additions work (unifying intuition)}{1716}{section*.3851}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Experiments and Ablations (Concise)}{1717}{section*.3852}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{What we learn (vs.\ SigLIP/BLIP/BLIP-2) \& which to choose}{1717}{section*.3853}\protected@file@percent }
\BKM@entry{id=957,dest={73656374696F6E2A2E33383534},srcline={1400}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030345C3030302E5C303030365C3030303A5C3030305C3034305C303030535C303030655C3030306C5C303030665C3030302D5C303030535C303030755C303030705C303030655C303030725C303030765C303030695C303030735C303030655C303030645C3030305C3034305C303030565C303030695C303030645C303030655C3030306F5C3030305C3034305C303030505C303030725C303030655C303030745C303030725C303030615C303030695C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030565C3030304C5C3030304C5C3030304D5C30303073}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{wang2023_videomaev2}
\abx@aux@segm{0}{0}{wang2023_videomaev2}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\BKM@entry{id=958,dest={73656374696F6E2A2E33383535},srcline={1403}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030345C3030302E5C303030365C3030302E5C303030315C3030303A5C3030305C3034305C303030565C303030695C303030645C303030655C3030306F5C3030304D5C303030415C303030455C3030303A5C3030305C3034305C3030304D5C303030615C303030735C3030306B5C303030655C303030645C3030305C3034305C303030415C303030755C303030745C3030306F5C303030655C3030306E5C303030635C3030306F5C303030645C303030655C303030725C303030735C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030565C303030695C303030645C303030655C3030306F5C3030305C3034305C303030535C303030535C3030304C}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\@writefile{toc}{\contentsline {section}{Enrichment 24.6: Self-Supervised Video Pretraining for VLLMs}{1718}{section*.3854}\protected@file@percent }
\abx@aux@backref{2753}{tong2022_videomae}{0}{1718}{1718}
\abx@aux@backref{2754}{wang2023_videomaev2}{0}{1718}{1718}
\abx@aux@backref{2755}{wang2023_mvd}{0}{1718}{1718}
\@writefile{toc}{\contentsline {subsection}{Enrichment 24.6.1: VideoMAE: Masked Autoencoders for Video SSL}{1718}{section*.3855}\protected@file@percent }
\newlabel{enr:subsec_chapter24_videomae}{{24.6.1}{1718}{\color {ocre}Enrichment \thesubsection : VideoMAE: Masked Autoencoders for Video SSL}{section*.3855}{}}
\@writefile{toc}{\contentsline {paragraph}{Scope and positioning}{1718}{section*.3856}\protected@file@percent }
\abx@aux@backref{2756}{tong2022_videomae}{0}{1718}{1718}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{1718}{section*.3857}\protected@file@percent }
\newlabel{subsubsec_chapter24_videomae_motivation}{{24.6.1}{1718}{Motivation}{section*.3857}{}}
\@writefile{toc}{\contentsline {paragraph}{Why masked autoencoding for video}{1718}{section*.3858}\protected@file@percent }
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\@writefile{lof}{\contentsline {figure}{\numberline {24.45}{\ignorespaces \textbf  {Overview of VideoMAE}. VideoMAE masks random spatio\-temporal cubes and reconstructs them with an asymmetric encoder–decoder. Owing to high redundancy and temporal correlation, the authors introduce \emph  {tube masking} with an extremely high ratio (90–95\%), which yields a harder and more meaningful self-supervised task and drives the encoder to capture useful spatiotemporal structure \blx@tocontentsinit {0}\cite {tong2022_videomae}.}}{1719}{figure.caption.3859}\protected@file@percent }
\abx@aux@backref{2758}{tong2022_videomae}{0}{1719}{1719}
\newlabel{fig:chapter24_videomae_overview}{{24.45}{1719}{\textbf {Overview of VideoMAE}. VideoMAE masks random spatio\-temporal cubes and reconstructs them with an asymmetric encoder–decoder. Owing to high redundancy and temporal correlation, the authors introduce \emph {tube masking} with an extremely high ratio (90–95\%), which yields a harder and more meaningful self-supervised task and drives the encoder to capture useful spatiotemporal structure \cite {tong2022_videomae}}{figure.caption.3859}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24.46}{\ignorespaces \textbf  {Tube masking vs.\ alternatives}. (a) Slowness induces temporal redundancy and correlation. (b) Frame masking and (c) random masking risk information leakage by leaving correlated duplicates unmasked. (d) Tube masking enforces the same spatial mask for all frames, removing easy copies and promoting representative spatiotemporal learning \blx@tocontentsinit {0}\cite {tong2022_videomae}.}}{1719}{figure.caption.3860}\protected@file@percent }
\abx@aux@backref{2760}{tong2022_videomae}{0}{1719}{1719}
\newlabel{fig:chapter24_videomae_tube}{{24.46}{1719}{\textbf {Tube masking vs.\ alternatives}. (a) Slowness induces temporal redundancy and correlation. (b) Frame masking and (c) random masking risk information leakage by leaving correlated duplicates unmasked. (d) Tube masking enforces the same spatial mask for all frames, removing easy copies and promoting representative spatiotemporal learning \cite {tong2022_videomae}}{figure.caption.3860}{}}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\@writefile{toc}{\contentsline {subsubsection}{Method}{1720}{section*.3861}\protected@file@percent }
\newlabel{subsubsec_chapter24_videomae_method}{{24.6.1}{1720}{Method}{section*.3861}{}}
\@writefile{toc}{\contentsline {paragraph}{Preliminaries and notation}{1720}{section*.3862}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Tube masking with extremely high ratios}{1720}{section*.3863}\protected@file@percent }
\newlabel{eq:chapter24_videomae_tube}{{24.12}{1720}{Tube masking with extremely high ratios}{equation.24.12}{}}
\abx@aux@backref{2761}{tong2022_videomae}{0}{1720}{1720}
\@writefile{toc}{\contentsline {paragraph}{Asymmetric encoder–decoder}{1720}{section*.3864}\protected@file@percent }
\abx@aux@backref{2762}{tong2022_videomae}{0}{1720}{1720}
\@writefile{toc}{\contentsline {paragraph}{Reconstruction objective on masked cubes}{1720}{section*.3865}\protected@file@percent }
\newlabel{eq:chapter24_videomae_loss}{{24.13}{1720}{Reconstruction objective on masked cubes}{equation.24.13}{}}
\abx@aux@backref{2763}{tong2022_videomae}{0}{1720}{1720}
\@writefile{toc}{\contentsline {paragraph}{Design choices justified}{1720}{section*.3866}\protected@file@percent }
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\@writefile{toc}{\contentsline {paragraph}{Algorithmic flow (pseudo code)}{1721}{section*.3867}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Architecture, Training, and Datasets}{1721}{section*.3868}\protected@file@percent }
\newlabel{subsubsec_chapter24_videomae_arch}{{24.6.1}{1721}{Architecture, Training, and Datasets}{section*.3868}{}}
\@writefile{toc}{\contentsline {paragraph}{Backbone and attention}{1721}{section*.3869}\protected@file@percent }
\abx@aux@backref{2764}{tong2022_videomae}{0}{1721}{1721}
\@writefile{toc}{\contentsline {paragraph}{Training setup}{1721}{section*.3870}\protected@file@percent }
\abx@aux@backref{2765}{tong2022_videomae}{0}{1721}{1721}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\@writefile{toc}{\contentsline {paragraph}{Datasets used in experiments and ablations}{1722}{section*.3871}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Experiments}{1722}{section*.3872}\protected@file@percent }
\newlabel{subsubsec_chapter24_videomae_expts}{{24.6.1}{1722}{Experiments}{section*.3872}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24.47}{\ignorespaces \textbf  {Effect of masking ratio}. With 16-frame ViT-B, SSV2 and K400 peak around $\rho {=}90\%$; $\rho {>}95\%$ hurts as context becomes too sparse \blx@tocontentsinit {0}\cite {tong2022_videomae}.}}{1722}{figure.caption.3873}\protected@file@percent }
\abx@aux@backref{2767}{tong2022_videomae}{0}{1722}{1722}
\newlabel{fig:chapter24_videomae_maskratio}{{24.47}{1722}{\textbf {Effect of masking ratio}. With 16-frame ViT-B, SSV2 and K400 peak around $\rho {=}90\%$; $\rho {>}95\%$ hurts as context becomes too sparse \cite {tong2022_videomae}}{figure.caption.3873}{}}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\@writefile{lof}{\contentsline {figure}{\numberline {24.48}{\ignorespaces \textbf  {Data efficiency on SSV2}. Fixed-iteration pre-training (green) at $132$k steps outperforms fixed-epoch pre-training (blue) when using subsets; notably, $25\%$ SSV2 with more iterations surpasses a K400-pretrained baseline, highlighting the value of domain-matched data \blx@tocontentsinit {0}\cite {tong2022_videomae}.}}{1723}{figure.caption.3874}\protected@file@percent }
\abx@aux@backref{2769}{tong2022_videomae}{0}{1723}{1723}
\newlabel{fig:chapter24_videomae_dataeff}{{24.48}{1723}{\textbf {Data efficiency on SSV2}. Fixed-iteration pre-training (green) at $132$k steps outperforms fixed-epoch pre-training (blue) when using subsets; notably, $25\%$ SSV2 with more iterations surpasses a K400-pretrained baseline, highlighting the value of domain-matched data \cite {tong2022_videomae}}{figure.caption.3874}{}}
\@writefile{toc}{\contentsline {subsubsection}{Ablations}{1723}{section*.3875}\protected@file@percent }
\newlabel{subsubsec_chapter24_videomae_ablations}{{24.6.1}{1723}{Ablations}{section*.3875}{}}
\@writefile{lot}{\contentsline {table}{\numberline {24.3}{\ignorespaces Table 1(a). Decoder depth on SSV2/K400 with 16-frame ViT-B (reproduced from \blx@tocontentsinit {0}\cite {tong2022_videomae}).}}{1723}{table.caption.3876}\protected@file@percent }
\abx@aux@backref{2771}{tong2022_videomae}{0}{1723}{1723}
\newlabel{tab:videomae_tbl1a}{{24.3}{1723}{Table 1(a). Decoder depth on SSV2/K400 with 16-frame ViT-B (reproduced from \cite {tong2022_videomae})}{table.caption.3876}{}}
\@writefile{lot}{\contentsline {table}{\numberline {24.4}{\ignorespaces Table 1(b). Mask sampling on SSV2/K400 (16-frame ViT-B) (reproduced from \blx@tocontentsinit {0}\cite {tong2022_videomae}). $^\ast $Frame masking hides $14/16$ frames on SSV2.}}{1723}{table.caption.3877}\protected@file@percent }
\abx@aux@backref{2773}{tong2022_videomae}{0}{1723}{1723}
\newlabel{tab:videomae_tbl1b}{{24.4}{1723}{Table 1(b). Mask sampling on SSV2/K400 (16-frame ViT-B) (reproduced from \cite {tong2022_videomae}). $^\ast $Frame masking hides $14/16$ frames on SSV2}{table.caption.3877}{}}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\@writefile{lot}{\contentsline {table}{\numberline {24.5}{\ignorespaces Table 1(c). Reconstruction target on SSV2/K400 (16-frame ViT-B) (reproduced from \blx@tocontentsinit {0}\cite {tong2022_videomae}).}}{1724}{table.caption.3878}\protected@file@percent }
\abx@aux@backref{2775}{tong2022_videomae}{0}{1724}{1724}
\newlabel{tab:videomae_tbl1c}{{24.5}{1724}{Table 1(c). Reconstruction target on SSV2/K400 (16-frame ViT-B) (reproduced from \cite {tong2022_videomae})}{table.caption.3878}{}}
\@writefile{lot}{\contentsline {table}{\numberline {24.6}{\ignorespaces Table 1(d). Pre-training strategy on SSV2/K400 (16-frame ViT-B) (reproduced from \blx@tocontentsinit {0}\cite {tong2022_videomae}).}}{1724}{table.caption.3879}\protected@file@percent }
\abx@aux@backref{2777}{tong2022_videomae}{0}{1724}{1724}
\newlabel{tab:videomae_tbl1d}{{24.6}{1724}{Table 1(d). Pre-training strategy on SSV2/K400 (16-frame ViT-B) (reproduced from \cite {tong2022_videomae})}{table.caption.3879}{}}
\@writefile{lot}{\contentsline {table}{\numberline {24.7}{\ignorespaces Table 1(e). Pre-training dataset comparison (16-frame ViT-B) (reproduced from \blx@tocontentsinit {0}\cite {tong2022_videomae}).}}{1724}{table.caption.3880}\protected@file@percent }
\abx@aux@backref{2779}{tong2022_videomae}{0}{1724}{1724}
\newlabel{tab:videomae_tbl1e}{{24.7}{1724}{Table 1(e). Pre-training dataset comparison (16-frame ViT-B) (reproduced from \cite {tong2022_videomae})}{table.caption.3880}{}}
\@writefile{lot}{\contentsline {table}{\numberline {24.8}{\ignorespaces Loss function on SSV2/K400 (16-frame ViT-B) (reproduced from \blx@tocontentsinit {0}\cite {tong2022_videomae}).}}{1724}{table.caption.3881}\protected@file@percent }
\abx@aux@backref{2781}{tong2022_videomae}{0}{1724}{1724}
\newlabel{tab:videomae_tbl1f}{{24.8}{1724}{Loss function on SSV2/K400 (16-frame ViT-B) (reproduced from \cite {tong2022_videomae})}{table.caption.3881}{}}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{feichtenhofer2019_slowfast}
\abx@aux@segm{0}{0}{feichtenhofer2019_slowfast}
\abx@aux@cite{0}{qian2021_cvrl}
\abx@aux@segm{0}{0}{qian2021_cvrl}
\abx@aux@cite{0}{feichtenhofer2021_r2plus1d_ssl}
\abx@aux@segm{0}{0}{feichtenhofer2021_r2plus1d_ssl}
\abx@aux@cite{0}{feichtenhofer2021_r2plus1d_ssl}
\abx@aux@segm{0}{0}{feichtenhofer2021_r2plus1d_ssl}
\abx@aux@cite{0}{wei2022_maskfeat}
\abx@aux@segm{0}{0}{wei2022_maskfeat}
\abx@aux@cite{0}{wei2022_maskfeat}
\abx@aux@segm{0}{0}{wei2022_maskfeat}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\@writefile{lot}{\contentsline {table}{\numberline {24.9}{\ignorespaces Comparison with prior self-supervised pre-training using 16-frame ViT-B and only unlabeled training splits (reproduced from \blx@tocontentsinit {0}\cite {tong2022_videomae}).}}{1725}{table.caption.3882}\protected@file@percent }
\abx@aux@backref{2783}{tong2022_videomae}{0}{1725}{1725}
\newlabel{tab:videomae_tbl2}{{24.9}{1725}{Comparison with prior self-supervised pre-training using 16-frame ViT-B and only unlabeled training splits (reproduced from \cite {tong2022_videomae})}{table.caption.3882}{}}
\@writefile{lot}{\contentsline {table}{\numberline {24.10}{\ignorespaces Pre-training efficiency on SSV2 with 16-frame ViT-B (64$\times $V100), reproduced from \blx@tocontentsinit {0}\cite {tong2022_videomae}.}}{1725}{table.caption.3883}\protected@file@percent }
\abx@aux@backref{2785}{tong2022_videomae}{0}{1725}{1725}
\newlabel{tab:videomae_tbl3}{{24.10}{1725}{Pre-training efficiency on SSV2 with 16-frame ViT-B (64$\times $V100), reproduced from \cite {tong2022_videomae}}{table.caption.3883}{}}
\@writefile{lot}{\contentsline {table}{\numberline {24.11}{\ignorespaces Feature transferability: pre-train on K400 (unlabeled), then fine-tune on target datasets (reproduced from \blx@tocontentsinit {0}\cite {tong2022_videomae}).}}{1725}{table.caption.3884}\protected@file@percent }
\abx@aux@backref{2787}{tong2022_videomae}{0}{1725}{1725}
\newlabel{tab:videomae_tbl4}{{24.11}{1725}{Feature transferability: pre-train on K400 (unlabeled), then fine-tune on target datasets (reproduced from \cite {tong2022_videomae})}{table.caption.3884}{}}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{li2021_teinet}
\abx@aux@segm{0}{0}{li2021_teinet}
\abx@aux@cite{0}{li2021_tanet}
\abx@aux@segm{0}{0}{li2021_tanet}
\abx@aux@cite{0}{wang2021_tdn}
\abx@aux@segm{0}{0}{wang2021_tdn}
\abx@aux@cite{0}{feichtenhofer2019_slowfast}
\abx@aux@segm{0}{0}{feichtenhofer2019_slowfast}
\abx@aux@cite{0}{fan2021_mvit}
\abx@aux@segm{0}{0}{fan2021_mvit}
\abx@aux@cite{0}{bertasius2021_timesformer}
\abx@aux@segm{0}{0}{bertasius2021_timesformer}
\abx@aux@cite{0}{bertasius2021_timesformer}
\abx@aux@segm{0}{0}{bertasius2021_timesformer}
\abx@aux@cite{0}{arnab2021_vivit}
\abx@aux@segm{0}{0}{arnab2021_vivit}
\abx@aux@cite{0}{patrick2021_motionformer}
\abx@aux@segm{0}{0}{patrick2021_motionformer}
\abx@aux@cite{0}{patrick2021_motionformer}
\abx@aux@segm{0}{0}{patrick2021_motionformer}
\abx@aux@cite{0}{liu2022_videoswin}
\abx@aux@segm{0}{0}{liu2022_videoswin}
\abx@aux@cite{0}{tan2021_vimpac}
\abx@aux@segm{0}{0}{tan2021_vimpac}
\abx@aux@cite{0}{wang2022_bevt}
\abx@aux@segm{0}{0}{wang2022_bevt}
\abx@aux@cite{0}{wei2022_maskfeat}
\abx@aux@segm{0}{0}{wei2022_maskfeat}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{wang2018_nonlocal_nn}
\abx@aux@segm{0}{0}{wang2018_nonlocal_nn}
\abx@aux@cite{0}{li2021_tanet}
\abx@aux@segm{0}{0}{li2021_tanet}
\abx@aux@cite{0}{wang2021_tdn}
\abx@aux@segm{0}{0}{wang2021_tdn}
\abx@aux@cite{0}{bertasius2021_timesformer}
\abx@aux@segm{0}{0}{bertasius2021_timesformer}
\abx@aux@cite{0}{arnab2021_vivit}
\abx@aux@segm{0}{0}{arnab2021_vivit}
\abx@aux@cite{0}{patrick2021_motionformer}
\abx@aux@segm{0}{0}{patrick2021_motionformer}
\abx@aux@cite{0}{liu2022_videoswin}
\abx@aux@segm{0}{0}{liu2022_videoswin}
\abx@aux@cite{0}{arnab2021_vivit}
\abx@aux@segm{0}{0}{arnab2021_vivit}
\abx@aux@cite{0}{arnab2021_vivit}
\abx@aux@segm{0}{0}{arnab2021_vivit}
\abx@aux@cite{0}{tan2021_vimpac}
\abx@aux@segm{0}{0}{tan2021_vimpac}
\abx@aux@cite{0}{wang2022_bevt}
\abx@aux@segm{0}{0}{wang2022_bevt}
\abx@aux@cite{0}{wei2022_maskfeat}
\abx@aux@segm{0}{0}{wei2022_maskfeat}
\abx@aux@cite{0}{tran2019_ipcsn}
\abx@aux@segm{0}{0}{tran2019_ipcsn}
\abx@aux@cite{0}{feichtenhofer2019_slowfast}
\abx@aux@segm{0}{0}{feichtenhofer2019_slowfast}
\abx@aux@cite{0}{fan2021_mvit}
\abx@aux@segm{0}{0}{fan2021_mvit}
\abx@aux@cite{0}{wei2022_maskfeat}
\abx@aux@segm{0}{0}{wei2022_maskfeat}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\@writefile{lot}{\contentsline {table}{\numberline {24.12}{\ignorespaces Comparison with the state of the art on AVA v2.2. All models are pre-trained and fine-tuned at image size $224^2$. We report validation mAP. “Ex.\ labels {\fontfamily  {pzd}\fontencoding  {U}\fontseries  {m}\fontshape  {n}\selectfont  \char 55}” means only \emph  {unlabelled} data is used during pre-training and the pre-trained models are directly transferred to AVA; “Ex.\ labels {\fontfamily  {pzd}\fontencoding  {U}\fontseries  {m}\fontshape  {n}\selectfont  \char 51}” additionally fine-tunes on the pre-training dataset with labels before transfer. $T{\times }\tau $ denotes frames$\times $sample-rate. (Numbers from \blx@tocontentsinit {0}\cite {tong2022_videomae}.)}}{1726}{table.caption.3885}\protected@file@percent }
\abx@aux@backref{2789}{tong2022_videomae}{0}{1726}{1726}
\newlabel{tab:chapter24_videomae_ava_fixed}{{24.12}{1726}{Comparison with the state of the art on AVA v2.2. All models are pre-trained and fine-tuned at image size $224^2$. We report validation mAP. “Ex.\ labels \xmark ” means only \emph {unlabelled} data is used during pre-training and the pre-trained models are directly transferred to AVA; “Ex.\ labels \cmark ” additionally fine-tunes on the pre-training dataset with labels before transfer. $T{\times }\tau $ denotes frames$\times $sample-rate. (Numbers from \cite {tong2022_videomae}.)}{table.caption.3885}{}}
\abx@aux@backref{2790}{feichtenhofer2019_slowfast}{0}{1726}{1726}
\abx@aux@backref{2791}{qian2021_cvrl}{0}{1726}{1726}
\abx@aux@backref{2792}{feichtenhofer2021_r2plus1d_ssl}{0}{1726}{1726}
\abx@aux@backref{2793}{feichtenhofer2021_r2plus1d_ssl}{0}{1726}{1726}
\abx@aux@backref{2794}{wei2022_maskfeat}{0}{1726}{1726}
\abx@aux@backref{2795}{wei2022_maskfeat}{0}{1726}{1726}
\abx@aux@backref{2796}{tong2022_videomae}{0}{1726}{1726}
\abx@aux@backref{2797}{tong2022_videomae}{0}{1726}{1726}
\abx@aux@backref{2798}{tong2022_videomae}{0}{1726}{1726}
\abx@aux@backref{2799}{tong2022_videomae}{0}{1726}{1726}
\abx@aux@backref{2800}{tong2022_videomae}{0}{1726}{1726}
\abx@aux@backref{2801}{tong2022_videomae}{0}{1726}{1726}
\abx@aux@backref{2802}{tong2022_videomae}{0}{1726}{1726}
\abx@aux@backref{2803}{tong2022_videomae}{0}{1726}{1726}
\abx@aux@backref{2804}{tong2022_videomae}{0}{1726}{1726}
\abx@aux@backref{2805}{tong2022_videomae}{0}{1726}{1726}
\@writefile{lot}{\contentsline {table}{\numberline {24.13}{\ignorespaces Comparison with the state of the art on Something–Something V2. VideoMAE reconstructs normalized cube pixels and is pre-trained with 90\% masking for 2400 epochs. “Ex.\ labels {\fontfamily  {pzd}\fontencoding  {U}\fontseries  {m}\fontshape  {n}\selectfont  \char 55}” means only \emph  {unlabelled} data is used during pre-training. (Numbers from \blx@tocontentsinit {0}\cite {tong2022_videomae}.)}}{1726}{table.caption.3886}\protected@file@percent }
\abx@aux@backref{2807}{tong2022_videomae}{0}{1726}{1726}
\newlabel{tab:chapter24_videomae_ssv2_fixed}{{24.13}{1726}{Comparison with the state of the art on Something–Something V2. VideoMAE reconstructs normalized cube pixels and is pre-trained with 90\% masking for 2400 epochs. “Ex.\ labels \xmark ” means only \emph {unlabelled} data is used during pre-training. (Numbers from \cite {tong2022_videomae}.)}{table.caption.3886}{}}
\abx@aux@backref{2808}{li2021_teinet}{0}{1726}{1726}
\abx@aux@backref{2809}{li2021_tanet}{0}{1726}{1726}
\abx@aux@backref{2810}{wang2021_tdn}{0}{1726}{1726}
\abx@aux@backref{2811}{feichtenhofer2019_slowfast}{0}{1726}{1726}
\abx@aux@backref{2812}{fan2021_mvit}{0}{1726}{1726}
\abx@aux@backref{2813}{bertasius2021_timesformer}{0}{1726}{1726}
\abx@aux@backref{2814}{bertasius2021_timesformer}{0}{1726}{1726}
\abx@aux@backref{2815}{arnab2021_vivit}{0}{1726}{1726}
\abx@aux@backref{2816}{patrick2021_motionformer}{0}{1726}{1726}
\abx@aux@backref{2817}{patrick2021_motionformer}{0}{1726}{1726}
\abx@aux@backref{2818}{liu2022_videoswin}{0}{1726}{1726}
\abx@aux@backref{2819}{tan2021_vimpac}{0}{1726}{1726}
\abx@aux@backref{2820}{wang2022_bevt}{0}{1726}{1726}
\abx@aux@backref{2821}{wei2022_maskfeat}{0}{1726}{1726}
\abx@aux@backref{2822}{tong2022_videomae}{0}{1726}{1726}
\abx@aux@backref{2823}{tong2022_videomae}{0}{1726}{1726}
\abx@aux@backref{2824}{tong2022_videomae}{0}{1726}{1726}
\abx@aux@backref{2825}{tong2022_videomae}{0}{1726}{1726}
\abx@aux@backref{2826}{tong2022_videomae}{0}{1726}{1726}
\abx@aux@backref{2827}{tong2022_videomae}{0}{1726}{1726}
\@writefile{lot}{\contentsline {table}{\numberline {24.14}{\ignorespaces Comparison with the state of the art on Kinetics-400. VideoMAE models are self-supervised with 90\% masking for 1600 epochs on K400. \textbf  {VideoMAE}$^{\uparrow 320}$ is initialized from its $224^2$ counterpart and then fine-tuned at $320^2$. “Ex.\ labels {\fontfamily  {pzd}\fontencoding  {U}\fontseries  {m}\fontshape  {n}\selectfont  \char 55}” means only \emph  {unlabelled} data is used during pre-training. (Numbers from \blx@tocontentsinit {0}\cite {tong2022_videomae}.)}}{1727}{table.caption.3887}\protected@file@percent }
\abx@aux@backref{2829}{tong2022_videomae}{0}{1727}{1727}
\newlabel{tab:chapter24_videomae_k400_fixed}{{24.14}{1727}{Comparison with the state of the art on Kinetics-400. VideoMAE models are self-supervised with 90\% masking for 1600 epochs on K400. \textbf {VideoMAE}$^{\uparrow 320}$ is initialized from its $224^2$ counterpart and then fine-tuned at $320^2$. “Ex.\ labels \xmark ” means only \emph {unlabelled} data is used during pre-training. (Numbers from \cite {tong2022_videomae}.)}{table.caption.3887}{}}
\abx@aux@backref{2830}{wang2018_nonlocal_nn}{0}{1727}{1727}
\abx@aux@backref{2831}{li2021_tanet}{0}{1727}{1727}
\abx@aux@backref{2832}{wang2021_tdn}{0}{1727}{1727}
\abx@aux@backref{2833}{bertasius2021_timesformer}{0}{1727}{1727}
\abx@aux@backref{2834}{arnab2021_vivit}{0}{1727}{1727}
\abx@aux@backref{2835}{patrick2021_motionformer}{0}{1727}{1727}
\abx@aux@backref{2836}{liu2022_videoswin}{0}{1727}{1727}
\abx@aux@backref{2837}{arnab2021_vivit}{0}{1727}{1727}
\abx@aux@backref{2838}{arnab2021_vivit}{0}{1727}{1727}
\abx@aux@backref{2839}{tan2021_vimpac}{0}{1727}{1727}
\abx@aux@backref{2840}{wang2022_bevt}{0}{1727}{1727}
\abx@aux@backref{2841}{wei2022_maskfeat}{0}{1727}{1727}
\abx@aux@backref{2842}{tran2019_ipcsn}{0}{1727}{1727}
\abx@aux@backref{2843}{feichtenhofer2019_slowfast}{0}{1727}{1727}
\abx@aux@backref{2844}{fan2021_mvit}{0}{1727}{1727}
\abx@aux@backref{2845}{wei2022_maskfeat}{0}{1727}{1727}
\abx@aux@backref{2846}{tong2022_videomae}{0}{1727}{1727}
\abx@aux@backref{2847}{tong2022_videomae}{0}{1727}{1727}
\abx@aux@backref{2848}{tong2022_videomae}{0}{1727}{1727}
\abx@aux@backref{2849}{tong2022_videomae}{0}{1727}{1727}
\abx@aux@backref{2850}{tong2022_videomae}{0}{1727}{1727}
\abx@aux@backref{2851}{tong2022_videomae}{0}{1727}{1727}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\@writefile{toc}{\contentsline {subsubsection}{Limitations and Future Work}{1728}{section*.3888}\protected@file@percent }
\newlabel{subsubsec_chapter24_videomae_limits}{{24.6.1}{1728}{Limitations and Future Work}{section*.3888}{}}
\@writefile{toc}{\contentsline {paragraph}{Observed constraints}{1728}{section*.3889}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Promising directions}{1728}{section*.3890}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{1728}{section*.3891}\protected@file@percent }
\abx@aux@backref{2852}{tong2022_videomae}{0}{1728}{1728}
\BKM@entry{id=959,dest={73656374696F6E2A2E33383932},srcline={1899}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030345C3030302E5C303030365C3030302E5C303030325C3030303A5C3030305C3034305C303030565C303030695C303030645C303030655C3030306F5C3030304D5C303030415C303030455C303030765C303030325C3030303A5C3030305C3034305C303030445C303030755C303030615C3030306C5C3030305C3034305C3030304D5C303030615C303030735C3030306B5C303030695C3030306E5C303030675C3030305C3034305C303030615C303030745C3030305C3034305C303030535C303030635C303030615C3030306C5C30303065}
\abx@aux@cite{0}{wang2023_videomaev2}
\abx@aux@segm{0}{0}{wang2023_videomaev2}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\@writefile{toc}{\contentsline {subsection}{Enrichment 24.6.2: VideoMAEv2: Dual Masking at Scale}{1729}{section*.3892}\protected@file@percent }
\newlabel{enr:subsec_chapter24_videomaev2}{{24.6.2}{1729}{\color {ocre}Enrichment \thesubsection : VideoMAEv2: Dual Masking at Scale}{section*.3892}{}}
\@writefile{toc}{\contentsline {paragraph}{Scope and positioning}{1729}{section*.3893}\protected@file@percent }
\abx@aux@backref{2853}{wang2023_videomaev2}{0}{1729}{1729}
\abx@aux@backref{2854}{tong2022_videomae}{0}{1729}{1729}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{1729}{section*.3894}\protected@file@percent }
\newlabel{subsubsec_chapter24_videomaev2_motivation}{{24.6.2}{1729}{Motivation}{section*.3894}{}}
\@writefile{toc}{\contentsline {paragraph}{Why mask the decoder too}{1729}{section*.3895}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24.49}{\ignorespaces \textbf  {VideoMAE with dual masking.} To improve the overall efficiency of computation and memory in video masked autoencoding, the authors mask the decoder as well and devise the dual masking strategy. Like the encoder, the method applies a masking map to the decoder and reconstructs only a subset of pixel cubes selected by \emph  {running cell} masking. The final reconstruction loss is computed only for the invisible tokens dropped by the encoder.}}{1729}{figure.caption.3896}\protected@file@percent }
\newlabel{fig:chapter24_videomaev2_overview}{{24.49}{1729}{\textbf {VideoMAE with dual masking.} To improve the overall efficiency of computation and memory in video masked autoencoding, the authors mask the decoder as well and devise the dual masking strategy. Like the encoder, the method applies a masking map to the decoder and reconstructs only a subset of pixel cubes selected by \emph {running cell} masking. The final reconstruction loss is computed only for the invisible tokens dropped by the encoder}{figure.caption.3896}{}}
\@writefile{toc}{\contentsline {subsubsection}{Method}{1730}{section*.3897}\protected@file@percent }
\newlabel{subsubsec_chapter24_videomaev2_method}{{24.6.2}{1730}{Method}{section*.3897}{}}
\@writefile{toc}{\contentsline {paragraph}{Preliminaries and notation}{1730}{section*.3898}\protected@file@percent }
\newlabel{eq:chapter24_videomaev2_enc}{{24.14}{1730}{Preliminaries and notation}{equation.24.14}{}}
\@writefile{toc}{\contentsline {paragraph}{Dual masking: decoder-side selection}{1730}{section*.3899}\protected@file@percent }
\newlabel{eq:chapter24_videomaev2_decinput}{{24.15}{1730}{Dual masking: decoder-side selection}{equation.24.15}{}}
\@writefile{toc}{\contentsline {paragraph}{Loss on encoder-invisible \& decoder-visible cubes}{1730}{section*.3900}\protected@file@percent }
\newlabel{eq:chapter24_videomaev2_loss}{{24.16}{1730}{Loss on encoder-invisible \& decoder-visible cubes}{equation.24.16}{}}
\@writefile{toc}{\contentsline {paragraph}{Running-cell masking for decoder supervision}{1730}{section*.3901}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Algorithmic flow (pseudo code)}{1732}{section*.3902}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Architecture and Implementation Details}{1732}{section*.3903}\protected@file@percent }
\newlabel{subsubsec_chapter24_videomaev2_arch}{{24.6.2}{1732}{Architecture and Implementation Details}{section*.3903}{}}
\@writefile{toc}{\contentsline {paragraph}{Backbones and decoder}{1732}{section*.3904}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Masking specifics}{1732}{section*.3905}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Data and schedules}{1732}{section*.3906}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Experiments and Ablation}{1733}{section*.3907}\protected@file@percent }
\newlabel{subsubsec_chapter24_videomaev2_experiments}{{24.6.2}{1733}{Experiments and Ablation}{section*.3907}{}}
\@writefile{toc}{\contentsline {paragraph}{Decoder masking strategies}{1733}{section*.3908}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {24.15}{\ignorespaces Ablation study on decoder masking strategies (ViT-B, SSv2, $800$ epochs). ``None'' is encoder-only masking (original VideoMAE). The default VideoMAEv2 setting is shaded.}}{1733}{table.caption.3909}\protected@file@percent }
\newlabel{tab:videomaev2_ablation_decoder}{{24.15}{1733}{Ablation study on decoder masking strategies (ViT-B, SSv2, $800$ epochs). ``None'' is encoder-only masking (original VideoMAE). The default VideoMAEv2 setting is shaded}{table.caption.3909}{}}
\abx@aux@cite{0}{feichtenhofer2022_mae_st}
\abx@aux@segm{0}{0}{feichtenhofer2022_mae_st}
\abx@aux@cite{0}{feichtenhofer2022_mae_st}
\abx@aux@segm{0}{0}{feichtenhofer2022_mae_st}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{feichtenhofer2022_mae_st}
\abx@aux@segm{0}{0}{feichtenhofer2022_mae_st}
\abx@aux@cite{0}{feichtenhofer2022_mae_st}
\abx@aux@segm{0}{0}{feichtenhofer2022_mae_st}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{wang2018_nonlocal_nn}
\abx@aux@segm{0}{0}{wang2018_nonlocal_nn}
\abx@aux@cite{0}{wang2021_tdn}
\abx@aux@segm{0}{0}{wang2021_tdn}
\abx@aux@cite{0}{feichtenhofer2019_slowfast}
\abx@aux@segm{0}{0}{feichtenhofer2019_slowfast}
\abx@aux@cite{0}{bertasius2021_timesformer}
\abx@aux@segm{0}{0}{bertasius2021_timesformer}
\abx@aux@cite{0}{yan2022_mtv}
\abx@aux@segm{0}{0}{yan2022_mtv}
\abx@aux@cite{0}{liu2022_videoswin}
\abx@aux@segm{0}{0}{liu2022_videoswin}
\abx@aux@cite{0}{arnab2021_vivit}
\abx@aux@segm{0}{0}{arnab2021_vivit}
\abx@aux@cite{0}{li2021_improved_mvit}
\abx@aux@segm{0}{0}{li2021_improved_mvit}
\abx@aux@cite{0}{wei2022_maskfeat}
\abx@aux@segm{0}{0}{wei2022_maskfeat}
\abx@aux@cite{0}{feichtenhofer2022_mae_st}
\abx@aux@segm{0}{0}{feichtenhofer2022_mae_st}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{wang2023_videomaev2}
\abx@aux@segm{0}{0}{wang2023_videomaev2}
\abx@aux@cite{0}{wang2023_videomaev2}
\abx@aux@segm{0}{0}{wang2023_videomaev2}
\abx@aux@cite{0}{wang2023_videomaev2}
\abx@aux@segm{0}{0}{wang2023_videomaev2}
\@writefile{toc}{\contentsline {paragraph}{Efficiency of dual masking}{1734}{section*.3910}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {24.16}{\ignorespaces \textbf  {Dual masking vs.\ encoder-only masking.} Computational cost, memory, and runtime (1200 epochs on 64 GPUs).}}{1734}{table.caption.3911}\protected@file@percent }
\newlabel{tab:videomaev2_efficiency}{{24.16}{1734}{\textbf {Dual masking vs.\ encoder-only masking.} Computational cost, memory, and runtime (1200 epochs on 64 GPUs)}{table.caption.3911}{}}
\@writefile{toc}{\contentsline {paragraph}{Kinetics-400}{1734}{section*.3912}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {24.17}{\ignorespaces \textbf  {Results on Kinetics-400.} Multi-view ($5{\times }3$) accuracy; single-view in brackets. Input $16{\times }224^2$, stride $\tau \!=\!4$.}}{1734}{table.caption.3913}\protected@file@percent }
\newlabel{tab:videomaev2_k400}{{24.17}{1734}{\textbf {Results on Kinetics-400.} Multi-view ($5{\times }3$) accuracy; single-view in brackets. Input $16{\times }224^2$, stride $\tau \!=\!4$}{table.caption.3913}{}}
\abx@aux@backref{2855}{feichtenhofer2022_mae_st}{0}{1734}{1734}
\abx@aux@backref{2856}{feichtenhofer2022_mae_st}{0}{1734}{1734}
\abx@aux@backref{2857}{tong2022_videomae}{0}{1734}{1734}
\@writefile{toc}{\contentsline {paragraph}{Something-Something V2}{1734}{section*.3914}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {24.18}{\ignorespaces \textbf  {Results on Something-Something V2.} Multi-view ($2{\times }3$) accuracy; single-view in brackets. Input $16{\times }224^2$, stride $\tau \!=\!2$.}}{1734}{table.caption.3915}\protected@file@percent }
\newlabel{tab:videomaev2_ssv2}{{24.18}{1734}{\textbf {Results on Something-Something V2.} Multi-view ($2{\times }3$) accuracy; single-view in brackets. Input $16{\times }224^2$, stride $\tau \!=\!2$}{table.caption.3915}{}}
\abx@aux@backref{2858}{feichtenhofer2022_mae_st}{0}{1734}{1734}
\abx@aux@backref{2859}{feichtenhofer2022_mae_st}{0}{1734}{1734}
\abx@aux@backref{2860}{tong2022_videomae}{0}{1734}{1734}
\@writefile{toc}{\contentsline {paragraph}{Progressive pre-training (K710)}{1734}{section*.3916}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {24.19}{\ignorespaces \textbf  {Study on progressive pre-training.} Kinetics-400 fine-tuning with multi-view ($5{\times }3$); single-view in brackets.}}{1734}{table.caption.3917}\protected@file@percent }
\newlabel{tab:videomaev2_progressive}{{24.19}{1734}{\textbf {Study on progressive pre-training.} Kinetics-400 fine-tuning with multi-view ($5{\times }3$); single-view in brackets}{table.caption.3917}{}}
\abx@aux@cite{0}{feichtenhofer2019_slowfast}
\abx@aux@segm{0}{0}{feichtenhofer2019_slowfast}
\abx@aux@cite{0}{bertasius2021_timesformer}
\abx@aux@segm{0}{0}{bertasius2021_timesformer}
\abx@aux@cite{0}{yan2022_mtv}
\abx@aux@segm{0}{0}{yan2022_mtv}
\abx@aux@cite{0}{arnab2021_vivit}
\abx@aux@segm{0}{0}{arnab2021_vivit}
\abx@aux@cite{0}{li2021_improved_mvit}
\abx@aux@segm{0}{0}{li2021_improved_mvit}
\abx@aux@cite{0}{wei2022_maskfeat}
\abx@aux@segm{0}{0}{wei2022_maskfeat}
\abx@aux@cite{0}{wang2023_videomaev2}
\abx@aux@segm{0}{0}{wang2023_videomaev2}
\abx@aux@cite{0}{wang2023_videomaev2}
\abx@aux@segm{0}{0}{wang2023_videomaev2}
\abx@aux@cite{0}{wang2023_videomaev2}
\abx@aux@segm{0}{0}{wang2023_videomaev2}
\abx@aux@cite{0}{feichtenhofer2019_slowfast}
\abx@aux@segm{0}{0}{feichtenhofer2019_slowfast}
\abx@aux@cite{0}{li2021_teinet}
\abx@aux@segm{0}{0}{li2021_teinet}
\abx@aux@cite{0}{li2020_tea}
\abx@aux@segm{0}{0}{li2020_tea}
\abx@aux@cite{0}{wang2021_tdn}
\abx@aux@segm{0}{0}{wang2021_tdn}
\abx@aux@cite{0}{bertasius2021_timesformer}
\abx@aux@segm{0}{0}{bertasius2021_timesformer}
\abx@aux@cite{0}{patrick2021_motionformer}
\abx@aux@segm{0}{0}{patrick2021_motionformer}
\abx@aux@cite{0}{arnab2021_vivit}
\abx@aux@segm{0}{0}{arnab2021_vivit}
\abx@aux@cite{0}{liu2022_videoswin}
\abx@aux@segm{0}{0}{liu2022_videoswin}
\abx@aux@cite{0}{li2021_improved_mvit}
\abx@aux@segm{0}{0}{li2021_improved_mvit}
\abx@aux@cite{0}{yan2022_mtv}
\abx@aux@segm{0}{0}{yan2022_mtv}
\abx@aux@cite{0}{wang2022_bevt}
\abx@aux@segm{0}{0}{wang2022_bevt}
\abx@aux@cite{0}{tan2021_vimpac}
\abx@aux@segm{0}{0}{tan2021_vimpac}
\abx@aux@cite{0}{li2022_uniformer}
\abx@aux@segm{0}{0}{li2022_uniformer}
\abx@aux@cite{0}{wei2022_maskfeat}
\abx@aux@segm{0}{0}{wei2022_maskfeat}
\abx@aux@cite{0}{feichtenhofer2022_mae_st}
\abx@aux@segm{0}{0}{feichtenhofer2022_mae_st}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{wang2023_videomaev2}
\abx@aux@segm{0}{0}{wang2023_videomaev2}
\abx@aux@cite{0}{wang2023_videomaev2}
\abx@aux@segm{0}{0}{wang2023_videomaev2}
\abx@aux@cite{0}{carreira2017_i3d}
\abx@aux@segm{0}{0}{carreira2017_i3d}
\abx@aux@cite{0}{wang2018_nonlocal_nn}
\abx@aux@segm{0}{0}{wang2018_nonlocal_nn}
\abx@aux@cite{0}{wang2018_videograph}
\abx@aux@segm{0}{0}{wang2018_videograph}
\abx@aux@cite{0}{lin2019_tsm}
\abx@aux@segm{0}{0}{lin2019_tsm}
\abx@aux@cite{0}{yue2020_v4d}
\abx@aux@segm{0}{0}{yue2020_v4d}
\abx@aux@cite{0}{li2020_tanet}
\abx@aux@segm{0}{0}{li2020_tanet}
\abx@aux@cite{0}{li2021_teinet}
\abx@aux@segm{0}{0}{li2021_teinet}
\abx@aux@cite{0}{li2020_tea}
\abx@aux@segm{0}{0}{li2020_tea}
\abx@aux@cite{0}{wang2020_corrnet}
\abx@aux@segm{0}{0}{wang2020_corrnet}
\abx@aux@cite{0}{wang2021_tdn}
\abx@aux@segm{0}{0}{wang2021_tdn}
\abx@aux@cite{0}{li2022_uniformer}
\abx@aux@segm{0}{0}{li2022_uniformer}
\abx@aux@cite{0}{wang2023_videomaev2}
\abx@aux@segm{0}{0}{wang2023_videomaev2}
\abx@aux@cite{0}{wang2023_videomaev2}
\abx@aux@segm{0}{0}{wang2023_videomaev2}
\@writefile{toc}{\contentsline {paragraph}{State of the art (selected benchmarks)}{1735}{section*.3918}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {24.20}{\ignorespaces (a) Kinetics\,400 — Top-1/Top-5 accuracy, views, and TFLOPs.}}{1735}{table.caption.3919}\protected@file@percent }
\abx@aux@backref{2861}{wang2018_nonlocal_nn}{0}{1735}{1735}
\abx@aux@backref{2862}{wang2021_tdn}{0}{1735}{1735}
\abx@aux@backref{2863}{feichtenhofer2019_slowfast}{0}{1735}{1735}
\abx@aux@backref{2864}{bertasius2021_timesformer}{0}{1735}{1735}
\abx@aux@backref{2865}{yan2022_mtv}{0}{1735}{1735}
\abx@aux@backref{2866}{liu2022_videoswin}{0}{1735}{1735}
\abx@aux@backref{2867}{arnab2021_vivit}{0}{1735}{1735}
\abx@aux@backref{2868}{li2021_improved_mvit}{0}{1735}{1735}
\abx@aux@backref{2869}{wei2022_maskfeat}{0}{1735}{1735}
\abx@aux@backref{2870}{feichtenhofer2022_mae_st}{0}{1735}{1735}
\abx@aux@backref{2871}{tong2022_videomae}{0}{1735}{1735}
\abx@aux@backref{2872}{wang2023_videomaev2}{0}{1735}{1735}
\abx@aux@backref{2873}{wang2023_videomaev2}{0}{1735}{1735}
\abx@aux@backref{2874}{wang2023_videomaev2}{0}{1735}{1735}
\@writefile{lot}{\contentsline {table}{\numberline {24.21}{\ignorespaces (b) Kinetics\,600 — Top-1/Top-5 accuracy, views, and TFLOPs.}}{1735}{table.caption.3920}\protected@file@percent }
\abx@aux@backref{2875}{feichtenhofer2019_slowfast}{0}{1735}{1735}
\abx@aux@backref{2876}{bertasius2021_timesformer}{0}{1735}{1735}
\abx@aux@backref{2877}{yan2022_mtv}{0}{1735}{1735}
\abx@aux@backref{2878}{arnab2021_vivit}{0}{1735}{1735}
\abx@aux@backref{2879}{li2021_improved_mvit}{0}{1735}{1735}
\abx@aux@backref{2880}{wei2022_maskfeat}{0}{1735}{1735}
\abx@aux@backref{2881}{wang2023_videomaev2}{0}{1735}{1735}
\abx@aux@backref{2882}{wang2023_videomaev2}{0}{1735}{1735}
\abx@aux@backref{2883}{wang2023_videomaev2}{0}{1735}{1735}
\@writefile{lot}{\contentsline {table}{\numberline {24.22}{\ignorespaces (c) Something-Something V2 — Top-1/Top-5 accuracy.}}{1735}{table.caption.3921}\protected@file@percent }
\abx@aux@backref{2884}{feichtenhofer2019_slowfast}{0}{1735}{1735}
\abx@aux@backref{2885}{li2021_teinet}{0}{1735}{1735}
\abx@aux@backref{2886}{li2020_tea}{0}{1735}{1735}
\abx@aux@backref{2887}{wang2021_tdn}{0}{1735}{1735}
\abx@aux@backref{2888}{bertasius2021_timesformer}{0}{1735}{1735}
\abx@aux@backref{2889}{patrick2021_motionformer}{0}{1735}{1735}
\abx@aux@backref{2890}{arnab2021_vivit}{0}{1735}{1735}
\abx@aux@backref{2891}{liu2022_videoswin}{0}{1735}{1735}
\abx@aux@backref{2892}{li2021_improved_mvit}{0}{1735}{1735}
\abx@aux@backref{2893}{yan2022_mtv}{0}{1735}{1735}
\abx@aux@backref{2894}{wang2022_bevt}{0}{1735}{1735}
\abx@aux@backref{2895}{tan2021_vimpac}{0}{1735}{1735}
\abx@aux@backref{2896}{li2022_uniformer}{0}{1735}{1735}
\abx@aux@backref{2897}{wei2022_maskfeat}{0}{1735}{1735}
\abx@aux@backref{2898}{feichtenhofer2022_mae_st}{0}{1735}{1735}
\abx@aux@backref{2899}{tong2022_videomae}{0}{1735}{1735}
\abx@aux@backref{2900}{wang2023_videomaev2}{0}{1735}{1735}
\abx@aux@backref{2901}{wang2023_videomaev2}{0}{1735}{1735}
\abx@aux@cite{0}{feichtenhofer2019_slowfast}
\abx@aux@segm{0}{0}{feichtenhofer2019_slowfast}
\abx@aux@cite{0}{zhao2022_tuber}
\abx@aux@segm{0}{0}{zhao2022_tuber}
\abx@aux@cite{0}{wei2022_maskfeat}
\abx@aux@segm{0}{0}{wei2022_maskfeat}
\abx@aux@cite{0}{feichtenhofer2022_mae_st}
\abx@aux@segm{0}{0}{feichtenhofer2022_mae_st}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{wang2023_videomaev2}
\abx@aux@segm{0}{0}{wang2023_videomaev2}
\abx@aux@cite{0}{wu2022_aiapp}
\abx@aux@segm{0}{0}{wu2022_aiapp}
\abx@aux@cite{0}{wu2020_msf}
\abx@aux@segm{0}{0}{wu2020_msf}
\abx@aux@cite{0}{pan2021_acar}
\abx@aux@segm{0}{0}{pan2021_acar}
\abx@aux@cite{0}{wang2023_videomaev2}
\abx@aux@segm{0}{0}{wang2023_videomaev2}
\abx@aux@cite{0}{xu2020_rtdnet}
\abx@aux@segm{0}{0}{xu2020_rtdnet}
\abx@aux@cite{0}{zhu2021_daotad}
\abx@aux@segm{0}{0}{zhu2021_daotad}
\abx@aux@cite{0}{lin2021_salient_boundary}
\abx@aux@segm{0}{0}{lin2021_salient_boundary}
\abx@aux@cite{0}{zeng2021_dcan}
\abx@aux@segm{0}{0}{zeng2021_dcan}
\abx@aux@cite{0}{liu2022_tadtr}
\abx@aux@segm{0}{0}{liu2022_tadtr}
\abx@aux@cite{0}{zhang2022_tallformer}
\abx@aux@segm{0}{0}{zhang2022_tallformer}
\abx@aux@cite{0}{bai2023_basictad}
\abx@aux@segm{0}{0}{bai2023_basictad}
\abx@aux@cite{0}{zhang2022_actionformer}
\abx@aux@segm{0}{0}{zhang2022_actionformer}
\abx@aux@cite{0}{wang2023_videomaev2}
\abx@aux@segm{0}{0}{wang2023_videomaev2}
\abx@aux@cite{0}{lin2019_bmn}
\abx@aux@segm{0}{0}{lin2019_bmn}
\abx@aux@cite{0}{xu2020_rtdnet}
\abx@aux@segm{0}{0}{xu2020_rtdnet}
\abx@aux@cite{0}{bai2023_basictad}
\abx@aux@segm{0}{0}{bai2023_basictad}
\abx@aux@cite{0}{zhang2022_actionformer}
\abx@aux@segm{0}{0}{zhang2022_actionformer}
\abx@aux@cite{0}{wang2023_videomaev2}
\abx@aux@segm{0}{0}{wang2023_videomaev2}
\@writefile{lot}{\contentsline {table}{\numberline {24.23}{\ignorespaces (d) Something-Something V1 — Top-1/Top-5 accuracy.}}{1736}{table.caption.3922}\protected@file@percent }
\abx@aux@backref{2902}{carreira2017_i3d}{0}{1736}{1736}
\abx@aux@backref{2903}{wang2018_videograph}{0}{1736}{1736}
\abx@aux@backref{2904}{wang2018_nonlocal_nn}{0}{1736}{1736}
\abx@aux@backref{2905}{lin2019_tsm}{0}{1736}{1736}
\abx@aux@backref{2906}{yue2020_v4d}{0}{1736}{1736}
\abx@aux@backref{2907}{li2020_tanet}{0}{1736}{1736}
\abx@aux@backref{2908}{li2021_teinet}{0}{1736}{1736}
\abx@aux@backref{2909}{li2020_tea}{0}{1736}{1736}
\abx@aux@backref{2910}{wang2020_corrnet}{0}{1736}{1736}
\abx@aux@backref{2911}{wang2021_tdn}{0}{1736}{1736}
\abx@aux@backref{2912}{li2022_uniformer}{0}{1736}{1736}
\abx@aux@backref{2913}{wang2023_videomaev2}{0}{1736}{1736}
\abx@aux@backref{2914}{wang2023_videomaev2}{0}{1736}{1736}
\@writefile{lot}{\contentsline {table}{\numberline {24.24}{\ignorespaces (e) AVA v2.2 — mAP with and without long feature.}}{1736}{table.caption.3923}\protected@file@percent }
\abx@aux@backref{2915}{feichtenhofer2019_slowfast}{0}{1736}{1736}
\abx@aux@backref{2916}{zhao2022_tuber}{0}{1736}{1736}
\abx@aux@backref{2917}{wei2022_maskfeat}{0}{1736}{1736}
\abx@aux@backref{2918}{feichtenhofer2022_mae_st}{0}{1736}{1736}
\abx@aux@backref{2919}{tong2022_videomae}{0}{1736}{1736}
\abx@aux@backref{2920}{wang2023_videomaev2}{0}{1736}{1736}
\@writefile{lot}{\contentsline {table}{\numberline {24.25}{\ignorespaces (f) AVA\textendash Kinetics — ensembled mAP.}}{1736}{table.caption.3924}\protected@file@percent }
\abx@aux@backref{2921}{wu2022_aiapp}{0}{1736}{1736}
\abx@aux@backref{2922}{wu2020_msf}{0}{1736}{1736}
\abx@aux@backref{2923}{pan2021_acar}{0}{1736}{1736}
\abx@aux@backref{2924}{wang2023_videomaev2}{0}{1736}{1736}
\@writefile{lot}{\contentsline {table}{\numberline {24.26}{\ignorespaces (g) THUMOS14 — temporal action detection mAP.}}{1736}{table.caption.3925}\protected@file@percent }
\abx@aux@backref{2925}{xu2020_rtdnet}{0}{1736}{1736}
\abx@aux@backref{2926}{zhu2021_daotad}{0}{1736}{1736}
\abx@aux@backref{2927}{lin2021_salient_boundary}{0}{1736}{1736}
\abx@aux@backref{2928}{zeng2021_dcan}{0}{1736}{1736}
\abx@aux@backref{2929}{liu2022_tadtr}{0}{1736}{1736}
\abx@aux@backref{2930}{zhang2022_tallformer}{0}{1736}{1736}
\abx@aux@backref{2931}{bai2023_basictad}{0}{1736}{1736}
\abx@aux@backref{2932}{zhang2022_actionformer}{0}{1736}{1736}
\abx@aux@backref{2933}{wang2023_videomaev2}{0}{1736}{1736}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\@writefile{lot}{\contentsline {table}{\numberline {24.27}{\ignorespaces (h) FineAction — temporal action detection mAP.}}{1737}{table.caption.3926}\protected@file@percent }
\abx@aux@backref{2934}{lin2019_bmn}{0}{1737}{1737}
\abx@aux@backref{2935}{xu2020_rtdnet}{0}{1737}{1737}
\abx@aux@backref{2936}{bai2023_basictad}{0}{1737}{1737}
\abx@aux@backref{2937}{zhang2022_actionformer}{0}{1737}{1737}
\abx@aux@backref{2938}{wang2023_videomaev2}{0}{1737}{1737}
\@writefile{toc}{\contentsline {subsubsection}{Limitations and Future Work}{1737}{section*.3927}\protected@file@percent }
\newlabel{subsubsec_chapter24_videomaev2_limits}{{24.6.2}{1737}{Limitations and Future Work}{section*.3927}{}}
\@writefile{toc}{\contentsline {paragraph}{Observed constraints}{1737}{section*.3928}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Future directions (path toward distillation and beyond)}{1737}{section*.3929}\protected@file@percent }
\abx@aux@backref{2939}{wang2023_mvd}{0}{1737}{1737}
\BKM@entry{id=960,dest={73656374696F6E2A2E33393330},srcline={2385}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030345C3030302E5C303030365C3030302E5C303030335C3030303A5C3030305C3034305C3030304D5C303030565C303030445C3030303A5C3030305C3034305C3030304D5C303030615C303030735C3030306B5C303030655C303030645C3030305C3034305C303030565C303030695C303030645C303030655C3030306F5C3030305C3034305C303030445C303030695C303030735C303030745C303030695C3030306C5C3030306C5C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\@writefile{toc}{\contentsline {subsection}{Enrichment 24.6.3: MVD: Masked Video Distillation}{1738}{section*.3930}\protected@file@percent }
\newlabel{enr:subsec_chapter24_mvd}{{24.6.3}{1738}{\color {ocre}Enrichment \thesubsection : MVD: Masked Video Distillation}{section*.3930}{}}
\@writefile{toc}{\contentsline {paragraph}{Scope and positioning}{1738}{section*.3931}\protected@file@percent }
\newlabel{par:chapter24_mvd_scope}{{24.6.3}{1738}{Scope and positioning}{section*.3931}{}}
\abx@aux@backref{2940}{tong2022_videomae}{0}{1738}{1738}
\abx@aux@backref{2941}{wang2023_mvd}{0}{1738}{1738}
\abx@aux@backref{2942}{wang2023_mvd}{0}{1738}{1738}
\@writefile{lof}{\contentsline {figure}{\numberline {24.50}{\ignorespaces Overview of MVD \blx@tocontentsinit {0}\cite {wang2023_mvd}. A student video encoder observes only visible tokens from a tube-masked clip and is trained to reconstruct masked \emph  {teacher features} with two shallow decoders: one targets an image teacher’s spatial features and the other a video teacher’s spatio-temporal features.}}{1738}{figure.caption.3932}\protected@file@percent }
\abx@aux@backref{2944}{wang2023_mvd}{0}{1738}{1738}
\newlabel{fig:chapter24_mvd_overview}{{24.50}{1738}{Overview of MVD \cite {wang2023_mvd}. A student video encoder observes only visible tokens from a tube-masked clip and is trained to reconstruct masked \emph {teacher features} with two shallow decoders: one targets an image teacher’s spatial features and the other a video teacher’s spatio-temporal features}{figure.caption.3932}{}}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{1738}{section*.3933}\protected@file@percent }
\newlabel{subsubsec:chapter24_mvd_motivation}{{24.6.3}{1738}{Motivation}{section*.3933}{}}
\@writefile{toc}{\contentsline {paragraph}{Limits of pixel-level MVM (VideoMAE).}{1738}{section*.3934}\protected@file@percent }
\abx@aux@backref{2945}{tong2022_videomae}{0}{1738}{1738}
\abx@aux@backref{2946}{wang2023_mvd}{0}{1738}{1738}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\@writefile{toc}{\contentsline {paragraph}{From pixels to features: cleaner targets and inductive bias.}{1739}{section*.3935}\protected@file@percent }
\abx@aux@backref{2947}{wang2023_mvd}{0}{1739}{1739}
\@writefile{toc}{\contentsline {paragraph}{Why two teachers: complementary spatial and temporal cues.}{1739}{section*.3936}\protected@file@percent }
\abx@aux@backref{2948}{wang2023_mvd}{0}{1739}{1739}
\@writefile{lof}{\contentsline {figure}{\numberline {24.51}{\ignorespaces Teacher feature similarity across frames (cosine). Image-teacher features are highly similar across time, indicating spatial dominance; video-teacher features decorrelate with temporal distance, indicating motion sensitivity \blx@tocontentsinit {0}\cite {wang2023_mvd}.}}{1739}{figure.caption.3937}\protected@file@percent }
\abx@aux@backref{2950}{wang2023_mvd}{0}{1739}{1739}
\newlabel{fig:chapter24_mvd_similarity}{{24.51}{1739}{Teacher feature similarity across frames (cosine). Image-teacher features are highly similar across time, indicating spatial dominance; video-teacher features decorrelate with temporal distance, indicating motion sensitivity \cite {wang2023_mvd}}{figure.caption.3937}{}}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\@writefile{toc}{\contentsline {subsubsection}{Method}{1740}{section*.3938}\protected@file@percent }
\newlabel{subsubsec:chapter24_mvd_method}{{24.6.3}{1740}{Method}{section*.3938}{}}
\@writefile{toc}{\contentsline {paragraph}{Preliminaries: masked feature modeling}{1740}{section*.3939}\protected@file@percent }
\newlabel{par:chapter24_mvd_mfm}{{24.6.3}{1740}{Preliminaries: masked feature modeling}{section*.3939}{}}
\newlabel{eq:chapter24_mvd_forward}{{24.17}{1740}{Preliminaries: masked feature modeling}{equation.24.17}{}}
\newlabel{eq:chapter24_mvd_mfm}{{24.18}{1740}{Preliminaries: masked feature modeling}{equation.24.18}{}}
\abx@aux@backref{2951}{wang2023_mvd}{0}{1740}{1740}
\@writefile{toc}{\contentsline {paragraph}{Teacher targets}{1740}{section*.3940}\protected@file@percent }
\newlabel{par:chapter24_mvd_targets}{{24.6.3}{1740}{Teacher targets}{section*.3940}{}}
\abx@aux@backref{2952}{wang2023_mvd}{0}{1740}{1740}
\@writefile{toc}{\contentsline {paragraph}{Spatial–temporal co-teaching}{1740}{section*.3941}\protected@file@percent }
\newlabel{par:chapter24_mvd_coteach}{{24.6.3}{1740}{Spatial–temporal co-teaching}{section*.3941}{}}
\newlabel{eq:chapter24_mvd_total}{{24.19}{1740}{Spatial–temporal co-teaching}{equation.24.19}{}}
\abx@aux@backref{2953}{wang2023_mvd}{0}{1740}{1740}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\@writefile{toc}{\contentsline {paragraph}{Algorithmic view}{1741}{section*.3942}\protected@file@percent }
\newlabel{par:chapter24_mvd_algo}{{24.6.3}{1741}{Algorithmic view}{section*.3942}{}}
\newlabel{alg:chapter24_mvd_algo}{{24.6.3}{1741}{Algorithmic view}{section*.3942}{}}
\@writefile{toc}{\contentsline {paragraph}{Intuition and failure-mode mitigation}{1741}{section*.3943}\protected@file@percent }
\newlabel{par:chapter24_mvd_intuition}{{24.6.3}{1741}{Intuition and failure-mode mitigation}{section*.3943}{}}
\@writefile{toc}{\contentsline {subsubsection}{Architecture and implementation details}{1741}{section*.3944}\protected@file@percent }
\newlabel{subsubsec:chapter24_mvd_arch}{{24.6.3}{1741}{Architecture and implementation details}{section*.3944}{}}
\@writefile{toc}{\contentsline {paragraph}{Backbone and tokenization}{1741}{section*.3945}\protected@file@percent }
\abx@aux@backref{2954}{tong2022_videomae}{0}{1741}{1741}
\abx@aux@backref{2955}{wang2023_mvd}{0}{1741}{1741}
\@writefile{toc}{\contentsline {paragraph}{Attention}{1741}{section*.3946}\protected@file@percent }
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\@writefile{toc}{\contentsline {paragraph}{Decoders and objectives}{1742}{section*.3947}\protected@file@percent }
\abx@aux@backref{2956}{wang2023_mvd}{0}{1742}{1742}
\@writefile{toc}{\contentsline {paragraph}{Pretraining schedules}{1742}{section*.3948}\protected@file@percent }
\abx@aux@backref{2957}{wang2023_mvd}{0}{1742}{1742}
\@writefile{toc}{\contentsline {subsubsection}{Experiments and ablation}{1742}{section*.3949}\protected@file@percent }
\newlabel{subsubsec:chapter24_mvd_experiments}{{24.6.3}{1742}{Experiments and ablation}{section*.3949}{}}
\@writefile{toc}{\contentsline {paragraph}{Main results and efficiency}{1742}{section*.3950}\protected@file@percent }
\abx@aux@backref{2958}{tong2022_videomae}{0}{1742}{1742}
\abx@aux@backref{2959}{wang2023_mvd}{0}{1742}{1742}
\@writefile{lof}{\contentsline {figure}{\numberline {24.52}{\ignorespaces SSv2 accuracy versus GFLOPs per video for supervised and self-supervised models. MVD (red stars) attains higher accuracy at comparable or lower cost across scales \blx@tocontentsinit {0}\cite {wang2023_mvd}.}}{1742}{figure.caption.3951}\protected@file@percent }
\abx@aux@backref{2961}{wang2023_mvd}{0}{1742}{1742}
\newlabel{fig:chpapter24_chapter24_mvd_flops}{{24.52}{1742}{SSv2 accuracy versus GFLOPs per video for supervised and self-supervised models. MVD (red stars) attains higher accuracy at comparable or lower cost across scales \cite {wang2023_mvd}}{figure.caption.3951}{}}
\@writefile{toc}{\contentsline {paragraph}{Gains over VideoMAE across scales}{1742}{section*.3952}\protected@file@percent }
\newlabel{par:chapter24_mvd_vs_videomae}{{24.6.3}{1742}{Gains over VideoMAE across scales}{section*.3952}{}}
\@writefile{lot}{\contentsline {table}{\numberline {24.28}{\ignorespaces MVD vs. VideoMAE across student/teacher scales on K400 and SSv2 \blx@tocontentsinit {0}\cite {wang2023_mvd,tong2022_videomae}.}}{1742}{table.caption.3953}\protected@file@percent }
\abx@aux@backref{2964}{tong2022_videomae}{0}{1742}{1742}
\abx@aux@backref{2965}{wang2023_mvd}{0}{1742}{1742}
\newlabel{tab:chapter24_mvd_vs_videomae}{{24.28}{1742}{MVD vs. VideoMAE across student/teacher scales on K400 and SSv2 \cite {wang2023_mvd,tong2022_videomae}}{table.caption.3953}{}}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{feichtenhofer2022_mae_st}
\abx@aux@segm{0}{0}{feichtenhofer2022_mae_st}
\abx@aux@cite{0}{girdhar2023_omnimae}
\abx@aux@segm{0}{0}{girdhar2023_omnimae}
\abx@aux@cite{0}{wang2022_bevt}
\abx@aux@segm{0}{0}{wang2022_bevt}
\abx@aux@cite{0}{wei2022_maskfeat}
\abx@aux@segm{0}{0}{wei2022_maskfeat}
\abx@aux@cite{0}{li2021_improved_mvit}
\abx@aux@segm{0}{0}{li2021_improved_mvit}
\abx@aux@cite{0}{liu2022_videoswin}
\abx@aux@segm{0}{0}{liu2022_videoswin}
\abx@aux@cite{0}{bertasius2021_timesformer}
\abx@aux@segm{0}{0}{bertasius2021_timesformer}
\abx@aux@cite{0}{arnab2021_vivit}
\abx@aux@segm{0}{0}{arnab2021_vivit}
\abx@aux@cite{0}{feichtenhofer2019_slowfast}
\abx@aux@segm{0}{0}{feichtenhofer2019_slowfast}
\abx@aux@cite{0}{wang2018_nonlocal}
\abx@aux@segm{0}{0}{wang2018_nonlocal}
\abx@aux@cite{0}{tran2019_ipcsn}
\abx@aux@segm{0}{0}{tran2019_ipcsn}
\abx@aux@cite{0}{feichtenhofer2020_x3d}
\abx@aux@segm{0}{0}{feichtenhofer2020_x3d}
\abx@aux@cite{0}{fan2021_mvit}
\abx@aux@segm{0}{0}{fan2021_mvit}
\abx@aux@cite{0}{li2022_uniformer}
\abx@aux@segm{0}{0}{li2022_uniformer}
\abx@aux@cite{0}{patrick2021_mformer}
\abx@aux@segm{0}{0}{patrick2021_mformer}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2018_nonlocal}
\abx@aux@segm{0}{0}{wang2018_nonlocal}
\abx@aux@cite{0}{tran2019_ipcsn}
\abx@aux@segm{0}{0}{tran2019_ipcsn}
\abx@aux@cite{0}{feichtenhofer2019_slowfast}
\abx@aux@segm{0}{0}{feichtenhofer2019_slowfast}
\abx@aux@cite{0}{feichtenhofer2020_x3d}
\abx@aux@segm{0}{0}{feichtenhofer2020_x3d}
\abx@aux@cite{0}{fan2021_mvit}
\abx@aux@segm{0}{0}{fan2021_mvit}
\abx@aux@cite{0}{liu2022_videoswin}
\abx@aux@segm{0}{0}{liu2022_videoswin}
\abx@aux@cite{0}{li2022_uniformer}
\abx@aux@segm{0}{0}{li2022_uniformer}
\abx@aux@cite{0}{bertasius2021_timesformer}
\abx@aux@segm{0}{0}{bertasius2021_timesformer}
\abx@aux@cite{0}{patrick2021_mformer}
\abx@aux@segm{0}{0}{patrick2021_mformer}
\abx@aux@cite{0}{patrick2021_mformer}
\abx@aux@segm{0}{0}{patrick2021_mformer}
\abx@aux@cite{0}{arnab2021_vivit}
\abx@aux@segm{0}{0}{arnab2021_vivit}
\abx@aux@cite{0}{liu2022_videoswin}
\abx@aux@segm{0}{0}{liu2022_videoswin}
\abx@aux@cite{0}{tan2021_vimpac}
\abx@aux@segm{0}{0}{tan2021_vimpac}
\abx@aux@cite{0}{wang2022_bevt}
\abx@aux@segm{0}{0}{wang2022_bevt}
\abx@aux@cite{0}{wei2022_maskfeat}
\abx@aux@segm{0}{0}{wei2022_maskfeat}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{feichtenhofer2022_mae_st}
\abx@aux@segm{0}{0}{feichtenhofer2022_mae_st}
\abx@aux@cite{0}{feichtenhofer2022_mae_st}
\abx@aux@segm{0}{0}{feichtenhofer2022_mae_st}
\abx@aux@cite{0}{feichtenhofer2022_mae_st}
\abx@aux@segm{0}{0}{feichtenhofer2022_mae_st}
\abx@aux@cite{0}{girdhar2023_omnimae}
\abx@aux@segm{0}{0}{girdhar2023_omnimae}
\abx@aux@cite{0}{girdhar2023_omnimae}
\abx@aux@segm{0}{0}{girdhar2023_omnimae}
\abx@aux@cite{0}{girdhar2023_omnimae}
\abx@aux@segm{0}{0}{girdhar2023_omnimae}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\@writefile{toc}{\contentsline {paragraph}{Co-teaching vs single teacher}{1743}{section*.3954}\protected@file@percent }
\newlabel{par:chapter24_mvd_coteach_table}{{24.6.3}{1743}{Co-teaching vs single teacher}{section*.3954}{}}
\@writefile{lot}{\contentsline {table}{\numberline {24.29}{\ignorespaces Spatial–temporal co-teaching outperforms single-teacher distillation on K400 and SSv2 \blx@tocontentsinit {0}\cite {wang2023_mvd}.}}{1743}{table.caption.3955}\protected@file@percent }
\abx@aux@backref{2967}{wang2023_mvd}{0}{1743}{1743}
\newlabel{tab:chapter24_mvd_coteaching}{{24.29}{1743}{Spatial–temporal co-teaching outperforms single-teacher distillation on K400 and SSv2 \cite {wang2023_mvd}}{table.caption.3955}{}}
\@writefile{toc}{\contentsline {paragraph}{Gains over VideoMAE across scales}{1743}{section*.3956}\protected@file@percent }
\newlabel{par:chapter24_mvd_vs_videomae_gains}{{24.6.3}{1743}{Gains over VideoMAE across scales}{section*.3956}{}}
\@writefile{lot}{\contentsline {table}{\numberline {24.30}{\ignorespaces MVD vs. VideoMAE across student/teacher scales on K400 and SSv2 \blx@tocontentsinit {0}\cite {wang2023_mvd,tong2022_videomae}.}}{1743}{table.caption.3957}\protected@file@percent }
\abx@aux@backref{2970}{tong2022_videomae}{0}{1743}{1743}
\abx@aux@backref{2971}{wang2023_mvd}{0}{1743}{1743}
\newlabel{tab:chapter24_mvd_vs_videomae_scales}{{24.30}{1743}{MVD vs. VideoMAE across student/teacher scales on K400 and SSv2 \cite {wang2023_mvd,tong2022_videomae}}{table.caption.3957}{}}
\@writefile{toc}{\contentsline {paragraph}{End-to-end comparisons}{1743}{section*.3958}\protected@file@percent }
\abx@aux@backref{2972}{wang2023_mvd}{0}{1743}{1743}
\abx@aux@backref{2973}{feichtenhofer2022_mae_st}{0}{1743}{1743}
\abx@aux@backref{2974}{girdhar2023_omnimae}{0}{1743}{1743}
\abx@aux@backref{2975}{wang2022_bevt}{0}{1743}{1743}
\abx@aux@backref{2976}{wei2022_maskfeat}{0}{1743}{1743}
\abx@aux@backref{2977}{li2021_improved_mvit}{0}{1743}{1743}
\abx@aux@backref{2978}{liu2022_videoswin}{0}{1743}{1743}
\abx@aux@backref{2979}{bertasius2021_timesformer}{0}{1743}{1743}
\abx@aux@backref{2980}{arnab2021_vivit}{0}{1743}{1743}
\abx@aux@backref{2981}{feichtenhofer2019_slowfast}{0}{1743}{1743}
\abx@aux@backref{2982}{wang2018_nonlocal}{0}{1743}{1743}
\abx@aux@backref{2983}{tran2019_ipcsn}{0}{1743}{1743}
\abx@aux@backref{2984}{feichtenhofer2020_x3d}{0}{1743}{1743}
\abx@aux@backref{2985}{fan2021_mvit}{0}{1743}{1743}
\abx@aux@backref{2986}{li2022_uniformer}{0}{1743}{1743}
\abx@aux@backref{2987}{patrick2021_mformer}{0}{1743}{1743}
\@writefile{toc}{\contentsline {paragraph}{Co-teaching vs single teacher}{1743}{section*.3959}\protected@file@percent }
\newlabel{par:chapter24_mvd_coteach_table}{{24.6.3}{1743}{Co-teaching vs single teacher}{section*.3959}{}}
\@writefile{lot}{\contentsline {table}{\numberline {24.31}{\ignorespaces Spatial–temporal co-teaching outperforms single-teacher distillation on K400 and SSv2 \blx@tocontentsinit {0}\cite {wang2023_mvd}.}}{1743}{table.caption.3960}\protected@file@percent }
\abx@aux@backref{2989}{wang2023_mvd}{0}{1743}{1743}
\newlabel{tab:chapter24_mvd_coteaching}{{24.31}{1743}{Spatial–temporal co-teaching outperforms single-teacher distillation on K400 and SSv2 \cite {wang2023_mvd}}{table.caption.3960}{}}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{feichtenhofer2019_slowfast}
\abx@aux@segm{0}{0}{feichtenhofer2019_slowfast}
\abx@aux@cite{0}{lin2019_tsm}
\abx@aux@segm{0}{0}{lin2019_tsm}
\abx@aux@cite{0}{liu2021_tam}
\abx@aux@segm{0}{0}{liu2021_tam}
\abx@aux@cite{0}{wang2021_tdn}
\abx@aux@segm{0}{0}{wang2021_tdn}
\abx@aux@cite{0}{fan2021_mvit}
\abx@aux@segm{0}{0}{fan2021_mvit}
\abx@aux@cite{0}{li2021_improved_mvit}
\abx@aux@segm{0}{0}{li2021_improved_mvit}
\abx@aux@cite{0}{li2022_uniformer}
\abx@aux@segm{0}{0}{li2022_uniformer}
\abx@aux@cite{0}{bertasius2021_timesformer}
\abx@aux@segm{0}{0}{bertasius2021_timesformer}
\abx@aux@cite{0}{arnab2021_vivit}
\abx@aux@segm{0}{0}{arnab2021_vivit}
\abx@aux@cite{0}{patrick2021_mformer}
\abx@aux@segm{0}{0}{patrick2021_mformer}
\abx@aux@cite{0}{patrick2021_mformer}
\abx@aux@segm{0}{0}{patrick2021_mformer}
\abx@aux@cite{0}{liu2022_videoswin}
\abx@aux@segm{0}{0}{liu2022_videoswin}
\abx@aux@cite{0}{li2021_improved_mvit}
\abx@aux@segm{0}{0}{li2021_improved_mvit}
\abx@aux@cite{0}{tan2021_vimpac}
\abx@aux@segm{0}{0}{tan2021_vimpac}
\abx@aux@cite{0}{wang2022_bevt}
\abx@aux@segm{0}{0}{wang2022_bevt}
\abx@aux@cite{0}{wei2022_maskfeat}
\abx@aux@segm{0}{0}{wei2022_maskfeat}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{feichtenhofer2022_mae_st}
\abx@aux@segm{0}{0}{feichtenhofer2022_mae_st}
\abx@aux@cite{0}{feichtenhofer2022_mae_st}
\abx@aux@segm{0}{0}{feichtenhofer2022_mae_st}
\abx@aux@cite{0}{girdhar2023_omnimae}
\abx@aux@segm{0}{0}{girdhar2023_omnimae}
\abx@aux@cite{0}{girdhar2023_omnimae}
\abx@aux@segm{0}{0}{girdhar2023_omnimae}
\abx@aux@cite{0}{girdhar2023_omnimae}
\abx@aux@segm{0}{0}{girdhar2023_omnimae}
\abx@aux@cite{0}{girdhar2023_omnimae}
\abx@aux@segm{0}{0}{girdhar2023_omnimae}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\@writefile{lot}{\contentsline {table}{\numberline {24.32}{\ignorespaces Kinetics-400 comparisons (single-view cost $\times $ \#views). Bold rows indicate MVD \blx@tocontentsinit {0}\cite {wang2023_mvd}.}}{1744}{table.caption.3961}\protected@file@percent }
\abx@aux@backref{2991}{wang2023_mvd}{0}{1744}{1744}
\newlabel{tab:chapter24_mvd_k400}{{24.32}{1744}{Kinetics-400 comparisons (single-view cost $\times $ \#views). Bold rows indicate MVD \cite {wang2023_mvd}}{table.caption.3961}{}}
\abx@aux@backref{2992}{wang2018_nonlocal}{0}{1744}{1744}
\abx@aux@backref{2993}{tran2019_ipcsn}{0}{1744}{1744}
\abx@aux@backref{2994}{feichtenhofer2019_slowfast}{0}{1744}{1744}
\abx@aux@backref{2995}{feichtenhofer2020_x3d}{0}{1744}{1744}
\abx@aux@backref{2996}{fan2021_mvit}{0}{1744}{1744}
\abx@aux@backref{2997}{liu2022_videoswin}{0}{1744}{1744}
\abx@aux@backref{2998}{li2022_uniformer}{0}{1744}{1744}
\abx@aux@backref{2999}{bertasius2021_timesformer}{0}{1744}{1744}
\abx@aux@backref{3000}{patrick2021_mformer}{0}{1744}{1744}
\abx@aux@backref{3001}{patrick2021_mformer}{0}{1744}{1744}
\abx@aux@backref{3002}{arnab2021_vivit}{0}{1744}{1744}
\abx@aux@backref{3003}{liu2022_videoswin}{0}{1744}{1744}
\abx@aux@backref{3004}{tan2021_vimpac}{0}{1744}{1744}
\abx@aux@backref{3005}{wang2022_bevt}{0}{1744}{1744}
\abx@aux@backref{3006}{wei2022_maskfeat}{0}{1744}{1744}
\abx@aux@backref{3007}{tong2022_videomae}{0}{1744}{1744}
\abx@aux@backref{3008}{tong2022_videomae}{0}{1744}{1744}
\abx@aux@backref{3009}{tong2022_videomae}{0}{1744}{1744}
\abx@aux@backref{3010}{tong2022_videomae}{0}{1744}{1744}
\abx@aux@backref{3011}{feichtenhofer2022_mae_st}{0}{1744}{1744}
\abx@aux@backref{3012}{feichtenhofer2022_mae_st}{0}{1744}{1744}
\abx@aux@backref{3013}{feichtenhofer2022_mae_st}{0}{1744}{1744}
\abx@aux@backref{3014}{girdhar2023_omnimae}{0}{1744}{1744}
\abx@aux@backref{3015}{girdhar2023_omnimae}{0}{1744}{1744}
\abx@aux@backref{3016}{girdhar2023_omnimae}{0}{1744}{1744}
\abx@aux@backref{3017}{wang2023_mvd}{0}{1744}{1744}
\abx@aux@backref{3018}{wang2023_mvd}{0}{1744}{1744}
\abx@aux@backref{3019}{wang2023_mvd}{0}{1744}{1744}
\abx@aux@backref{3020}{wang2023_mvd}{0}{1744}{1744}
\abx@aux@backref{3021}{wang2023_mvd}{0}{1744}{1744}
\abx@aux@backref{3022}{wang2023_mvd}{0}{1744}{1744}
\abx@aux@backref{3023}{wang2023_mvd}{0}{1744}{1744}
\@writefile{lot}{\contentsline {table}{\numberline {24.33}{\ignorespaces Something-Something V2 comparisons. $^\dagger $ indicates 800-epoch distillation \blx@tocontentsinit {0}\cite {wang2023_mvd}.}}{1745}{table.caption.3962}\protected@file@percent }
\abx@aux@backref{3025}{wang2023_mvd}{0}{1745}{1745}
\newlabel{tab:chapter24_mvd_ssv2}{{24.33}{1745}{Something-Something V2 comparisons. $^\dagger $ indicates 800-epoch distillation \cite {wang2023_mvd}}{table.caption.3962}{}}
\abx@aux@backref{3026}{feichtenhofer2019_slowfast}{0}{1745}{1745}
\abx@aux@backref{3027}{lin2019_tsm}{0}{1745}{1745}
\abx@aux@backref{3028}{liu2021_tam}{0}{1745}{1745}
\abx@aux@backref{3029}{wang2021_tdn}{0}{1745}{1745}
\abx@aux@backref{3030}{fan2021_mvit}{0}{1745}{1745}
\abx@aux@backref{3031}{li2021_improved_mvit}{0}{1745}{1745}
\abx@aux@backref{3032}{li2022_uniformer}{0}{1745}{1745}
\abx@aux@backref{3033}{bertasius2021_timesformer}{0}{1745}{1745}
\abx@aux@backref{3034}{arnab2021_vivit}{0}{1745}{1745}
\abx@aux@backref{3035}{patrick2021_mformer}{0}{1745}{1745}
\abx@aux@backref{3036}{patrick2021_mformer}{0}{1745}{1745}
\abx@aux@backref{3037}{liu2022_videoswin}{0}{1745}{1745}
\abx@aux@backref{3038}{li2021_improved_mvit}{0}{1745}{1745}
\abx@aux@backref{3039}{tan2021_vimpac}{0}{1745}{1745}
\abx@aux@backref{3040}{wang2022_bevt}{0}{1745}{1745}
\abx@aux@backref{3041}{wei2022_maskfeat}{0}{1745}{1745}
\abx@aux@backref{3042}{tong2022_videomae}{0}{1745}{1745}
\abx@aux@backref{3043}{tong2022_videomae}{0}{1745}{1745}
\abx@aux@backref{3044}{tong2022_videomae}{0}{1745}{1745}
\abx@aux@backref{3045}{tong2022_videomae}{0}{1745}{1745}
\abx@aux@backref{3046}{tong2022_videomae}{0}{1745}{1745}
\abx@aux@backref{3047}{tong2022_videomae}{0}{1745}{1745}
\abx@aux@backref{3048}{feichtenhofer2022_mae_st}{0}{1745}{1745}
\abx@aux@backref{3049}{feichtenhofer2022_mae_st}{0}{1745}{1745}
\abx@aux@backref{3050}{girdhar2023_omnimae}{0}{1745}{1745}
\abx@aux@backref{3051}{girdhar2023_omnimae}{0}{1745}{1745}
\abx@aux@backref{3052}{girdhar2023_omnimae}{0}{1745}{1745}
\abx@aux@backref{3053}{girdhar2023_omnimae}{0}{1745}{1745}
\abx@aux@backref{3054}{wang2023_mvd}{0}{1745}{1745}
\abx@aux@backref{3055}{wang2023_mvd}{0}{1745}{1745}
\abx@aux@backref{3056}{wang2023_mvd}{0}{1745}{1745}
\abx@aux@backref{3057}{wang2023_mvd}{0}{1745}{1745}
\abx@aux@backref{3058}{wang2023_mvd}{0}{1745}{1745}
\abx@aux@backref{3059}{wang2023_mvd}{0}{1745}{1745}
\abx@aux@backref{3060}{wang2023_mvd}{0}{1745}{1745}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{feichtenhofer2019_slowfast}
\abx@aux@segm{0}{0}{feichtenhofer2019_slowfast}
\abx@aux@cite{0}{li2021_improved_mvit}
\abx@aux@segm{0}{0}{li2021_improved_mvit}
\abx@aux@cite{0}{li2021_improved_mvit}
\abx@aux@segm{0}{0}{li2021_improved_mvit}
\abx@aux@cite{0}{wei2022_maskfeat}
\abx@aux@segm{0}{0}{wei2022_maskfeat}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{feichtenhofer2022_mae_st}
\abx@aux@segm{0}{0}{feichtenhofer2022_mae_st}
\abx@aux@cite{0}{feichtenhofer2022_mae_st}
\abx@aux@segm{0}{0}{feichtenhofer2022_mae_st}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{pan2021_videomoco}
\abx@aux@segm{0}{0}{pan2021_videomoco}
\abx@aux@cite{0}{han2020_memdpc}
\abx@aux@segm{0}{0}{han2020_memdpc}
\abx@aux@cite{0}{chen2021_corp}
\abx@aux@segm{0}{0}{chen2021_corp}
\abx@aux@cite{0}{qian2021_cvrl}
\abx@aux@segm{0}{0}{qian2021_cvrl}
\abx@aux@cite{0}{recasens2021_broaden}
\abx@aux@segm{0}{0}{recasens2021_broaden}
\abx@aux@cite{0}{tan2021_vimpac}
\abx@aux@segm{0}{0}{tan2021_vimpac}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{pan2021_videomoco}
\abx@aux@segm{0}{0}{pan2021_videomoco}
\abx@aux@cite{0}{han2020_memdpc}
\abx@aux@segm{0}{0}{han2020_memdpc}
\abx@aux@cite{0}{chen2021_corp}
\abx@aux@segm{0}{0}{chen2021_corp}
\abx@aux@cite{0}{qian2021_cvrl}
\abx@aux@segm{0}{0}{qian2021_cvrl}
\abx@aux@cite{0}{recasens2021_broaden}
\abx@aux@segm{0}{0}{recasens2021_broaden}
\abx@aux@cite{0}{tan2021_vimpac}
\abx@aux@segm{0}{0}{tan2021_vimpac}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{pan2021_videomoco}
\abx@aux@segm{0}{0}{pan2021_videomoco}
\abx@aux@cite{0}{han2020_memdpc}
\abx@aux@segm{0}{0}{han2020_memdpc}
\abx@aux@cite{0}{qian2021_cvrl}
\abx@aux@segm{0}{0}{qian2021_cvrl}
\abx@aux@cite{0}{chen2021_corp}
\abx@aux@segm{0}{0}{chen2021_corp}
\abx@aux@cite{0}{qian2021_cvrl}
\abx@aux@segm{0}{0}{qian2021_cvrl}
\abx@aux@cite{0}{qian2021_cvrl}
\abx@aux@segm{0}{0}{qian2021_cvrl}
\abx@aux@cite{0}{recasens2021_broaden}
\abx@aux@segm{0}{0}{recasens2021_broaden}
\abx@aux@cite{0}{tan2021_vimpac}
\abx@aux@segm{0}{0}{tan2021_vimpac}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\@writefile{lot}{\contentsline {table}{\numberline {24.34}{\ignorespaces AVA v2.2 action detection (mAP) comparisons with and without intermediate labeled finetuning on the pretraining dataset \blx@tocontentsinit {0}\cite {wang2023_mvd}. “Extra labels” denotes whether the pretrained model is intermediately finetuned on the pretraining video dataset with labels before transfer to AVA.}}{1746}{table.caption.3963}\protected@file@percent }
\abx@aux@backref{3062}{wang2023_mvd}{0}{1746}{1746}
\newlabel{tab:chapter24_mvd_ava}{{24.34}{1746}{AVA v2.2 action detection (mAP) comparisons with and without intermediate labeled finetuning on the pretraining dataset \cite {wang2023_mvd}. “Extra labels” denotes whether the pretrained model is intermediately finetuned on the pretraining video dataset with labels before transfer to AVA}{table.caption.3963}{}}
\abx@aux@backref{3063}{feichtenhofer2019_slowfast}{0}{1746}{1746}
\abx@aux@backref{3064}{li2021_improved_mvit}{0}{1746}{1746}
\abx@aux@backref{3065}{li2021_improved_mvit}{0}{1746}{1746}
\abx@aux@backref{3066}{wei2022_maskfeat}{0}{1746}{1746}
\abx@aux@backref{3067}{tong2022_videomae}{0}{1746}{1746}
\abx@aux@backref{3068}{tong2022_videomae}{0}{1746}{1746}
\abx@aux@backref{3069}{tong2022_videomae}{0}{1746}{1746}
\abx@aux@backref{3070}{tong2022_videomae}{0}{1746}{1746}
\abx@aux@backref{3071}{tong2022_videomae}{0}{1746}{1746}
\abx@aux@backref{3072}{tong2022_videomae}{0}{1746}{1746}
\abx@aux@backref{3073}{feichtenhofer2022_mae_st}{0}{1746}{1746}
\abx@aux@backref{3074}{feichtenhofer2022_mae_st}{0}{1746}{1746}
\abx@aux@backref{3075}{wang2023_mvd}{0}{1746}{1746}
\abx@aux@backref{3076}{wang2023_mvd}{0}{1746}{1746}
\abx@aux@backref{3077}{wang2023_mvd}{0}{1746}{1746}
\abx@aux@backref{3078}{wang2023_mvd}{0}{1746}{1746}
\abx@aux@backref{3079}{wang2023_mvd}{0}{1746}{1746}
\abx@aux@backref{3080}{wang2023_mvd}{0}{1746}{1746}
\abx@aux@backref{3081}{wang2023_mvd}{0}{1746}{1746}
\abx@aux@backref{3082}{wang2023_mvd}{0}{1746}{1746}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\@writefile{toc}{\contentsline {paragraph}{Transfer: UCF101 and HMDB51}{1747}{section*.3964}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {24.35}{\ignorespaces Comparison with previous SOTA on UCF101 and HMDB51 (averaged over standard splits) \blx@tocontentsinit {0}\cite {wang2023_mvd,pan2021_videomoco,han2020_memdpc,chen2021_corp,qian2021_cvrl,recasens2021_broaden,tan2021_vimpac,tong2022_videomae}.}}{1747}{table.caption.3965}\protected@file@percent }
\abx@aux@backref{3091}{chen2021_corp}{0}{1747}{1747}
\abx@aux@backref{3092}{han2020_memdpc}{0}{1747}{1747}
\abx@aux@backref{3093}{pan2021_videomoco}{0}{1747}{1747}
\abx@aux@backref{3094}{qian2021_cvrl}{0}{1747}{1747}
\abx@aux@backref{3095}{recasens2021_broaden}{0}{1747}{1747}
\abx@aux@backref{3096}{tan2021_vimpac}{0}{1747}{1747}
\abx@aux@backref{3097}{tong2022_videomae}{0}{1747}{1747}
\abx@aux@backref{3098}{wang2023_mvd}{0}{1747}{1747}
\newlabel{tab:chapter24_mvd_ucf_hmdb}{{24.35}{1747}{Comparison with previous SOTA on UCF101 and HMDB51 (averaged over standard splits) \cite {wang2023_mvd,pan2021_videomoco,han2020_memdpc,chen2021_corp,qian2021_cvrl,recasens2021_broaden,tan2021_vimpac,tong2022_videomae}}{table.caption.3965}{}}
\abx@aux@backref{3099}{pan2021_videomoco}{0}{1747}{1747}
\abx@aux@backref{3100}{han2020_memdpc}{0}{1747}{1747}
\abx@aux@backref{3101}{qian2021_cvrl}{0}{1747}{1747}
\abx@aux@backref{3102}{chen2021_corp}{0}{1747}{1747}
\abx@aux@backref{3103}{qian2021_cvrl}{0}{1747}{1747}
\abx@aux@backref{3104}{qian2021_cvrl}{0}{1747}{1747}
\abx@aux@backref{3105}{recasens2021_broaden}{0}{1747}{1747}
\abx@aux@backref{3106}{tan2021_vimpac}{0}{1747}{1747}
\abx@aux@backref{3107}{tong2022_videomae}{0}{1747}{1747}
\abx@aux@backref{3108}{wang2023_mvd}{0}{1747}{1747}
\abx@aux@backref{3109}{wang2023_mvd}{0}{1747}{1747}
\@writefile{toc}{\contentsline {paragraph}{Training time}{1747}{section*.3966}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {24.36}{\ignorespaces ViT-B training time on 32$\times $V100 GPUs (teacher cost included for MVD) \blx@tocontentsinit {0}\cite {wang2023_mvd}.}}{1747}{table.caption.3967}\protected@file@percent }
\abx@aux@backref{3111}{wang2023_mvd}{0}{1747}{1747}
\newlabel{tab:chapter24_mvd_time}{{24.36}{1747}{ViT-B training time on 32$\times $V100 GPUs (teacher cost included for MVD) \cite {wang2023_mvd}}{table.caption.3967}{}}
\abx@aux@backref{3112}{tong2022_videomae}{0}{1747}{1747}
\abx@aux@backref{3113}{tong2022_videomae}{0}{1747}{1747}
\abx@aux@backref{3114}{wang2023_mvd}{0}{1747}{1747}
\@writefile{toc}{\contentsline {paragraph}{Ablations: pixels during distillation}{1747}{section*.3968}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {24.37}{\ignorespaces Effect of regressing pixels during distillation on SSv2 (student ViT-S, teacher ViT-B, 300 epochs) \blx@tocontentsinit {0}\cite {wang2023_mvd}.}}{1747}{table.caption.3969}\protected@file@percent }
\abx@aux@backref{3116}{wang2023_mvd}{0}{1747}{1747}
\newlabel{tab:chapter24_mvd_pixels}{{24.37}{1747}{Effect of regressing pixels during distillation on SSv2 (student ViT-S, teacher ViT-B, 300 epochs) \cite {wang2023_mvd}}{table.caption.3969}{}}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\@writefile{toc}{\contentsline {paragraph}{Bootstrapped teachers and IN1K-initialized students}{1748}{section*.3970}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {24.38}{\ignorespaces Comparison with bootstrapped teachers and students initialized with IN1K-pretrained models (ViT-B) \blx@tocontentsinit {0}\cite {wang2023_mvd}.}}{1748}{table.caption.3971}\protected@file@percent }
\abx@aux@backref{3118}{wang2023_mvd}{0}{1748}{1748}
\newlabel{tab:chapter24_mvd_bootstrap}{{24.38}{1748}{Comparison with bootstrapped teachers and students initialized with IN1K-pretrained models (ViT-B) \cite {wang2023_mvd}}{table.caption.3971}{}}
\@writefile{toc}{\contentsline {paragraph}{Ablations: masked reconstruction vs. per-token feature distillation}{1748}{section*.3972}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {24.39}{\ignorespaces Masked reconstruction vs. per-token feature distillation (teacher ViT-B) on K400 and SSv2 \blx@tocontentsinit {0}\cite {wang2023_mvd}.}}{1748}{table.caption.3973}\protected@file@percent }
\abx@aux@backref{3120}{wang2023_mvd}{0}{1748}{1748}
\newlabel{tab:chapter24_mvd_featuredistill}{{24.39}{1748}{Masked reconstruction vs. per-token feature distillation (teacher ViT-B) on K400 and SSv2 \cite {wang2023_mvd}}{table.caption.3973}{}}
\@writefile{toc}{\contentsline {subsubsection}{Limitations and future directions}{1748}{section*.3974}\protected@file@percent }
\newlabel{subsubsec:chapter24_mvd_limits}{{24.6.3}{1748}{Limitations and future directions}{section*.3974}{}}
\@writefile{toc}{\contentsline {paragraph}{Observed constraints}{1748}{section*.3975}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Future work}{1748}{section*.3976}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{1748}{section*.3977}\protected@file@percent }
\abx@aux@backref{3121}{tong2022_videomae}{0}{1748}{1748}
\abx@aux@backref{3122}{wang2023_mvd}{0}{1748}{1748}
\BKM@entry{id=961,dest={73656374696F6E2A2E33393738},srcline={2882}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030345C3030302E5C303030375C3030303A5C3030305C3034305C303030495C3030306E5C303030735C303030745C303030725C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030302D5C303030545C303030755C3030306E5C303030655C303030645C3030305C3034305C303030565C3030304C5C3030304C5C3030304D5C3030305C3034305C303030505C303030725C303030655C303030635C303030755C303030725C303030735C3030306F5C303030725C30303073}
\abx@aux@cite{0}{dai2023_instructblip}
\abx@aux@segm{0}{0}{dai2023_instructblip}
\abx@aux@cite{0}{liu2023_llava}
\abx@aux@segm{0}{0}{liu2023_llava}
\BKM@entry{id=962,dest={73656374696F6E2A2E33393739},srcline={2886}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030345C3030302E5C303030375C3030302E5C303030315C3030303A5C3030305C3034305C303030495C3030306E5C303030735C303030745C303030725C303030755C303030635C303030745C303030425C3030304C5C303030495C303030505C3030303A5C3030305C3034305C303030495C3030306E5C303030735C303030745C303030725C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030302D5C303030545C303030755C3030306E5C303030655C303030645C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C3030306D5C3030306F5C303030645C303030615C3030306C5C3030305C3034305C303030415C3030306C5C303030695C303030675C3030306E5C3030306D5C303030655C3030306E5C30303074}
\abx@aux@cite{0}{dai2023_instructblip}
\abx@aux@segm{0}{0}{dai2023_instructblip}
\abx@aux@cite{0}{li2023_blip2}
\abx@aux@segm{0}{0}{li2023_blip2}
\abx@aux@cite{0}{dai2023_instructblip}
\abx@aux@segm{0}{0}{dai2023_instructblip}
\abx@aux@cite{0}{dai2023_instructblip}
\abx@aux@segm{0}{0}{dai2023_instructblip}
\abx@aux@cite{0}{dai2023_instructblip}
\abx@aux@segm{0}{0}{dai2023_instructblip}
\abx@aux@cite{0}{dai2023_instructblip}
\abx@aux@segm{0}{0}{dai2023_instructblip}
\abx@aux@cite{0}{li2023_blip2}
\abx@aux@segm{0}{0}{li2023_blip2}
\abx@aux@cite{0}{dai2023_instructblip}
\abx@aux@segm{0}{0}{dai2023_instructblip}
\@writefile{toc}{\contentsline {section}{Enrichment 24.7: Instruction-Tuned VLLM Precursors}{1749}{section*.3978}\protected@file@percent }
\newlabel{enr:sec_chapter24_instrprecursors}{{24.7}{1749}{\color {ocre}Enrichment \thesection : Instruction-Tuned VLLM Precursors}{section*.3978}{}}
\abx@aux@backref{3123}{dai2023_instructblip}{0}{1749}{1749}
\abx@aux@backref{3124}{liu2023_llava}{0}{1749}{1749}
\@writefile{toc}{\contentsline {subsection}{Enrichment 24.7.1: InstructBLIP: Instruction-Tuned Multimodal Alignment}{1749}{section*.3979}\protected@file@percent }
\newlabel{enr:subsec_chapter24_instructblip}{{24.7.1}{1749}{\color {ocre}Enrichment \thesubsection : InstructBLIP: Instruction-Tuned Multimodal Alignment}{section*.3979}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation and Positioning}{1749}{section*.3980}\protected@file@percent }
\abx@aux@backref{3125}{dai2023_instructblip}{0}{1749}{1749}
\abx@aux@backref{3126}{li2023_blip2}{0}{1749}{1749}
\@writefile{toc}{\contentsline {paragraph}{High-Level Idea}{1749}{section*.3981}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24.53}{\ignorespaces \textbf  {InstructBLIP architecture}~\blx@tocontentsinit {0}\cite {dai2023_instructblip}. A frozen image encoder (e.g., ViT-g/14 from CLIP/EVA-CLIP) feeds patch embeddings to a trainable \emph  {Q-Former}. The Q-Former uses learnable queries that \emph  {attend to the instruction tokens and the visual tokens} to produce \emph  {instruction-aware} visual features. A linear projection maps these features to the frozen LLM’s embedding space (e.g., FlanT5 or Vicuna), where they serve as \emph  {soft prompts}. Training uses a next-token LM objective over instruction-formatted data; at inference the model follows arbitrary prompts in a conversational loop. Source:\blx@tocontentsinit {0}\cite {dai2023_instructblip}.}}{1749}{figure.caption.3982}\protected@file@percent }
\abx@aux@backref{3129}{dai2023_instructblip}{0}{1749}{1749}
\abx@aux@backref{3130}{dai2023_instructblip}{0}{1749}{1749}
\newlabel{fig:chpapter24_instructblip_arch}{{24.53}{1749}{\textbf {InstructBLIP architecture}~\cite {dai2023_instructblip}. A frozen image encoder (e.g., ViT-g/14 from CLIP/EVA-CLIP) feeds patch embeddings to a trainable \emph {Q-Former}. The Q-Former uses learnable queries that \emph {attend to the instruction tokens and the visual tokens} to produce \emph {instruction-aware} visual features. A linear projection maps these features to the frozen LLM’s embedding space (e.g., FlanT5 or Vicuna), where they serve as \emph {soft prompts}. Training uses a next-token LM objective over instruction-formatted data; at inference the model follows arbitrary prompts in a conversational loop. Source:\cite {dai2023_instructblip}}{figure.caption.3982}{}}
\abx@aux@cite{0}{dai2023_instructblip}
\abx@aux@segm{0}{0}{dai2023_instructblip}
\abx@aux@cite{0}{dai2023_instructblip}
\abx@aux@segm{0}{0}{dai2023_instructblip}
\abx@aux@cite{0}{dai2023_instructblip}
\abx@aux@segm{0}{0}{dai2023_instructblip}
\abx@aux@cite{0}{dai2023_instructblip}
\abx@aux@segm{0}{0}{dai2023_instructblip}
\@writefile{toc}{\contentsline {paragraph}{How It Works (Mechanism)}{1750}{section*.3983}\protected@file@percent }
\abx@aux@backref{3131}{li2023_blip2}{0}{1750}{1750}
\abx@aux@backref{3132}{dai2023_instructblip}{0}{1750}{1750}
\@writefile{toc}{\contentsline {paragraph}{Why Instruction Tuning Helps (Intuition)}{1750}{section*.3984}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24.54}{\ignorespaces \textbf  {Qualitative behaviors of InstructBLIP (Vicuna variants)}~\blx@tocontentsinit {0}\cite {dai2023_instructblip}. The model follows open-form instructions: (i) rich descriptions that list attributes and composition; (ii) visual commonsense reasoning (e.g., infer damage cause); (iii) abstract/hypothetical queries (metaphor vs.\ literal); (iv) knowledge-grounded recognition (e.g., famous artworks); (v) practical steps and multi-turn dialogue. These examples illustrate that instruction tuning teaches \emph  {how to follow instructions} rather than memorizing dataset formats. Source:\blx@tocontentsinit {0}\cite {dai2023_instructblip}.}}{1751}{figure.caption.3985}\protected@file@percent }
\abx@aux@backref{3135}{dai2023_instructblip}{0}{1751}{1751}
\abx@aux@backref{3136}{dai2023_instructblip}{0}{1751}{1751}
\newlabel{fig:chpapter24_instructblip_examples}{{24.54}{1751}{\textbf {Qualitative behaviors of InstructBLIP (Vicuna variants)}~\cite {dai2023_instructblip}. The model follows open-form instructions: (i) rich descriptions that list attributes and composition; (ii) visual commonsense reasoning (e.g., infer damage cause); (iii) abstract/hypothetical queries (metaphor vs.\ literal); (iv) knowledge-grounded recognition (e.g., famous artworks); (v) practical steps and multi-turn dialogue. These examples illustrate that instruction tuning teaches \emph {how to follow instructions} rather than memorizing dataset formats. Source:\cite {dai2023_instructblip}}{figure.caption.3985}{}}
\abx@aux@cite{0}{dai2023_instructblip}
\abx@aux@segm{0}{0}{dai2023_instructblip}
\abx@aux@cite{0}{dai2023_instructblip}
\abx@aux@segm{0}{0}{dai2023_instructblip}
\abx@aux@cite{0}{dai2023_instructblip}
\abx@aux@segm{0}{0}{dai2023_instructblip}
\abx@aux@cite{0}{dai2023_instructblip}
\abx@aux@segm{0}{0}{dai2023_instructblip}
\abx@aux@cite{0}{dai2023_instructblip}
\abx@aux@segm{0}{0}{dai2023_instructblip}
\abx@aux@cite{0}{dai2023_instructblip}
\abx@aux@segm{0}{0}{dai2023_instructblip}
\abx@aux@cite{0}{dai2023_instructblip}
\abx@aux@segm{0}{0}{dai2023_instructblip}
\abx@aux@cite{0}{dai2023_instructblip}
\abx@aux@segm{0}{0}{dai2023_instructblip}
\abx@aux@cite{0}{flamingo2022_fewshot}
\abx@aux@segm{0}{0}{flamingo2022_fewshot}
\abx@aux@cite{0}{flamingo2022_fewshot}
\abx@aux@segm{0}{0}{flamingo2022_fewshot}
\abx@aux@cite{0}{flamingo2022_fewshot}
\abx@aux@segm{0}{0}{flamingo2022_fewshot}
\abx@aux@cite{0}{li2023_blip2}
\abx@aux@segm{0}{0}{li2023_blip2}
\abx@aux@cite{0}{li2023_blip2}
\abx@aux@segm{0}{0}{li2023_blip2}
\@writefile{toc}{\contentsline {paragraph}{Data \& Formatting: From Multi-Task to Instruction-Tuning}{1752}{section*.3986}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24.55}{\ignorespaces \textbf  {Instruction-tuning sources}~\blx@tocontentsinit {0}\cite {dai2023_instructblip}. InstructBLIP formats a broad mixture of vision–language datasets as \emph  {instruction $\rightarrow $ response}. “Held-in” sets are used for tuning and evaluation; “held-out” sets are reserved for zero-shot generalization. Diversity (OCR, knowledge, reasoning, dialogue, video) is crucial for task transfer under natural prompts. Source:\blx@tocontentsinit {0}\cite {dai2023_instructblip}.}}{1752}{figure.caption.3987}\protected@file@percent }
\abx@aux@backref{3139}{dai2023_instructblip}{0}{1752}{1752}
\abx@aux@backref{3140}{dai2023_instructblip}{0}{1752}{1752}
\newlabel{fig:chpapter24_instructblip_datasets}{{24.55}{1752}{\textbf {Instruction-tuning sources}~\cite {dai2023_instructblip}. InstructBLIP formats a broad mixture of vision–language datasets as \emph {instruction $\rightarrow $ response}. “Held-in” sets are used for tuning and evaluation; “held-out” sets are reserved for zero-shot generalization. Diversity (OCR, knowledge, reasoning, dialogue, video) is crucial for task transfer under natural prompts. Source:\cite {dai2023_instructblip}}{figure.caption.3987}{}}
\@writefile{lot}{\contentsline {table}{\numberline {24.40}{\ignorespaces Zero-shot results on held-out datasets using \textsc  {InstructBLIP}~\blx@tocontentsinit {0}\cite {dai2023_instructblip}. VisDial: Visual Dialog, HM: HatefulMemes, SciQA: ScienceQA (image-context split). For NoCaps/Flickr we report CIDEr; for iVQA we report iVQA accuracy; for HM we report AUC; for VisDial we report MRR; others are top-1 accuracy (\%). Source:\blx@tocontentsinit {0}\cite {dai2023_instructblip}.}}{1752}{table.caption.3988}\protected@file@percent }
\abx@aux@backref{3143}{dai2023_instructblip}{0}{1752}{1752}
\abx@aux@backref{3144}{dai2023_instructblip}{0}{1752}{1752}
\newlabel{tab:instructblip_zeroshot}{{24.40}{1752}{Zero-shot results on held-out datasets using \textsc {InstructBLIP}~\cite {dai2023_instructblip}. VisDial: Visual Dialog, HM: HatefulMemes, SciQA: ScienceQA (image-context split). For NoCaps/Flickr we report CIDEr; for iVQA we report iVQA accuracy; for HM we report AUC; for VisDial we report MRR; others are top-1 accuracy (\%). Source:\cite {dai2023_instructblip}}{table.caption.3988}{}}
\abx@aux@backref{3145}{flamingo2022_fewshot}{0}{1752}{1752}
\abx@aux@backref{3146}{flamingo2022_fewshot}{0}{1752}{1752}
\abx@aux@backref{3147}{flamingo2022_fewshot}{0}{1752}{1752}
\abx@aux@backref{3148}{li2023_blip2}{0}{1752}{1752}
\abx@aux@backref{3149}{li2023_blip2}{0}{1752}{1752}
\abx@aux@cite{0}{dai2023_instructblip}
\abx@aux@segm{0}{0}{dai2023_instructblip}
\abx@aux@cite{0}{dai2023_instructblip}
\abx@aux@segm{0}{0}{dai2023_instructblip}
\abx@aux@cite{0}{dai2023_instructblip}
\abx@aux@segm{0}{0}{dai2023_instructblip}
\abx@aux@cite{0}{dai2023_instructblip}
\abx@aux@segm{0}{0}{dai2023_instructblip}
\abx@aux@cite{0}{dai2023_instructblip}
\abx@aux@segm{0}{0}{dai2023_instructblip}
\abx@aux@cite{0}{dai2023_instructblip}
\abx@aux@segm{0}{0}{dai2023_instructblip}
\abx@aux@cite{0}{dai2023_instructblip}
\abx@aux@segm{0}{0}{dai2023_instructblip}
\abx@aux@cite{0}{dai2023_instructblip}
\abx@aux@segm{0}{0}{dai2023_instructblip}
\@writefile{toc}{\contentsline {paragraph}{Ablations: What Matters}{1753}{section*.3989}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {24.41}{\ignorespaces Ablations from \textsc  {InstructBLIP}~\blx@tocontentsinit {0}\cite {dai2023_instructblip}. Held-in Avg.\ averages COCO Caption, OKVQA, A-OKVQA, TextCaps; held-out columns report across distinct tasks. Parentheses show deltas vs.\ the full model. Source:\blx@tocontentsinit {0}\cite {dai2023_instructblip}.}}{1753}{table.caption.3990}\protected@file@percent }
\abx@aux@backref{3152}{dai2023_instructblip}{0}{1753}{1753}
\abx@aux@backref{3153}{dai2023_instructblip}{0}{1753}{1753}
\newlabel{tab:instructblip_ablation}{{24.41}{1753}{Ablations from \textsc {InstructBLIP}~\cite {dai2023_instructblip}. Held-in Avg.\ averages COCO Caption, OKVQA, A-OKVQA, TextCaps; held-out columns report across distinct tasks. Parentheses show deltas vs.\ the full model. Source:\cite {dai2023_instructblip}}{table.caption.3990}{}}
\@writefile{toc}{\contentsline {paragraph}{Instruction Tuning vs.\ Multi-Task Training}{1753}{section*.3991}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24.56}{\ignorespaces \textbf  {Instruction tuning vs.\ multi-task training (BLIP-2 FlanT5-XL backbone)}~\blx@tocontentsinit {0}\cite {dai2023_instructblip}. Models trained on plain inputs or dataset-tag prompts excel on held-in but lag on \emph  {held-out} tasks. InstructBLIP, trained with instruction formatting and an instruction-aware Q-Former, achieves the strongest held-out generalization with competitive held-in results. Source:\blx@tocontentsinit {0}\cite {dai2023_instructblip}.}}{1753}{figure.caption.3992}\protected@file@percent }
\abx@aux@backref{3156}{dai2023_instructblip}{0}{1753}{1753}
\abx@aux@backref{3157}{dai2023_instructblip}{0}{1753}{1753}
\newlabel{fig:chpapter24_instructblip_multitask}{{24.56}{1753}{\textbf {Instruction tuning vs.\ multi-task training (BLIP-2 FlanT5-XL backbone)}~\cite {dai2023_instructblip}. Models trained on plain inputs or dataset-tag prompts excel on held-in but lag on \emph {held-out} tasks. InstructBLIP, trained with instruction formatting and an instruction-aware Q-Former, achieves the strongest held-out generalization with competitive held-in results. Source:\cite {dai2023_instructblip}}{figure.caption.3992}{}}
\abx@aux@cite{0}{dai2023_instructblip}
\abx@aux@segm{0}{0}{dai2023_instructblip}
\abx@aux@cite{0}{liu2023_llava}
\abx@aux@segm{0}{0}{liu2023_llava}
\abx@aux@cite{0}{wang2022_git}
\abx@aux@segm{0}{0}{wang2022_git}
\abx@aux@cite{0}{driess2023_palme}
\abx@aux@segm{0}{0}{driess2023_palme}
\abx@aux@cite{0}{hu2023_promptcap}
\abx@aux@segm{0}{0}{hu2023_promptcap}
\abx@aux@cite{0}{shao2023_answer_heuristics}
\abx@aux@segm{0}{0}{shao2023_answer_heuristics}
\abx@aux@cite{0}{dai2023_instructblip}
\abx@aux@segm{0}{0}{dai2023_instructblip}
\abx@aux@cite{0}{dai2023_instructblip}
\abx@aux@segm{0}{0}{dai2023_instructblip}
\abx@aux@cite{0}{liu2023_llava}
\abx@aux@segm{0}{0}{liu2023_llava}
\abx@aux@cite{0}{wang2022_git}
\abx@aux@segm{0}{0}{wang2022_git}
\abx@aux@cite{0}{driess2023_palme}
\abx@aux@segm{0}{0}{driess2023_palme}
\abx@aux@cite{0}{hu2023_promptcap}
\abx@aux@segm{0}{0}{hu2023_promptcap}
\abx@aux@cite{0}{shao2023_answer_heuristics}
\abx@aux@segm{0}{0}{shao2023_answer_heuristics}
\abx@aux@cite{0}{dai2023_instructblip}
\abx@aux@segm{0}{0}{dai2023_instructblip}
\abx@aux@cite{0}{liu2023_llava}
\abx@aux@segm{0}{0}{liu2023_llava}
\abx@aux@cite{0}{wang2022_git}
\abx@aux@segm{0}{0}{wang2022_git}
\abx@aux@cite{0}{driess2023_palme}
\abx@aux@segm{0}{0}{driess2023_palme}
\abx@aux@cite{0}{hu2023_promptcap}
\abx@aux@segm{0}{0}{hu2023_promptcap}
\abx@aux@cite{0}{shao2023_answer_heuristics}
\abx@aux@segm{0}{0}{shao2023_answer_heuristics}
\abx@aux@cite{0}{hu2023_promptcap}
\abx@aux@segm{0}{0}{hu2023_promptcap}
\abx@aux@cite{0}{shao2023_answer_heuristics}
\abx@aux@segm{0}{0}{shao2023_answer_heuristics}
\abx@aux@cite{0}{li2023_blip2}
\abx@aux@segm{0}{0}{li2023_blip2}
\abx@aux@cite{0}{dai2023_instructblip}
\abx@aux@segm{0}{0}{dai2023_instructblip}
\abx@aux@cite{0}{li2023_blip2}
\abx@aux@segm{0}{0}{li2023_blip2}
\abx@aux@cite{0}{dai2023_instructblip}
\abx@aux@segm{0}{0}{dai2023_instructblip}
\abx@aux@cite{0}{dai2023_instructblip}
\abx@aux@segm{0}{0}{dai2023_instructblip}
\@writefile{toc}{\contentsline {paragraph}{Downstream Fine-Tuning}{1754}{section*.3993}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {24.42}{\ignorespaces Fine-tuning \textsc  {BLIP-2} vs.\ \textsc  {InstructBLIP} on downstream sets~\blx@tocontentsinit {0}\cite {dai2023_instructblip}. “Previous SOTA”: LLaVA~\blx@tocontentsinit {0}\cite {liu2023_llava} (ScienceQA IMG), GIT~\blx@tocontentsinit {0}\cite {wang2022_git} (OCR-VQA), PaLM-E (562B)~\blx@tocontentsinit {0}\cite {driess2023_palme} (OKVQA), PromptCap~\blx@tocontentsinit {0}\cite {hu2023_promptcap}/Answer Heuristics~\blx@tocontentsinit {0}\cite {shao2023_answer_heuristics} (A-OKVQA). Source:\blx@tocontentsinit {0}\cite {dai2023_instructblip}.}}{1754}{table.caption.3994}\protected@file@percent }
\abx@aux@backref{3165}{dai2023_instructblip}{0}{1754}{1754}
\abx@aux@backref{3166}{liu2023_llava}{0}{1754}{1754}
\abx@aux@backref{3167}{wang2022_git}{0}{1754}{1754}
\abx@aux@backref{3168}{driess2023_palme}{0}{1754}{1754}
\abx@aux@backref{3169}{hu2023_promptcap}{0}{1754}{1754}
\abx@aux@backref{3170}{shao2023_answer_heuristics}{0}{1754}{1754}
\abx@aux@backref{3171}{dai2023_instructblip}{0}{1754}{1754}
\newlabel{tab:instructblip_finetune}{{24.42}{1754}{Fine-tuning \textsc {BLIP-2} vs.\ \textsc {InstructBLIP} on downstream sets~\cite {dai2023_instructblip}. “Previous SOTA”: LLaVA~\cite {liu2023_llava} (ScienceQA IMG), GIT~\cite {wang2022_git} (OCR-VQA), PaLM-E (562B)~\cite {driess2023_palme} (OKVQA), PromptCap~\cite {hu2023_promptcap}/Answer Heuristics~\cite {shao2023_answer_heuristics} (A-OKVQA). Source:\cite {dai2023_instructblip}}{table.caption.3994}{}}
\abx@aux@backref{3172}{liu2023_llava}{0}{1754}{1754}
\abx@aux@backref{3173}{wang2022_git}{0}{1754}{1754}
\abx@aux@backref{3174}{driess2023_palme}{0}{1754}{1754}
\abx@aux@backref{3175}{hu2023_promptcap}{0}{1754}{1754}
\abx@aux@backref{3176}{shao2023_answer_heuristics}{0}{1754}{1754}
\abx@aux@backref{3177}{hu2023_promptcap}{0}{1754}{1754}
\abx@aux@backref{3178}{shao2023_answer_heuristics}{0}{1754}{1754}
\abx@aux@backref{3179}{li2023_blip2}{0}{1754}{1754}
\abx@aux@backref{3180}{dai2023_instructblip}{0}{1754}{1754}
\abx@aux@backref{3181}{li2023_blip2}{0}{1754}{1754}
\abx@aux@backref{3182}{dai2023_instructblip}{0}{1754}{1754}
\@writefile{toc}{\contentsline {paragraph}{Takeaways (Sharper Reading of the Evidence)}{1754}{section*.3995}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Limitations and Future Work}{1754}{section*.3996}\protected@file@percent }
\abx@aux@backref{3183}{dai2023_instructblip}{0}{1754}{1754}
\abx@aux@cite{0}{liu2023_llava}
\abx@aux@segm{0}{0}{liu2023_llava}
\abx@aux@cite{0}{liu2024_llava_next}
\abx@aux@segm{0}{0}{liu2024_llava_next}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{liu2023_llava}
\abx@aux@segm{0}{0}{liu2023_llava}
\abx@aux@backref{3184}{li2024_llavaonevision}{0}{1755}{1755}
\abx@aux@backref{3185}{liu2024_llava_next}{0}{1755}{1755}
\abx@aux@backref{3186}{liu2023_llava}{0}{1755}{1755}
\abx@aux@backref{3187}{liu2023_llava}{0}{1755}{1755}
\BKM@entry{id=963,dest={73656374696F6E2A2E33393937},srcline={3087}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030345C3030302E5C303030375C3030302E5C303030325C3030303A5C3030305C3034305C3030304C5C3030304C5C303030615C303030565C303030415C3030303A5C3030305C3034305C3030304C5C303030615C303030725C303030675C303030655C3030305C3034305C3030304C5C303030615C3030306E5C303030675C303030755C303030615C303030675C303030655C3030305C3034305C303030615C3030306E5C303030645C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030305C3034305C303030415C303030735C303030735C303030695C303030735C303030745C303030615C3030306E5C30303074}
\abx@aux@cite{0}{liu2023_llava}
\abx@aux@segm{0}{0}{liu2023_llava}
\abx@aux@cite{0}{liu2023_llava}
\abx@aux@segm{0}{0}{liu2023_llava}
\abx@aux@cite{0}{liu2023_llava}
\abx@aux@segm{0}{0}{liu2023_llava}
\abx@aux@cite{0}{liu2023_llava}
\abx@aux@segm{0}{0}{liu2023_llava}
\abx@aux@cite{0}{liu2023_llava}
\abx@aux@segm{0}{0}{liu2023_llava}
\@writefile{toc}{\contentsline {subsection}{Enrichment 24.7.2: LLaVA: Large Language and Vision Assistant}{1756}{section*.3997}\protected@file@percent }
\newlabel{enr:subsec_chapter24_llava}{{24.7.2}{1756}{\color {ocre}Enrichment \thesubsection : LLaVA: Large Language and Vision Assistant}{section*.3997}{}}
\@writefile{toc}{\contentsline {paragraph}{High-Level Idea}{1756}{section*.3998}\protected@file@percent }
\abx@aux@backref{3188}{liu2023_llava}{0}{1756}{1756}
\@writefile{toc}{\contentsline {paragraph}{Architecture}{1756}{section*.3999}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24.57}{\ignorespaces \textbf  {LLaVA network.} A frozen CLIP ViT-L/14 produces grid features; a linear projector $W$ maps them to the LLM token space. Visual tokens are concatenated with the dialogue tokens and trained via LM loss. (As discussed by~\blx@tocontentsinit {0}\cite {liu2023_llava}, more sophisticated connectors—e.g., Flamingo’s gated cross-attention or BLIP-2’s Q-Former—are possible but were not the focus.). Source:\blx@tocontentsinit {0}\cite {liu2023_llava}.}}{1756}{figure.caption.4000}\protected@file@percent }
\abx@aux@backref{3191}{liu2023_llava}{0}{1756}{1756}
\abx@aux@backref{3192}{liu2023_llava}{0}{1756}{1756}
\newlabel{fig:chpapter24_llava_architecture}{{24.57}{1756}{\textbf {LLaVA network.} A frozen CLIP ViT-L/14 produces grid features; a linear projector $W$ maps them to the LLM token space. Visual tokens are concatenated with the dialogue tokens and trained via LM loss. (As discussed by~\cite {liu2023_llava}, more sophisticated connectors—e.g., Flamingo’s gated cross-attention or BLIP-2’s Q-Former—are possible but were not the focus.). Source:\cite {liu2023_llava}}{figure.caption.4000}{}}
\@writefile{toc}{\contentsline {paragraph}{Why freeze vision but (partly) train the LLM?}{1756}{section*.4001}\protected@file@percent }
\abx@aux@cite{0}{liu2023_llava}
\abx@aux@segm{0}{0}{liu2023_llava}
\@writefile{toc}{\contentsline {paragraph}{Data Pipeline: Visual Instruction Tuning}{1757}{section*.4002}\protected@file@percent }
\abx@aux@backref{3193}{liu2023_llava}{0}{1757}{1757}
\abx@aux@cite{0}{liu2023_llava}
\abx@aux@segm{0}{0}{liu2023_llava}
\abx@aux@cite{0}{liu2023_llava}
\abx@aux@segm{0}{0}{liu2023_llava}
\abx@aux@cite{0}{liu2023_llava}
\abx@aux@segm{0}{0}{liu2023_llava}
\abx@aux@cite{0}{liu2023_llava}
\abx@aux@segm{0}{0}{liu2023_llava}
\@writefile{lof}{\contentsline {figure}{\numberline {24.58}{\ignorespaces \textbf  {Instruction-following data construction (illustrative snippet).} Top: text-only \emph  {contexts} (captions / boxes) shown to GPT; bottom: three \emph  {response types}. The raw image is \emph  {not} fed to GPT—only used for human reference in the paper. Source:\blx@tocontentsinit {0}\cite {liu2023_llava}.}}{1758}{figure.caption.4003}\protected@file@percent }
\abx@aux@backref{3195}{liu2023_llava}{0}{1758}{1758}
\newlabel{fig:chpapter24_llava_examples}{{24.58}{1758}{\textbf {Instruction-following data construction (illustrative snippet).} Top: text-only \emph {contexts} (captions / boxes) shown to GPT; bottom: three \emph {response types}. The raw image is \emph {not} fed to GPT—only used for human reference in the paper. Source:\cite {liu2023_llava}}{figure.caption.4003}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24.59}{\ignorespaces \textbf  {Input sequence for training.} Visual tokens from $H_v$ are concatenated with dialogue tokens. The model learns to generate assistant answers and the stop symbol (\texttt  {<STOP>=\#\#\#}) autoregressively; only assistant tokens contribute to the loss. Source:\blx@tocontentsinit {0}\cite {liu2023_llava}.}}{1758}{figure.caption.4004}\protected@file@percent }
\abx@aux@backref{3197}{liu2023_llava}{0}{1758}{1758}
\newlabel{fig:chpapter24_llava_input_sequence}{{24.59}{1758}{\textbf {Input sequence for training.} Visual tokens from $H_v$ are concatenated with dialogue tokens. The model learns to generate assistant answers and the stop symbol (\texttt {<STOP>=\#\#\#}) autoregressively; only assistant tokens contribute to the loss. Source:\cite {liu2023_llava}}{figure.caption.4004}{}}
\@writefile{toc}{\contentsline {paragraph}{Why It Works (vs.\ BLIP/BLIP-2)}{1758}{section*.4005}\protected@file@percent }
\abx@aux@cite{0}{liu2023_llava}
\abx@aux@segm{0}{0}{liu2023_llava}
\abx@aux@cite{0}{liu2023_llava}
\abx@aux@segm{0}{0}{liu2023_llava}
\abx@aux@cite{0}{liu2023_llava}
\abx@aux@segm{0}{0}{liu2023_llava}
\abx@aux@cite{0}{liu2023_llava}
\abx@aux@segm{0}{0}{liu2023_llava}
\@writefile{toc}{\contentsline {paragraph}{Instruction Following and Reasoning (Qualitative)}{1759}{section*.4006}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24.60}{\ignorespaces \textbf  {LLaVA-Bench (In-the-Wild): challenging, high-resolution cases with detailed human annotations.} The benchmark probes real-world capabilities beyond generic captioning, stressing OCR, fine-grained recognition, spatial reasoning, and knowledge grounding. \emph  {Example 1 (left, ``ICHIRAN Ramen''):} requires reading small text in-the-wild (OCR) and linking it to world knowledge to answer queries such as \emph  {``What’s the name of the restaurant?''}. \emph  {Example 2 (right, ``Filled Fridge''):} demands locating fine-grained items, reading brand labels (e.g., \emph  {Fage} variants), and reasoning over cluttered layouts to answer compositional questions (e.g., brand identification, presence/absence of a flavor). These cases illustrate why instruction-following VLMs must combine accurate text extraction, object/attribute recognition, and commonsense knowledge to succeed. Source:\blx@tocontentsinit {0}\cite {liu2023_llava}.}}{1759}{figure.caption.4007}\protected@file@percent }
\abx@aux@backref{3199}{liu2023_llava}{0}{1759}{1759}
\newlabel{fig:chpapter24_llava_in_the_wild}{{24.60}{1759}{\textbf {LLaVA-Bench (In-the-Wild): challenging, high-resolution cases with detailed human annotations.} The benchmark probes real-world capabilities beyond generic captioning, stressing OCR, fine-grained recognition, spatial reasoning, and knowledge grounding. \emph {Example 1 (left, ``ICHIRAN Ramen''):} requires reading small text in-the-wild (OCR) and linking it to world knowledge to answer queries such as \emph {``What’s the name of the restaurant?''}. \emph {Example 2 (right, ``Filled Fridge''):} demands locating fine-grained items, reading brand labels (e.g., \emph {Fage} variants), and reasoning over cluttered layouts to answer compositional questions (e.g., brand identification, presence/absence of a flavor). These cases illustrate why instruction-following VLMs must combine accurate text extraction, object/attribute recognition, and commonsense knowledge to succeed. Source:\cite {liu2023_llava}}{figure.caption.4007}{}}
\abx@aux@cite{0}{liu2023_llava}
\abx@aux@segm{0}{0}{liu2023_llava}
\abx@aux@cite{0}{liu2023_llava}
\abx@aux@segm{0}{0}{liu2023_llava}
\abx@aux@cite{0}{liu2023_llava}
\abx@aux@segm{0}{0}{liu2023_llava}
\abx@aux@cite{0}{liu2023_llava}
\abx@aux@segm{0}{0}{liu2023_llava}
\abx@aux@cite{0}{awadalla2023_openflamingo}
\abx@aux@segm{0}{0}{awadalla2023_openflamingo}
\abx@aux@cite{0}{li2023_blip2}
\abx@aux@segm{0}{0}{li2023_blip2}
\abx@aux@cite{0}{liu2023_llava}
\abx@aux@segm{0}{0}{liu2023_llava}
\abx@aux@cite{0}{liu2023_llava}
\abx@aux@segm{0}{0}{liu2023_llava}
\@writefile{lof}{\contentsline {figure}{\numberline {24.61}{\ignorespaces \textbf  {Following instructions vs.\ scene description.} LLaVA answers “what is unusual?” with multi-step reasoning and safety considerations, outperforming BLIP-2 / OpenFlamingo on instruction adherence; GPT-4 is concise but less conversational. Source:\blx@tocontentsinit {0}\cite {liu2023_llava}.}}{1760}{figure.caption.4008}\protected@file@percent }
\abx@aux@backref{3201}{liu2023_llava}{0}{1760}{1760}
\@writefile{toc}{\contentsline {paragraph}{Benchmarks: LLaVA-Bench, COCO ablations, In-the-Wild, ScienceQA}{1760}{section*.4009}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {24.43}{\ignorespaces Ablation on LLaVA-Bench (COCO). Scores are relative to a text-only GPT-4 that sees ground-truth captions/boxes. Removing instruction tuning is catastrophic, highlighting its centrality. Source:\blx@tocontentsinit {0}\cite {liu2023_llava}.}}{1760}{table.caption.4010}\protected@file@percent }
\abx@aux@backref{3203}{liu2023_llava}{0}{1760}{1760}
\newlabel{tab:llava_coco_ablation}{{24.43}{1760}{Ablation on LLaVA-Bench (COCO). Scores are relative to a text-only GPT-4 that sees ground-truth captions/boxes. Removing instruction tuning is catastrophic, highlighting its centrality. Source:\cite {liu2023_llava}}{table.caption.4010}{}}
\abx@aux@cite{0}{lu2022_scienceqa}
\abx@aux@segm{0}{0}{lu2022_scienceqa}
\abx@aux@cite{0}{liu2023_llava}
\abx@aux@segm{0}{0}{liu2023_llava}
\abx@aux@cite{0}{lu2022_scienceqa}
\abx@aux@segm{0}{0}{lu2022_scienceqa}
\abx@aux@cite{0}{liu2023_llava}
\abx@aux@segm{0}{0}{liu2023_llava}
\abx@aux@cite{0}{lu2022_scienceqa}
\abx@aux@segm{0}{0}{lu2022_scienceqa}
\abx@aux@cite{0}{lu2022_scienceqa}
\abx@aux@segm{0}{0}{lu2022_scienceqa}
\abx@aux@cite{0}{lu2022_scienceqa}
\abx@aux@segm{0}{0}{lu2022_scienceqa}
\abx@aux@cite{0}{zhang2023_llama_adapter}
\abx@aux@segm{0}{0}{zhang2023_llama_adapter}
\abx@aux@cite{0}{zhang2024_mcot}
\abx@aux@segm{0}{0}{zhang2024_mcot}
\abx@aux@cite{0}{zhang2024_mcot}
\abx@aux@segm{0}{0}{zhang2024_mcot}
\abx@aux@cite{0}{liu2023_llava}
\abx@aux@segm{0}{0}{liu2023_llava}
\abx@aux@cite{0}{liu2023_llava}
\abx@aux@segm{0}{0}{liu2023_llava}
\abx@aux@cite{0}{liu2023_llava}
\abx@aux@segm{0}{0}{liu2023_llava}
\@writefile{lot}{\contentsline {table}{\numberline {24.44}{\ignorespaces Instruction-following comparison (relative scores) on LLaVA-Bench (In-the-Wild). Means $\pm $ std over three runs for the first three rows; for LLaVA$^\dagger $, GPT-4 is queried three times as judge. Source:\blx@tocontentsinit {0}\cite {liu2023_llava}.}}{1761}{table.caption.4011}\protected@file@percent }
\abx@aux@backref{3205}{liu2023_llava}{0}{1761}{1761}
\newlabel{tab:llava_inthewild}{{24.44}{1761}{Instruction-following comparison (relative scores) on LLaVA-Bench (In-the-Wild). Means $\pm $ std over three runs for the first three rows; for LLaVA$^\dagger $, GPT-4 is queried three times as judge. Source:\cite {liu2023_llava}}{table.caption.4011}{}}
\abx@aux@backref{3206}{awadalla2023_openflamingo}{0}{1761}{1761}
\abx@aux@backref{3207}{li2023_blip2}{0}{1761}{1761}
\abx@aux@backref{3208}{liu2023_llava}{0}{1761}{1761}
\abx@aux@backref{3209}{liu2023_llava}{0}{1761}{1761}
\@writefile{lot}{\contentsline {table}{\numberline {24.45}{\ignorespaces ScienceQA~\blx@tocontentsinit {0}\cite {lu2022_scienceqa} accuracy (\%). NAT/SOC/LAN: domains; TXT/IMG/NO: context types; G1–6/G7–12: grade levels. $^\dagger $Text-only GPT-4 (our eval.). Source:\blx@tocontentsinit {0}\cite {liu2023_llava}.}}{1761}{table.caption.4012}\protected@file@percent }
\abx@aux@backref{3212}{lu2022_scienceqa}{0}{1761}{1761}
\abx@aux@backref{3213}{liu2023_llava}{0}{1761}{1761}
\newlabel{tab:llava_scienceqa}{{24.45}{1761}{ScienceQA~\cite {lu2022_scienceqa} accuracy (\%). NAT/SOC/LAN: domains; TXT/IMG/NO: context types; G1–6/G7–12: grade levels. $^\dagger $Text-only GPT-4 (our eval.). Source:\cite {liu2023_llava}}{table.caption.4012}{}}
\abx@aux@backref{3214}{lu2022_scienceqa}{0}{1761}{1761}
\abx@aux@backref{3215}{lu2022_scienceqa}{0}{1761}{1761}
\abx@aux@backref{3216}{lu2022_scienceqa}{0}{1761}{1761}
\abx@aux@backref{3217}{zhang2023_llama_adapter}{0}{1761}{1761}
\abx@aux@backref{3218}{zhang2024_mcot}{0}{1761}{1761}
\abx@aux@backref{3219}{zhang2024_mcot}{0}{1761}{1761}
\abx@aux@backref{3220}{liu2023_llava}{0}{1761}{1761}
\@writefile{lot}{\contentsline {table}{\numberline {24.46}{\ignorespaces Design ablations on ScienceQA (Average \%). Differences vs.\ best variant in parentheses. Source:\blx@tocontentsinit {0}\cite {liu2023_llava}.}}{1761}{table.caption.4013}\protected@file@percent }
\abx@aux@backref{3222}{liu2023_llava}{0}{1761}{1761}
\newlabel{tab:llava_design_ablations}{{24.46}{1761}{Design ablations on ScienceQA (Average \%). Differences vs.\ best variant in parentheses. Source:\cite {liu2023_llava}}{table.caption.4013}{}}
\@writefile{toc}{\contentsline {paragraph}{What the Ablations Say (and How This Differs from BLIP-2)}{1761}{section*.4014}\protected@file@percent }
\abx@aux@cite{0}{liu2023_llava}
\abx@aux@segm{0}{0}{liu2023_llava}
\abx@aux@cite{0}{liu2024_llava_next}
\abx@aux@segm{0}{0}{liu2024_llava_next}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\@writefile{toc}{\contentsline {paragraph}{Positioning vs.\ BLIP/BLIP-2}{1762}{section*.4015}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Limitations and Next Steps (segue to LLaVA-NeXT / OneVision)}{1762}{section*.4016}\protected@file@percent }
\abx@aux@backref{3223}{liu2023_llava}{0}{1762}{1762}
\abx@aux@backref{3224}{liu2024_llava_next}{0}{1762}{1762}
\abx@aux@backref{3225}{li2024_llavaonevision}{0}{1762}{1762}
\BKM@entry{id=964,dest={73656374696F6E2A2E34303137},srcline={3316}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030345C3030302E5C303030375C3030302E5C303030335C3030303A5C3030305C3034305C3030304C5C3030304C5C303030615C303030565C303030415C3030302D5C3030304F5C3030306E5C303030655C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3030303A5C3030305C3034305C303030555C3030306E5C303030695C303030665C303030695C303030655C303030645C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C3030306D5C3030306F5C303030645C303030615C3030306C5C3030305C3034305C303030545C303030725C303030615C3030306E5C303030735C303030665C303030655C30303072}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\@writefile{toc}{\contentsline {subsection}{Enrichment 24.7.3: LLaVA-OneVision: Unified Multimodal Transfer}{1763}{section*.4017}\protected@file@percent }
\newlabel{enr:subsec_chapter24_llava_onevision}{{24.7.3}{1763}{\color {ocre}Enrichment \thesubsection : LLaVA-OneVision: Unified Multimodal Transfer}{section*.4017}{}}
\@writefile{toc}{\contentsline {paragraph}{From LLaVA to OneVision: Motivation \& Goal}{1763}{section*.4018}\protected@file@percent }
\abx@aux@backref{3226}{li2024_llavaonevision}{0}{1763}{1763}
\@writefile{toc}{\contentsline {paragraph}{High-Level Idea}{1763}{section*.4019}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24.62}{\ignorespaces \textbf  {LLaVA-OneVision architecture.} Left: a concrete instantiation; Right: the generalized LLaVA form extended to support single images, multi-image sets, and video clips in one pipeline. \emph  {Source:}~\blx@tocontentsinit {0}\cite {li2024_llavaonevision}.}}{1763}{figure.caption.4020}\protected@file@percent }
\abx@aux@backref{3228}{li2024_llavaonevision}{0}{1763}{1763}
\newlabel{fig:chapter24_llava_onevision_architecture}{{24.62}{1763}{\textbf {LLaVA-OneVision architecture.} Left: a concrete instantiation; Right: the generalized LLaVA form extended to support single images, multi-image sets, and video clips in one pipeline. \emph {Source:}~\cite {li2024_llavaonevision}}{figure.caption.4020}{}}
\@writefile{toc}{\contentsline {paragraph}{Architecture Overview (What changes vs.\ LLaVA)}{1763}{section*.4022}\protected@file@percent }
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@backref{3229}{li2024_llavaonevision}{0}{1764}{1764}
\abx@aux@backref{3230}{li2024_llavaonevision}{0}{1764}{1764}
\abx@aux@backref{3231}{li2024_llavaonevision}{0}{1764}{1764}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\@writefile{lof}{\contentsline {figure}{\numberline {24.63}{\ignorespaces \textbf  {Higher AnyRes vs.\ original AnyRes.} Upgraded tiling/merging with bilinear interpolation preserves high-resolution fidelity (top) compared to the original scheme (bottom), improving OCR and small-object recognition. \emph  {Source:}~\blx@tocontentsinit {0}\cite {li2024_llavaonevision}.}}{1765}{figure.caption.4023}\protected@file@percent }
\abx@aux@backref{3233}{li2024_llavaonevision}{0}{1765}{1765}
\newlabel{fig:chapter24_llava_onevision_anyres}{{24.63}{1765}{\textbf {Higher AnyRes vs.\ original AnyRes.} Upgraded tiling/merging with bilinear interpolation preserves high-resolution fidelity (top) compared to the original scheme (bottom), improving OCR and small-object recognition. \emph {Source:}~\cite {li2024_llavaonevision}}{figure.caption.4023}{}}
\abx@aux@backref{3234}{li2024_llavaonevision}{0}{1765}{1765}
\abx@aux@backref{3235}{li2024_llavaonevision}{0}{1765}{1765}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@backref{3236}{li2024_llavaonevision}{0}{1766}{1766}
\abx@aux@backref{3237}{li2024_llavaonevision}{0}{1766}{1766}
\abx@aux@backref{3238}{li2024_llavaonevision}{0}{1766}{1766}
\abx@aux@backref{3239}{li2024_llavaonevision}{0}{1766}{1766}
\abx@aux@backref{3240}{li2024_llavaonevision}{0}{1766}{1766}
\@writefile{lof}{\contentsline {figure}{\numberline {24.64}{\ignorespaces \textbf  {Balanced visual token allocation across modalities.} OneVision caps tokens so single-image, multi-image, and video inputs receive comparable visual capacity (e.g., $\sim $729 tokens $\approx $ SigLIP at $384{\times }384$), preserving LLM context and encouraging cross-scenario transfer. \emph  {Source:}~\blx@tocontentsinit {0}\cite {li2024_llavaonevision}.}}{1766}{figure.caption.4024}\protected@file@percent }
\abx@aux@backref{3242}{li2024_llavaonevision}{0}{1766}{1766}
\newlabel{fig:chapter24_llava_onevision_token_strategy}{{24.64}{1766}{\textbf {Balanced visual token allocation across modalities.} OneVision caps tokens so single-image, multi-image, and video inputs receive comparable visual capacity (e.g., $\sim $729 tokens $\approx $ SigLIP at $384{\times }384$), preserving LLM context and encouraging cross-scenario transfer. \emph {Source:}~\cite {li2024_llavaonevision}}{figure.caption.4024}{}}
\abx@aux@backref{3243}{li2024_llavaonevision}{0}{1766}{1766}
\abx@aux@backref{3244}{li2024_llavaonevision}{0}{1766}{1766}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@backref{3245}{li2024_llavaonevision}{0}{1767}{1767}
\abx@aux@backref{3246}{li2024_llavaonevision}{0}{1767}{1767}
\abx@aux@backref{3247}{li2024_llavaonevision}{0}{1767}{1767}
\@writefile{toc}{\contentsline {paragraph}{Training Curriculum (How capabilities are built)}{1767}{section*.4025}\protected@file@percent }
\abx@aux@backref{3248}{li2024_llavaonevision}{0}{1767}{1767}
\abx@aux@backref{3249}{li2024_llavaonevision}{0}{1767}{1767}
\abx@aux@backref{3250}{li2024_llavaonevision}{0}{1767}{1767}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@backref{3251}{li2024_llavaonevision}{0}{1768}{1768}
\abx@aux@backref{3252}{li2024_llavaonevision}{0}{1768}{1768}
\abx@aux@backref{3253}{li2024_llavaonevision}{0}{1768}{1768}
\abx@aux@backref{3254}{li2024_llavaonevision}{0}{1768}{1768}
\@writefile{lof}{\contentsline {figure}{\numberline {24.65}{\ignorespaces \textbf  {Training stages and configurations.} Vision backbone, token budget, datasets, model scale, and hyperparameters per stage illustrate the curriculum from alignment to mixed-modality instruction tuning. \emph  {Source:}~\blx@tocontentsinit {0}\cite {li2024_llavaonevision}.}}{1768}{figure.caption.4026}\protected@file@percent }
\abx@aux@backref{3256}{li2024_llavaonevision}{0}{1768}{1768}
\newlabel{fig:chapter24_llava_onevision_stages}{{24.65}{1768}{\textbf {Training stages and configurations.} Vision backbone, token budget, datasets, model scale, and hyperparameters per stage illustrate the curriculum from alignment to mixed-modality instruction tuning. \emph {Source:}~\cite {li2024_llavaonevision}}{figure.caption.4026}{}}
\@writefile{toc}{\contentsline {paragraph}{Data Collections (for SFT)}{1768}{section*.4027}\protected@file@percent }
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@backref{3257}{li2024_llavaonevision}{0}{1769}{1769}
\@writefile{lof}{\contentsline {figure}{\numberline {24.66}{\ignorespaces \textbf  {Single-Image (3.2M) collection.} Left: category distribution (general QA/captioning, docs/charts/screens, math/reasoning, language, OCR). Right: dataset counts. Curated coverage builds a strong single-image instruction base before introducing multi-image/video. \emph  {Source:}~\blx@tocontentsinit {0}\cite {li2024_llavaonevision}.}}{1769}{figure.caption.4028}\protected@file@percent }
\abx@aux@backref{3259}{li2024_llavaonevision}{0}{1769}{1769}
\newlabel{fig:chapter24_llava_onevision_singleimage_data}{{24.66}{1769}{\textbf {Single-Image (3.2M) collection.} Left: category distribution (general QA/captioning, docs/charts/screens, math/reasoning, language, OCR). Right: dataset counts. Curated coverage builds a strong single-image instruction base before introducing multi-image/video. \emph {Source:}~\cite {li2024_llavaonevision}}{figure.caption.4028}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24.67}{\ignorespaces \textbf  {OneVision mixed set (1.6M).} Left: distribution over \emph  {multi-image}, \emph  {video}, and \emph  {single-image}; Right: dataset counts (``MI'' denotes multi-image variants). Mixed-modality SFT promotes coherent reasoning across images and over time under a shared token budget. \emph  {Source:}~\blx@tocontentsinit {0}\cite {li2024_llavaonevision}.}}{1769}{figure.caption.4029}\protected@file@percent }
\abx@aux@backref{3261}{li2024_llavaonevision}{0}{1769}{1769}
\newlabel{fig:chapter24_llava_onevision_mixed_data}{{24.67}{1769}{\textbf {OneVision mixed set (1.6M).} Left: distribution over \emph {multi-image}, \emph {video}, and \emph {single-image}; Right: dataset counts (``MI'' denotes multi-image variants). Mixed-modality SFT promotes coherent reasoning across images and over time under a shared token budget. \emph {Source:}~\cite {li2024_llavaonevision}}{figure.caption.4029}{}}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@backref{3262}{li2024_llavaonevision}{0}{1770}{1770}
\abx@aux@backref{3263}{li2024_llavaonevision}{0}{1770}{1770}
\@writefile{toc}{\contentsline {paragraph}{What the Experiments Show}{1770}{section*.4031}\protected@file@percent }
\abx@aux@backref{3264}{li2024_llavaonevision}{0}{1770}{1770}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\@writefile{toc}{\contentsline {paragraph}{Ablation Themes (High-Level)}{1771}{section*.4032}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Qualitative Capabilities (Selected Examples)}{1771}{section*.4033}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24.68}{\ignorespaces \textbf  {Across-image diagram/table understanding.} The model synthesizes evidence across multiple diagrams/tables (e.g., cross-referencing axes, legends, and cells) to answer compositional questions—illustrating robust \emph  {multi-image transfer} beyond single-image captioning. \emph  {Source:}~\blx@tocontentsinit {0}\cite {li2024_llavaonevision}.}}{1771}{figure.caption.4034}\protected@file@percent }
\abx@aux@backref{3266}{li2024_llavaonevision}{0}{1771}{1771}
\newlabel{fig:chapter24_llava_onevision_charts}{{24.68}{1771}{\textbf {Across-image diagram/table understanding.} The model synthesizes evidence across multiple diagrams/tables (e.g., cross-referencing axes, legends, and cells) to answer compositional questions—illustrating robust \emph {multi-image transfer} beyond single-image captioning. \emph {Source:}~\cite {li2024_llavaonevision}}{figure.caption.4034}{}}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\@writefile{lof}{\contentsline {figure}{\numberline {24.69}{\ignorespaces \textbf  {Agentic reasoning on UIs.} Given several phone screenshots, the model plans step-wise actions (e.g., tap/scroll/type) grounded in on-screen text and layout, demonstrating instruction following that bridges \emph  {vision $\rightarrow $ action suggestions}. \emph  {Source:}~\blx@tocontentsinit {0}\cite {li2024_llavaonevision}.}}{1772}{figure.caption.4035}\protected@file@percent }
\abx@aux@backref{3268}{li2024_llavaonevision}{0}{1772}{1772}
\newlabel{fig:chapter24_llava_onevision_agents}{{24.69}{1772}{\textbf {Agentic reasoning on UIs.} Given several phone screenshots, the model plans step-wise actions (e.g., tap/scroll/type) grounded in on-screen text and layout, demonstrating instruction following that bridges \emph {vision $\rightarrow $ action suggestions}. \emph {Source:}~\cite {li2024_llavaonevision}}{figure.caption.4035}{}}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\@writefile{lof}{\contentsline {figure}{\numberline {24.70}{\ignorespaces \textbf  {Set-of-mark prompting.} The model uses numbered marks to localize and describe fine-grained regions (e.g., “mark~3 is a pressure gauge”), enabling precise references without extra detection heads. \emph  {Source:}~\blx@tocontentsinit {0}\cite {li2024_llavaonevision}.}}{1773}{figure.caption.4036}\protected@file@percent }
\abx@aux@backref{3270}{li2024_llavaonevision}{0}{1773}{1773}
\newlabel{fig:chapter24_llava_onevision_setofmark}{{24.70}{1773}{\textbf {Set-of-mark prompting.} The model uses numbered marks to localize and describe fine-grained regions (e.g., “mark~3 is a pressure gauge”), enabling precise references without extra detection heads. \emph {Source:}~\cite {li2024_llavaonevision}}{figure.caption.4036}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24.71}{\ignorespaces \textbf  {Image-to-video prompt transfer.} From a static image, the model drafts detailed, temporally-aware prompts for video generation/editing (e.g., motions, transitions, camera moves), showcasing \emph  {image$\rightarrow $video} transfer of high-level intent. \emph  {Source:}~\blx@tocontentsinit {0}\cite {li2024_llavaonevision}.}}{1773}{figure.caption.4037}\protected@file@percent }
\abx@aux@backref{3272}{li2024_llavaonevision}{0}{1773}{1773}
\newlabel{fig:chapter24_llava_onevision_img2vid_edit}{{24.71}{1773}{\textbf {Image-to-video prompt transfer.} From a static image, the model drafts detailed, temporally-aware prompts for video generation/editing (e.g., motions, transitions, camera moves), showcasing \emph {image$\rightarrow $video} transfer of high-level intent. \emph {Source:}~\cite {li2024_llavaonevision}}{figure.caption.4037}{}}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\@writefile{lof}{\contentsline {figure}{\numberline {24.72}{\ignorespaces \textbf  {Video-to-video difference (same start, different endings).} The model contrasts two clips that share an opening but diverge later, identifying \emph  {when} and \emph  {how} outcomes differ—evidence of native temporal reasoning. \emph  {Source:}~\blx@tocontentsinit {0}\cite {li2024_llavaonevision}.}}{1774}{figure.caption.4038}\protected@file@percent }
\abx@aux@backref{3274}{li2024_llavaonevision}{0}{1774}{1774}
\newlabel{fig:chapter24_llava_onevision_vid2vid_diff_a}{{24.72}{1774}{\textbf {Video-to-video difference (same start, different endings).} The model contrasts two clips that share an opening but diverge later, identifying \emph {when} and \emph {how} outcomes differ—evidence of native temporal reasoning. \emph {Source:}~\cite {li2024_llavaonevision}}{figure.caption.4038}{}}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\@writefile{lof}{\contentsline {figure}{\numberline {24.73}{\ignorespaces \textbf  {Video-to-video difference (similar background, different foreground).} With background held constant across clips, the model focuses on foreground actors/objects to explain semantic changes—probing \emph  {foreground-aware} temporal understanding. \emph  {Source:}~\blx@tocontentsinit {0}\cite {li2024_llavaonevision}.}}{1775}{figure.caption.4039}\protected@file@percent }
\abx@aux@backref{3276}{li2024_llavaonevision}{0}{1775}{1775}
\newlabel{fig:chapter24_llava_onevision_vid2vid_diff_b}{{24.73}{1775}{\textbf {Video-to-video difference (similar background, different foreground).} With background held constant across clips, the model focuses on foreground actors/objects to explain semantic changes—probing \emph {foreground-aware} temporal understanding. \emph {Source:}~\cite {li2024_llavaonevision}}{figure.caption.4039}{}}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\@writefile{lof}{\contentsline {figure}{\numberline {24.74}{\ignorespaces \textbf  {Multi-camera driving videos.} The model integrates synchronized views (front/side/rear) to explain traffic participants and events, reflecting \emph  {multi-view + temporal} fusion useful for autonomy-style reasoning. \emph  {Source:}~\blx@tocontentsinit {0}\cite {li2024_llavaonevision}.}}{1776}{figure.caption.4040}\protected@file@percent }
\abx@aux@backref{3278}{li2024_llavaonevision}{0}{1776}{1776}
\newlabel{fig:chapter24_llava_onevision_selfdriving}{{24.74}{1776}{\textbf {Multi-camera driving videos.} The model integrates synchronized views (front/side/rear) to explain traffic participants and events, reflecting \emph {multi-view + temporal} fusion useful for autonomy-style reasoning. \emph {Source:}~\cite {li2024_llavaonevision}}{figure.caption.4040}{}}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\@writefile{lof}{\contentsline {figure}{\numberline {24.75}{\ignorespaces \textbf  {Composed sub-videos.} The model narrates a timeline formed by ordered sub-clips, keeping track of entities and transitions—showcasing long-range \emph  {event composition} rather than frame-level description. \emph  {Source:}~\blx@tocontentsinit {0}\cite {li2024_llavaonevision}.}}{1777}{figure.caption.4041}\protected@file@percent }
\abx@aux@backref{3280}{li2024_llavaonevision}{0}{1777}{1777}
\newlabel{fig:chapter24_llava_onevision_subvideos}{{24.75}{1777}{\textbf {Composed sub-videos.} The model narrates a timeline formed by ordered sub-clips, keeping track of entities and transitions—showcasing long-range \emph {event composition} rather than frame-level description. \emph {Source:}~\cite {li2024_llavaonevision}}{figure.caption.4041}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24.76}{\ignorespaces \textbf  {Referring image \& video understanding.} Given a reference image, the model grounds identities in a target video (presence/absence, re-identification), unifying \emph  {multi-image linkage} with \emph  {temporal tracking}. \emph  {Source:}~\blx@tocontentsinit {0}\cite {li2024_llavaonevision}.}}{1778}{figure.caption.4042}\protected@file@percent }
\abx@aux@backref{3282}{li2024_llavaonevision}{0}{1778}{1778}
\newlabel{fig:chapter24_llava_onevision_referring}{{24.76}{1778}{\textbf {Referring image \& video understanding.} Given a reference image, the model grounds identities in a target video (presence/absence, re-identification), unifying \emph {multi-image linkage} with \emph {temporal tracking}. \emph {Source:}~\cite {li2024_llavaonevision}}{figure.caption.4042}{}}
\abx@aux@cite{0}{an2025_llavaonevision15}
\abx@aux@segm{0}{0}{an2025_llavaonevision15}
\@writefile{toc}{\contentsline {paragraph}{Current Constraints}{1779}{section*.4044}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Directions and the Move to OV-1.5}{1779}{section*.4045}\protected@file@percent }
\abx@aux@backref{3283}{an2025_llavaonevision15}{0}{1779}{1779}
\@writefile{toc}{\contentsline {paragraph}{Future Directions}{1779}{section*.4046}\protected@file@percent }
\BKM@entry{id=965,dest={73656374696F6E2A2E34303437},srcline={3620}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030345C3030302E5C303030385C3030303A5C3030305C3034305C3030304C5C303030615C303030725C303030675C303030655C3030302D5C303030535C303030635C303030615C3030306C5C303030655C3030305C3034305C303030565C303030695C303030645C303030655C3030306F5C3030305C3034305C303030465C3030306F5C303030755C3030306E5C303030645C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\abx@aux@cite{0}{wang2022_internvideo}
\abx@aux@segm{0}{0}{wang2022_internvideo}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{wang2022_omnivl}
\abx@aux@segm{0}{0}{wang2022_omnivl}
\BKM@entry{id=966,dest={73656374696F6E2A2E34303438},srcline={3624}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030345C3030302E5C303030385C3030302E5C303030315C3030303A5C3030305C3034305C303030495C3030306E5C303030745C303030655C303030725C3030306E5C303030565C303030695C303030645C303030655C3030306F5C3030303A5C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C3030306C5C3030305C3034305C303030565C303030695C303030645C303030655C3030306F5C3030305C3034305C303030425C303030615C303030635C3030306B5C303030625C3030306F5C3030306E5C303030655C30303073}
\abx@aux@cite{0}{wang2022_internvideo}
\abx@aux@segm{0}{0}{wang2022_internvideo}
\abx@aux@cite{0}{feichtenhofer2019_slowfast}
\abx@aux@segm{0}{0}{feichtenhofer2019_slowfast}
\abx@aux@cite{0}{lin2019_tsm}
\abx@aux@segm{0}{0}{lin2019_tsm}
\abx@aux@cite{0}{arnab2021_vivit}
\abx@aux@segm{0}{0}{arnab2021_vivit}
\abx@aux@cite{0}{yan2022_mtv}
\abx@aux@segm{0}{0}{yan2022_mtv}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\abx@aux@cite{0}{li2022_uniformerv2}
\abx@aux@segm{0}{0}{li2022_uniformerv2}
\abx@aux@cite{0}{zellers2022_merlotreserve}
\abx@aux@segm{0}{0}{zellers2022_merlotreserve}
\abx@aux@cite{0}{wang2022_internvideo}
\abx@aux@segm{0}{0}{wang2022_internvideo}
\abx@aux@cite{0}{feichtenhofer2019_slowfast}
\abx@aux@segm{0}{0}{feichtenhofer2019_slowfast}
\abx@aux@cite{0}{lin2019_tsm}
\abx@aux@segm{0}{0}{lin2019_tsm}
\abx@aux@cite{0}{arnab2021_vivit}
\abx@aux@segm{0}{0}{arnab2021_vivit}
\abx@aux@cite{0}{yan2022_mtv}
\abx@aux@segm{0}{0}{yan2022_mtv}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\abx@aux@cite{0}{li2022_uniformerv2}
\abx@aux@segm{0}{0}{li2022_uniformerv2}
\abx@aux@cite{0}{zellers2022_merlotreserve}
\abx@aux@segm{0}{0}{zellers2022_merlotreserve}
\abx@aux@cite{0}{wang2022_internvideo}
\abx@aux@segm{0}{0}{wang2022_internvideo}
\@writefile{toc}{\contentsline {section}{Enrichment 24.8: Large-Scale Video Foundation Models}{1780}{section*.4047}\protected@file@percent }
\newlabel{enr:sec_chapter24_foundation_models}{{24.8}{1780}{\color {ocre}Enrichment \thesection : Large-Scale Video Foundation Models}{section*.4047}{}}
\abx@aux@backref{3284}{wang2022_internvideo}{0}{1780}{1780}
\abx@aux@backref{3285}{wang2024_internvideo2}{0}{1780}{1780}
\abx@aux@backref{3286}{wang2022_omnivl}{0}{1780}{1780}
\@writefile{toc}{\contentsline {subsection}{Enrichment 24.8.1: InternVideo: General Video Backbones}{1780}{section*.4048}\protected@file@percent }
\newlabel{enr:subsec_chapter24_internvideo}{{24.8.1}{1780}{\color {ocre}Enrichment \thesubsection : InternVideo: General Video Backbones}{section*.4048}{}}
\@writefile{toc}{\contentsline {paragraph}{Scope and positioning}{1780}{section*.4049}\protected@file@percent }
\abx@aux@backref{3287}{wang2022_internvideo}{0}{1780}{1780}
\@writefile{lof}{\contentsline {figure}{\numberline {24.77}{\ignorespaces \textbf  {SOTA overview.} InternVideo delivers the best performance on extensive video-related tasks, compared with specialized~\blx@tocontentsinit {0}\cite {feichtenhofer2019_slowfast,lin2019_tsm,arnab2021_vivit,yan2022_mtv} and foundation models~\blx@tocontentsinit {0}\cite {radford2021_clip,li2022_uniformerv2,zellers2022_merlotreserve}. Abbreviations: v2t/t2v retrieval, STA, FHP, NLQ, SCOD, MQ. Figure adapted from \blx@tocontentsinit {0}\cite {wang2022_internvideo}.}}{1780}{figure.caption.4050}\protected@file@percent }
\abx@aux@backref{3296}{arnab2021_vivit}{0}{1780}{1780}
\abx@aux@backref{3297}{feichtenhofer2019_slowfast}{0}{1780}{1780}
\abx@aux@backref{3298}{lin2019_tsm}{0}{1780}{1780}
\abx@aux@backref{3299}{yan2022_mtv}{0}{1780}{1780}
\abx@aux@backref{3300}{li2022_uniformerv2}{0}{1780}{1780}
\abx@aux@backref{3301}{radford2021_clip}{0}{1780}{1780}
\abx@aux@backref{3302}{zellers2022_merlotreserve}{0}{1780}{1780}
\abx@aux@backref{3303}{wang2022_internvideo}{0}{1780}{1780}
\newlabel{fig:chapter24_internvideo_sota}{{24.77}{1780}{\textbf {SOTA overview.} InternVideo delivers the best performance on extensive video-related tasks, compared with specialized~\cite {feichtenhofer2019_slowfast,lin2019_tsm,arnab2021_vivit,yan2022_mtv} and foundation models~\cite {radford2021_clip,li2022_uniformerv2,zellers2022_merlotreserve}. Abbreviations: v2t/t2v retrieval, STA, FHP, NLQ, SCOD, MQ. Figure adapted from \cite {wang2022_internvideo}}{figure.caption.4050}{}}
\abx@aux@cite{0}{wang2022_internvideo}
\abx@aux@segm{0}{0}{wang2022_internvideo}
\abx@aux@cite{0}{wang2022_internvideo}
\abx@aux@segm{0}{0}{wang2022_internvideo}
\abx@aux@cite{0}{wang2022_internvideo}
\abx@aux@segm{0}{0}{wang2022_internvideo}
\abx@aux@cite{0}{wang2022_internvideo}
\abx@aux@segm{0}{0}{wang2022_internvideo}
\abx@aux@cite{0}{li2022_uniformer}
\abx@aux@segm{0}{0}{li2022_uniformer}
\abx@aux@cite{0}{li2022_uniformerv2}
\abx@aux@segm{0}{0}{li2022_uniformerv2}
\abx@aux@cite{0}{wang2022_internvideo}
\abx@aux@segm{0}{0}{wang2022_internvideo}
\abx@aux@cite{0}{wang2022_internvideo}
\abx@aux@segm{0}{0}{wang2022_internvideo}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{1781}{section*.4051}\protected@file@percent }
\newlabel{subsubsec_chapter24_internvideo_motivation}{{24.8.1}{1781}{Motivation}{section*.4051}{}}
\abx@aux@backref{3304}{wang2022_internvideo}{0}{1781}{1781}
\abx@aux@backref{3305}{wang2022_internvideo}{0}{1781}{1781}
\abx@aux@backref{3306}{wang2022_internvideo}{0}{1781}{1781}
\abx@aux@backref{3307}{wang2022_internvideo}{0}{1781}{1781}
\abx@aux@backref{3308}{li2022_uniformer}{0}{1781}{1781}
\abx@aux@backref{3309}{li2022_uniformerv2}{0}{1781}{1781}
\abx@aux@cite{0}{li2022_uniformer}
\abx@aux@segm{0}{0}{li2022_uniformer}
\abx@aux@cite{0}{li2022_uniformerv2}
\abx@aux@segm{0}{0}{li2022_uniformerv2}
\abx@aux@cite{0}{li2022_uniformer}
\abx@aux@segm{0}{0}{li2022_uniformer}
\@writefile{lof}{\contentsline {figure}{\numberline {24.78}{\ignorespaces \textbf  {Unified framework.} Dual pretraining pathways (masked video reconstruction and multimodal contrastive learning) are coordinated via CMA for broad downstream transfer. Adapted from \blx@tocontentsinit {0}\cite {wang2022_internvideo}.}}{1782}{figure.caption.4052}\protected@file@percent }
\abx@aux@backref{3311}{wang2022_internvideo}{0}{1782}{1782}
\newlabel{fig:chapter24_internvideo_framework}{{24.78}{1782}{\textbf {Unified framework.} Dual pretraining pathways (masked video reconstruction and multimodal contrastive learning) are coordinated via CMA for broad downstream transfer. Adapted from \cite {wang2022_internvideo}}{figure.caption.4052}{}}
\@writefile{toc}{\contentsline {subsubsection}{Preliminaries: UniFormer and UniFormerV2}{1782}{section*.4053}\protected@file@percent }
\newlabel{subsubsec_chapter24_uniformer}{{24.8.1}{1782}{Preliminaries: UniFormer and UniFormerV2}{section*.4053}{}}
\@writefile{toc}{\contentsline {paragraph}{Why these preliminaries matter here.}{1782}{section*.4054}\protected@file@percent }
\abx@aux@backref{3312}{li2022_uniformer}{0}{1782}{1782}
\abx@aux@backref{3313}{li2022_uniformerv2}{0}{1782}{1782}
\abx@aux@backref{3314}{li2022_uniformer}{0}{1782}{1782}
\@writefile{toc}{\contentsline {paragraph}{UniFormer (CVPR’22)~\blx@tocontentsinit {0}\cite {li2022_uniformer}}{1782}{section*.4055}\protected@file@percent }
\newlabel{eq:uni_dpe}{{24.20}{1782}{UniFormer (CVPR’22)~\cite {li2022_uniformer}}{equation.24.20}{}}
\newlabel{eq:uni_mhra_uniformer}{{24.21}{1782}{UniFormer (CVPR’22)~\cite {li2022_uniformer}}{equation.24.21}{}}
\newlabel{eq:uni_ffn_uniformer}{{24.22}{1782}{UniFormer (CVPR’22)~\cite {li2022_uniformer}}{equation.24.22}{}}
\@writefile{toc}{\contentsline {paragraph}{Dynamic Position Embedding (DPE): learnable relative spatiotemporal bias.}{1783}{section*.4056}\protected@file@percent }
\newlabel{eq:uni_dpe_def}{{24.23}{1783}{Dynamic Position Embedding (DPE): learnable relative spatiotemporal bias}{equation.24.23}{}}
\@writefile{toc}{\contentsline {paragraph}{MHRA (general form): one template that adapts with depth.}{1783}{section*.4057}\protected@file@percent }
\newlabel{eq:uni_mhra}{{24.24}{1783}{MHRA (general form): one template that adapts with depth}{equation.24.24}{}}
\newlabel{eq:uni_mhra_general}{{24.25}{1783}{MHRA (general form): one template that adapts with depth}{equation.24.25}{}}
\@writefile{toc}{\contentsline {paragraph}{MHRA—Local (shallow stages): cheap neighborhood mixing.}{1783}{section*.4058}\protected@file@percent }
\newlabel{eq:uni_local}{{24.26}{1783}{MHRA—Local (shallow stages): cheap neighborhood mixing}{equation.24.26}{}}
\@writefile{toc}{\contentsline {paragraph}{MHRA—Global (deep stages): full space–time self-attention when it counts.}{1784}{section*.4059}\protected@file@percent }
\newlabel{eq:uni_global}{{24.27}{1784}{MHRA—Global (deep stages): full space–time self-attention when it counts}{equation.24.27}{}}
\@writefile{toc}{\contentsline {paragraph}{FFN: per-token refinement.}{1784}{section*.4060}\protected@file@percent }
\newlabel{eq:uni_ffn}{{24.28}{1784}{FFN: per-token refinement}{equation.24.28}{}}
\@writefile{toc}{\contentsline {paragraph}{Putting it together: why this staging works for video.}{1784}{section*.4061}\protected@file@percent }
\abx@aux@cite{0}{li2022_uniformer}
\abx@aux@segm{0}{0}{li2022_uniformer}
\abx@aux@cite{0}{li2022_uniformer}
\abx@aux@segm{0}{0}{li2022_uniformer}
\@writefile{toc}{\contentsline {paragraph}{Concrete cue.}{1785}{section*.4062}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24.79}{\ignorespaces \textbf  {UniFormer architecture.} A UniFormer block combines DPE, MHRA, and FFN. Early blocks employ local MHRA; deeper blocks employ global MHRA to capture long-range space--time dependencies. Adapted from \blx@tocontentsinit {0}\cite {li2022_uniformer}.}}{1785}{figure.caption.4063}\protected@file@percent }
\abx@aux@backref{3316}{li2022_uniformer}{0}{1785}{1785}
\newlabel{fig:chapter24_uniformer_arch}{{24.79}{1785}{\textbf {UniFormer architecture.} A UniFormer block combines DPE, MHRA, and FFN. Early blocks employ local MHRA; deeper blocks employ global MHRA to capture long-range space--time dependencies. Adapted from \cite {li2022_uniformer}}{figure.caption.4063}{}}
\@writefile{toc}{\contentsline {paragraph}{From UniFormer (V1): what we gained, and what still needs fixing.}{1785}{section*.4064}\protected@file@percent }
\abx@aux@cite{0}{li2022_uniformerv2}
\abx@aux@segm{0}{0}{li2022_uniformerv2}
\abx@aux@backref{3317}{li2022_uniformerv2}{0}{1786}{1786}
\@writefile{toc}{\contentsline {paragraph}{UniFormerV2 (ICCV’22)~\blx@tocontentsinit {0}\cite {li2022_uniformerv2}: arming image ViTs for video, with full formulation and clear integration.}{1786}{section*.4065}\protected@file@percent }
\newlabel{eq:uv2_lt}{{24.29}{1786}{UniFormerV2 (ICCV’22)~\cite {li2022_uniformerv2}: arming image ViTs for video, with full formulation and clear integration}{equation.24.29}{}}
\newlabel{eq:uv2_gs}{{24.30}{1786}{UniFormerV2 (ICCV’22)~\cite {li2022_uniformerv2}: arming image ViTs for video, with full formulation and clear integration}{equation.24.30}{}}
\newlabel{eq:uv2_ffn}{{24.31}{1786}{UniFormerV2 (ICCV’22)~\cite {li2022_uniformerv2}: arming image ViTs for video, with full formulation and clear integration}{equation.24.31}{}}
\newlabel{eq:uv2_lt_aff}{{24.32}{1786}{UniFormerV2 (ICCV’22)~\cite {li2022_uniformerv2}: arming image ViTs for video, with full formulation and clear integration}{equation.24.32}{}}
\newlabel{eq:uv2_gs_aff}{{24.33}{1786}{UniFormerV2 (ICCV’22)~\cite {li2022_uniformerv2}: arming image ViTs for video, with full formulation and clear integration}{equation.24.33}{}}
\newlabel{eq:uv2_dpe}{{24.34}{1786}{UniFormerV2 (ICCV’22)~\cite {li2022_uniformerv2}: arming image ViTs for video, with full formulation and clear integration}{equation.24.34}{}}
\newlabel{eq:uv2_cmhra}{{24.35}{1786}{UniFormerV2 (ICCV’22)~\cite {li2022_uniformerv2}: arming image ViTs for video, with full formulation and clear integration}{equation.24.35}{}}
\newlabel{eq:uv2_g_ffn}{{24.36}{1786}{UniFormerV2 (ICCV’22)~\cite {li2022_uniformerv2}: arming image ViTs for video, with full formulation and clear integration}{equation.24.36}{}}
\newlabel{eq:uv2_cross}{{24.38}{1787}{UniFormerV2 (ICCV’22)~\cite {li2022_uniformerv2}: arming image ViTs for video, with full formulation and clear integration}{equation.24.38}{}}
\@writefile{toc}{\contentsline {paragraph}{Bridging to the method: why UniFormer/UniFormerV2 set the stage.}{1787}{section*.4066}\protected@file@percent }
\abx@aux@cite{0}{li2022_uniformerv2}
\abx@aux@segm{0}{0}{li2022_uniformerv2}
\abx@aux@cite{0}{li2022_uniformerv2}
\abx@aux@segm{0}{0}{li2022_uniformerv2}
\abx@aux@cite{0}{li2022_uniformerv2}
\abx@aux@segm{0}{0}{li2022_uniformerv2}
\abx@aux@cite{0}{li2022_uniformerv2}
\abx@aux@segm{0}{0}{li2022_uniformerv2}
\@writefile{lof}{\contentsline {figure}{\numberline {24.80}{\ignorespaces \textbf  {Why arm image ViTs.} Naïvely adding temporal MHSA to image ViTs tends to underperform for a given compute budget. UniFormerV2 keeps strong spatial priors and adds concise temporal modules to achieve superior accuracy--FLOPs trade-offs on video. Adapted from \blx@tocontentsinit {0}\cite {li2022_uniformerv2}.}}{1788}{figure.caption.4067}\protected@file@percent }
\abx@aux@backref{3319}{li2022_uniformerv2}{0}{1788}{1788}
\newlabel{fig:chapter24_uniformer_v1v2}{{24.80}{1788}{\textbf {Why arm image ViTs.} Naïvely adding temporal MHSA to image ViTs tends to underperform for a given compute budget. UniFormerV2 keeps strong spatial priors and adds concise temporal modules to achieve superior accuracy--FLOPs trade-offs on video. Adapted from \cite {li2022_uniformerv2}}{figure.caption.4067}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24.81}{\ignorespaces \textbf  {UniFormerV2 framework.} Each stage consists of a Local UniBlock (LT-MHRA adapter \(\rightarrow \) preserved ViT spatial attention \(\rightarrow \) FFN). Selected deep stages add a Global UniBlock that forms a per-clip video token via one-query cross-attention; multiple video tokens are fused to form the final descriptor. Adapted from \blx@tocontentsinit {0}\cite {li2022_uniformerv2}.}}{1788}{figure.caption.4068}\protected@file@percent }
\abx@aux@backref{3321}{li2022_uniformerv2}{0}{1788}{1788}
\newlabel{fig:chapter24_uniformer_v2_method}{{24.81}{1788}{\textbf {UniFormerV2 framework.} Each stage consists of a Local UniBlock (LT-MHRA adapter \(\rightarrow \) preserved ViT spatial attention \(\rightarrow \) FFN). Selected deep stages add a Global UniBlock that forms a per-clip video token via one-query cross-attention; multiple video tokens are fused to form the final descriptor. Adapted from \cite {li2022_uniformerv2}}{figure.caption.4068}{}}
\abx@aux@cite{0}{li2022_uniformerv2}
\abx@aux@segm{0}{0}{li2022_uniformerv2}
\abx@aux@cite{0}{wang2022_internvideo}
\abx@aux@segm{0}{0}{wang2022_internvideo}
\abx@aux@cite{0}{wang2022_internvideo}
\abx@aux@segm{0}{0}{wang2022_internvideo}
\abx@aux@cite{0}{wang2022_internvideo}
\abx@aux@segm{0}{0}{wang2022_internvideo}
\abx@aux@cite{0}{wang2022_internvideo}
\abx@aux@segm{0}{0}{wang2022_internvideo}
\@writefile{toc}{\contentsline {subsubsection}{Method}{1789}{section*.4069}\protected@file@percent }
\newlabel{subsubsec_chapter24_internvideo_method}{{24.8.1}{1789}{Method}{section*.4069}{}}
\@writefile{toc}{\contentsline {paragraph}{High-level overview}{1789}{section*.4070}\protected@file@percent }
\abx@aux@backref{3322}{li2022_uniformerv2}{0}{1789}{1789}
\abx@aux@backref{3323}{wang2022_internvideo}{0}{1789}{1789}
\abx@aux@backref{3324}{wang2022_internvideo}{0}{1789}{1789}
\@writefile{lof}{\contentsline {figure}{\numberline {24.82}{\ignorespaces \textbf  {Pretraining pathways.} (a) Masked video modeling with an asymmetric ViT encoder–decoder. (b) Multimodal learning with UniFormerV2 video encoder, CLIP-initialized text encoder, and a cross-modal caption decoder. Adapted from \blx@tocontentsinit {0}\cite {wang2022_internvideo}.}}{1789}{figure.caption.4071}\protected@file@percent }
\abx@aux@backref{3326}{wang2022_internvideo}{0}{1789}{1789}
\newlabel{fig:chapter24_internvideo_mve_mml}{{24.82}{1789}{\textbf {Pretraining pathways.} (a) Masked video modeling with an asymmetric ViT encoder–decoder. (b) Multimodal learning with UniFormerV2 video encoder, CLIP-initialized text encoder, and a cross-modal caption decoder. Adapted from \cite {wang2022_internvideo}}{figure.caption.4071}{}}
\@writefile{toc}{\contentsline {paragraph}{Notation}{1789}{section*.4072}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{1) Generative path --- Masked Video Encoder (MVE)}{1789}{section*.4073}\protected@file@percent }
\abx@aux@cite{0}{wang2022_internvideo}
\abx@aux@segm{0}{0}{wang2022_internvideo}
\abx@aux@cite{0}{li2022_uniformerv2}
\abx@aux@segm{0}{0}{li2022_uniformerv2}
\newlabel{eq:internvideo_pix}{{24.39}{1790}{1) Generative path --- Masked Video Encoder (MVE)}{equation.24.39}{}}
\@writefile{toc}{\contentsline {paragraph}{2) Discriminative path --- Multimodal Video Encoder (MMVE)}{1790}{section*.4074}\protected@file@percent }
\abx@aux@backref{3327}{li2022_uniformerv2}{0}{1790}{1790}
\abx@aux@backref{3328}{wang2022_internvideo}{0}{1790}{1790}
\newlabel{eq:internvideo_contrast}{{24.42}{1790}{2) Discriminative path --- Multimodal Video Encoder (MMVE)}{equation.24.42}{}}
\@writefile{toc}{\contentsline {paragraph}{Captioning loss: concise mechanics and intuition}{1790}{section*.4075}\protected@file@percent }
\abx@aux@cite{0}{wang2022_internvideo}
\abx@aux@segm{0}{0}{wang2022_internvideo}
\abx@aux@cite{0}{wang2022_internvideo}
\abx@aux@segm{0}{0}{wang2022_internvideo}
\@writefile{toc}{\contentsline {paragraph}{3) Coordination — Cross-Model Attention (CMA).}{1791}{section*.4076}\protected@file@percent }
\abx@aux@backref{3329}{wang2022_internvideo}{0}{1791}{1791}
\abx@aux@backref{3330}{wang2022_internvideo}{0}{1791}{1791}
\abx@aux@cite{0}{wang2022_internvideo}
\abx@aux@segm{0}{0}{wang2022_internvideo}
\abx@aux@cite{0}{wang2022_internvideo}
\abx@aux@segm{0}{0}{wang2022_internvideo}
\abx@aux@cite{0}{wang2022_internvideo}
\abx@aux@segm{0}{0}{wang2022_internvideo}
\@writefile{lof}{\contentsline {figure}{\numberline {24.83}{\ignorespaces \textbf  {Cross-Model Attention (CMA).} Middle: MVE tokens query MMVE tokens to import semantics into the masked stream. Final: the MMVE class token queries MVE tokens to add motion/detail to the multimodal summary used by the head. Adapted from \blx@tocontentsinit {0}\cite {wang2022_internvideo}.}}{1792}{figure.caption.4078}\protected@file@percent }
\abx@aux@backref{3332}{wang2022_internvideo}{0}{1792}{1792}
\newlabel{fig:chapter24_internvideo_cma}{{24.83}{1792}{\textbf {Cross-Model Attention (CMA).} Middle: MVE tokens query MMVE tokens to import semantics into the masked stream. Final: the MMVE class token queries MVE tokens to add motion/detail to the multimodal summary used by the head. Adapted from \cite {wang2022_internvideo}}{figure.caption.4078}{}}
\@writefile{toc}{\contentsline {paragraph}{4) Prediction heads and supervised adaptation}{1792}{section*.4079}\protected@file@percent }
\abx@aux@backref{3333}{wang2022_internvideo}{0}{1792}{1792}
\@writefile{toc}{\contentsline {paragraph}{5) End-to-end flow (one pass)}{1792}{section*.4080}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Architecture and Implementation Details}{1793}{section*.4081}\protected@file@percent }
\newlabel{subsubsec_chapter24_internvideo_arch_impl}{{24.8.1}{1793}{Architecture and Implementation Details}{section*.4081}{}}
\@writefile{toc}{\contentsline {paragraph}{Backbone choices}{1793}{section*.4082}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Tokenization and shapes}{1793}{section*.4083}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{UniFormerV2 block order in MMVE}{1793}{section*.4084}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Cross-Model Attention (CMA) placement}{1793}{section*.4085}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Training schedule}{1793}{section*.4086}\protected@file@percent }
\abx@aux@cite{0}{wang2022_internvideo}
\abx@aux@segm{0}{0}{wang2022_internvideo}
\abx@aux@cite{0}{wei2022_maskfeat}
\abx@aux@segm{0}{0}{wei2022_maskfeat}
\abx@aux@cite{0}{yu2022_coca}
\abx@aux@segm{0}{0}{yu2022_coca}
\abx@aux@cite{0}{yan2022_mtv}
\abx@aux@segm{0}{0}{yan2022_mtv}
\abx@aux@cite{0}{wang2022_internvideo}
\abx@aux@segm{0}{0}{wang2022_internvideo}
\abx@aux@cite{0}{wang2022_internvideo}
\abx@aux@segm{0}{0}{wang2022_internvideo}
\abx@aux@cite{0}{zhang2022_actionformer}
\abx@aux@segm{0}{0}{zhang2022_actionformer}
\abx@aux@cite{0}{xu2021_tcanet}
\abx@aux@segm{0}{0}{xu2021_tcanet}
\abx@aux@cite{0}{wang2022_internvideo}
\abx@aux@segm{0}{0}{wang2022_internvideo}
\abx@aux@cite{0}{pan2021_acar}
\abx@aux@segm{0}{0}{pan2021_acar}
\abx@aux@cite{0}{li2020_relationalmaps}
\abx@aux@segm{0}{0}{li2020_relationalmaps}
\abx@aux@cite{0}{wei2022_maskfeat}
\abx@aux@segm{0}{0}{wei2022_maskfeat}
\abx@aux@cite{0}{wang2022_internvideo}
\abx@aux@segm{0}{0}{wang2022_internvideo}
\abx@aux@cite{0}{luo2022_clip4clip}
\abx@aux@segm{0}{0}{luo2022_clip4clip}
\abx@aux@cite{0}{ma2022_xclip}
\abx@aux@segm{0}{0}{ma2022_xclip}
\abx@aux@cite{0}{liu2022_ts2net}
\abx@aux@segm{0}{0}{liu2022_ts2net}
\abx@aux@cite{0}{wang2022_internvideo}
\abx@aux@segm{0}{0}{wang2022_internvideo}
\abx@aux@cite{0}{lei2021_clipbert}
\abx@aux@segm{0}{0}{lei2021_clipbert}
\abx@aux@cite{0}{fu2021_violet}
\abx@aux@segm{0}{0}{fu2021_violet}
\abx@aux@cite{0}{zellers2021_merlot}
\abx@aux@segm{0}{0}{zellers2021_merlot}
\abx@aux@cite{0}{wang2022_allinone}
\abx@aux@segm{0}{0}{wang2022_allinone}
\abx@aux@cite{0}{wang2022_internvideo}
\abx@aux@segm{0}{0}{wang2022_internvideo}
\abx@aux@cite{0}{wang2022_internvideo}
\abx@aux@segm{0}{0}{wang2022_internvideo}
\abx@aux@cite{0}{wang2022_internvideo}
\abx@aux@segm{0}{0}{wang2022_internvideo}
\abx@aux@cite{0}{wang2022_internvideo}
\abx@aux@segm{0}{0}{wang2022_internvideo}
\abx@aux@cite{0}{li2022_uniformerv2}
\abx@aux@segm{0}{0}{li2022_uniformerv2}
\abx@aux@cite{0}{wang2022_internvideo}
\abx@aux@segm{0}{0}{wang2022_internvideo}
\abx@aux@cite{0}{wang2022_internvideo}
\abx@aux@segm{0}{0}{wang2022_internvideo}
\abx@aux@cite{0}{yu2022_coca}
\abx@aux@segm{0}{0}{yu2022_coca}
\abx@aux@cite{0}{yan2022_mtv}
\abx@aux@segm{0}{0}{yan2022_mtv}
\abx@aux@cite{0}{wang2022_internvideo}
\abx@aux@segm{0}{0}{wang2022_internvideo}
\@writefile{toc}{\contentsline {subsubsection}{Experiments and Ablations}{1794}{section*.4087}\protected@file@percent }
\newlabel{subsubsec_chapter24_internvideo_experiments}{{24.8.1}{1794}{Experiments and Ablations}{section*.4087}{}}
\@writefile{toc}{\contentsline {paragraph}{Bottom-line summary across tasks}{1794}{section*.4088}\protected@file@percent }
\abx@aux@backref{3334}{wang2022_internvideo}{0}{1794}{1794}
\abx@aux@backref{3335}{wei2022_maskfeat}{0}{1794}{1794}
\abx@aux@backref{3336}{yan2022_mtv}{0}{1794}{1794}
\abx@aux@backref{3337}{yu2022_coca}{0}{1794}{1794}
\abx@aux@backref{3338}{wang2022_internvideo}{0}{1794}{1794}
\abx@aux@backref{3339}{wang2022_internvideo}{0}{1794}{1794}
\abx@aux@backref{3340}{xu2021_tcanet}{0}{1794}{1794}
\abx@aux@backref{3341}{zhang2022_actionformer}{0}{1794}{1794}
\abx@aux@backref{3342}{li2020_relationalmaps}{0}{1794}{1794}
\abx@aux@backref{3343}{pan2021_acar}{0}{1794}{1794}
\abx@aux@backref{3344}{wang2022_internvideo}{0}{1794}{1794}
\abx@aux@backref{3345}{wei2022_maskfeat}{0}{1794}{1794}
\abx@aux@backref{3346}{liu2022_ts2net}{0}{1794}{1794}
\abx@aux@backref{3347}{luo2022_clip4clip}{0}{1794}{1794}
\abx@aux@backref{3348}{ma2022_xclip}{0}{1794}{1794}
\abx@aux@backref{3349}{wang2022_internvideo}{0}{1794}{1794}
\abx@aux@backref{3350}{fu2021_violet}{0}{1794}{1794}
\abx@aux@backref{3351}{lei2021_clipbert}{0}{1794}{1794}
\abx@aux@backref{3352}{wang2022_allinone}{0}{1794}{1794}
\abx@aux@backref{3353}{wang2022_internvideo}{0}{1794}{1794}
\abx@aux@backref{3354}{zellers2021_merlot}{0}{1794}{1794}
\abx@aux@backref{3355}{wang2022_internvideo}{0}{1794}{1794}
\@writefile{toc}{\contentsline {paragraph}{Key ablations and what they imply}{1794}{section*.4089}\protected@file@percent }
\abx@aux@backref{3356}{wang2022_internvideo}{0}{1794}{1794}
\abx@aux@backref{3357}{wang2022_internvideo}{0}{1794}{1794}
\abx@aux@backref{3358}{wang2022_internvideo}{0}{1794}{1794}
\abx@aux@backref{3359}{li2022_uniformerv2}{0}{1794}{1794}
\abx@aux@backref{3360}{wang2022_internvideo}{0}{1794}{1794}
\@writefile{toc}{\contentsline {paragraph}{Representative takeaways}{1794}{section*.4090}\protected@file@percent }
\abx@aux@backref{3361}{wang2022_internvideo}{0}{1794}{1794}
\abx@aux@backref{3362}{yan2022_mtv}{0}{1794}{1794}
\abx@aux@backref{3363}{yu2022_coca}{0}{1794}{1794}
\abx@aux@backref{3364}{wang2022_internvideo}{0}{1794}{1794}
\abx@aux@cite{0}{wang2022_internvideo}
\abx@aux@segm{0}{0}{wang2022_internvideo}
\abx@aux@cite{0}{wang2022_internvideo}
\abx@aux@segm{0}{0}{wang2022_internvideo}
\@writefile{toc}{\contentsline {subsubsection}{Limitations and Follow-up Works}{1795}{section*.4091}\protected@file@percent }
\newlabel{subsubsec_chapter24_internvideo_limits_future}{{24.8.1}{1795}{Limitations and Follow-up Works}{section*.4091}{}}
\@writefile{toc}{\contentsline {paragraph}{Current limitations}{1795}{section*.4092}\protected@file@percent }
\abx@aux@backref{3365}{wang2022_internvideo}{0}{1795}{1795}
\@writefile{toc}{\contentsline {paragraph}{Buildup toward InternVideoV2}{1795}{section*.4093}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Takeaway}{1795}{section*.4094}\protected@file@percent }
\abx@aux@backref{3366}{wang2022_internvideo}{0}{1795}{1795}
\BKM@entry{id=967,dest={73656374696F6E2A2E34303935},srcline={4117}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030345C3030302E5C303030385C3030302E5C303030325C3030303A5C3030305C3034305C3030304F5C3030306D5C3030306E5C303030695C303030565C3030304C5C3030303A5C3030305C3034305C3030304F5C3030306E5C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C303030495C3030306D5C303030615C303030675C303030655C3034305C3032335C303030565C303030695C303030645C303030655C3030306F5C3034305C3032335C3030304C5C303030615C3030306E5C303030675C303030755C303030615C303030675C30303065}
\abx@aux@cite{0}{wang2022_omnivl}
\abx@aux@segm{0}{0}{wang2022_omnivl}
\abx@aux@cite{0}{wang2022_omnivl}
\abx@aux@segm{0}{0}{wang2022_omnivl}
\abx@aux@cite{0}{wang2022_omnivl}
\abx@aux@segm{0}{0}{wang2022_omnivl}
\abx@aux@cite{0}{wang2022_omnivl}
\abx@aux@segm{0}{0}{wang2022_omnivl}
\@writefile{toc}{\contentsline {subsection}{Enrichment 24.8.2: OmniVL: One Model for Image–Video–Language}{1796}{section*.4095}\protected@file@percent }
\newlabel{enr:subsec_chapter24_omnivl}{{24.8.2}{1796}{\color {ocre}Enrichment \thesubsection : OmniVL: One Model for Image–Video–Language}{section*.4095}{}}
\@writefile{toc}{\contentsline {paragraph}{Scope and positioning}{1796}{section*.4096}\protected@file@percent }
\abx@aux@backref{3367}{wang2022_omnivl}{0}{1796}{1796}
\@writefile{lof}{\contentsline {figure}{\numberline {24.84}{\ignorespaces \textbf  {OmniVL overview.} The framework unifies the pretraining corpus (human-annotated labels and web-crawled captions), the modality space (image, video, and text), and functionality (visual-only classification, cross-modal alignment, and multi-modal understanding/generation) in a single encoder–decoder architecture. Source: \blx@tocontentsinit {0}\cite {wang2022_omnivl}}}{1796}{figure.caption.4097}\protected@file@percent }
\abx@aux@backref{3369}{wang2022_omnivl}{0}{1796}{1796}
\newlabel{fig:chapter24_omnivl_overview}{{24.84}{1796}{\textbf {OmniVL overview.} The framework unifies the pretraining corpus (human-annotated labels and web-crawled captions), the modality space (image, video, and text), and functionality (visual-only classification, cross-modal alignment, and multi-modal understanding/generation) in a single encoder–decoder architecture. Source: \cite {wang2022_omnivl}}{figure.caption.4097}{}}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{1796}{section*.4098}\protected@file@percent }
\newlabel{subsubsec:chapter24_omnivl_motivation}{{24.8.2}{1796}{Motivation}{section*.4098}{}}
\@writefile{toc}{\contentsline {paragraph}{Fragmentation problem}{1796}{section*.4099}\protected@file@percent }
\abx@aux@backref{3370}{wang2022_omnivl}{0}{1796}{1796}
\abx@aux@cite{0}{wang2022_omnivl}
\abx@aux@segm{0}{0}{wang2022_omnivl}
\abx@aux@cite{0}{wang2022_omnivl}
\abx@aux@segm{0}{0}{wang2022_omnivl}
\abx@aux@cite{0}{wang2022_omnivl}
\abx@aux@segm{0}{0}{wang2022_omnivl}
\abx@aux@cite{0}{wang2022_omnivl}
\abx@aux@segm{0}{0}{wang2022_omnivl}
\@writefile{toc}{\contentsline {paragraph}{Design hypothesis}{1797}{section*.4100}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Method: high-level flow and detailed breakdown}{1797}{section*.4101}\protected@file@percent }
\newlabel{subsubsec:chapter24_omnivl_method}{{24.8.2}{1797}{Method: high-level flow and detailed breakdown}{section*.4101}{}}
\@writefile{toc}{\contentsline {paragraph}{High-level overview}{1797}{section*.4102}\protected@file@percent }
\abx@aux@backref{3371}{wang2022_omnivl}{0}{1797}{1797}
\@writefile{toc}{\contentsline {paragraph}{Data format and prompting}{1797}{section*.4103}\protected@file@percent }
\abx@aux@backref{3372}{wang2022_omnivl}{0}{1797}{1797}
\@writefile{toc}{\contentsline {paragraph}{Step-by-step data flow}{1797}{section*.4104}\protected@file@percent }
\abx@aux@backref{3373}{wang2022_omnivl}{0}{1797}{1797}
\abx@aux@cite{0}{wang2022_omnivl}
\abx@aux@segm{0}{0}{wang2022_omnivl}
\abx@aux@cite{0}{wang2022_omnivl}
\abx@aux@segm{0}{0}{wang2022_omnivl}
\abx@aux@backref{3374}{wang2022_omnivl}{0}{1798}{1798}
\abx@aux@backref{3375}{wang2022_omnivl}{0}{1798}{1798}
\abx@aux@backref{3376}{wang2022_omnivl}{0}{1798}{1798}
\abx@aux@cite{0}{wang2022_omnivl}
\abx@aux@segm{0}{0}{wang2022_omnivl}
\abx@aux@cite{0}{wang2022_omnivl}
\abx@aux@segm{0}{0}{wang2022_omnivl}
\abx@aux@backref{3377}{wang2022_omnivl}{0}{1799}{1799}
\@writefile{toc}{\contentsline {paragraph}{Pretraining objectives}{1800}{section*.4105}\protected@file@percent }
\abx@aux@backref{3378}{wang2022_omnivl}{0}{1800}{1800}
\abx@aux@cite{0}{wang2022_omnivl}
\abx@aux@segm{0}{0}{wang2022_omnivl}
\abx@aux@cite{0}{wang2022_omnivl}
\abx@aux@segm{0}{0}{wang2022_omnivl}
\abx@aux@cite{0}{bertasius2021_timesformer}
\abx@aux@segm{0}{0}{bertasius2021_timesformer}
\abx@aux@cite{0}{devlin2019_bert}
\abx@aux@segm{0}{0}{devlin2019_bert}
\abx@aux@cite{0}{wang2022_omnivl}
\abx@aux@segm{0}{0}{wang2022_omnivl}
\@writefile{toc}{\contentsline {paragraph}{Decoupled joint pretraining}{1801}{section*.4111}\protected@file@percent }
\abx@aux@backref{3379}{wang2022_omnivl}{0}{1801}{1801}
\@writefile{toc}{\contentsline {paragraph}{Task routing and inference}{1801}{section*.4112}\protected@file@percent }
\abx@aux@cite{0}{wang2022_omnivl}
\abx@aux@segm{0}{0}{wang2022_omnivl}
\abx@aux@cite{0}{wang2022_omnivl}
\abx@aux@segm{0}{0}{wang2022_omnivl}
\abx@aux@cite{0}{bertasius2021_timesformer}
\abx@aux@segm{0}{0}{bertasius2021_timesformer}
\abx@aux@cite{0}{wang2022_omnivl}
\abx@aux@segm{0}{0}{wang2022_omnivl}
\abx@aux@cite{0}{wang2022_omnivl}
\abx@aux@segm{0}{0}{wang2022_omnivl}
\abx@aux@cite{0}{devlin2019_bert}
\abx@aux@segm{0}{0}{devlin2019_bert}
\abx@aux@cite{0}{wang2022_omnivl}
\abx@aux@segm{0}{0}{wang2022_omnivl}
\abx@aux@cite{0}{wang2022_omnivl}
\abx@aux@segm{0}{0}{wang2022_omnivl}
\abx@aux@cite{0}{wang2022_omnivl}
\abx@aux@segm{0}{0}{wang2022_omnivl}
\abx@aux@cite{0}{wang2022_omnivl}
\abx@aux@segm{0}{0}{wang2022_omnivl}
\abx@aux@cite{0}{wang2022_omnivl}
\abx@aux@segm{0}{0}{wang2022_omnivl}
\abx@aux@cite{0}{wang2022_omnivl}
\abx@aux@segm{0}{0}{wang2022_omnivl}
\abx@aux@cite{0}{wang2022_omnivl}
\abx@aux@segm{0}{0}{wang2022_omnivl}
\@writefile{toc}{\contentsline {subsubsection}{Architecture \& implementation details}{1802}{section*.4113}\protected@file@percent }
\newlabel{subsubsec:chapter24_omnivl_arch}{{24.8.2}{1802}{Architecture \& implementation details}{section*.4113}{}}
\@writefile{toc}{\contentsline {paragraph}{Backbone design at a glance}{1802}{section*.4114}\protected@file@percent }
\abx@aux@backref{3380}{bertasius2021_timesformer}{0}{1802}{1802}
\abx@aux@backref{3381}{devlin2019_bert}{0}{1802}{1802}
\abx@aux@backref{3382}{wang2022_omnivl}{0}{1802}{1802}
\abx@aux@backref{3383}{wang2022_omnivl}{0}{1802}{1802}
\@writefile{toc}{\contentsline {paragraph}{Unified visual encoder: shapes, blocks, and schedules}{1802}{section*.4115}\protected@file@percent }
\abx@aux@backref{3384}{wang2022_omnivl}{0}{1802}{1802}
\abx@aux@backref{3385}{wang2022_omnivl}{0}{1802}{1802}
\abx@aux@backref{3386}{bertasius2021_timesformer}{0}{1802}{1802}
\abx@aux@backref{3387}{wang2022_omnivl}{0}{1802}{1802}
\abx@aux@backref{3388}{wang2022_omnivl}{0}{1802}{1802}
\@writefile{toc}{\contentsline {paragraph}{Text encoder: tokenization and heads}{1802}{section*.4116}\protected@file@percent }
\abx@aux@backref{3389}{devlin2019_bert}{0}{1802}{1802}
\abx@aux@backref{3390}{wang2022_omnivl}{0}{1802}{1802}
\abx@aux@backref{3391}{wang2022_omnivl}{0}{1802}{1802}
\@writefile{toc}{\contentsline {paragraph}{Decoders: attention masks, fusion, and outputs}{1802}{section*.4117}\protected@file@percent }
\abx@aux@backref{3392}{wang2022_omnivl}{0}{1802}{1802}
\abx@aux@backref{3393}{wang2022_omnivl}{0}{1802}{1802}
\@writefile{toc}{\contentsline {paragraph}{Projection heads, similarities, and temperatures}{1802}{section*.4118}\protected@file@percent }
\abx@aux@backref{3394}{wang2022_omnivl}{0}{1802}{1802}
\abx@aux@cite{0}{wang2022_omnivl}
\abx@aux@segm{0}{0}{wang2022_omnivl}
\abx@aux@cite{0}{wang2022_omnivl}
\abx@aux@segm{0}{0}{wang2022_omnivl}
\abx@aux@cite{0}{wang2022_omnivl}
\abx@aux@segm{0}{0}{wang2022_omnivl}
\abx@aux@cite{0}{wang2022_omnivl}
\abx@aux@segm{0}{0}{wang2022_omnivl}
\abx@aux@cite{0}{wang2022_omnivl}
\abx@aux@segm{0}{0}{wang2022_omnivl}
\@writefile{toc}{\contentsline {paragraph}{Queues, EMA encoders, and retrieval runtime}{1803}{section*.4119}\protected@file@percent }
\abx@aux@backref{3395}{wang2022_omnivl}{0}{1803}{1803}
\abx@aux@backref{3396}{wang2022_omnivl}{0}{1803}{1803}
\@writefile{toc}{\contentsline {paragraph}{Data, batching, and curriculum specifics}{1803}{section*.4120}\protected@file@percent }
\abx@aux@backref{3397}{wang2022_omnivl}{0}{1803}{1803}
\@writefile{toc}{\contentsline {paragraph}{Optimization and training stability}{1803}{section*.4121}\protected@file@percent }
\abx@aux@backref{3398}{wang2022_omnivl}{0}{1803}{1803}
\abx@aux@backref{3399}{wang2022_omnivl}{0}{1803}{1803}
\@writefile{toc}{\contentsline {subsubsection}{Experiments and ablations}{1803}{section*.4122}\protected@file@percent }
\newlabel{subsubsec:chapter24_omnivl_expts}{{24.8.2}{1803}{Experiments and ablations}{section*.4122}{}}
\@writefile{toc}{\contentsline {paragraph}{Result highlights}{1803}{section*.4123}\protected@file@percent }
\abx@aux@backref{3400}{wang2022_omnivl}{0}{1803}{1803}
\abx@aux@backref{3401}{wang2022_omnivl}{0}{1803}{1803}
\@writefile{lot}{\contentsline {table}{\numberline {24.47}{\ignorespaces Comparison across pretraining schedules from the paper’s Table~10. Metrics: COCO retrieval (TR@1, IR@1), MSRVTT retrieval (IR@1), COCO captioning (BLEU@4, CIDEr), VQA (test-dev), and MSRVTT-QA accuracy.}}{1803}{table.caption.4124}\protected@file@percent }
\newlabel{tab:chapter24_omnivl_ablation_pretrain}{{24.47}{1803}{Comparison across pretraining schedules from the paper’s Table~10. Metrics: COCO retrieval (TR@1, IR@1), MSRVTT retrieval (IR@1), COCO captioning (BLEU@4, CIDEr), VQA (test-dev), and MSRVTT-QA accuracy}{table.caption.4124}{}}
\@writefile{toc}{\contentsline {paragraph}{What the curriculum buys}{1803}{section*.4125}\protected@file@percent }
\abx@aux@cite{0}{wang2022_omnivl}
\abx@aux@segm{0}{0}{wang2022_omnivl}
\abx@aux@cite{0}{wang2022_omnivl}
\abx@aux@segm{0}{0}{wang2022_omnivl}
\abx@aux@cite{0}{wang2022_omnivl}
\abx@aux@segm{0}{0}{wang2022_omnivl}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{weng2024_longvlm}
\abx@aux@segm{0}{0}{weng2024_longvlm}
\@writefile{toc}{\contentsline {paragraph}{What UniVLC adds}{1804}{section*.4126}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Retrieval pipeline ablation}{1804}{section*.4127}\protected@file@percent }
\abx@aux@backref{3402}{wang2022_omnivl}{0}{1804}{1804}
\@writefile{toc}{\contentsline {paragraph}{Takeaways}{1804}{section*.4128}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24.85}{\ignorespaces With and without UniVLC across tasks, showing consistent gains that support the unified contrastive design \blx@tocontentsinit {0}\cite {wang2022_omnivl}.}}{1804}{figure.caption.4129}\protected@file@percent }
\abx@aux@backref{3404}{wang2022_omnivl}{0}{1804}{1804}
\newlabel{fig:chapter24_omnivl_univlc_ablation}{{24.85}{1804}{With and without UniVLC across tasks, showing consistent gains that support the unified contrastive design \cite {wang2022_omnivl}}{figure.caption.4129}{}}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\@writefile{toc}{\contentsline {subsubsection}{Limitations and future directions}{1805}{section*.4130}\protected@file@percent }
\newlabel{subsubsec:chapter24_omnivl_limits}{{24.8.2}{1805}{Limitations and future directions}{section*.4130}{}}
\@writefile{toc}{\contentsline {paragraph}{Token budget and long-form video}{1805}{section*.4131}\protected@file@percent }
\abx@aux@backref{3405}{wang2024_internvideo2}{0}{1805}{1805}
\abx@aux@backref{3406}{weng2024_longvlm}{0}{1805}{1805}
\@writefile{toc}{\contentsline {paragraph}{Prompting sensitivity and text targets}{1805}{section*.4132}\protected@file@percent }
\abx@aux@backref{3407}{wang2024_internvideo2}{0}{1805}{1805}
\abx@aux@backref{3408}{cheng2024_videollama2}{0}{1805}{1805}
\@writefile{toc}{\contentsline {paragraph}{Fine-grained localization and grounding}{1805}{section*.4133}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Data curation and balance}{1805}{section*.4134}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{From unified encoders to instruction following}{1805}{section*.4135}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Scaling outlook}{1805}{section*.4136}\protected@file@percent }
\BKM@entry{id=968,dest={73656374696F6E2A2E34313337},srcline={4391}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030345C3030302E5C303030385C3030302E5C303030335C3030303A5C3030305C3034305C303030495C3030306E5C303030745C303030655C303030725C3030306E5C303030565C303030695C303030645C303030655C3030306F5C303030325C3030303A5C3030305C3034305C303030475C303030655C3030306E5C303030655C303030725C303030615C303030745C303030695C303030765C303030655C3030305C3034305C3030302B5C3030305C3034305C303030445C303030695C303030735C303030635C303030725C303030695C3030306D5C303030695C3030306E5C303030615C303030745C303030695C303030765C303030655C3030305C3034305C303030505C303030725C303030655C303030745C303030725C303030615C303030695C3030306E5C303030695C3030306E5C30303067}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{wang2022_internvideo}
\abx@aux@segm{0}{0}{wang2022_internvideo}
\@writefile{toc}{\contentsline {subsection}{Enrichment 24.8.3: InternVideo2: Generative + Discriminative Pretraining}{1806}{section*.4137}\protected@file@percent }
\newlabel{enr:subsec_chapter24_internvideo2}{{24.8.3}{1806}{\color {ocre}Enrichment \thesubsection : InternVideo2: Generative + Discriminative Pretraining}{section*.4137}{}}
\@writefile{toc}{\contentsline {paragraph}{Scope and positioning}{1806}{section*.4138}\protected@file@percent }
\abx@aux@backref{3409}{wang2024_internvideo2}{0}{1806}{1806}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{1806}{section*.4139}\protected@file@percent }
\newlabel{subsubsec:chapter24_internvideo2_motivation}{{24.8.3}{1806}{Motivation}{section*.4139}{}}
\@writefile{toc}{\contentsline {paragraph}{Problem framing}{1806}{section*.4140}\protected@file@percent }
\abx@aux@backref{3410}{wang2022_internvideo}{0}{1806}{1806}
\@writefile{toc}{\contentsline {paragraph}{Why InternVideo (V1) is not enough}{1806}{section*.4141}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Design principles for a scalable VFM}{1806}{section*.4142}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{What success looks like}{1806}{section*.4143}\protected@file@percent }
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{he2022_mae}
\abx@aux@segm{0}{0}{he2022_mae}
\@writefile{toc}{\contentsline {paragraph}{Key idea}{1807}{section*.4144}\protected@file@percent }
\abx@aux@backref{3411}{wang2024_internvideo2}{0}{1807}{1807}
\@writefile{lof}{\contentsline {figure}{\numberline {24.86}{\ignorespaces High-level overview and qualitative capabilities of InternVideo2 across recognition, retrieval, long-form reasoning, and dialogue. Source: \blx@tocontentsinit {0}\cite {wang2024_internvideo2}.}}{1807}{figure.caption.4145}\protected@file@percent }
\abx@aux@backref{3413}{wang2024_internvideo2}{0}{1807}{1807}
\newlabel{fig:chapter24_iv2_intro}{{24.86}{1807}{High-level overview and qualitative capabilities of InternVideo2 across recognition, retrieval, long-form reasoning, and dialogue. Source: \cite {wang2024_internvideo2}}{figure.caption.4145}{}}
\@writefile{toc}{\contentsline {subsubsection}{Method: objectives, training stages, and intuition}{1807}{section*.4146}\protected@file@percent }
\newlabel{subsubsec:chapter24_internvideo2_method}{{24.8.3}{1807}{Method: objectives, training stages, and intuition}{section*.4146}{}}
\@writefile{toc}{\contentsline {paragraph}{Notation}{1807}{section*.4147}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Stage 1: Video-only masked autoencoding}{1807}{section*.4148}\protected@file@percent }
\abx@aux@backref{3414}{he2022_mae}{0}{1807}{1807}
\newlabel{eq:chapter24_iv2_mae}{{24.43}{1807}{Stage 1: Video-only masked autoencoding}{equation.24.43}{}}
\@writefile{toc}{\contentsline {paragraph}{Stage 2: Multimodal contrastive alignment (image–text and video–text)}{1808}{section*.4149}\protected@file@percent }
\newlabel{eq:chapter24_iv2_clip}{{24.44}{1808}{Stage 2: Multimodal contrastive alignment (image–text and video–text)}{equation.24.44}{}}
\@writefile{toc}{\contentsline {paragraph}{Stage 3: Video-centric instruction tuning with a Q-Former bridge}{1808}{section*.4150}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Notation (simplified)}{1808}{section*.4151}\protected@file@percent }
\abx@aux@cite{0}{li2023_blip2}
\abx@aux@segm{0}{0}{li2023_blip2}
\abx@aux@cite{0}{li2023_blip2}
\abx@aux@segm{0}{0}{li2023_blip2}
\@writefile{toc}{\contentsline {paragraph}{One Q-Former layer}{1809}{section*.4152}\protected@file@percent }
\newlabel{eq:qformer-sa}{{24.45}{1809}{One Q-Former layer}{equation.24.45}{}}
\newlabel{eq:qformer-ca}{{24.46}{1809}{One Q-Former layer}{equation.24.46}{}}
\newlabel{eq:chapter24_iv2_lm}{{24.48}{1809}{One Q-Former layer}{equation.24.48}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24.87}{\ignorespaces Q-Former–LLM interface adapted from BLIP-2: a small set of learnable queries cross-attend to video tokens to produce a compact summary that conditions the LLM via soft visual prompts. Source: \blx@tocontentsinit {0}\cite {li2023_blip2}.}}{1810}{figure.caption.4153}\protected@file@percent }
\abx@aux@backref{3416}{li2023_blip2}{0}{1810}{1810}
\newlabel{fig:chapter24_blip2_qformer}{{24.87}{1810}{Q-Former–LLM interface adapted from BLIP-2: a small set of learnable queries cross-attend to video tokens to produce a compact summary that conditions the LLM via soft visual prompts. Source: \cite {li2023_blip2}}{figure.caption.4153}{}}
\@writefile{toc}{\contentsline {paragraph}{Total training recipe}{1810}{section*.4154}\protected@file@percent }
\newlabel{eq:chapter24_iv2_total}{{24.49}{1810}{Total training recipe}{equation.24.49}{}}
\@writefile{toc}{\contentsline {paragraph}{Practical schedule and hyperparameters}{1810}{section*.4155}\protected@file@percent }
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{hu2021_lora}
\abx@aux@segm{0}{0}{hu2021_lora}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{jiang2014_thumos14}
\abx@aux@segm{0}{0}{jiang2014_thumos14}
\abx@aux@cite{0}{krishna2017_activitynet_captions}
\abx@aux@segm{0}{0}{krishna2017_activitynet_captions}
\abx@aux@cite{0}{zhao2019_hacs}
\abx@aux@segm{0}{0}{zhao2019_hacs}
\abx@aux@cite{0}{yang2019_youtube_vis}
\abx@aux@segm{0}{0}{yang2019_youtube_vis}
\abx@aux@cite{0}{liu2021_swin}
\abx@aux@segm{0}{0}{liu2021_swin}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{jiang2014_thumos14}
\abx@aux@segm{0}{0}{jiang2014_thumos14}
\abx@aux@cite{0}{krishna2017_activitynet_captions}
\abx@aux@segm{0}{0}{krishna2017_activitynet_captions}
\abx@aux@cite{0}{zhao2019_hacs}
\abx@aux@segm{0}{0}{zhao2019_hacs}
\abx@aux@cite{0}{yang2019_youtube_vis}
\abx@aux@segm{0}{0}{yang2019_youtube_vis}
\abx@aux@cite{0}{liu2021_swin}
\abx@aux@segm{0}{0}{liu2021_swin}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{jiang2014_thumos14}
\abx@aux@segm{0}{0}{jiang2014_thumos14}
\abx@aux@cite{0}{krishna2017_activitynet_captions}
\abx@aux@segm{0}{0}{krishna2017_activitynet_captions}
\abx@aux@cite{0}{zhao2019_hacs}
\abx@aux@segm{0}{0}{zhao2019_hacs}
\abx@aux@cite{0}{yang2019_youtube_vis}
\abx@aux@segm{0}{0}{yang2019_youtube_vis}
\abx@aux@cite{0}{yang2019_youtube_vis}
\abx@aux@segm{0}{0}{yang2019_youtube_vis}
\abx@aux@cite{0}{liu2021_swin}
\abx@aux@segm{0}{0}{liu2021_swin}
\abx@aux@cite{0}{yang2019_youtube_vis}
\abx@aux@segm{0}{0}{yang2019_youtube_vis}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{carreira2017_i3d}
\abx@aux@segm{0}{0}{carreira2017_i3d}
\abx@aux@cite{0}{tran2018_closer}
\abx@aux@segm{0}{0}{tran2018_closer}
\abx@aux@cite{0}{wang2022_internvideo}
\abx@aux@segm{0}{0}{wang2022_internvideo}
\abx@aux@cite{0}{wang2023_videomaev2}
\abx@aux@segm{0}{0}{wang2023_videomaev2}
\@writefile{toc}{\contentsline {subsubsection}{Experiments}{1811}{section*.4156}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Experimental setup and scaling}{1811}{section*.4157}\protected@file@percent }
\abx@aux@backref{3417}{wang2024_internvideo2}{0}{1811}{1811}
\@writefile{toc}{\contentsline {paragraph}{Efficiency and compute}{1811}{section*.4158}\protected@file@percent }
\abx@aux@backref{3418}{wang2024_internvideo2}{0}{1811}{1811}
\abx@aux@backref{3419}{hu2021_lora}{0}{1811}{1811}
\@writefile{toc}{\contentsline {paragraph}{Headline results}{1811}{section*.4159}\protected@file@percent }
\abx@aux@backref{3420}{wang2024_internvideo2}{0}{1811}{1811}
\@writefile{toc}{\contentsline {paragraph}{Action understanding (``what'' and ``when'')}{1811}{section*.4160}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {24.48}{\ignorespaces Temporal action localization (avg.\ mAP) and video instance segmentation (mAP). Figures are reported in \blx@tocontentsinit {0}\cite {wang2024_internvideo2}. Datasets: THUMOS14~\blx@tocontentsinit {0}\cite {jiang2014_thumos14}, ActivityNet-Captions~\blx@tocontentsinit {0}\cite {krishna2017_activitynet_captions}, HACS~\blx@tocontentsinit {0}\cite {zhao2019_hacs}, YouTube-VIS19~\blx@tocontentsinit {0}\cite {yang2019_youtube_vis}. VIS baselines include Swin-L~\blx@tocontentsinit {0}\cite {liu2021_swin} and an image InternViT backbone (as referenced by \blx@tocontentsinit {0}\cite {wang2024_internvideo2}).}}{1811}{table.caption.4161}\protected@file@percent }
\abx@aux@backref{3428}{wang2024_internvideo2}{0}{1811}{1811}
\abx@aux@backref{3429}{jiang2014_thumos14}{0}{1811}{1811}
\abx@aux@backref{3430}{krishna2017_activitynet_captions}{0}{1811}{1811}
\abx@aux@backref{3431}{zhao2019_hacs}{0}{1811}{1811}
\abx@aux@backref{3432}{yang2019_youtube_vis}{0}{1811}{1811}
\abx@aux@backref{3433}{liu2021_swin}{0}{1811}{1811}
\abx@aux@backref{3434}{wang2024_internvideo2}{0}{1811}{1811}
\newlabel{tab:iv2_action}{{24.48}{1811}{Temporal action localization (avg.\ mAP) and video instance segmentation (mAP). Figures are reported in \cite {wang2024_internvideo2}. Datasets: THUMOS14~\cite {jiang2014_thumos14}, ActivityNet-Captions~\cite {krishna2017_activitynet_captions}, HACS~\cite {zhao2019_hacs}, YouTube-VIS19~\cite {yang2019_youtube_vis}. VIS baselines include Swin-L~\cite {liu2021_swin} and an image InternViT backbone (as referenced by \cite {wang2024_internvideo2})}{table.caption.4161}{}}
\abx@aux@backref{3435}{jiang2014_thumos14}{0}{1811}{1811}
\abx@aux@backref{3436}{krishna2017_activitynet_captions}{0}{1811}{1811}
\abx@aux@backref{3437}{zhao2019_hacs}{0}{1811}{1811}
\abx@aux@backref{3438}{yang2019_youtube_vis}{0}{1811}{1811}
\abx@aux@backref{3439}{yang2019_youtube_vis}{0}{1811}{1811}
\abx@aux@backref{3440}{liu2021_swin}{0}{1811}{1811}
\abx@aux@backref{3441}{yang2019_youtube_vis}{0}{1811}{1811}
\abx@aux@cite{0}{tong2022_videomae}
\abx@aux@segm{0}{0}{tong2022_videomae}
\abx@aux@cite{0}{wang2023_videomaev2}
\abx@aux@segm{0}{0}{wang2023_videomaev2}
\abx@aux@cite{0}{wang2022_internvideo}
\abx@aux@segm{0}{0}{wang2022_internvideo}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\abx@aux@cite{0}{luo2022_clip4clip}
\abx@aux@segm{0}{0}{luo2022_clip4clip}
\abx@aux@cite{0}{wang2024_internvid}
\abx@aux@segm{0}{0}{wang2024_internvid}
\abx@aux@cite{0}{wang2022_internvideo}
\abx@aux@segm{0}{0}{wang2022_internvideo}
\abx@aux@cite{0}{li2024_umt}
\abx@aux@segm{0}{0}{li2024_umt}
\abx@aux@cite{0}{yan2023_videococa}
\abx@aux@segm{0}{0}{yan2023_videococa}
\abx@aux@cite{0}{zhao2025_videoprism}
\abx@aux@segm{0}{0}{zhao2025_videoprism}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\abx@aux@cite{0}{luo2022_clip4clip}
\abx@aux@segm{0}{0}{luo2022_clip4clip}
\abx@aux@cite{0}{wang2024_internvid}
\abx@aux@segm{0}{0}{wang2024_internvid}
\abx@aux@cite{0}{li2024_umt}
\abx@aux@segm{0}{0}{li2024_umt}
\@writefile{lot}{\contentsline {table}{\numberline {24.49}{\ignorespaces Finetuned temporal action localization (avg.\ mAP). “Flow” uses ensembled I3D flow features; * with Flow. (From \blx@tocontentsinit {0}\cite [Tab.~7]{wang2024_internvideo2}.)}}{1812}{table.caption.4162}\protected@file@percent }
\abx@aux@backref{3443}{wang2024_internvideo2}{0}{1812}{1812}
\newlabel{tab:iv2_tal_compare}{{24.49}{1812}{Finetuned temporal action localization (avg.\ mAP). “Flow” uses ensembled I3D flow features; * with Flow. (From \cite [Tab.~7]{wang2024_internvideo2}.)}{table.caption.4162}{}}
\abx@aux@backref{3444}{carreira2017_i3d}{0}{1812}{1812}
\abx@aux@backref{3445}{tran2018_closer}{0}{1812}{1812}
\abx@aux@backref{3446}{wang2022_internvideo}{0}{1812}{1812}
\abx@aux@backref{3447}{wang2023_videomaev2}{0}{1812}{1812}
\abx@aux@backref{3448}{tong2022_videomae}{0}{1812}{1812}
\abx@aux@backref{3449}{wang2023_videomaev2}{0}{1812}{1812}
\abx@aux@backref{3450}{wang2022_internvideo}{0}{1812}{1812}
\abx@aux@backref{3451}{wang2024_internvideo2}{0}{1812}{1812}
\@writefile{toc}{\contentsline {paragraph}{Video–language retrieval (the ``search engine'')}{1812}{section*.4163}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {24.50}{\ignorespaces Zero-shot video retrieval R@1 on MSR-VTT, LSMDC, DiDeMo, MSVD, ANet, and VATEX (T2V/V2T). Baselines follow \blx@tocontentsinit {0}\cite [Tab.~9]{wang2024_internvideo2}.}}{1812}{table.caption.4164}\protected@file@percent }
\abx@aux@backref{3453}{wang2024_internvideo2}{0}{1812}{1812}
\newlabel{tab:iv2_zeroshot_retrieval_compare}{{24.50}{1812}{Zero-shot video retrieval R@1 on MSR-VTT, LSMDC, DiDeMo, MSVD, ANet, and VATEX (T2V/V2T). Baselines follow \cite [Tab.~9]{wang2024_internvideo2}}{table.caption.4164}{}}
\abx@aux@backref{3454}{radford2021_clip}{0}{1812}{1812}
\abx@aux@backref{3455}{luo2022_clip4clip}{0}{1812}{1812}
\abx@aux@backref{3456}{wang2024_internvid}{0}{1812}{1812}
\abx@aux@backref{3457}{wang2022_internvideo}{0}{1812}{1812}
\abx@aux@backref{3458}{li2024_umt}{0}{1812}{1812}
\abx@aux@backref{3459}{yan2023_videococa}{0}{1812}{1812}
\abx@aux@backref{3460}{zhao2025_videoprism}{0}{1812}{1812}
\@writefile{lot}{\contentsline {table}{\numberline {24.51}{\ignorespaces Finetuned video retrieval R@1 on MSR-VTT, LSMDC, DiDeMo, MSVD, ANet, VATEX (T2V/V2T) from \blx@tocontentsinit {0}\cite [Tab.~10]{wang2024_internvideo2}.}}{1812}{table.caption.4165}\protected@file@percent }
\abx@aux@backref{3462}{wang2024_internvideo2}{0}{1812}{1812}
\newlabel{tab:iv2_finetuned_retrieval_compare}{{24.51}{1812}{Finetuned video retrieval R@1 on MSR-VTT, LSMDC, DiDeMo, MSVD, ANet, VATEX (T2V/V2T) from \cite [Tab.~10]{wang2024_internvideo2}}{table.caption.4165}{}}
\abx@aux@backref{3463}{radford2021_clip}{0}{1812}{1812}
\abx@aux@backref{3464}{luo2022_clip4clip}{0}{1812}{1812}
\abx@aux@backref{3465}{wang2024_internvid}{0}{1812}{1812}
\abx@aux@backref{3466}{li2024_umt}{0}{1812}{1812}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\abx@aux@cite{0}{feichtenhofer2019_slowfast}
\abx@aux@segm{0}{0}{feichtenhofer2019_slowfast}
\abx@aux@cite{0}{lei2021_qvhighlight}
\abx@aux@segm{0}{0}{lei2021_qvhighlight}
\abx@aux@cite{0}{gao2017_charadessta}
\abx@aux@segm{0}{0}{gao2017_charadessta}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{lei2021_qvhighlight}
\abx@aux@segm{0}{0}{lei2021_qvhighlight}
\abx@aux@cite{0}{gao2017_charadessta}
\abx@aux@segm{0}{0}{gao2017_charadessta}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\abx@aux@cite{0}{feichtenhofer2019_slowfast}
\abx@aux@segm{0}{0}{feichtenhofer2019_slowfast}
\abx@aux@cite{0}{radford2021_clip}
\abx@aux@segm{0}{0}{radford2021_clip}
\abx@aux@cite{0}{feichtenhofer2019_slowfast}
\abx@aux@segm{0}{0}{feichtenhofer2019_slowfast}
\abx@aux@cite{0}{hu2021_lora}
\abx@aux@segm{0}{0}{hu2021_lora}
\abx@aux@cite{0}{li2024_mvbench}
\abx@aux@segm{0}{0}{li2024_mvbench}
\abx@aux@cite{0}{mangalam2023_egoschema}
\abx@aux@segm{0}{0}{mangalam2023_egoschema}
\abx@aux@cite{0}{patraucean2023_perceptiontest}
\abx@aux@segm{0}{0}{patraucean2023_perceptiontest}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{li2024_mvbench}
\abx@aux@segm{0}{0}{li2024_mvbench}
\abx@aux@cite{0}{mangalam2023_egoschema}
\abx@aux@segm{0}{0}{mangalam2023_egoschema}
\abx@aux@cite{0}{patraucean2023_perceptiontest}
\abx@aux@segm{0}{0}{patraucean2023_perceptiontest}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{openai2023b_gpt4v}
\abx@aux@segm{0}{0}{openai2023b_gpt4v}
\abx@aux@cite{0}{team2023_gemini}
\abx@aux@segm{0}{0}{team2023_gemini}
\abx@aux@cite{0}{liu2024_llava_next_video}
\abx@aux@segm{0}{0}{liu2024_llava_next_video}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{li2024_mvbench}
\abx@aux@segm{0}{0}{li2024_mvbench}
\abx@aux@cite{0}{li2024_umt}
\abx@aux@segm{0}{0}{li2024_umt}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\@writefile{toc}{\contentsline {paragraph}{Temporal grounding (finding the exact moment)}{1813}{section*.4166}\protected@file@percent }
\abx@aux@backref{3467}{radford2021_clip}{0}{1813}{1813}
\abx@aux@backref{3468}{feichtenhofer2019_slowfast}{0}{1813}{1813}
\@writefile{lot}{\contentsline {table}{\numberline {24.52}{\ignorespaces Finetuned temporal grounding on QVHighlights~\blx@tocontentsinit {0}\cite {lei2021_qvhighlight} and Charades-STA~\blx@tocontentsinit {0}\cite {gao2017_charadessta}. Metrics follow \blx@tocontentsinit {0}\cite [Tab.~11]{wang2024_internvideo2}.}}{1813}{table.caption.4167}\protected@file@percent }
\abx@aux@backref{3472}{lei2021_qvhighlight}{0}{1813}{1813}
\abx@aux@backref{3473}{gao2017_charadessta}{0}{1813}{1813}
\abx@aux@backref{3474}{wang2024_internvideo2}{0}{1813}{1813}
\newlabel{tab:iv2_grounding_compare}{{24.52}{1813}{Finetuned temporal grounding on QVHighlights~\cite {lei2021_qvhighlight} and Charades-STA~\cite {gao2017_charadessta}. Metrics follow \cite [Tab.~11]{wang2024_internvideo2}}{table.caption.4167}{}}
\abx@aux@backref{3475}{radford2021_clip}{0}{1813}{1813}
\abx@aux@backref{3476}{feichtenhofer2019_slowfast}{0}{1813}{1813}
\abx@aux@backref{3477}{radford2021_clip}{0}{1813}{1813}
\abx@aux@backref{3478}{feichtenhofer2019_slowfast}{0}{1813}{1813}
\@writefile{toc}{\contentsline {paragraph}{Video dialogue and reasoning (the ``conversational AI'')}{1813}{section*.4168}\protected@file@percent }
\abx@aux@backref{3479}{hu2021_lora}{0}{1813}{1813}
\@writefile{lot}{\contentsline {table}{\numberline {24.53}{\ignorespaces Chat-centric evaluation on MVBench~\blx@tocontentsinit {0}\cite {li2024_mvbench}, EgoSchema~\blx@tocontentsinit {0}\cite {mangalam2023_egoschema}, and Perception Test~\blx@tocontentsinit {0}\cite {patraucean2023_perceptiontest}. Numbers follow \blx@tocontentsinit {0}\cite [Tab.~14]{wang2024_internvideo2}.}}{1813}{table.caption.4169}\protected@file@percent }
\abx@aux@backref{3484}{li2024_mvbench}{0}{1813}{1813}
\abx@aux@backref{3485}{mangalam2023_egoschema}{0}{1813}{1813}
\abx@aux@backref{3486}{patraucean2023_perceptiontest}{0}{1813}{1813}
\abx@aux@backref{3487}{wang2024_internvideo2}{0}{1813}{1813}
\newlabel{tab:iv2_chat_compare}{{24.53}{1813}{Chat-centric evaluation on MVBench~\cite {li2024_mvbench}, EgoSchema~\cite {mangalam2023_egoschema}, and Perception Test~\cite {patraucean2023_perceptiontest}. Numbers follow \cite [Tab.~14]{wang2024_internvideo2}}{table.caption.4169}{}}
\abx@aux@backref{3488}{openai2023b_gpt4v}{0}{1813}{1813}
\abx@aux@backref{3489}{team2023_gemini}{0}{1813}{1813}
\abx@aux@backref{3490}{liu2024_llava_next_video}{0}{1813}{1813}
\abx@aux@backref{3491}{cheng2024_videollama2}{0}{1813}{1813}
\abx@aux@backref{3492}{li2024_mvbench}{0}{1813}{1813}
\abx@aux@backref{3493}{li2024_umt}{0}{1813}{1813}
\abx@aux@backref{3494}{wang2024_internvideo2}{0}{1813}{1813}
\@writefile{toc}{\contentsline {paragraph}{Scaling validation}{1813}{section*.4170}\protected@file@percent }
\abx@aux@backref{3495}{wang2024_internvideo2}{0}{1813}{1813}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{hu2021_lora}
\abx@aux@segm{0}{0}{hu2021_lora}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{hu2021_lora}
\abx@aux@segm{0}{0}{hu2021_lora}
\@writefile{toc}{\contentsline {subsubsection}{Ablations}{1814}{section*.4171}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{What is varied}{1814}{section*.4172}\protected@file@percent }
\abx@aux@backref{3496}{wang2024_internvideo2}{0}{1814}{1814}
\@writefile{lot}{\contentsline {table}{\numberline {24.54}{\ignorespaces Effect of VAS caption fusion in Stage~2 (normalized trends, higher is better). Adding VAS consistently improves retrieval and video QA by densifying temporal grounding; alt-text alone underperforms on temporally entangled content \blx@tocontentsinit {0}\cite {wang2024_internvideo2}.}}{1814}{table.caption.4173}\protected@file@percent }
\abx@aux@backref{3498}{wang2024_internvideo2}{0}{1814}{1814}
\newlabel{tab:chapter24_iv2_vas}{{24.54}{1814}{Effect of VAS caption fusion in Stage~2 (normalized trends, higher is better). Adding VAS consistently improves retrieval and video QA by densifying temporal grounding; alt-text alone underperforms on temporally entangled content \cite {wang2024_internvideo2}}{table.caption.4173}{}}
\@writefile{toc}{\contentsline {paragraph}{Takeaway}{1814}{section*.4174}\protected@file@percent }
\abx@aux@backref{3499}{wang2024_internvideo2}{0}{1814}{1814}
\@writefile{lot}{\contentsline {table}{\numberline {24.55}{\ignorespaces Stage~1 design: masking ratio and tubelet size (normalized trends). Aggressive tube masking and moderate tubelets encourage motion modeling and reduce redundancy; too high masking or too large tubelets harms fine detail \blx@tocontentsinit {0}\cite {wang2024_internvideo2}.}}{1814}{table.caption.4175}\protected@file@percent }
\abx@aux@backref{3501}{wang2024_internvideo2}{0}{1814}{1814}
\newlabel{tab:chapter24_iv2_stage1_mask}{{24.55}{1814}{Stage~1 design: masking ratio and tubelet size (normalized trends). Aggressive tube masking and moderate tubelets encourage motion modeling and reduce redundancy; too high masking or too large tubelets harms fine detail \cite {wang2024_internvideo2}}{table.caption.4175}{}}
\@writefile{toc}{\contentsline {paragraph}{Takeaway}{1814}{section*.4176}\protected@file@percent }
\abx@aux@backref{3502}{wang2024_internvideo2}{0}{1814}{1814}
\@writefile{lot}{\contentsline {table}{\numberline {24.56}{\ignorespaces Q-Former size: number of queries \(K\) and depth (normalized trends). More queries improve recall of fine events but increase LLM context; a shallow stack is sufficient when \(K\) is tuned \blx@tocontentsinit {0}\cite {wang2024_internvideo2}.}}{1814}{table.caption.4177}\protected@file@percent }
\abx@aux@backref{3504}{wang2024_internvideo2}{0}{1814}{1814}
\newlabel{tab:chapter24_iv2_qformer_size}{{24.56}{1814}{Q-Former size: number of queries \(K\) and depth (normalized trends). More queries improve recall of fine events but increase LLM context; a shallow stack is sufficient when \(K\) is tuned \cite {wang2024_internvideo2}}{table.caption.4177}{}}
\@writefile{toc}{\contentsline {paragraph}{Takeaway}{1814}{section*.4178}\protected@file@percent }
\abx@aux@backref{3505}{wang2024_internvideo2}{0}{1814}{1814}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{hu2021_lora}
\abx@aux@segm{0}{0}{hu2021_lora}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\@writefile{lot}{\contentsline {table}{\numberline {24.57}{\ignorespaces LoRA configuration on the LLM (normalized trends). Modest ranks and targeting attention projections give most gains; very high ranks show diminishing returns relative to cost \blx@tocontentsinit {0}\cite {wang2024_internvideo2,hu2021_lora}.}}{1815}{table.caption.4179}\protected@file@percent }
\abx@aux@backref{3508}{hu2021_lora}{0}{1815}{1815}
\abx@aux@backref{3509}{wang2024_internvideo2}{0}{1815}{1815}
\newlabel{tab:chapter24_iv2_lora}{{24.57}{1815}{LoRA configuration on the LLM (normalized trends). Modest ranks and targeting attention projections give most gains; very high ranks show diminishing returns relative to cost \cite {wang2024_internvideo2,hu2021_lora}}{table.caption.4179}{}}
\@writefile{toc}{\contentsline {paragraph}{Takeaway}{1815}{section*.4180}\protected@file@percent }
\abx@aux@backref{3510}{hu2021_lora}{0}{1815}{1815}
\abx@aux@backref{3511}{wang2024_internvideo2}{0}{1815}{1815}
\@writefile{lot}{\contentsline {table}{\numberline {24.58}{\ignorespaces Frame sampling for long videos (normalized trends). Mixing sparse long strides with short local windows and a global view improves long-form QA and temporal localization with small latency overhead \blx@tocontentsinit {0}\cite {wang2024_internvideo2}.}}{1815}{table.caption.4181}\protected@file@percent }
\abx@aux@backref{3513}{wang2024_internvideo2}{0}{1815}{1815}
\newlabel{tab:chapter24_iv2_sampling}{{24.58}{1815}{Frame sampling for long videos (normalized trends). Mixing sparse long strides with short local windows and a global view improves long-form QA and temporal localization with small latency overhead \cite {wang2024_internvideo2}}{table.caption.4181}{}}
\@writefile{toc}{\contentsline {paragraph}{Takeaway}{1815}{section*.4182}\protected@file@percent }
\abx@aux@backref{3514}{wang2024_internvideo2}{0}{1815}{1815}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\@writefile{toc}{\contentsline {paragraph}{Qualitative comparisons}{1816}{section*.4183}\protected@file@percent }
\abx@aux@backref{3515}{wang2024_internvideo2}{0}{1816}{1816}
\@writefile{lof}{\contentsline {figure}{\numberline {24.88}{\ignorespaces Temporal action recognition with a \emph  {before} query. The clip shows a person sitting with a remote, standing up, walking, taking a blanket, and returning. The question is ``What happened before the person took the blanket?'' InternVideo2-Chat answers using only visible evidence (sitting on the sofa, watching TV) and is marked correct, as is Gemini Pro; GPT-4V hallucinates a motive (feeling cold) not supported by the frames and is marked incorrect. This highlights the value of temporally grounded answers over plausible but ungrounded narratives. Source: \blx@tocontentsinit {0}\cite {wang2024_internvideo2}.}}{1816}{figure.caption.4184}\protected@file@percent }
\abx@aux@backref{3517}{wang2024_internvideo2}{0}{1816}{1816}
\newlabel{fig:chapter24_iv2_temporal_action}{{24.88}{1816}{Temporal action recognition with a \emph {before} query. The clip shows a person sitting with a remote, standing up, walking, taking a blanket, and returning. The question is ``What happened before the person took the blanket?'' InternVideo2-Chat answers using only visible evidence (sitting on the sofa, watching TV) and is marked correct, as is Gemini Pro; GPT-4V hallucinates a motive (feeling cold) not supported by the frames and is marked incorrect. This highlights the value of temporally grounded answers over plausible but ungrounded narratives. Source: \cite {wang2024_internvideo2}}{figure.caption.4184}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24.89}{\ignorespaces Confusing action recognition under deceptive motion. A rapid hand movement mimics banana peeling, but the final state shows the banana unpeeled and dropped. InternVideo2-Chat focuses on the outcome and answers ``dropping a banana'' (correct). Gemini Pro reports the misleading motion (``peeling'') and is incorrect. GPT-4V explains the deception but does not commit to the final physical action. The example shows why temporal endpoints, not transient cues, should anchor predictions. Source: \blx@tocontentsinit {0}\cite {wang2024_internvideo2}.}}{1816}{figure.caption.4185}\protected@file@percent }
\abx@aux@backref{3519}{wang2024_internvideo2}{0}{1816}{1816}
\newlabel{fig:chapter24_iv2_confusing_action}{{24.89}{1816}{Confusing action recognition under deceptive motion. A rapid hand movement mimics banana peeling, but the final state shows the banana unpeeled and dropped. InternVideo2-Chat focuses on the outcome and answers ``dropping a banana'' (correct). Gemini Pro reports the misleading motion (``peeling'') and is incorrect. GPT-4V explains the deception but does not commit to the final physical action. The example shows why temporal endpoints, not transient cues, should anchor predictions. Source: \cite {wang2024_internvideo2}}{figure.caption.4185}{}}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\@writefile{lof}{\contentsline {figure}{\numberline {24.90}{\ignorespaces Temporal ordering of objects (letters). The subject reveals letters sequentially next to a bottle. Gemini Pro misidentifies several letters and reverses order; GPT-4V mixes incorrect letters and order; InternVideo2-Chat yields the fewest errors and preserves the correct order (J\,\(\rightarrow \)K\,\(\rightarrow \)L\,\(\rightarrow \)M\,\(\rightarrow \)N). The task stresses joint recognition and sequence tracking over time. Source: \blx@tocontentsinit {0}\cite {wang2024_internvideo2}.}}{1817}{figure.caption.4186}\protected@file@percent }
\abx@aux@backref{3521}{wang2024_internvideo2}{0}{1817}{1817}
\newlabel{fig:chapter24_iv2_object_temporal}{{24.90}{1817}{Temporal ordering of objects (letters). The subject reveals letters sequentially next to a bottle. Gemini Pro misidentifies several letters and reverses order; GPT-4V mixes incorrect letters and order; InternVideo2-Chat yields the fewest errors and preserves the correct order (J\,\(\rightarrow \)K\,\(\rightarrow \)L\,\(\rightarrow \)M\,\(\rightarrow \)N). The task stresses joint recognition and sequence tracking over time. Source: \cite {wang2024_internvideo2}}{figure.caption.4186}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24.91}{\ignorespaces Event counting. The clip contains three distinct ``launch'' motions of a small object. InternVideo2-Chat and GPT-4V correctly count three events by grouping frames into actions; Gemini Pro confuses the number of frames with the number of events and answers six. Counting requires segmenting repeated motions and ignoring redundant frames. Source: \blx@tocontentsinit {0}\cite {wang2024_internvideo2}.}}{1817}{figure.caption.4187}\protected@file@percent }
\abx@aux@backref{3523}{wang2024_internvideo2}{0}{1817}{1817}
\newlabel{fig:chapter24_iv2_event_counting}{{24.91}{1817}{Event counting. The clip contains three distinct ``launch'' motions of a small object. InternVideo2-Chat and GPT-4V correctly count three events by grouping frames into actions; Gemini Pro confuses the number of frames with the number of events and answers six. Counting requires segmenting repeated motions and ignoring redundant frames. Source: \cite {wang2024_internvideo2}}{figure.caption.4187}{}}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\@writefile{lof}{\contentsline {figure}{\numberline {24.92}{\ignorespaces Unexpected action recognition (``magic'' transition). The scene transforms from a 2D elephant drawing to a 3D toy after a close-up occlusion. InternVideo2-Chat and Gemini Pro correctly describe the conceptual transition (2D\,\(\rightarrow \)3D), while GPT-4V focuses on filming mechanics (the occlusion) rather than the outcome. The example underscores modeling \emph  {state change} rather than camera tricks. Source: \blx@tocontentsinit {0}\cite {wang2024_internvideo2}.}}{1818}{figure.caption.4188}\protected@file@percent }
\abx@aux@backref{3525}{wang2024_internvideo2}{0}{1818}{1818}
\newlabel{fig:chapter24_iv2_unexpected_action}{{24.92}{1818}{Unexpected action recognition (``magic'' transition). The scene transforms from a 2D elephant drawing to a 3D toy after a close-up occlusion. InternVideo2-Chat and Gemini Pro correctly describe the conceptual transition (2D\,\(\rightarrow \)3D), while GPT-4V focuses on filming mechanics (the occlusion) rather than the outcome. The example underscores modeling \emph {state change} rather than camera tricks. Source: \cite {wang2024_internvideo2}}{figure.caption.4188}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24.93}{\ignorespaces Vision–language navigation with progress tracking. Instructions are: (1) go up the stairs, (2) turn left, (3) enter the left bedroom, (4) stop in the doorway. The video shows steps (1)–(2) completed. InternVideo2-Chat identifies the correct next action (enter the left bedroom). Gemini Pro jumps to the final step; GPT-4V repeats a completed step. Success requires aligning visual progress with instruction lists and selecting the pending action. Source: \blx@tocontentsinit {0}\cite {wang2024_internvideo2}.}}{1818}{figure.caption.4189}\protected@file@percent }
\abx@aux@backref{3527}{wang2024_internvideo2}{0}{1818}{1818}
\newlabel{fig:chapter24_iv2_vln}{{24.93}{1818}{Vision–language navigation with progress tracking. Instructions are: (1) go up the stairs, (2) turn left, (3) enter the left bedroom, (4) stop in the doorway. The video shows steps (1)–(2) completed. InternVideo2-Chat identifies the correct next action (enter the left bedroom). Gemini Pro jumps to the final step; GPT-4V repeats a completed step. Success requires aligning visual progress with instruction lists and selecting the pending action. Source: \cite {wang2024_internvideo2}}{figure.caption.4189}{}}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{wang2025internvideo2_5}
\abx@aux@segm{0}{0}{wang2025internvideo2_5}
\abx@aux@cite{0}{wang2025internvideo2_5}
\abx@aux@segm{0}{0}{wang2025internvideo2_5}
\abx@aux@cite{0}{wang2025internvideo2_5}
\abx@aux@segm{0}{0}{wang2025internvideo2_5}
\@writefile{toc}{\contentsline {subsubsection}{Limitations}{1819}{section*.4190}\protected@file@percent }
\abx@aux@backref{3528}{wang2024_internvideo2}{0}{1819}{1819}
\abx@aux@backref{3529}{wang2024_internvideo2}{0}{1819}{1819}
\abx@aux@backref{3530}{wang2024_internvideo2}{0}{1819}{1819}
\abx@aux@backref{3531}{wang2024_internvideo2}{0}{1819}{1819}
\abx@aux@backref{3532}{wang2024_internvideo2}{0}{1819}{1819}
\@writefile{toc}{\contentsline {subsubsection}{Future work and toward InternVideo2.5}{1819}{section*.4191}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24.94}{\ignorespaces \textbf  {InternVideo2.5 with LRC modeling.} LRC pairs hierarchical token compression for long context with task-grounded preference optimization to inject dense perception skills (temporal grounding, segmentation, tracking) into the MLLM. Reproduced from \blx@tocontentsinit {0}\cite {wang2025internvideo2_5}.}}{1819}{figure.caption.4192}\protected@file@percent }
\abx@aux@backref{3534}{wang2025internvideo2_5}{0}{1819}{1819}
\newlabel{fig:chpapter24_iv25_overview}{{24.94}{1819}{\textbf {InternVideo2.5 with LRC modeling.} LRC pairs hierarchical token compression for long context with task-grounded preference optimization to inject dense perception skills (temporal grounding, segmentation, tracking) into the MLLM. Reproduced from \cite {wang2025internvideo2_5}}{figure.caption.4192}{}}
\abx@aux@cite{0}{wang2025internvideo2_5}
\abx@aux@segm{0}{0}{wang2025internvideo2_5}
\abx@aux@cite{0}{wang2025internvideo2_5}
\abx@aux@segm{0}{0}{wang2025internvideo2_5}
\abx@aux@cite{0}{wang2025internvideo2_5}
\abx@aux@segm{0}{0}{wang2025internvideo2_5}
\abx@aux@cite{0}{wang2025internvideo2_5}
\abx@aux@segm{0}{0}{wang2025internvideo2_5}
\abx@aux@cite{0}{openai2023b_gpt4v}
\abx@aux@segm{0}{0}{openai2023b_gpt4v}
\abx@aux@cite{0}{team2023_gemini}
\abx@aux@segm{0}{0}{team2023_gemini}
\abx@aux@cite{0}{wang2025internvideo2_5}
\abx@aux@segm{0}{0}{wang2025internvideo2_5}
\abx@aux@cite{0}{openai2023b_gpt4v}
\abx@aux@segm{0}{0}{openai2023b_gpt4v}
\abx@aux@cite{0}{team2023_gemini}
\abx@aux@segm{0}{0}{team2023_gemini}
\abx@aux@cite{0}{bai2023_qwenvl2}
\abx@aux@segm{0}{0}{bai2023_qwenvl2}
\abx@aux@cite{0}{li2024_mvbench}
\abx@aux@segm{0}{0}{li2024_mvbench}
\abx@aux@cite{0}{bai2023_qwenvl2}
\abx@aux@segm{0}{0}{bai2023_qwenvl2}
\abx@aux@cite{0}{li2024_mvbench}
\abx@aux@segm{0}{0}{li2024_mvbench}
\abx@aux@cite{0}{li2024_mvbench}
\abx@aux@segm{0}{0}{li2024_mvbench}
\abx@aux@cite{0}{bai2023_qwenvl2}
\abx@aux@segm{0}{0}{bai2023_qwenvl2}
\abx@aux@cite{0}{li2024_mvbench}
\abx@aux@segm{0}{0}{li2024_mvbench}
\abx@aux@cite{0}{wang2025internvideo2_5}
\abx@aux@segm{0}{0}{wang2025internvideo2_5}
\abx@aux@cite{0}{wang2025internvideo2_5}
\abx@aux@segm{0}{0}{wang2025internvideo2_5}
\abx@aux@backref{3535}{wang2025internvideo2_5}{0}{1820}{1820}
\abx@aux@backref{3536}{wang2025internvideo2_5}{0}{1820}{1820}
\abx@aux@backref{3537}{wang2025internvideo2_5}{0}{1820}{1820}
\abx@aux@backref{3538}{wang2025internvideo2_5}{0}{1820}{1820}
\@writefile{toc}{\contentsline {paragraph}{Empirical findings and position vs.\ prior work}{1820}{section*.4193}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {24.59}{\ignorespaces InternVideo2.5 (7B, 16 tokens) vs.\ representative prior systems (scores \%). “Best prior open” is the strongest \emph  {open} baseline reported \emph  {before} adding LRC. Numbers consolidated from Table~2 in \blx@tocontentsinit {0}\cite {wang2025internvideo2_5}; proprietary rows from \blx@tocontentsinit {0}\cite {openai2023b_gpt4v,team2023_gemini}.}}{1820}{table.caption.4194}\protected@file@percent }
\abx@aux@backref{3542}{wang2025internvideo2_5}{0}{1820}{1820}
\abx@aux@backref{3543}{team2023_gemini}{0}{1820}{1820}
\abx@aux@backref{3544}{openai2023b_gpt4v}{0}{1820}{1820}
\newlabel{tab:iv25_compact_vs_baselines}{{24.59}{1820}{InternVideo2.5 (7B, 16 tokens) vs.\ representative prior systems (scores \%). “Best prior open” is the strongest \emph {open} baseline reported \emph {before} adding LRC. Numbers consolidated from Table~2 in \cite {wang2025internvideo2_5}; proprietary rows from \cite {openai2023b_gpt4v,team2023_gemini}}{table.caption.4194}{}}
\abx@aux@backref{3545}{bai2023_qwenvl2}{0}{1820}{1820}
\abx@aux@backref{3546}{li2024_mvbench}{0}{1820}{1820}
\abx@aux@backref{3547}{bai2023_qwenvl2}{0}{1820}{1820}
\abx@aux@backref{3548}{li2024_mvbench}{0}{1820}{1820}
\abx@aux@backref{3549}{li2024_mvbench}{0}{1820}{1820}
\abx@aux@backref{3550}{bai2023_qwenvl2}{0}{1820}{1820}
\abx@aux@backref{3551}{li2024_mvbench}{0}{1820}{1820}
\@writefile{toc}{\contentsline {paragraph}{What changes, how it is implemented, and why it helps}{1820}{section*.4195}\protected@file@percent }
\abx@aux@backref{3552}{wang2025internvideo2_5}{0}{1820}{1820}
\abx@aux@backref{3553}{wang2025internvideo2_5}{0}{1820}{1820}
\abx@aux@cite{0}{wang2025internvideo2_5}
\abx@aux@segm{0}{0}{wang2025internvideo2_5}
\abx@aux@cite{0}{wang2025internvideo2_5}
\abx@aux@segm{0}{0}{wang2025internvideo2_5}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{wang2025internvideo2_5}
\abx@aux@segm{0}{0}{wang2025internvideo2_5}
\abx@aux@backref{3554}{wang2025internvideo2_5}{0}{1821}{1821}
\abx@aux@backref{3555}{wang2025internvideo2_5}{0}{1821}{1821}
\@writefile{toc}{\contentsline {paragraph}{Intuition and expected impact}{1821}{section*.4196}\protected@file@percent }
\abx@aux@backref{3556}{wang2024_internvideo2}{0}{1821}{1821}
\abx@aux@backref{3557}{wang2025internvideo2_5}{0}{1821}{1821}
\BKM@entry{id=969,dest={73656374696F6E2A2E34313937},srcline={5002}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030345C3030302E5C303030395C3030303A5C3030305C3034305C303030565C303030695C303030645C303030655C3030306F5C3034305C3032335C3030304C5C303030615C3030306E5C303030675C303030755C303030615C303030675C303030655C3030305C3034305C3030304C5C303030615C303030725C303030675C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{wang2024_internvideo2}
\abx@aux@segm{0}{0}{wang2024_internvideo2}
\abx@aux@cite{0}{zhao2022_lavila}
\abx@aux@segm{0}{0}{zhao2022_lavila}
\abx@aux@cite{0}{zhang2023_videollama}
\abx@aux@segm{0}{0}{zhang2023_videollama}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\BKM@entry{id=970,dest={73656374696F6E2A2E34313938},srcline={5010}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030345C3030302E5C303030395C3030302E5C303030315C3030303A5C3030305C3034305C3030304C5C303030615C303030565C303030695C3030304C5C303030615C3030303A5C3030305C3034305C3030304C5C303030655C303030615C303030725C3030306E5C303030695C3030306E5C303030675C3030305C3034305C303030565C303030695C303030645C303030655C3030306F5C3030305C3034305C303030525C303030655C303030705C303030725C303030655C303030735C303030655C3030306E5C303030745C303030615C303030745C303030695C3030306F5C3030306E5C303030735C3030305C3034305C303030665C303030725C3030306F5C3030306D5C3030305C3034305C3030304C5C3030304C5C3030304D5C30303073}
\abx@aux@cite{0}{zhao2022_lavila}
\abx@aux@segm{0}{0}{zhao2022_lavila}
\abx@aux@cite{0}{zhao2022_lavila}
\abx@aux@segm{0}{0}{zhao2022_lavila}
\abx@aux@cite{0}{zhao2022_lavila}
\abx@aux@segm{0}{0}{zhao2022_lavila}
\@writefile{toc}{\contentsline {section}{Enrichment 24.9: Video--Language Large Models}{1822}{section*.4197}\protected@file@percent }
\newlabel{enr:sec_chapter24_vl_llms}{{24.9}{1822}{\color {ocre}Enrichment \thesection : Video--Language Large Models}{section*.4197}{}}
\abx@aux@backref{3558}{li2024_llavaonevision}{0}{1822}{1822}
\abx@aux@backref{3559}{wang2024_internvideo2}{0}{1822}{1822}
\abx@aux@backref{3560}{zhao2022_lavila}{0}{1822}{1822}
\abx@aux@backref{3561}{zhang2023_videollama}{0}{1822}{1822}
\abx@aux@backref{3562}{cheng2024_videollama2}{0}{1822}{1822}
\abx@aux@backref{3563}{zhang2025_videollama3}{0}{1822}{1822}
\abx@aux@backref{3564}{bai2023_qwenvl}{0}{1822}{1822}
\abx@aux@backref{3565}{wang2024_qwen2vl}{0}{1822}{1822}
\@writefile{toc}{\contentsline {subsection}{Enrichment 24.9.1: LaViLa: Learning Video Representations from LLMs}{1822}{section*.4198}\protected@file@percent }
\newlabel{enr:subsec_chapter24_lavila}{{24.9.1}{1822}{\color {ocre}Enrichment \thesubsection : LaViLa: Learning Video Representations from LLMs}{section*.4198}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24.95}{\ignorespaces LaViLa leverages LLMs to densely narrate long videos, and uses those narrations to train strong dual-encoders; compared to prior sparse human labels or weak ASR, LLM text is denser, more diverse, and temporally aligned. Source: \blx@tocontentsinit {0}\cite {zhao2022_lavila}.}}{1822}{figure.caption.4199}\protected@file@percent }
\abx@aux@backref{3567}{zhao2022_lavila}{0}{1822}{1822}
\newlabel{fig:chapter24_lavila_idea}{{24.95}{1822}{LaViLa leverages LLMs to densely narrate long videos, and uses those narrations to train strong dual-encoders; compared to prior sparse human labels or weak ASR, LLM text is denser, more diverse, and temporally aligned. Source: \cite {zhao2022_lavila}}{figure.caption.4199}{}}
\abx@aux@cite{0}{zhao2022_lavila}
\abx@aux@segm{0}{0}{zhao2022_lavila}
\abx@aux@cite{0}{zhao2022_lavila}
\abx@aux@segm{0}{0}{zhao2022_lavila}
\@writefile{toc}{\contentsline {paragraph}{Scope and positioning}{1823}{section*.4200}\protected@file@percent }
\abx@aux@backref{3568}{zhao2022_lavila}{0}{1823}{1823}
\@writefile{toc}{\contentsline {paragraph}{Motivation / Problem framing}{1823}{section*.4201}\protected@file@percent }
\abx@aux@backref{3569}{zhao2022_lavila}{0}{1823}{1823}
\@writefile{toc}{\contentsline {subsubsection}{Method: narration-supervised contrastive learning}{1823}{section*.4202}\protected@file@percent }
\newlabel{subsubsec:chapter24_lavila_method}{{24.9.1}{1823}{Method: narration-supervised contrastive learning}{section*.4202}{}}
\@writefile{toc}{\contentsline {paragraph}{Highlevel flow}{1823}{section*.4203}\protected@file@percent }
\abx@aux@backref{3570}{zhao2022_lavila}{0}{1823}{1823}
\@writefile{toc}{\contentsline {paragraph}{Why NARRATOR and REPHRASER}{1823}{section*.4204}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Setup and notation}{1823}{section*.4205}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Contrastive objective on mixed sources}{1823}{section*.4206}\protected@file@percent }
\abx@aux@cite{0}{zhao2022_lavila}
\abx@aux@segm{0}{0}{zhao2022_lavila}
\abx@aux@cite{0}{zhao2022_lavila}
\abx@aux@segm{0}{0}{zhao2022_lavila}
\abx@aux@cite{0}{zhao2022_lavila}
\abx@aux@segm{0}{0}{zhao2022_lavila}
\abx@aux@cite{0}{zhao2022_lavila}
\abx@aux@segm{0}{0}{zhao2022_lavila}
\abx@aux@cite{0}{zhao2022_lavila}
\abx@aux@segm{0}{0}{zhao2022_lavila}
\newlabel{eq:chapter24_lavila_infonce}{{24.50}{1824}{Contrastive objective on mixed sources}{equation.24.50}{}}
\abx@aux@backref{3571}{zhao2022_lavila}{0}{1824}{1824}
\@writefile{toc}{\contentsline {paragraph}{Offline generators and their training}{1824}{section*.4207}\protected@file@percent }
\abx@aux@backref{3572}{zhao2022_lavila}{0}{1824}{1824}
\abx@aux@backref{3573}{zhao2022_lavila}{0}{1824}{1824}
\@writefile{toc}{\contentsline {paragraph}{Visual conditioning mechanism}{1824}{section*.4208}\protected@file@percent }
\newlabel{eq:chapter24_lavila_attnpool}{{24.51}{1824}{Visual conditioning mechanism}{equation.24.51}{}}
\abx@aux@backref{3574}{zhao2022_lavila}{0}{1824}{1824}
\newlabel{eq:chapter24_lavila_narrator}{{24.52}{1824}{Visual conditioning mechanism}{equation.24.52}{}}
\@writefile{toc}{\contentsline {paragraph}{Batching and curriculum in practice}{1824}{section*.4209}\protected@file@percent }
\abx@aux@backref{3575}{zhao2022_lavila}{0}{1824}{1824}
\@writefile{toc}{\contentsline {paragraph}{Why this design works}{1824}{section*.4210}\protected@file@percent }
\abx@aux@cite{0}{zhao2022_lavila}
\abx@aux@segm{0}{0}{zhao2022_lavila}
\abx@aux@backref{3576}{zhao2022_lavila}{0}{1825}{1825}
\@writefile{toc}{\contentsline {paragraph}{High-level training loop}{1825}{section*.4211}\protected@file@percent }
\newlabel{alg:chapter24_lavila_training}{{24.9.1}{1825}{High-level training loop}{section*.4211}{}}
\@writefile{toc}{\contentsline {subsubsection}{Architecture and implementation details}{1825}{section*.4212}\protected@file@percent }
\newlabel{subsubsec:chapter24_lavila_arch}{{24.9.1}{1825}{Architecture and implementation details}{section*.4212}{}}
\@writefile{toc}{\contentsline {paragraph}{Dual-encoder backbone}{1825}{section*.4213}\protected@file@percent }
\abx@aux@cite{0}{zhao2022_lavila}
\abx@aux@segm{0}{0}{zhao2022_lavila}
\abx@aux@cite{0}{zhao2022_lavila}
\abx@aux@segm{0}{0}{zhao2022_lavila}
\abx@aux@cite{0}{zhao2022_lavila}
\abx@aux@segm{0}{0}{zhao2022_lavila}
\abx@aux@cite{0}{zhao2022_lavila}
\abx@aux@segm{0}{0}{zhao2022_lavila}
\@writefile{toc}{\contentsline {paragraph}{NARRATOR design and training}{1826}{section*.4214}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24.96}{\ignorespaces Language supervision from REPHRASER (text$\rightarrow $text) and NARRATOR (video$\rightarrow $text); the latter uses attention pooling over video tokens and cross-attention modules inside a frozen GPT-2 decoder \,\,\, Source: \blx@tocontentsinit {0}\cite {zhao2022_lavila}.}}{1826}{figure.caption.4215}\protected@file@percent }
\abx@aux@backref{3578}{zhao2022_lavila}{0}{1826}{1826}
\newlabel{fig:chapter24_lavila_langsup}{{24.96}{1826}{Language supervision from REPHRASER (text$\rightarrow $text) and NARRATOR (video$\rightarrow $text); the latter uses attention pooling over video tokens and cross-attention modules inside a frozen GPT-2 decoder \,\,\, Source: \cite {zhao2022_lavila}}{figure.caption.4215}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24.97}{\ignorespaces Qualitative outputs from NARRATOR and REPHRASER; the former focuses on actions and interacted objects, the latter diversifies phrasing via synonymy and reordering \,\,\, Source: \blx@tocontentsinit {0}\cite {zhao2022_lavila}.}}{1826}{figure.caption.4216}\protected@file@percent }
\abx@aux@backref{3580}{zhao2022_lavila}{0}{1826}{1826}
\newlabel{fig:chapter24_lavila_qual}{{24.97}{1826}{Qualitative outputs from NARRATOR and REPHRASER; the former focuses on actions and interacted objects, the latter diversifies phrasing via synonymy and reordering \,\,\, Source: \cite {zhao2022_lavila}}{figure.caption.4216}{}}
\@writefile{toc}{\contentsline {paragraph}{Pretraining schedule and input processing}{1826}{section*.4217}\protected@file@percent }
\abx@aux@cite{0}{zhao2022_lavila}
\abx@aux@segm{0}{0}{zhao2022_lavila}
\abx@aux@cite{0}{zhao2022_lavila}
\abx@aux@segm{0}{0}{zhao2022_lavila}
\abx@aux@cite{0}{zhao2022_lavila}
\abx@aux@segm{0}{0}{zhao2022_lavila}
\abx@aux@cite{0}{zhao2022_lavila}
\abx@aux@segm{0}{0}{zhao2022_lavila}
\abx@aux@cite{0}{zhao2022_lavila}
\abx@aux@segm{0}{0}{zhao2022_lavila}
\@writefile{toc}{\contentsline {subsubsection}{Experiments}{1827}{section*.4218}\protected@file@percent }
\newlabel{subsubsec:chapter24_lavila_experiments}{{24.9.1}{1827}{Experiments}{section*.4218}{}}
\@writefile{toc}{\contentsline {paragraph}{Benchmarks and protocols}{1827}{section*.4219}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {24.60}{\ignorespaces Downstream datasets and evaluation protocols for LaViLa.}}{1827}{table.caption.4220}\protected@file@percent }
\newlabel{tab:chapter24_lavila_datasets}{{24.60}{1827}{Downstream datasets and evaluation protocols for LaViLa}{table.caption.4220}{}}
\@writefile{toc}{\contentsline {paragraph}{Headline results}{1827}{section*.4221}\protected@file@percent }
\abx@aux@backref{3581}{zhao2022_lavila}{0}{1827}{1827}
\abx@aux@backref{3582}{zhao2022_lavila}{0}{1827}{1827}
\@writefile{lof}{\contentsline {figure}{\numberline {24.98}{\ignorespaces Comparison to prior SOTA across egocentric and third-person video understanding; LaViLa attains new state of the art via narration-supervised alignment. Source: \blx@tocontentsinit {0}\cite {zhao2022_lavila}.}}{1827}{figure.caption.4222}\protected@file@percent }
\abx@aux@backref{3584}{zhao2022_lavila}{0}{1827}{1827}
\newlabel{fig:chapter24_lavila_sota}{{24.98}{1827}{Comparison to prior SOTA across egocentric and third-person video understanding; LaViLa attains new state of the art via narration-supervised alignment. Source: \cite {zhao2022_lavila}}{figure.caption.4222}{}}
\abx@aux@cite{0}{zhao2022_lavila}
\abx@aux@segm{0}{0}{zhao2022_lavila}
\abx@aux@cite{0}{zhao2022_lavila}
\abx@aux@segm{0}{0}{zhao2022_lavila}
\abx@aux@cite{0}{zhao2022_lavila}
\abx@aux@segm{0}{0}{zhao2022_lavila}
\abx@aux@cite{0}{zhao2022_lavila}
\abx@aux@segm{0}{0}{zhao2022_lavila}
\abx@aux@cite{0}{zhao2022_lavila}
\abx@aux@segm{0}{0}{zhao2022_lavila}
\@writefile{toc}{\contentsline {paragraph}{Summary of main experiments and ablations}{1828}{section*.4223}\protected@file@percent }
\abx@aux@backref{3585}{zhao2022_lavila}{0}{1828}{1828}
\abx@aux@backref{3586}{zhao2022_lavila}{0}{1828}{1828}
\abx@aux@backref{3587}{zhao2022_lavila}{0}{1828}{1828}
\abx@aux@backref{3588}{zhao2022_lavila}{0}{1828}{1828}
\abx@aux@backref{3589}{zhao2022_lavila}{0}{1828}{1828}
\abx@aux@backref{3590}{zhao2022_lavila}{0}{1828}{1828}
\abx@aux@cite{0}{zhao2022_lavila}
\abx@aux@segm{0}{0}{zhao2022_lavila}
\abx@aux@cite{0}{zhao2022_lavila}
\abx@aux@segm{0}{0}{zhao2022_lavila}
\abx@aux@cite{0}{zhao2022_lavila}
\abx@aux@segm{0}{0}{zhao2022_lavila}
\abx@aux@cite{0}{zhao2022_lavila}
\abx@aux@segm{0}{0}{zhao2022_lavila}
\abx@aux@cite{0}{zhao2022_lavila}
\abx@aux@segm{0}{0}{zhao2022_lavila}
\abx@aux@cite{0}{zhao2022_lavila}
\abx@aux@segm{0}{0}{zhao2022_lavila}
\abx@aux@cite{0}{zhao2022_lavila}
\abx@aux@segm{0}{0}{zhao2022_lavila}
\@writefile{toc}{\contentsline {paragraph}{Ablations}{1829}{section*.4224}\protected@file@percent }
\abx@aux@backref{3591}{zhao2022_lavila}{0}{1829}{1829}
\abx@aux@backref{3592}{zhao2022_lavila}{0}{1829}{1829}
\abx@aux@backref{3593}{zhao2022_lavila}{0}{1829}{1829}
\abx@aux@backref{3594}{zhao2022_lavila}{0}{1829}{1829}
\abx@aux@backref{3595}{zhao2022_lavila}{0}{1829}{1829}
\abx@aux@backref{3596}{zhao2022_lavila}{0}{1829}{1829}
\abx@aux@backref{3597}{zhao2022_lavila}{0}{1829}{1829}
\abx@aux@cite{0}{zhao2022_lavila}
\abx@aux@segm{0}{0}{zhao2022_lavila}
\abx@aux@cite{0}{zhao2022_lavila}
\abx@aux@segm{0}{0}{zhao2022_lavila}
\abx@aux@cite{0}{zhao2022_lavila}
\abx@aux@segm{0}{0}{zhao2022_lavila}
\abx@aux@cite{0}{zhao2022_lavila}
\abx@aux@segm{0}{0}{zhao2022_lavila}
\abx@aux@cite{0}{zhao2022_lavila}
\abx@aux@segm{0}{0}{zhao2022_lavila}
\abx@aux@cite{0}{zhao2022_lavila}
\abx@aux@segm{0}{0}{zhao2022_lavila}
\abx@aux@cite{0}{zhao2022_lavila}
\abx@aux@segm{0}{0}{zhao2022_lavila}
\abx@aux@cite{0}{zhang2023_videollama}
\abx@aux@segm{0}{0}{zhang2023_videollama}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\@writefile{toc}{\contentsline {subsubsection}{Limitations and future directions}{1830}{section*.4225}\protected@file@percent }
\newlabel{subsubsec:chapter24_lavila_limits}{{24.9.1}{1830}{Limitations and future directions}{section*.4225}{}}
\@writefile{toc}{\contentsline {paragraph}{Observed constraints}{1830}{section*.4226}\protected@file@percent }
\abx@aux@backref{3598}{zhao2022_lavila}{0}{1830}{1830}
\abx@aux@backref{3599}{zhao2022_lavila}{0}{1830}{1830}
\abx@aux@backref{3600}{zhao2022_lavila}{0}{1830}{1830}
\abx@aux@backref{3601}{zhao2022_lavila}{0}{1830}{1830}
\abx@aux@backref{3602}{zhao2022_lavila}{0}{1830}{1830}
\abx@aux@backref{3603}{zhao2022_lavila}{0}{1830}{1830}
\@writefile{toc}{\contentsline {paragraph}{Future work}{1830}{section*.4227}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Bridge to instruction-tuned video–LLMs}{1830}{section*.4228}\protected@file@percent }
\abx@aux@backref{3604}{zhao2022_lavila}{0}{1830}{1830}
\abx@aux@backref{3605}{cheng2024_videollama2}{0}{1830}{1830}
\abx@aux@backref{3606}{zhang2023_videollama}{0}{1830}{1830}
\BKM@entry{id=971,dest={73656374696F6E2A2E34323239},srcline={5269}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030345C3030302E5C303030395C3030302E5C303030325C3030303A5C3030305C3034305C303030565C303030695C303030645C303030655C3030306F5C3030302D5C3030304C5C3030304C5C303030615C3030304D5C303030415C3030305C3034305C303030315C3030303A5C3030305C3034305C303030495C3030306E5C303030735C303030745C303030725C303030755C303030635C303030745C303030695C3030306F5C3030306E5C3030302D5C303030545C303030755C3030306E5C303030655C303030645C3030305C3034305C303030565C303030695C303030645C303030655C3030306F5C3030305C3034305C3030304C5C3030304C5C3030304D}
\abx@aux@cite{0}{li2023_blip2}
\abx@aux@segm{0}{0}{li2023_blip2}
\abx@aux@cite{0}{liu2023_llava}
\abx@aux@segm{0}{0}{liu2023_llava}
\abx@aux@cite{0}{zhu2023_minigpt4}
\abx@aux@segm{0}{0}{zhu2023_minigpt4}
\abx@aux@cite{0}{huang2023_audiogpt}
\abx@aux@segm{0}{0}{huang2023_audiogpt}
\abx@aux@cite{0}{li2024_videochat}
\abx@aux@segm{0}{0}{li2024_videochat}
\abx@aux@cite{0}{maaz2024_video_chatgpt}
\abx@aux@segm{0}{0}{maaz2024_video_chatgpt}
\abx@aux@cite{0}{zhang2023_videollama}
\abx@aux@segm{0}{0}{zhang2023_videollama}
\abx@aux@cite{0}{zhang2023_videollama}
\abx@aux@segm{0}{0}{zhang2023_videollama}
\@writefile{toc}{\contentsline {subsection}{Enrichment 24.9.2: Video\mbox  {-}LLaMA 1: Instruction\mbox  {-}Tuned Video LLM}{1831}{section*.4229}\protected@file@percent }
\newlabel{enr:subsec_chapter24_videollama1}{{24.9.2}{1831}{\color {ocre}Enrichment \thesubsection : Video\mbox {-}LLaMA 1: Instruction\mbox {-}Tuned Video LLM}{section*.4229}{}}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{1831}{section*.4230}\protected@file@percent }
\newlabel{subsubsec:chapter24_videollama1_motivation}{{24.9.2}{1831}{Motivation}{section*.4230}{}}
\@writefile{toc}{\contentsline {paragraph}{Why audio--visual LLMs?}{1831}{section*.4231}\protected@file@percent }
\abx@aux@backref{3607}{li2023_blip2}{0}{1831}{1831}
\abx@aux@backref{3608}{liu2023_llava}{0}{1831}{1831}
\abx@aux@backref{3609}{zhu2023_minigpt4}{0}{1831}{1831}
\abx@aux@backref{3610}{huang2023_audiogpt}{0}{1831}{1831}
\@writefile{toc}{\contentsline {paragraph}{Design goal}{1831}{section*.4232}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24.99}{\ignorespaces Overall architecture of Video-LLaMA: a dual-branch design that converts video frames (left, Vision–Language branch) and audio segments (right, Audio–Language branch) into small sets of \emph  {query tokens}, projects them to the LLM embedding space, and concatenates them with text tokens to condition a frozen LLM (Vicuna/LLaMA). Vision: frozen image encoder (ViT) + Video Q-Former + linear projector. Audio: frozen audio encoder (ImageBind) + Audio Q-Former + projector. This lets the LLM reason jointly over sight and sound. Adapted from \blx@tocontentsinit {0}\cite {zhang2023_videollama}.}}{1831}{figure.caption.4233}\protected@file@percent }
\abx@aux@backref{3614}{zhang2023_videollama}{0}{1831}{1831}
\newlabel{fig:chapter24_videollama_arch}{{24.99}{1831}{Overall architecture of Video-LLaMA: a dual-branch design that converts video frames (left, Vision–Language branch) and audio segments (right, Audio–Language branch) into small sets of \emph {query tokens}, projects them to the LLM embedding space, and concatenates them with text tokens to condition a frozen LLM (Vicuna/LLaMA). Vision: frozen image encoder (ViT) + Video Q-Former + linear projector. Audio: frozen audio encoder (ImageBind) + Audio Q-Former + projector. This lets the LLM reason jointly over sight and sound. Adapted from \cite {zhang2023_videollama}}{figure.caption.4233}{}}
\abx@aux@backref{3611}{li2024_videochat}{0}{1831}{1831}
\abx@aux@backref{3612}{maaz2024_video_chatgpt}{0}{1831}{1831}
\abx@aux@cite{0}{girdhar2023_imagebind}
\abx@aux@segm{0}{0}{girdhar2023_imagebind}
\abx@aux@cite{0}{girdhar2023_imagebind}
\abx@aux@segm{0}{0}{girdhar2023_imagebind}
\abx@aux@cite{0}{zhang2023_videollama}
\abx@aux@segm{0}{0}{zhang2023_videollama}
\@writefile{toc}{\contentsline {subsubsection}{Method: Multi-Branch Cross-Modal Training with Q-Formers}{1832}{section*.4234}\protected@file@percent }
\newlabel{subsubsec:chapter24_videollama1_method}{{24.9.2}{1832}{Method: Multi-Branch Cross-Modal Training with Q-Formers}{section*.4234}{}}
\@writefile{toc}{\contentsline {paragraph}{Problem setup and notation}{1832}{section*.4235}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Vision–Language branch}{1832}{section*.4236}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Audio–Language branch}{1832}{section*.4237}\protected@file@percent }
\abx@aux@backref{3615}{girdhar2023_imagebind}{0}{1832}{1832}
\abx@aux@backref{3616}{girdhar2023_imagebind}{0}{1832}{1832}
\@writefile{toc}{\contentsline {paragraph}{Training curriculum}{1832}{section*.4238}\protected@file@percent }
\abx@aux@backref{3617}{zhang2023_videollama}{0}{1832}{1832}
\abx@aux@cite{0}{zhang2023_videollama}
\abx@aux@segm{0}{0}{zhang2023_videollama}
\abx@aux@cite{0}{zhang2023_videollama}
\abx@aux@segm{0}{0}{zhang2023_videollama}
\abx@aux@cite{0}{girdhar2023_imagebind}
\abx@aux@segm{0}{0}{girdhar2023_imagebind}
\abx@aux@cite{0}{zhang2023_videollama}
\abx@aux@segm{0}{0}{zhang2023_videollama}
\abx@aux@cite{0}{girdhar2023_imagebind}
\abx@aux@segm{0}{0}{girdhar2023_imagebind}
\abx@aux@cite{0}{zhang2023_videollama}
\abx@aux@segm{0}{0}{zhang2023_videollama}
\abx@aux@cite{0}{zhang2023_videollama}
\abx@aux@segm{0}{0}{zhang2023_videollama}
\abx@aux@cite{0}{zhang2023_videollama}
\abx@aux@segm{0}{0}{zhang2023_videollama}
\abx@aux@backref{3618}{zhang2023_videollama}{0}{1833}{1833}
\abx@aux@backref{3619}{zhang2023_videollama}{0}{1833}{1833}
\abx@aux@backref{3620}{girdhar2023_imagebind}{0}{1833}{1833}
\abx@aux@backref{3621}{girdhar2023_imagebind}{0}{1833}{1833}
\abx@aux@backref{3622}{zhang2023_videollama}{0}{1833}{1833}
\@writefile{toc}{\contentsline {paragraph}{How images and videos share one encoder}{1833}{section*.4239}\protected@file@percent }
\abx@aux@backref{3623}{zhang2023_videollama}{0}{1833}{1833}
\@writefile{toc}{\contentsline {paragraph}{Positional encoding (vision \& audio)}{1833}{section*.4240}\protected@file@percent }
\abx@aux@backref{3624}{zhang2023_videollama}{0}{1833}{1833}
\@writefile{toc}{\contentsline {paragraph}{Learning objective (unified view)}{1833}{section*.4241}\protected@file@percent }
\abx@aux@cite{0}{zhang2023_videollama}
\abx@aux@segm{0}{0}{zhang2023_videollama}
\abx@aux@cite{0}{zhang2023_videollama}
\abx@aux@segm{0}{0}{zhang2023_videollama}
\abx@aux@cite{0}{li2023_blip2}
\abx@aux@segm{0}{0}{li2023_blip2}
\abx@aux@cite{0}{zhu2023_minigpt4}
\abx@aux@segm{0}{0}{zhu2023_minigpt4}
\abx@aux@cite{0}{liu2023_llava}
\abx@aux@segm{0}{0}{liu2023_llava}
\abx@aux@cite{0}{ye2024_mplug_owl}
\abx@aux@segm{0}{0}{ye2024_mplug_owl}
\abx@aux@cite{0}{li2024_videochat}
\abx@aux@segm{0}{0}{li2024_videochat}
\abx@aux@cite{0}{huang2023_audiogpt}
\abx@aux@segm{0}{0}{huang2023_audiogpt}
\abx@aux@cite{0}{maaz2024_video_chatgpt}
\abx@aux@segm{0}{0}{maaz2024_video_chatgpt}
\abx@aux@cite{0}{zhang2023_videollama}
\abx@aux@segm{0}{0}{zhang2023_videollama}
\abx@aux@cite{0}{zhang2023_videollama}
\abx@aux@segm{0}{0}{zhang2023_videollama}
\abx@aux@backref{3625}{zhang2023_videollama}{0}{1834}{1834}
\@writefile{toc}{\contentsline {paragraph}{Intuition and roles}{1834}{section*.4242}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Architecture \& Implementation Details}{1834}{section*.4243}\protected@file@percent }
\newlabel{subsubsec:chapter24_videollama1_arch_impl}{{24.9.2}{1834}{Architecture \& Implementation Details}{section*.4243}{}}
\@writefile{toc}{\contentsline {paragraph}{Backbones and frozen parts}{1834}{section*.4244}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Video tokens}{1834}{section*.4245}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Audio tokens}{1834}{section*.4246}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{GEMINI additions (intuitive recap)}{1834}{section*.4247}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {24.61}{\ignorespaces Comparison with popular multimodal LLMs: Video-LLaMA uniquely handles \emph  {images}, \emph  {silent videos}, and \emph  {audio} jointly (Adapted from Table~1 of \blx@tocontentsinit {0}\cite {zhang2023_videollama}).}}{1834}{table.caption.4248}\protected@file@percent }
\abx@aux@backref{3627}{zhang2023_videollama}{0}{1834}{1834}
\newlabel{tab:videollama_model_capabilities}{{24.61}{1834}{Comparison with popular multimodal LLMs: Video-LLaMA uniquely handles \emph {images}, \emph {silent videos}, and \emph {audio} jointly (Adapted from Table~1 of \cite {zhang2023_videollama})}{table.caption.4248}{}}
\abx@aux@backref{3628}{li2023_blip2}{0}{1834}{1834}
\abx@aux@backref{3629}{zhu2023_minigpt4}{0}{1834}{1834}
\abx@aux@backref{3630}{liu2023_llava}{0}{1834}{1834}
\abx@aux@backref{3631}{ye2024_mplug_owl}{0}{1834}{1834}
\abx@aux@backref{3632}{li2024_videochat}{0}{1834}{1834}
\abx@aux@backref{3633}{huang2023_audiogpt}{0}{1834}{1834}
\abx@aux@backref{3634}{maaz2024_video_chatgpt}{0}{1834}{1834}
\abx@aux@backref{3635}{zhang2023_videollama}{0}{1834}{1834}
\abx@aux@cite{0}{zhang2023_videollama}
\abx@aux@segm{0}{0}{zhang2023_videollama}
\abx@aux@cite{0}{zhang2023_videollama}
\abx@aux@segm{0}{0}{zhang2023_videollama}
\abx@aux@cite{0}{zhang2023_videollama}
\abx@aux@segm{0}{0}{zhang2023_videollama}
\@writefile{toc}{\contentsline {subsubsection}{Experiments and Ablations}{1835}{section*.4249}\protected@file@percent }
\newlabel{subsubsec:chapter24_videollama1_expts}{{24.9.2}{1835}{Experiments and Ablations}{section*.4249}{}}
\@writefile{toc}{\contentsline {paragraph}{Qualitative capabilities}{1835}{section*.4250}\protected@file@percent }
\abx@aux@backref{3636}{zhang2023_videollama}{0}{1835}{1835}
\@writefile{lof}{\contentsline {figure}{\numberline {24.100}{\ignorespaces Examples generated by Video-LLaMA. (a) Answers based on background sound and visual content. (b) Identifies actions over time. (c) Understands static images. (d) Recognizes famous landmarks. Adapted from \blx@tocontentsinit {0}\cite {zhang2023_videollama}.}}{1835}{figure.caption.4251}\protected@file@percent }
\abx@aux@backref{3638}{zhang2023_videollama}{0}{1835}{1835}
\newlabel{fig:chapter24_videollama_examples}{{24.100}{1835}{Examples generated by Video-LLaMA. (a) Answers based on background sound and visual content. (b) Identifies actions over time. (c) Understands static images. (d) Recognizes famous landmarks. Adapted from \cite {zhang2023_videollama}}{figure.caption.4251}{}}
\@writefile{toc}{\contentsline {paragraph}{Tasks and metrics (at a glance)}{1835}{section*.4252}\protected@file@percent }
\abx@aux@backref{3639}{zhang2023_videollama}{0}{1835}{1835}
\abx@aux@cite{0}{zhang2023_videollama}
\abx@aux@segm{0}{0}{zhang2023_videollama}
\abx@aux@cite{0}{zhang2023_videollama}
\abx@aux@segm{0}{0}{zhang2023_videollama}
\abx@aux@cite{0}{zhang2023_videollama}
\abx@aux@segm{0}{0}{zhang2023_videollama}
\abx@aux@cite{0}{zhang2023_videollama}
\abx@aux@segm{0}{0}{zhang2023_videollama}
\@writefile{toc}{\contentsline {paragraph}{Training stages and ablations}{1836}{section*.4253}\protected@file@percent }
\abx@aux@backref{3640}{zhang2023_videollama}{0}{1836}{1836}
\abx@aux@backref{3641}{zhang2023_videollama}{0}{1836}{1836}
\abx@aux@backref{3642}{zhang2023_videollama}{0}{1836}{1836}
\abx@aux@backref{3643}{zhang2023_videollama}{0}{1836}{1836}
\@writefile{toc}{\contentsline {paragraph}{Positioning w.r.t.\ LaViLa and related LMMs}{1836}{section*.4254}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Limitations and Future Directions}{1836}{section*.4255}\protected@file@percent }
\newlabel{subsubsec:chapter24_videollama1_limits}{{24.9.2}{1836}{Limitations and Future Directions}{section*.4255}{}}
\@writefile{toc}{\contentsline {paragraph}{Observed constraints}{1836}{section*.4256}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Future work}{1836}{section*.4257}\protected@file@percent }
\BKM@entry{id=972,dest={73656374696F6E2A2E34323538},srcline={5428}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030345C3030302E5C303030395C3030302E5C303030335C3030303A5C3030305C3034305C303030565C303030695C303030645C303030655C3030306F5C3030302D5C3030304C5C3030304C5C303030615C3030304D5C303030415C3030305C3034305C303030325C3030303A5C3030305C3034305C303030455C3030306E5C303030685C303030615C3030306E5C303030635C303030655C303030645C3030305C3034305C303030555C3030306E5C303030645C303030655C303030725C303030735C303030745C303030615C3030306E5C303030645C303030695C3030306E5C303030675C3030302C5C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030635C30303079}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\@writefile{toc}{\contentsline {subsection}{Enrichment 24.9.3: Video\mbox  {-}LLaMA 2: Enhanced Understanding, Efficiency}{1837}{section*.4258}\protected@file@percent }
\newlabel{enr:subsec_chapter24_videollama2}{{24.9.3}{1837}{\color {ocre}Enrichment \thesubsection : Video\mbox {-}LLaMA 2: Enhanced Understanding, Efficiency}{section*.4258}{}}
\@writefile{toc}{\contentsline {paragraph}{Overview and motivation}{1837}{section*.4259}\protected@file@percent }
\abx@aux@backref{3644}{cheng2024_videollama2}{0}{1837}{1837}
\@writefile{lof}{\contentsline {figure}{\numberline {24.101}{\ignorespaces Overall pipeline of \emph  {Video-LLaMA2}. Frames are encoded by a frozen image encoder and passed through the STC connector before entering the LLM; audio is converted to log-Mel features, encoded, and aligned via an MLP block. Adapted from \blx@tocontentsinit {0}\cite {cheng2024_videollama2}.}}{1837}{figure.caption.4260}\protected@file@percent }
\abx@aux@backref{3646}{cheng2024_videollama2}{0}{1837}{1837}
\newlabel{fig:chapter24_videollama2_pipeline}{{24.101}{1837}{Overall pipeline of \emph {Video-LLaMA2}. Frames are encoded by a frozen image encoder and passed through the STC connector before entering the LLM; audio is converted to log-Mel features, encoded, and aligned via an MLP block. Adapted from \cite {cheng2024_videollama2}}{figure.caption.4260}{}}
\@writefile{toc}{\contentsline {subsubsection}{Method}{1837}{section*.4261}\protected@file@percent }
\newlabel{subsubsec:chapter24_videollama2_method}{{24.9.3}{1837}{Method}{section*.4261}{}}
\@writefile{toc}{\contentsline {paragraph}{Modality branches (concise)}{1837}{section*.4262}\protected@file@percent }
\abx@aux@backref{3647}{cheng2024_videollama2}{0}{1837}{1837}
\abx@aux@backref{3648}{cheng2024_videollama2}{0}{1837}{1837}
\abx@aux@backref{3649}{cheng2024_videollama2}{0}{1837}{1837}
\abx@aux@cite{0}{radosavovic2020_dnds}
\abx@aux@segm{0}{0}{radosavovic2020_dnds}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{radosavovic2020_dnds}
\abx@aux@segm{0}{0}{radosavovic2020_dnds}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\@writefile{toc}{\contentsline {paragraph}{STC connector: step\mbox  {-}by\mbox  {-}step mechanics and intuition}{1838}{section*.4263}\protected@file@percent }
\abx@aux@backref{3650}{radosavovic2020_dnds}{0}{1838}{1838}
\abx@aux@backref{3651}{cheng2024_videollama2}{0}{1838}{1838}
\abx@aux@backref{3652}{radosavovic2020_dnds}{0}{1838}{1838}
\abx@aux@backref{3653}{cheng2024_videollama2}{0}{1838}{1838}
\abx@aux@backref{3654}{cheng2024_videollama2}{0}{1838}{1838}
\abx@aux@backref{3655}{cheng2024_videollama2}{0}{1838}{1838}
\abx@aux@backref{3656}{cheng2024_videollama2}{0}{1838}{1838}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@backref{3657}{cheng2024_videollama2}{0}{1839}{1839}
\abx@aux@backref{3658}{cheng2024_videollama2}{0}{1839}{1839}
\@writefile{lof}{\contentsline {figure}{\numberline {24.102}{\ignorespaces STC connector: RegStage $\rightarrow $ 3D convolution for spatio–temporal aggregation (e.g., downsampling $(2,2,2)$) $\rightarrow $ RegStage, followed by a small MLP to produce LLM tokens; preserves temporal order while reducing token count. Adapted from \blx@tocontentsinit {0}\cite {cheng2024_videollama2}.}}{1839}{figure.caption.4264}\protected@file@percent }
\abx@aux@backref{3660}{cheng2024_videollama2}{0}{1839}{1839}
\newlabel{fig:chapter24_videollama2_stc}{{24.102}{1839}{STC connector: RegStage $\rightarrow $ 3D convolution for spatio–temporal aggregation (e.g., downsampling $(2,2,2)$) $\rightarrow $ RegStage, followed by a small MLP to produce LLM tokens; preserves temporal order while reducing token count. Adapted from \cite {cheng2024_videollama2}}{figure.caption.4264}{}}
\@writefile{toc}{\contentsline {paragraph}{Why STC instead of a plain 3D CNN or a Q\mbox  {-}Former?}{1839}{section*.4265}\protected@file@percent }
\abx@aux@backref{3661}{cheng2024_videollama2}{0}{1839}{1839}
\abx@aux@backref{3662}{cheng2024_videollama2}{0}{1839}{1839}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\@writefile{toc}{\contentsline {paragraph}{Implementation of STC in Python (from the paper)}{1840}{section*.4266}\protected@file@percent }
\abx@aux@backref{3663}{cheng2024_videollama2}{0}{1840}{1840}
\@writefile{toc}{\contentsline {paragraph}{Training signal and integration}{1840}{section*.4267}\protected@file@percent }
\abx@aux@backref{3664}{cheng2024_videollama2}{0}{1840}{1840}
\@writefile{toc}{\contentsline {paragraph}{Key changes vs.\ V1 (what changed and why)}{1840}{section*.4268}\protected@file@percent }
\abx@aux@backref{3665}{cheng2024_videollama2}{0}{1840}{1840}
\abx@aux@backref{3666}{cheng2024_videollama2}{0}{1840}{1840}
\abx@aux@backref{3667}{cheng2024_videollama2}{0}{1840}{1840}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@backref{3668}{cheng2024_videollama2}{0}{1841}{1841}
\abx@aux@backref{3669}{cheng2024_videollama2}{0}{1841}{1841}
\@writefile{toc}{\contentsline {paragraph}{Architecture and implementation details}{1841}{section*.4269}\protected@file@percent }
\abx@aux@backref{3670}{cheng2024_videollama2}{0}{1841}{1841}
\abx@aux@backref{3671}{cheng2024_videollama2}{0}{1841}{1841}
\abx@aux@backref{3672}{cheng2024_videollama2}{0}{1841}{1841}
\@writefile{toc}{\contentsline {paragraph}{Training curriculum}{1841}{section*.4270}\protected@file@percent }
\abx@aux@backref{3673}{cheng2024_videollama2}{0}{1841}{1841}
\abx@aux@backref{3674}{cheng2024_videollama2}{0}{1841}{1841}
\abx@aux@backref{3675}{cheng2024_videollama2}{0}{1841}{1841}
\abx@aux@backref{3676}{cheng2024_videollama2}{0}{1841}{1841}
\abx@aux@backref{3677}{cheng2024_videollama2}{0}{1841}{1841}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\@writefile{toc}{\contentsline {subsubsection}{Experiments and Ablations}{1842}{section*.4271}\protected@file@percent }
\newlabel{subsubsec:chapter24_videollama2_expts}{{24.9.3}{1842}{Experiments and Ablations}{section*.4271}{}}
\@writefile{toc}{\contentsline {paragraph}{STC Ablations}{1842}{section*.4272}\protected@file@percent }
\abx@aux@backref{3678}{cheng2024_videollama2}{0}{1842}{1842}
\@writefile{toc}{\contentsline {paragraph}{Data Recipe Overview}{1842}{section*.4273}\protected@file@percent }
\abx@aux@backref{3679}{cheng2024_videollama2}{0}{1842}{1842}
\@writefile{toc}{\contentsline {paragraph}{Multiple\mbox  {-}Choice VQA and Perception}{1842}{section*.4274}\protected@file@percent }
\abx@aux@backref{3680}{cheng2024_videollama2}{0}{1842}{1842}
\@writefile{toc}{\contentsline {paragraph}{Open\mbox  {-}Ended Video QA}{1842}{section*.4275}\protected@file@percent }
\abx@aux@backref{3681}{cheng2024_videollama2}{0}{1842}{1842}
\@writefile{toc}{\contentsline {paragraph}{Audio QA}{1842}{section*.4276}\protected@file@percent }
\abx@aux@backref{3682}{cheng2024_videollama2}{0}{1842}{1842}
\@writefile{toc}{\contentsline {paragraph}{Open\mbox  {-}Ended Audio--Video QA}{1842}{section*.4277}\protected@file@percent }
\abx@aux@backref{3683}{cheng2024_videollama2}{0}{1842}{1842}
\@writefile{lof}{\contentsline {figure}{\numberline {24.103}{\ignorespaces Qualitative cases from \emph  {Video\mbox  {-}LLaMA2}: (a) Global scene description and affect. (b) Spatial--temporal orientation awareness. (c) Commonsense reasoning with environmental cues. (d) Fine\mbox  {-}grained OCR in video. Adapted from \blx@tocontentsinit {0}\cite {cheng2024_videollama2}.}}{1843}{figure.caption.4278}\protected@file@percent }
\abx@aux@backref{3685}{cheng2024_videollama2}{0}{1843}{1843}
\newlabel{fig:chapter24_videollama2_qual}{{24.103}{1843}{Qualitative cases from \emph {Video\mbox {-}LLaMA2}: (a) Global scene description and affect. (b) Spatial--temporal orientation awareness. (c) Commonsense reasoning with environmental cues. (d) Fine\mbox {-}grained OCR in video. Adapted from \cite {cheng2024_videollama2}}{figure.caption.4278}{}}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{li2024_mvbench}
\abx@aux@segm{0}{0}{li2024_mvbench}
\abx@aux@cite{0}{liu2024_llava_next_video}
\abx@aux@segm{0}{0}{liu2024_llava_next_video}
\@writefile{toc}{\contentsline {paragraph}{Limitations and future directions}{1844}{section*.4279}\protected@file@percent }
\newlabel{par:chapter24_videollama2_limits}{{24.9.3}{1844}{Limitations and future directions}{section*.4279}{}}
\abx@aux@backref{3686}{cheng2024_videollama2}{0}{1844}{1844}
\@writefile{lot}{\contentsline {table}{\numberline {24.62}{\ignorespaces Selected MC\mbox  {-}VQA/perception results from the paper at 7B (16 frames). \emph  {Video\mbox  {-}LLaMA2} and the 2.1 refresh improve over contemporaries under comparable settings. Metrics are accuracies unless noted.}}{1844}{table.caption.4280}\protected@file@percent }
\newlabel{tab:videollama2_mcq_compact}{{24.62}{1844}{Selected MC\mbox {-}VQA/perception results from the paper at 7B (16 frames). \emph {Video\mbox {-}LLaMA2} and the 2.1 refresh improve over contemporaries under comparable settings. Metrics are accuracies unless noted}{table.caption.4280}{}}
\abx@aux@backref{3687}{cheng2024_videollama2}{0}{1844}{1844}
\abx@aux@backref{3688}{cheng2024_videollama2}{0}{1844}{1844}
\abx@aux@backref{3689}{li2024_mvbench}{0}{1844}{1844}
\abx@aux@backref{3690}{liu2024_llava_next_video}{0}{1844}{1844}
\BKM@entry{id=973,dest={73656374696F6E2A2E34323831},srcline={5622}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030345C3030302E5C303030395C3030302E5C303030345C3030303A5C3030305C3034305C303030565C303030695C303030645C303030655C3030306F5C3030302D5C3030304C5C3030304C5C303030615C3030304D5C303030415C3030305C3034305C303030335C3030303A5C3030305C3034305C303030465C303030725C3030306F5C3030306E5C303030745C303030695C303030655C303030725C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C3030306D5C3030306F5C303030645C303030615C3030306C5C3030305C3034305C303030465C3030306F5C303030755C3030306E5C303030645C303030615C303030745C303030695C3030306F5C3030306E5C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C30303073}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\@writefile{toc}{\contentsline {subsection}{Enrichment 24.9.4: Video\mbox  {-}LLaMA 3: Frontier Multimodal Foundation Models}{1845}{section*.4281}\protected@file@percent }
\newlabel{enr:subsec_chapter24_videollama3}{{24.9.4}{1845}{\color {ocre}Enrichment \thesubsection : Video\mbox {-}LLaMA 3: Frontier Multimodal Foundation Models}{section*.4281}{}}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{1845}{section*.4282}\protected@file@percent }
\newlabel{subsubsec:chapter24_videollama3_motivation}{{24.9.4}{1845}{Motivation}{section*.4282}{}}
\@writefile{toc}{\contentsline {paragraph}{A vision\mbox  {-}first redesign}{1845}{section*.4283}\protected@file@percent }
\abx@aux@backref{3691}{cheng2024_videollama2}{0}{1845}{1845}
\abx@aux@backref{3692}{zhang2025_videollama3}{0}{1845}{1845}
\@writefile{lof}{\contentsline {figure}{\numberline {24.104}{\ignorespaces Pipeline of Video-LLaMA3 with two key techniques: Any-resolution Vision Tokenization (AVT) and Difference-aware Frame Pruning (DiffFP). AVT turns images/videos of any resolution into 1-D token sequences; DiffFP drops low-change regions across adjacent frames for efficient long-video processing. Adapted from \blx@tocontentsinit {0}\cite {zhang2025_videollama3}.}}{1845}{figure.caption.4284}\protected@file@percent }
\abx@aux@backref{3694}{zhang2025_videollama3}{0}{1845}{1845}
\newlabel{fig:chapter24_videollama3_overview}{{24.104}{1845}{Pipeline of Video-LLaMA3 with two key techniques: Any-resolution Vision Tokenization (AVT) and Difference-aware Frame Pruning (DiffFP). AVT turns images/videos of any resolution into 1-D token sequences; DiffFP drops low-change regions across adjacent frames for efficient long-video processing. Adapted from \cite {zhang2025_videollama3}}{figure.caption.4284}{}}
\@writefile{toc}{\contentsline {paragraph}{Design objectives}{1845}{section*.4285}\protected@file@percent }
\abx@aux@backref{3695}{zhang2025_videollama3}{0}{1845}{1845}
\abx@aux@backref{3696}{zhang2025_videollama3}{0}{1845}{1845}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@backref{3697}{zhang2025_videollama3}{0}{1846}{1846}
\abx@aux@backref{3698}{zhang2025_videollama3}{0}{1846}{1846}
\@writefile{toc}{\contentsline {paragraph}{Mechanisms chosen to meet these goals}{1846}{section*.4286}\protected@file@percent }
\abx@aux@backref{3699}{zhang2025_videollama3}{0}{1846}{1846}
\abx@aux@backref{3700}{zhang2025_videollama3}{0}{1846}{1846}
\abx@aux@backref{3701}{zhang2025_videollama3}{0}{1846}{1846}
\@writefile{toc}{\contentsline {paragraph}{Scope: vision focus in V3}{1846}{section*.4287}\protected@file@percent }
\abx@aux@backref{3702}{zhang2025_videollama3}{0}{1846}{1846}
\@writefile{toc}{\contentsline {paragraph}{Anticipated benefits over V2}{1846}{section*.4288}\protected@file@percent }
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@cite{0}{heo2024_rotarype}
\abx@aux@segm{0}{0}{heo2024_rotarype}
\@writefile{toc}{\contentsline {subsubsection}{Method}{1847}{section*.4289}\protected@file@percent }
\newlabel{subsubsec:chapter24_videollama3_method}{{24.9.4}{1847}{Method}{section*.4289}{}}
\@writefile{toc}{\contentsline {paragraph}{Pipeline at a glance}{1847}{section*.4290}\protected@file@percent }
\abx@aux@backref{3703}{zhang2025_videollama3}{0}{1847}{1847}
\abx@aux@backref{3704}{zhang2025_videollama3}{0}{1847}{1847}
\@writefile{toc}{\contentsline {paragraph}{Why a resolution\mbox  {-}agnostic encoder}{1847}{section*.4291}\protected@file@percent }
\abx@aux@backref{3705}{zhang2025_videollama3}{0}{1847}{1847}
\@writefile{toc}{\contentsline {paragraph}{Any\mbox  {-}resolution Vision Tokenization (AVT)}{1847}{section*.4292}\protected@file@percent }
\abx@aux@backref{3706}{zhang2025_videollama3}{0}{1847}{1847}
\abx@aux@backref{3707}{heo2024_rotarype}{0}{1847}{1847}
\abx@aux@backref{3708}{zhang2025_videollama3}{0}{1847}{1847}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@cite{0}{heo2024_rotarype}
\abx@aux@segm{0}{0}{heo2024_rotarype}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@backref{3709}{zhang2025_videollama3}{0}{1848}{1848}
\@writefile{toc}{\contentsline {paragraph}{How 2D\mbox  {-}RoPE encodes spatial relations}{1848}{section*.4293}\protected@file@percent }
\abx@aux@backref{3710}{heo2024_rotarype}{0}{1848}{1848}
\abx@aux@backref{3711}{zhang2025_videollama3}{0}{1848}{1848}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@backref{3712}{zhang2025_videollama3}{0}{1850}{1850}
\@writefile{toc}{\contentsline {paragraph}{Differential Frame Pruner (DiffFP)}{1850}{section*.4294}\protected@file@percent }
\abx@aux@backref{3713}{zhang2025_videollama3}{0}{1850}{1850}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@cite{0}{zhai2023_siglip}
\abx@aux@segm{0}{0}{zhai2023_siglip}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@cite{0}{qwen2025_qwen25technicalreport}
\abx@aux@segm{0}{0}{qwen2025_qwen25technicalreport}
\@writefile{lof}{\contentsline {figure}{\numberline {24.105}{\ignorespaces Difference\mbox  {-}aware Frame Pruning (DiffFP). Patches with small $\ell _1$ differences to the previous frame are pruned; high\mbox  {-}difference regions and frames with large global change are kept, yielding a compact stream of key frames plus motion patches. Adapted from \blx@tocontentsinit {0}\cite {zhang2025_videollama3}.}}{1851}{figure.caption.4295}\protected@file@percent }
\abx@aux@backref{3715}{zhang2025_videollama3}{0}{1851}{1851}
\newlabel{fig:chapter24_videollama3_difffp}{{24.105}{1851}{Difference\mbox {-}aware Frame Pruning (DiffFP). Patches with small $\ell _1$ differences to the previous frame are pruned; high\mbox {-}difference regions and frames with large global change are kept, yielding a compact stream of key frames plus motion patches. Adapted from \cite {zhang2025_videollama3}}{figure.caption.4295}{}}
\@writefile{toc}{\contentsline {paragraph}{Data representations for multi\mbox  {-}image, video, and streaming}{1851}{section*.4296}\protected@file@percent }
\abx@aux@backref{3716}{zhang2025_videollama3}{0}{1851}{1851}
\abx@aux@backref{3717}{zhang2025_videollama3}{0}{1851}{1851}
\@writefile{lof}{\contentsline {figure}{\numberline {24.106}{\ignorespaces Data formats for different input types. (1) Image sequences use ``\texttt  {\textbackslash n}'' to separate tokens from different images. (2) Video sequences prefix each frame with ``\texttt  {Time: xxs}'', use commas to separate frames, and ``\texttt  {\textbackslash n}'' to separate different videos. (3) Streaming sequences interleave timestamped video tokens with text turns. Adapted from \blx@tocontentsinit {0}\cite [Fig.~6]{zhang2025_videollama3}.}}{1851}{figure.caption.4297}\protected@file@percent }
\abx@aux@backref{3719}{zhang2025_videollama3}{0}{1851}{1851}
\newlabel{fig:chapter24_videollama3_data_types}{{24.106}{1851}{Data formats for different input types. (1) Image sequences use ``\texttt {\textbackslash n}'' to separate tokens from different images. (2) Video sequences prefix each frame with ``\texttt {Time: xxs}'', use commas to separate frames, and ``\texttt {\textbackslash n}'' to separate different videos. (3) Streaming sequences interleave timestamped video tokens with text turns. Adapted from \cite [Fig.~6]{zhang2025_videollama3}}{figure.caption.4297}{}}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\@writefile{toc}{\contentsline {subsubsection}{Architecture \& Implementation Details}{1852}{section*.4298}\protected@file@percent }
\newlabel{subsubsec:chapter24_videollama3_arch}{{24.9.4}{1852}{Architecture \& Implementation Details}{section*.4298}{}}
\@writefile{toc}{\contentsline {paragraph}{Backbone and projector}{1852}{section*.4299}\protected@file@percent }
\abx@aux@backref{3720}{qwen2025_qwen25technicalreport}{0}{1852}{1852}
\abx@aux@backref{3721}{zhai2023_siglip}{0}{1852}{1852}
\abx@aux@backref{3722}{zhang2025_videollama3}{0}{1852}{1852}
\@writefile{toc}{\contentsline {paragraph}{Training paradigm}{1852}{section*.4300}\protected@file@percent }
\abx@aux@backref{3723}{zhang2025_videollama3}{0}{1852}{1852}
\@writefile{toc}{\contentsline {paragraph}{Where AVT and DiffFP plug in}{1852}{section*.4301}\protected@file@percent }
\abx@aux@backref{3724}{zhang2025_videollama3}{0}{1852}{1852}
\abx@aux@cite{0}{zhai2023_siglip}
\abx@aux@segm{0}{0}{zhai2023_siglip}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@cite{0}{qwen2025_qwen25technicalreport}
\abx@aux@segm{0}{0}{qwen2025_qwen25technicalreport}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\@writefile{toc}{\contentsline {paragraph}{Summary of design choices}{1853}{section*.4302}\protected@file@percent }
\abx@aux@backref{3725}{zhai2023_siglip}{0}{1853}{1853}
\abx@aux@backref{3726}{zhang2025_videollama3}{0}{1853}{1853}
\abx@aux@backref{3727}{qwen2025_qwen25technicalreport}{0}{1853}{1853}
\abx@aux@backref{3728}{zhang2025_videollama3}{0}{1853}{1853}
\abx@aux@backref{3729}{zhang2025_videollama3}{0}{1853}{1853}
\@writefile{lof}{\contentsline {figure}{\numberline {24.107}{\ignorespaces Four-stage training paradigm: Vision Encoder Adaptation, Vision--Language Alignment, Multi-task Fine-tuning, and Video-centric Fine-tuning. Adapted from \blx@tocontentsinit {0}\cite {zhang2025_videollama3}.}}{1853}{figure.caption.4303}\protected@file@percent }
\abx@aux@backref{3731}{zhang2025_videollama3}{0}{1853}{1853}
\newlabel{fig:chapter24_videollama3_training}{{24.107}{1853}{Four-stage training paradigm: Vision Encoder Adaptation, Vision--Language Alignment, Multi-task Fine-tuning, and Video-centric Fine-tuning. Adapted from \cite {zhang2025_videollama3}}{figure.caption.4303}{}}
\@writefile{toc}{\contentsline {subsubsection}{Experiments and Ablations}{1853}{section*.4304}\protected@file@percent }
\newlabel{subsubsec:chapter24_videollama3_experiments}{{24.9.4}{1853}{Experiments and Ablations}{section*.4304}{}}
\@writefile{toc}{\contentsline {paragraph}{Benchmarks and headline performance}{1853}{section*.4305}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24.108}{\ignorespaces Representative comparison across image and video benchmarks. Image\mbox  {-}centric baselines (e.g., LLaVA\mbox  {-}OneVision) are reported on image tasks; video\mbox  {-}centric baselines (e.g., LLaVA\mbox  {-}Video) on video tasks. Adapted from \blx@tocontentsinit {0}\cite {zhang2025_videollama3}.}}{1853}{figure.caption.4306}\protected@file@percent }
\abx@aux@backref{3733}{zhang2025_videollama3}{0}{1853}{1853}
\newlabel{fig:chapter24_videollama3_comparisons}{{24.108}{1853}{Representative comparison across image and video benchmarks. Image\mbox {-}centric baselines (e.g., LLaVA\mbox {-}OneVision) are reported on image tasks; video\mbox {-}centric baselines (e.g., LLaVA\mbox {-}Video) on video tasks. Adapted from \cite {zhang2025_videollama3}}{figure.caption.4306}{}}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@cite{0}{zhai2023_siglip}
\abx@aux@segm{0}{0}{zhai2023_siglip}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\@writefile{lot}{\contentsline {table}{\numberline {24.63}{\ignorespaces Selected headline results for \emph  {Video\mbox  {-}LLaMA3} (7B). Accuracies (\%).}}{1854}{table.caption.4307}\protected@file@percent }
\newlabel{tab:videollama3_headline}{{24.63}{1854}{Selected headline results for \emph {Video\mbox {-}LLaMA3} (7B). Accuracies (\%)}{table.caption.4307}{}}
\@writefile{toc}{\contentsline {paragraph}{Effect of AVT and DiffFP}{1854}{section*.4308}\protected@file@percent }
\abx@aux@backref{3734}{zhang2025_videollama3}{0}{1854}{1854}
\abx@aux@backref{3735}{zhang2025_videollama3}{0}{1854}{1854}
\abx@aux@backref{3736}{zhang2025_videollama3}{0}{1854}{1854}
\abx@aux@backref{3737}{zhang2025_videollama3}{0}{1854}{1854}
\@writefile{toc}{\contentsline {paragraph}{Comparisons to related systems}{1854}{section*.4309}\protected@file@percent }
\abx@aux@backref{3738}{cheng2024_videollama2}{0}{1854}{1854}
\abx@aux@backref{3739}{zhang2025_videollama3}{0}{1854}{1854}
\abx@aux@backref{3740}{zhang2025_videollama3}{0}{1854}{1854}
\abx@aux@backref{3741}{zhang2025_videollama3}{0}{1854}{1854}
\@writefile{toc}{\contentsline {paragraph}{Vision backbone ablation.}{1854}{section*.4310}\protected@file@percent }
\abx@aux@backref{3742}{zhai2023_siglip}{0}{1854}{1854}
\abx@aux@backref{3743}{zhang2025_videollama3}{0}{1854}{1854}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\@writefile{toc}{\contentsline {paragraph}{Data curation and mixtures}{1855}{section*.4311}\protected@file@percent }
\abx@aux@backref{3744}{zhang2025_videollama3}{0}{1855}{1855}
\abx@aux@backref{3745}{zhang2025_videollama3}{0}{1855}{1855}
\@writefile{toc}{\contentsline {subsubsection}{Limitations and Future Work}{1855}{section*.4312}\protected@file@percent }
\newlabel{subsubsec:chapter24_videollama3_limits}{{24.9.4}{1855}{Limitations and Future Work}{section*.4312}{}}
\@writefile{toc}{\contentsline {paragraph}{Long-context and token budgets.}{1855}{section*.4313}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Temporal precision and rare events}{1855}{section*.4314}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Data biases and domain transfer}{1855}{section*.4315}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Toward \emph  {Video-LLaMA4}}{1855}{section*.4316}\protected@file@percent }
\BKM@entry{id=974,dest={73656374696F6E2A2E34333137},srcline={5951}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030345C3030302E5C303030395C3030302E5C303030355C3030303A5C3030305C3034305C303030515C303030775C303030655C3030306E5C3030302D5C303030565C3030304C5C3030303A5C3030305C3034305C303030565C303030655C303030725C303030735C303030615C303030745C303030695C3030306C5C303030655C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3034305C3032335C3030304C5C303030615C3030306E5C303030675C303030755C303030615C303030675C303030655C3030305C3034305C303030465C3030306F5C303030755C3030306E5C303030645C303030615C303030745C303030695C3030306F5C3030306E}
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\@writefile{toc}{\contentsline {subsection}{Enrichment 24.9.5: Qwen-VL: Versatile Vision--Language Foundation}{1856}{section*.4317}\protected@file@percent }
\newlabel{enr:subsec_chapter24_qwenvl}{{24.9.5}{1856}{\color {ocre}Enrichment \thesubsection : Qwen-VL: Versatile Vision--Language Foundation}{section*.4317}{}}
\@writefile{toc}{\contentsline {subsubsection}{Motivation}{1856}{section*.4318}\protected@file@percent }
\newlabel{subsubsec:chapter24_qwenvl_motivation}{{24.9.5}{1856}{Motivation}{section*.4318}{}}
\abx@aux@backref{3746}{bai2023_qwenvl}{0}{1856}{1856}
\@writefile{lof}{\contentsline {figure}{\numberline {24.109}{\ignorespaces At publication time, Qwen\mbox  {-}VL achieved state\mbox  {-}of\mbox  {-}the\mbox  {-}art results among generalist models across diverse benchmarks (schematic radar chart). Adapted from \blx@tocontentsinit {0}\cite {bai2023_qwenvl}.}}{1856}{figure.caption.4319}\protected@file@percent }
\abx@aux@backref{3748}{bai2023_qwenvl}{0}{1856}{1856}
\newlabel{fig:chapter24_qwenvl_radar}{{24.109}{1856}{At publication time, Qwen\mbox {-}VL achieved state\mbox {-}of\mbox {-}the\mbox {-}art results among generalist models across diverse benchmarks (schematic radar chart). Adapted from \cite {bai2023_qwenvl}}{figure.caption.4319}{}}
\@writefile{toc}{\contentsline {paragraph}{Reading the radar chart (intuition)}{1856}{section*.4320}\protected@file@percent }
\abx@aux@backref{3749}{bai2023_qwenvl}{0}{1856}{1856}
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\@writefile{toc}{\contentsline {subsubsection}{Method}{1857}{section*.4321}\protected@file@percent }
\newlabel{subsubsec:chapter24_qwenvl_method}{{24.9.5}{1857}{Method}{section*.4321}{}}
\@writefile{toc}{\contentsline {paragraph}{Architecture (visual receptor + LLM)}{1857}{section*.4322}\protected@file@percent }
\abx@aux@backref{3750}{bai2023_qwenvl}{0}{1857}{1857}
\abx@aux@backref{3751}{bai2023_qwenvl}{0}{1857}{1857}
\abx@aux@backref{3752}{bai2023_qwenvl}{0}{1857}{1857}
\abx@aux@backref{3753}{bai2023_qwenvl}{0}{1857}{1857}
\@writefile{lot}{\contentsline {table}{\numberline {24.64}{\ignorespaces Qwen\mbox  {-}VL model parameters (billions).}}{1857}{table.caption.4323}\protected@file@percent }
\newlabel{tab:chapter24_qwenvl_params}{{24.64}{1857}{Qwen\mbox {-}VL model parameters (billions)}{table.caption.4323}{}}
\@writefile{toc}{\contentsline {paragraph}{Input\mbox  {--}output interface (tokenization and special tokens)}{1857}{section*.4324}\protected@file@percent }
\abx@aux@backref{3754}{bai2023_qwenvl}{0}{1857}{1857}
\@writefile{toc}{\contentsline {paragraph}{Cross\mbox  {-}attention compression (derivation and intuition)}{1857}{section*.4325}\protected@file@percent }
\abx@aux@backref{3755}{bai2023_qwenvl}{0}{1857}{1857}
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\@writefile{toc}{\contentsline {paragraph}{Training pipeline (three stages)}{1858}{section*.4326}\protected@file@percent }
\abx@aux@backref{3756}{bai2023_qwenvl}{0}{1858}{1858}
\@writefile{toc}{\contentsline {paragraph}{Why this design}{1858}{section*.4327}\protected@file@percent }
\abx@aux@backref{3757}{bai2023_qwenvl}{0}{1858}{1858}
\@writefile{lof}{\contentsline {figure}{\numberline {24.110}{\ignorespaces Qwen\mbox  {-}VL training pipeline: Stage~1 (low\mbox  {-}res pretraining; LLM frozen), Stage~2 (high\mbox  {-}res multi\mbox  {-}task; all unfrozen), Stage~3 (SFT with ViT frozen). Adapted from \blx@tocontentsinit {0}\cite {bai2023_qwenvl}.}}{1858}{figure.caption.4328}\protected@file@percent }
\abx@aux@backref{3759}{bai2023_qwenvl}{0}{1858}{1858}
\newlabel{fig:chapter24_qwenvl_pipeline}{{24.110}{1858}{Qwen\mbox {-}VL training pipeline: Stage~1 (low\mbox {-}res pretraining; LLM frozen), Stage~2 (high\mbox {-}res multi\mbox {-}task; all unfrozen), Stage~3 (SFT with ViT frozen). Adapted from \cite {bai2023_qwenvl}}{figure.caption.4328}{}}
\@writefile{toc}{\contentsline {paragraph}{Data}{1858}{section*.4329}\protected@file@percent }
\abx@aux@backref{3760}{bai2023_qwenvl}{0}{1858}{1858}
\abx@aux@backref{3761}{bai2023_qwenvl}{0}{1859}{1859}
\@writefile{toc}{\contentsline {paragraph}{Pseudo\mbox  {-}code}{1859}{section*.4330}\protected@file@percent }
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\@writefile{toc}{\contentsline {subsubsection}{Architecture \& Implementation Details}{1860}{section*.4331}\protected@file@percent }
\newlabel{subsubsec:chapter24_qwenvl_arch}{{24.9.5}{1860}{Architecture \& Implementation Details}{section*.4331}{}}
\@writefile{toc}{\contentsline {paragraph}{Backbone and adapter}{1860}{section*.4332}\protected@file@percent }
\abx@aux@backref{3762}{bai2023_qwenvl}{0}{1860}{1860}
\@writefile{toc}{\contentsline {paragraph}{Resolution and sequence length}{1860}{section*.4333}\protected@file@percent }
\abx@aux@backref{3763}{bai2023_qwenvl}{0}{1860}{1860}
\@writefile{toc}{\contentsline {paragraph}{Special tokens and grounding format}{1860}{section*.4334}\protected@file@percent }
\abx@aux@backref{3764}{bai2023_qwenvl}{0}{1860}{1860}
\@writefile{toc}{\contentsline {subsubsection}{Experiments and Ablations}{1860}{section*.4335}\protected@file@percent }
\newlabel{subsubsec:chapter24_qwenvl_experiments}{{24.9.5}{1860}{Experiments and Ablations}{section*.4335}{}}
\@writefile{toc}{\contentsline {paragraph}{Benchmarks and headline performance}{1860}{section*.4336}\protected@file@percent }
\abx@aux@backref{3765}{bai2023_qwenvl}{0}{1860}{1860}
\abx@aux@backref{3766}{bai2023_qwenvl}{0}{1860}{1860}
\abx@aux@backref{3767}{bai2023_qwenvl}{0}{1860}{1860}
\abx@aux@backref{3768}{bai2023_qwenvl}{0}{1860}{1860}
\@writefile{toc}{\contentsline {paragraph}{What the ablations test}{1860}{section*.4337}\protected@file@percent }
\abx@aux@backref{3769}{bai2023_qwenvl}{0}{1860}{1860}
\abx@aux@backref{3770}{bai2023_qwenvl}{0}{1860}{1860}
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\@writefile{toc}{\contentsline {paragraph}{How these results compare}{1861}{section*.4338}\protected@file@percent }
\abx@aux@backref{3771}{bai2023_qwenvl}{0}{1861}{1861}
\abx@aux@backref{3772}{bai2023_qwenvl}{0}{1861}{1861}
\@writefile{toc}{\contentsline {paragraph}{Design choices the ablations support}{1861}{section*.4339}\protected@file@percent }
\abx@aux@backref{3773}{bai2023_qwenvl}{0}{1861}{1861}
\abx@aux@backref{3774}{bai2023_qwenvl}{0}{1861}{1861}
\abx@aux@backref{3775}{bai2023_qwenvl}{0}{1861}{1861}
\@writefile{toc}{\contentsline {paragraph}{Takeaways}{1861}{section*.4340}\protected@file@percent }
\abx@aux@backref{3776}{bai2023_qwenvl}{0}{1861}{1861}
\@writefile{lof}{\contentsline {figure}{\numberline {24.111}{\ignorespaces Representative Qwen\mbox  {-}VL\mbox  {-}Chat capabilities: multi\mbox  {-}image dialogue, multilingual text reading, region grounding/localization, and code understanding. Adapted from \blx@tocontentsinit {0}\cite {bai2023_qwenvl}.}}{1861}{figure.caption.4341}\protected@file@percent }
\abx@aux@backref{3778}{bai2023_qwenvl}{0}{1861}{1861}
\newlabel{fig:chapter24_qwenvl_chat}{{24.111}{1861}{Representative Qwen\mbox {-}VL\mbox {-}Chat capabilities: multi\mbox {-}image dialogue, multilingual text reading, region grounding/localization, and code understanding. Adapted from \cite {bai2023_qwenvl}}{figure.caption.4341}{}}
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\@writefile{toc}{\contentsline {paragraph}{Qualitative capabilities}{1862}{section*.4342}\protected@file@percent }
\abx@aux@backref{3779}{bai2023_qwenvl}{0}{1862}{1862}
\@writefile{toc}{\contentsline {subsubsection}{Limitations and Future Work}{1862}{section*.4343}\protected@file@percent }
\newlabel{subsubsec:chapter24_qwenvl_limits}{{24.9.5}{1862}{Limitations and Future Work}{section*.4343}{}}
\abx@aux@backref{3780}{bai2023_qwenvl}{0}{1862}{1862}
\abx@aux@backref{3781}{bai2023_qwenvl}{0}{1862}{1862}
\abx@aux@backref{3782}{bai2023_qwenvl}{0}{1862}{1862}
\abx@aux@backref{3783}{bai2023_qwenvl}{0}{1862}{1862}
\abx@aux@backref{3784}{bai2023_qwenvl}{0}{1862}{1862}
\@writefile{toc}{\contentsline {paragraph}{Bridge to Qwen2\mbox  {-}VL}{1862}{section*.4344}\protected@file@percent }
\abx@aux@backref{3785}{wang2024_qwen2vl}{0}{1862}{1862}
\BKM@entry{id=975,dest={73656374696F6E2A2E34333435},srcline={6154}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030345C3030302E5C303030395C3030302E5C303030365C3030303A5C3030305C3034305C303030515C303030775C303030655C3030306E5C303030325C3030302D5C303030565C3030304C5C3030303A5C3030305C3034305C303030445C303030795C3030306E5C303030615C3030306D5C303030695C303030635C3030305C3034305C303030525C303030655C303030735C3030306F5C3030306C5C303030755C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030565C303030695C303030735C303030695C3030306F5C3030306E5C3034305C3032335C3030304C5C303030615C3030306E5C303030675C303030755C303030615C303030675C303030655C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030695C3030306E5C30303067}
\abx@aux@cite{0}{li2024_llavaonevision}
\abx@aux@segm{0}{0}{li2024_llavaonevision}
\abx@aux@cite{0}{cheng2024_videollama2}
\abx@aux@segm{0}{0}{cheng2024_videollama2}
\abx@aux@cite{0}{zhang2025_videollama3}
\abx@aux@segm{0}{0}{zhang2025_videollama3}
\abx@aux@cite{0}{bai2023_qwenvl}
\abx@aux@segm{0}{0}{bai2023_qwenvl}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\@writefile{toc}{\contentsline {subsection}{Enrichment 24.9.6: Qwen2-VL: Dynamic Resolution Vision--Language Modeling}{1863}{section*.4345}\protected@file@percent }
\newlabel{enr:subsec_chapter24_qwen2vl}{{24.9.6}{1863}{\color {ocre}Enrichment \thesubsection : Qwen2-VL: Dynamic Resolution Vision--Language Modeling}{section*.4345}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation}{1863}{section*.4346}\protected@file@percent }
\abx@aux@backref{3786}{cheng2024_videollama2}{0}{1863}{1863}
\abx@aux@backref{3787}{li2024_llavaonevision}{0}{1863}{1863}
\abx@aux@backref{3788}{zhang2025_videollama3}{0}{1863}{1863}
\abx@aux@backref{3789}{bai2023_qwenvl}{0}{1863}{1863}
\abx@aux@backref{3790}{wang2024_qwen2vl}{0}{1863}{1863}
\@writefile{lof}{\contentsline {figure}{\numberline {24.112}{\ignorespaces Illustrative capabilities of Qwen2\mbox  {-}VL: multilingual OCR, document and diagram parsing, math/code reasoning, video analysis, live chat, grounding, and tool/agent interactions. Adapted from \blx@tocontentsinit {0}\cite {wang2024_qwen2vl}.}}{1863}{figure.caption.4347}\protected@file@percent }
\abx@aux@backref{3792}{wang2024_qwen2vl}{0}{1863}{1863}
\newlabel{fig:chapter24_qwen2vl_caps}{{24.112}{1863}{Illustrative capabilities of Qwen2\mbox {-}VL: multilingual OCR, document and diagram parsing, math/code reasoning, video analysis, live chat, grounding, and tool/agent interactions. Adapted from \cite {wang2024_qwen2vl}}{figure.caption.4347}{}}
\@writefile{toc}{\contentsline {subsubsection}{Method}{1863}{section*.4348}\protected@file@percent }
\newlabel{subsubsec:chapter24_qwen2vl_method}{{24.9.6}{1863}{Method}{section*.4348}{}}
\@writefile{toc}{\contentsline {paragraph}{Design overview}{1863}{section*.4349}\protected@file@percent }
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\@writefile{toc}{\contentsline {paragraph}{Naive dynamic resolution}{1864}{section*.4350}\protected@file@percent }
\abx@aux@backref{3793}{wang2024_qwen2vl}{0}{1864}{1864}
\abx@aux@backref{3794}{wang2024_qwen2vl}{0}{1864}{1864}
\abx@aux@backref{3795}{wang2024_qwen2vl}{0}{1864}{1864}
\@writefile{toc}{\contentsline {paragraph}{M\mbox  {-}RoPE for space--time}{1864}{section*.4351}\protected@file@percent }
\abx@aux@backref{3796}{wang2024_qwen2vl}{0}{1864}{1864}
\abx@aux@backref{3797}{wang2024_qwen2vl}{0}{1864}{1864}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\@writefile{toc}{\contentsline {paragraph}{Pseudo\mbox  {-}code for dynamic resolution and M\mbox  {-}RoPE}{1865}{section*.4352}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Why M\mbox  {-}RoPE instead of 2D absolute encodings}{1865}{section*.4353}\protected@file@percent }
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\abx@aux@backref{3798}{wang2024_qwen2vl}{0}{1866}{1866}
\@writefile{toc}{\contentsline {paragraph}{Unified multimodal serialization}{1866}{section*.4354}\protected@file@percent }
\abx@aux@backref{3799}{wang2024_qwen2vl}{0}{1866}{1866}
\abx@aux@backref{3800}{wang2024_qwen2vl}{0}{1866}{1866}
\@writefile{toc}{\contentsline {subsubsection}{Architecture \& Implementation Details}{1866}{section*.4355}\protected@file@percent }
\newlabel{subsubsec:chapter24_qwen2vl_arch}{{24.9.6}{1866}{Architecture \& Implementation Details}{section*.4355}{}}
\@writefile{toc}{\contentsline {paragraph}{Model variants}{1866}{section*.4356}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {24.65}{\ignorespaces Qwen2\mbox  {-}VL variants and sizes.}}{1866}{table.caption.4357}\protected@file@percent }
\newlabel{tab:chapter24_qwen2vl_models}{{24.65}{1866}{Qwen2\mbox {-}VL variants and sizes}{table.caption.4357}{}}
\@writefile{toc}{\contentsline {paragraph}{Implementation notes}{1866}{section*.4358}\protected@file@percent }
\abx@aux@backref{3801}{wang2024_qwen2vl}{0}{1866}{1866}
\abx@aux@backref{3802}{wang2024_qwen2vl}{0}{1866}{1866}
\abx@aux@backref{3803}{wang2024_qwen2vl}{0}{1866}{1866}
\@writefile{lof}{\contentsline {figure}{\numberline {24.113}{\ignorespaces Adaptiveness to native resolutions and extreme aspect ratios: token counts scale with visual content rather than a fixed canvas. Adapted from \blx@tocontentsinit {0}\cite {wang2024_qwen2vl}.}}{1866}{figure.caption.4359}\protected@file@percent }
\abx@aux@backref{3805}{wang2024_qwen2vl}{0}{1866}{1866}
\newlabel{fig:chapter24_qwen2vl_adapt}{{24.113}{1866}{Adaptiveness to native resolutions and extreme aspect ratios: token counts scale with visual content rather than a fixed canvas. Adapted from \cite {wang2024_qwen2vl}}{figure.caption.4359}{}}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\@writefile{lof}{\contentsline {figure}{\numberline {24.114}{\ignorespaces M\mbox  {-}RoPE decomposes rotary embeddings into temporal, height, and width components, unifying position encoding for text, images, and videos. Adapted from \blx@tocontentsinit {0}\cite {wang2024_qwen2vl}.}}{1867}{figure.caption.4360}\protected@file@percent }
\abx@aux@backref{3807}{wang2024_qwen2vl}{0}{1867}{1867}
\newlabel{fig:chapter24_qwen2vl_mrope}{{24.114}{1867}{M\mbox {-}RoPE decomposes rotary embeddings into temporal, height, and width components, unifying position encoding for text, images, and videos. Adapted from \cite {wang2024_qwen2vl}}{figure.caption.4360}{}}
\@writefile{toc}{\contentsline {subsubsection}{Experiments and Ablations}{1867}{section*.4361}\protected@file@percent }
\newlabel{subsubsec:chapter24_qwen2vl_experiments}{{24.9.6}{1867}{Experiments and Ablations}{section*.4361}{}}
\@writefile{toc}{\contentsline {paragraph}{Benchmarks and headline performance}{1867}{section*.4362}\protected@file@percent }
\abx@aux@backref{3808}{wang2024_qwen2vl}{0}{1867}{1867}
\abx@aux@backref{3809}{wang2024_qwen2vl}{0}{1867}{1867}
\abx@aux@backref{3810}{wang2024_qwen2vl}{0}{1867}{1867}
\@writefile{toc}{\contentsline {paragraph}{Video understanding}{1867}{section*.4363}\protected@file@percent }
\abx@aux@backref{3811}{wang2024_qwen2vl}{0}{1867}{1867}
\@writefile{toc}{\contentsline {paragraph}{Grounding}{1867}{section*.4364}\protected@file@percent }
\abx@aux@backref{3812}{wang2024_qwen2vl}{0}{1867}{1867}
\@writefile{toc}{\contentsline {paragraph}{Multilingual OCR (internal)}{1867}{section*.4365}\protected@file@percent }
\abx@aux@backref{3813}{wang2024_qwen2vl}{0}{1867}{1867}
\@writefile{toc}{\contentsline {paragraph}{Why dynamic resolution helps}{1867}{section*.4366}\protected@file@percent }
\abx@aux@backref{3814}{wang2024_qwen2vl}{0}{1867}{1867}
\abx@aux@backref{3815}{wang2024_qwen2vl}{0}{1867}{1867}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\@writefile{toc}{\contentsline {paragraph}{Why M\mbox  {-}RoPE matters}{1868}{section*.4367}\protected@file@percent }
\abx@aux@backref{3816}{wang2024_qwen2vl}{0}{1868}{1868}
\@writefile{toc}{\contentsline {paragraph}{Length extrapolation}{1868}{section*.4368}\protected@file@percent }
\abx@aux@backref{3817}{wang2024_qwen2vl}{0}{1868}{1868}
\@writefile{lof}{\contentsline {figure}{\numberline {24.115}{\ignorespaces Inference length extrapolation on Video\mbox  {-}MME: accuracy remains robust beyond the 16K training context, with strong performance up to long contexts. Adapted from \blx@tocontentsinit {0}\cite {wang2024_qwen2vl}.}}{1868}{figure.caption.4369}\protected@file@percent }
\abx@aux@backref{3819}{wang2024_qwen2vl}{0}{1868}{1868}
\newlabel{fig:chapter24_qwen2vl_len}{{24.115}{1868}{Inference length extrapolation on Video\mbox {-}MME: accuracy remains robust beyond the 16K training context, with strong performance up to long contexts. Adapted from \cite {wang2024_qwen2vl}}{figure.caption.4369}{}}
\@writefile{toc}{\contentsline {paragraph}{Resolution sensitivity}{1868}{section*.4370}\protected@file@percent }
\abx@aux@backref{3820}{wang2024_qwen2vl}{0}{1868}{1868}
\@writefile{lof}{\contentsline {figure}{\numberline {24.116}{\ignorespaces Effect of \emph  {min\_pixels}: modest upscaling tends to help text\mbox  {-}rich and fine\mbox  {-}structure tasks, while extreme upscaling can be counterproductive. Adapted from \blx@tocontentsinit {0}\cite {wang2024_qwen2vl}.}}{1868}{figure.caption.4371}\protected@file@percent }
\abx@aux@backref{3822}{wang2024_qwen2vl}{0}{1868}{1868}
\newlabel{fig:chapter24_qwen2vl_minpix}{{24.116}{1868}{Effect of \emph {min\_pixels}: modest upscaling tends to help text\mbox {-}rich and fine\mbox {-}structure tasks, while extreme upscaling can be counterproductive. Adapted from \cite {wang2024_qwen2vl}}{figure.caption.4371}{}}
\@writefile{toc}{\contentsline {paragraph}{Scaling behavior and training curriculum}{1868}{section*.4372}\protected@file@percent }
\abx@aux@backref{3823}{wang2024_qwen2vl}{0}{1868}{1868}
\abx@aux@backref{3824}{wang2024_qwen2vl}{0}{1868}{1868}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\abx@aux@cite{0}{wang2024_qwen2vl}
\abx@aux@segm{0}{0}{wang2024_qwen2vl}
\abx@aux@cite{0}{qwen2025_qwen25technicalreport}
\abx@aux@segm{0}{0}{qwen2025_qwen25technicalreport}
\@writefile{toc}{\contentsline {subsubsection}{Limitations and Future Work}{1869}{section*.4373}\protected@file@percent }
\newlabel{subsubsec:chapter24_qwen2vl_limits}{{24.9.6}{1869}{Limitations and Future Work}{section*.4373}{}}
\abx@aux@backref{3825}{wang2024_qwen2vl}{0}{1869}{1869}
\abx@aux@backref{3826}{wang2024_qwen2vl}{0}{1869}{1869}
\abx@aux@backref{3827}{wang2024_qwen2vl}{0}{1869}{1869}
\abx@aux@backref{3828}{wang2024_qwen2vl}{0}{1869}{1869}
\abx@aux@backref{3829}{wang2024_qwen2vl}{0}{1869}{1869}
\abx@aux@backref{3830}{wang2024_qwen2vl}{0}{1869}{1869}
\abx@aux@backref{3831}{qwen2025_qwen25technicalreport}{0}{1869}{1869}
\BKM@entry{id=976,dest={73656374696F6E2A2E34333734},srcline={6368}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030345C3030302E5C303030315C303030305C3030303A5C3030305C3034305C3030304C5C3030306F5C3030306E5C303030675C3030302D5C303030435C3030306F5C3030306E5C303030745C303030655C303030785C303030745C3030305C3034305C3030304D5C3030306F5C303030645C303030655C3030306C5C303030695C3030306E5C30303067}
\abx@aux@cite{0}{wu2022_memvit}
\abx@aux@segm{0}{0}{wu2022_memvit}
\abx@aux@cite{0}{mamba2023_selective}
\abx@aux@segm{0}{0}{mamba2023_selective}
\abx@aux@cite{0}{weng2024_longvlm}
\abx@aux@segm{0}{0}{weng2024_longvlm}
\abx@aux@cite{0}{liu2025_lwm}
\abx@aux@segm{0}{0}{liu2025_lwm}
\@writefile{toc}{\contentsline {section}{Enrichment 24.10: Long-Context Modeling}{1870}{section*.4374}\protected@file@percent }
\newlabel{enr:sec_chapter24_long_context}{{24.10}{1870}{\color {ocre}Enrichment \thesection : Long-Context Modeling}{section*.4374}{}}
\abx@aux@backref{3832}{wu2022_memvit}{0}{1870}{1870}
\abx@aux@backref{3833}{mamba2023_selective}{0}{1870}{1870}
\abx@aux@backref{3834}{weng2024_longvlm}{0}{1870}{1870}
\abx@aux@backref{3835}{liu2025_lwm}{0}{1870}{1870}
\BKM@entry{id=977,dest={73656374696F6E2A2E34333735},srcline={6385}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030345C3030302E5C303030315C303030305C3030302E5C303030315C3030303A5C3030305C3034305C3030304D5C303030655C3030304D5C303030565C303030695C303030545C3030303A5C3030305C3034305C3030304D5C303030655C3030306D5C3030306F5C303030725C303030795C3030302D5C303030415C303030755C303030675C3030306D5C303030655C3030306E5C303030745C303030655C303030645C3030305C3034305C3030304D5C303030755C3030306C5C303030745C303030695C303030735C303030635C303030615C3030306C5C303030655C3030305C3034305C303030565C303030695C303030545C30303073}
\abx@aux@cite{0}{wu2022_memvit}
\abx@aux@segm{0}{0}{wu2022_memvit}
\abx@aux@cite{0}{wu2022_memvit}
\abx@aux@segm{0}{0}{wu2022_memvit}
\abx@aux@cite{0}{wu2022_memvit}
\abx@aux@segm{0}{0}{wu2022_memvit}
\abx@aux@cite{0}{wu2022_memvit}
\abx@aux@segm{0}{0}{wu2022_memvit}
\@writefile{toc}{\contentsline {subsection}{Enrichment 24.10.1: MeMViT: Memory-Augmented Multiscale ViTs}{1871}{section*.4375}\protected@file@percent }
\newlabel{enr:subsec_chapter24_memvit}{{24.10.1}{1871}{\color {ocre}Enrichment \thesubsection : MeMViT: Memory-Augmented Multiscale ViTs}{section*.4375}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation}{1871}{section*.4376}\protected@file@percent }
\abx@aux@backref{3836}{wu2022_memvit}{0}{1871}{1871}
\@writefile{lof}{\contentsline {figure}{\numberline {24.117}{\ignorespaces Problem setup and key idea. Traditional long\mbox  {-}term scaling increases input frames and explodes compute/memory; MeMViT maintains a cached, hierarchically compressed memory and lets current queries attend to it efficiently. Adapted from \blx@tocontentsinit {0}\cite {wu2022_memvit}.}}{1871}{figure.caption.4377}\protected@file@percent }
\abx@aux@backref{3839}{wu2022_memvit}{0}{1871}{1871}
\newlabel{fig:chapter24_memvit_intro}{{24.117}{1871}{Problem setup and key idea. Traditional long\mbox {-}term scaling increases input frames and explodes compute/memory; MeMViT maintains a cached, hierarchically compressed memory and lets current queries attend to it efficiently. Adapted from \cite {wu2022_memvit}}{figure.caption.4377}{}}
\abx@aux@backref{3837}{wu2022_memvit}{0}{1871}{1871}
\abx@aux@cite{0}{fan2021_mvit}
\abx@aux@segm{0}{0}{fan2021_mvit}
\abx@aux@cite{0}{li2021_improved_mvit}
\abx@aux@segm{0}{0}{li2021_improved_mvit}
\abx@aux@cite{0}{li2021_improved_mvit}
\abx@aux@segm{0}{0}{li2021_improved_mvit}
\@writefile{toc}{\contentsline {paragraph}{Preliminaries: ViT and MViT}{1872}{section*.4378}\protected@file@percent }
\newlabel{eq:memvit_linear}{{24.53}{1872}{Preliminaries: ViT and MViT}{equation.24.53}{}}
\newlabel{eq:memvit_attn}{{24.54}{1872}{Preliminaries: ViT and MViT}{equation.24.54}{}}
\abx@aux@backref{3840}{fan2021_mvit}{0}{1872}{1872}
\abx@aux@backref{3841}{li2021_improved_mvit}{0}{1872}{1872}
\abx@aux@backref{3842}{li2021_improved_mvit}{0}{1872}{1872}
\newlabel{eq:memvit_pool_then_linear}{{24.55}{1872}{Preliminaries: ViT and MViT}{equation.24.55}{}}
\abx@aux@cite{0}{wu2022_memvit}
\abx@aux@segm{0}{0}{wu2022_memvit}
\abx@aux@cite{0}{wu2022_memvit}
\abx@aux@segm{0}{0}{wu2022_memvit}
\abx@aux@cite{0}{wu2022_memvit}
\abx@aux@segm{0}{0}{wu2022_memvit}
\@writefile{toc}{\contentsline {paragraph}{Method: Memory Attention and Hierarchical Caching}{1873}{section*.4379}\protected@file@percent }
\newlabel{subsubsec:chapter24_memvit_method}{{24.10.1}{1873}{Method: Memory Attention and Hierarchical Caching}{section*.4379}{}}
\abx@aux@backref{3843}{wu2022_memvit}{0}{1873}{1873}
\abx@aux@backref{3844}{wu2022_memvit}{0}{1873}{1873}
\newlabel{eq:memvit_basic_K}{{24.56}{1873}{Method: Memory Attention and Hierarchical Caching}{equation.24.56}{}}
\newlabel{eq:memvit_basic_V}{{24.57}{1873}{Method: Memory Attention and Hierarchical Caching}{equation.24.57}{}}
\abx@aux@cite{0}{wu2022_memvit}
\abx@aux@segm{0}{0}{wu2022_memvit}
\abx@aux@cite{0}{wu2022_memvit}
\abx@aux@segm{0}{0}{wu2022_memvit}
\abx@aux@cite{0}{wu2022_memvit}
\abx@aux@segm{0}{0}{wu2022_memvit}
\abx@aux@cite{0}{wu2022_memvit}
\abx@aux@segm{0}{0}{wu2022_memvit}
\abx@aux@backref{3845}{wu2022_memvit}{0}{1874}{1874}
\abx@aux@backref{3846}{wu2022_memvit}{0}{1874}{1874}
\newlabel{eq:memvit_pipeline}{{24.58}{1874}{Method: Memory Attention and Hierarchical Caching}{equation.24.58}{}}
\abx@aux@backref{3847}{wu2022_memvit}{0}{1874}{1874}
\abx@aux@backref{3848}{wu2022_memvit}{0}{1874}{1874}
\abx@aux@backref{3849}{wu2022_memvit}{0}{1874}{1874}
\abx@aux@cite{0}{wu2022_memvit}
\abx@aux@segm{0}{0}{wu2022_memvit}
\abx@aux@cite{0}{wu2022_memvit}
\abx@aux@segm{0}{0}{wu2022_memvit}
\@writefile{toc}{\contentsline {paragraph}{Algorithmic sketch (from the paper)}{1875}{section*.4380}\protected@file@percent }
\abx@aux@cite{0}{fan2021_mvit}
\abx@aux@segm{0}{0}{fan2021_mvit}
\abx@aux@cite{0}{li2021_improved_mvit}
\abx@aux@segm{0}{0}{li2021_improved_mvit}
\abx@aux@cite{0}{wu2022_memvit}
\abx@aux@segm{0}{0}{wu2022_memvit}
\abx@aux@cite{0}{wu2022_memvit}
\abx@aux@segm{0}{0}{wu2022_memvit}
\abx@aux@cite{0}{wu2022_memvit}
\abx@aux@segm{0}{0}{wu2022_memvit}
\@writefile{lof}{\contentsline {figure}{\numberline {24.118}{\ignorespaces MeMViT caching and attention. Left: An online, clip\mbox  {-}wise pipeline with an uncompressed cache for the immediate past and compressed caches for earlier steps. Right: At a memory\mbox  {-}augmented layer, current queries attend to current keys/values plus cached, compressed memory from the past. Adapted from \blx@tocontentsinit {0}\cite {wu2022_memvit}.}}{1876}{figure.caption.4381}\protected@file@percent }
\abx@aux@backref{3851}{wu2022_memvit}{0}{1876}{1876}
\newlabel{fig:chapter24_memvit_cache}{{24.118}{1876}{MeMViT caching and attention. Left: An online, clip\mbox {-}wise pipeline with an uncompressed cache for the immediate past and compressed caches for earlier steps. Right: At a memory\mbox {-}augmented layer, current queries attend to current keys/values plus cached, compressed memory from the past. Adapted from \cite {wu2022_memvit}}{figure.caption.4381}{}}
\@writefile{toc}{\contentsline {subsubsection}{Architecture \& Implementation Details}{1876}{section*.4382}\protected@file@percent }
\newlabel{subsubsec:chapter24_memvit_arch}{{24.10.1}{1876}{Architecture \& Implementation Details}{section*.4382}{}}
\@writefile{toc}{\contentsline {paragraph}{Backbone and stages}{1876}{section*.4383}\protected@file@percent }
\abx@aux@backref{3852}{fan2021_mvit}{0}{1876}{1876}
\abx@aux@backref{3853}{li2021_improved_mvit}{0}{1876}{1876}
\@writefile{toc}{\contentsline {paragraph}{Data loading and training}{1876}{section*.4384}\protected@file@percent }
\abx@aux@backref{3855}{wu2022_memvit}{0}{1876}{1876}
\@writefile{toc}{\contentsline {subsubsection}{Experiments and Ablations}{1876}{section*.4385}\protected@file@percent }
\newlabel{subsubsec:chapter24_memvit_experiments}{{24.10.1}{1876}{Experiments and Ablations}{section*.4385}{}}
\@writefile{toc}{\contentsline {paragraph}{Scaling strategies}{1876}{section*.4386}\protected@file@percent }
\abx@aux@backref{3856}{wu2022_memvit}{0}{1876}{1876}
\abx@aux@backref{3854}{wu2022_memvit}{0}{1876}{1876}
\abx@aux@cite{0}{wu2022_memvit}
\abx@aux@segm{0}{0}{wu2022_memvit}
\abx@aux@cite{0}{wu2022_memvit}
\abx@aux@segm{0}{0}{wu2022_memvit}
\abx@aux@cite{0}{gu2018_ava}
\abx@aux@segm{0}{0}{gu2018_ava}
\abx@aux@cite{0}{li2021_improved_mvit}
\abx@aux@segm{0}{0}{li2021_improved_mvit}
\abx@aux@cite{0}{wu2022_memvit}
\abx@aux@segm{0}{0}{wu2022_memvit}
\abx@aux@cite{0}{wu2022_memvit}
\abx@aux@segm{0}{0}{wu2022_memvit}
\abx@aux@cite{0}{wu2022_memvit}
\abx@aux@segm{0}{0}{wu2022_memvit}
\abx@aux@cite{0}{wu2022_memvit}
\abx@aux@segm{0}{0}{wu2022_memvit}
\abx@aux@cite{0}{wu2022_memvit}
\abx@aux@segm{0}{0}{wu2022_memvit}
\abx@aux@cite{0}{wu2022_memvit}
\abx@aux@segm{0}{0}{wu2022_memvit}
\abx@aux@cite{0}{wu2022_memvit}
\abx@aux@segm{0}{0}{wu2022_memvit}
\abx@aux@cite{0}{wu2022_memvit}
\abx@aux@segm{0}{0}{wu2022_memvit}
\abx@aux@cite{0}{wu2022_memvit}
\abx@aux@segm{0}{0}{wu2022_memvit}
\abx@aux@cite{0}{wu2022_memvit}
\abx@aux@segm{0}{0}{wu2022_memvit}
\@writefile{lof}{\contentsline {figure}{\numberline {24.119}{\ignorespaces Comparison of scaling strategies. Increasing frames $T$ quickly explodes compute and memory; MeMViT maintains near–flat costs versus temporal support and achieves higher mAP under the same FLOPs. Adapted from \blx@tocontentsinit {0}\cite {wu2022_memvit}.}}{1877}{figure.caption.4387}\protected@file@percent }
\abx@aux@backref{3858}{wu2022_memvit}{0}{1877}{1877}
\newlabel{fig:chapter24_memvit_scaling}{{24.119}{1877}{Comparison of scaling strategies. Increasing frames $T$ quickly explodes compute and memory; MeMViT maintains near–flat costs versus temporal support and achieves higher mAP under the same FLOPs. Adapted from \cite {wu2022_memvit}}{figure.caption.4387}{}}
\@writefile{toc}{\contentsline {paragraph}{Ablations: how memory is used}{1877}{section*.4388}\protected@file@percent }
\abx@aux@backref{3859}{gu2018_ava}{0}{1877}{1877}
\abx@aux@backref{3860}{li2021_improved_mvit}{0}{1877}{1877}
\abx@aux@backref{3861}{wu2022_memvit}{0}{1877}{1877}
\abx@aux@backref{3862}{wu2022_memvit}{0}{1877}{1877}
\abx@aux@backref{3863}{wu2022_memvit}{0}{1877}{1877}
\abx@aux@backref{3864}{wu2022_memvit}{0}{1877}{1877}
\@writefile{toc}{\contentsline {paragraph}{Pipeline vs.\ naive compression}{1877}{section*.4389}\protected@file@percent }
\abx@aux@backref{3865}{wu2022_memvit}{0}{1877}{1877}
\@writefile{lof}{\contentsline {figure}{\numberline {24.120}{\ignorespaces Compression strategy. Pipelined memory compression lowers GPU memory and runtime compared with naively recompressing all cached steps each iteration. Adapted from \blx@tocontentsinit {0}\cite {wu2022_memvit}.}}{1877}{figure.caption.4390}\protected@file@percent }
\abx@aux@backref{3867}{wu2022_memvit}{0}{1877}{1877}
\newlabel{fig:chapter24_memvit_compression}{{24.120}{1877}{Compression strategy. Pipelined memory compression lowers GPU memory and runtime compared with naively recompressing all cached steps each iteration. Adapted from \cite {wu2022_memvit}}{figure.caption.4390}{}}
\@writefile{toc}{\contentsline {paragraph}{Generalization across backbones and datasets}{1877}{section*.4391}\protected@file@percent }
\abx@aux@backref{3868}{wu2022_memvit}{0}{1877}{1877}
\abx@aux@backref{3869}{wu2022_memvit}{0}{1877}{1877}
\abx@aux@cite{0}{wu2022_memvit}
\abx@aux@segm{0}{0}{wu2022_memvit}
\abx@aux@cite{0}{wu2022_memvit}
\abx@aux@segm{0}{0}{wu2022_memvit}
\abx@aux@cite{0}{wu2022_memvit}
\abx@aux@segm{0}{0}{wu2022_memvit}
\abx@aux@cite{0}{li2021_improved_mvit}
\abx@aux@segm{0}{0}{li2021_improved_mvit}
\abx@aux@cite{0}{wu2022_memvit}
\abx@aux@segm{0}{0}{wu2022_memvit}
\abx@aux@cite{0}{wu2022_memvit}
\abx@aux@segm{0}{0}{wu2022_memvit}
\@writefile{toc}{\contentsline {paragraph}{Takeaways from the ablations}{1878}{section*.4392}\protected@file@percent }
\abx@aux@backref{3870}{wu2022_memvit}{0}{1878}{1878}
\@writefile{toc}{\contentsline {subsubsection}{Limitations and Future Work}{1878}{section*.4393}\protected@file@percent }
\newlabel{subsubsec:chapter24_memvit_limits}{{24.10.1}{1878}{Limitations and Future Work}{section*.4393}{}}
\abx@aux@backref{3871}{wu2022_memvit}{0}{1878}{1878}
\abx@aux@backref{3872}{wu2022_memvit}{0}{1878}{1878}
\abx@aux@backref{3873}{wu2022_memvit}{0}{1878}{1878}
\abx@aux@backref{3874}{li2021_improved_mvit}{0}{1878}{1878}
\abx@aux@backref{3875}{wu2022_memvit}{0}{1878}{1878}
\abx@aux@backref{3876}{wu2022_memvit}{0}{1878}{1878}
\BKM@entry{id=978,dest={73656374696F6E2A2E34333934},srcline={6619}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030345C3030302E5C303030315C303030305C3030302E5C303030325C3030303A5C3030305C3034305C3030304C5C3030306F5C3030306E5C303030675C303030565C3030304C5C3030304D5C3030303A5C3030305C3034305C303030455C303030665C303030665C303030695C303030635C303030695C303030655C3030306E5C303030745C3030305C3034305C3030304C5C3030306F5C3030306E5C303030675C3030302D5C303030565C303030695C303030645C303030655C3030306F5C3030305C3034305C303030525C303030655C303030615C303030735C3030306F5C3030306E5C303030695C3030306E5C30303067}
\abx@aux@cite{0}{weng2024_longvlm}
\abx@aux@segm{0}{0}{weng2024_longvlm}
\abx@aux@cite{0}{weng2024_longvlm}
\abx@aux@segm{0}{0}{weng2024_longvlm}
\abx@aux@cite{0}{weng2024_longvlm}
\abx@aux@segm{0}{0}{weng2024_longvlm}
\@writefile{toc}{\contentsline {subsection}{Enrichment 24.10.2: LongVLM: Efficient Long-Video Reasoning}{1879}{section*.4394}\protected@file@percent }
\newlabel{enr:subsec_chapter24_longvlm}{{24.10.2}{1879}{\color {ocre}Enrichment \thesubsection : LongVLM: Efficient Long-Video Reasoning}{section*.4394}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation}{1879}{section*.4395}\protected@file@percent }
\abx@aux@backref{3877}{weng2024_longvlm}{0}{1879}{1879}
\@writefile{lof}{\contentsline {figure}{\numberline {24.121}{\ignorespaces Architectural contrast and qualitative examples. Prior Video-LLMs (e.g., Video-ChatGPT, Video-LLaMA) aggressively compress to a few tokens (pooling/Q-Former), risking an information bottleneck; LongVLM preserves a longer sequence via token merging and attains more faithful, temporally grounded responses (green indicates correct text; red indicates errors). Adapted from \blx@tocontentsinit {0}\cite {weng2024_longvlm}.}}{1879}{figure.caption.4396}\protected@file@percent }
\abx@aux@backref{3879}{weng2024_longvlm}{0}{1879}{1879}
\newlabel{fig:chapter24_longvlm_compare}{{24.121}{1879}{Architectural contrast and qualitative examples. Prior Video-LLMs (e.g., Video-ChatGPT, Video-LLaMA) aggressively compress to a few tokens (pooling/Q-Former), risking an information bottleneck; LongVLM preserves a longer sequence via token merging and attains more faithful, temporally grounded responses (green indicates correct text; red indicates errors). Adapted from \cite {weng2024_longvlm}}{figure.caption.4396}{}}
\@writefile{toc}{\contentsline {paragraph}{Method}{1879}{section*.4397}\protected@file@percent }
\newlabel{enr:subsubsec_chapter24_longvlm_method}{{24.10.2}{1879}{Method}{section*.4397}{}}
\newlabel{eq:longvlm_vs}{{24.59}{1879}{Method}{equation.24.59}{}}
\abx@aux@cite{0}{weng2024_longvlm}
\abx@aux@segm{0}{0}{weng2024_longvlm}
\abx@aux@cite{0}{weng2024_longvlm}
\abx@aux@segm{0}{0}{weng2024_longvlm}
\newlabel{eq:longvlm_cos}{{24.60}{1880}{Method}{equation.24.60}{}}
\newlabel{eq:longvlm_merge}{{24.61}{1880}{Method}{equation.24.61}{}}
\newlabel{eq:longvlm_seg_z}{{24.62}{1880}{Method}{equation.24.62}{}}
\newlabel{eq:longvlm_local_seq}{{24.63}{1880}{Method}{equation.24.63}{}}
\newlabel{eq:longvlm_global}{{24.64}{1880}{Method}{equation.24.64}{}}
\abx@aux@cite{0}{weng2024_longvlm}
\abx@aux@segm{0}{0}{weng2024_longvlm}
\abx@aux@cite{0}{weng2024_longvlm}
\abx@aux@segm{0}{0}{weng2024_longvlm}
\@writefile{lof}{\contentsline {figure}{\numberline {24.122}{\ignorespaces LongVLM overview. Frames $\to $ visual encoder features $\to $ two streams: (i) short-term \emph  {local} segment features via hierarchical token merging; (ii) \emph  {global} semantics via temporally averaged \texttt  {[CLS]} tokens from multiple encoder layers. Global tokens are prepended to the local, time-ordered tokens; a small projection aligns to the frozen LLM input space for instruction-following. Adapted from \blx@tocontentsinit {0}\cite {weng2024_longvlm}.}}{1881}{figure.caption.4398}\protected@file@percent }
\abx@aux@backref{3881}{weng2024_longvlm}{0}{1881}{1881}
\newlabel{fig:chapter24_longvlm_arch}{{24.122}{1881}{LongVLM overview. Frames $\to $ visual encoder features $\to $ two streams: (i) short-term \emph {local} segment features via hierarchical token merging; (ii) \emph {global} semantics via temporally averaged \texttt {[CLS]} tokens from multiple encoder layers. Global tokens are prepended to the local, time-ordered tokens; a small projection aligns to the frozen LLM input space for instruction-following. Adapted from \cite {weng2024_longvlm}}{figure.caption.4398}{}}
\@writefile{toc}{\contentsline {paragraph}{Algorithmic sketch (token merging within a segment)}{1881}{section*.4399}\protected@file@percent }
\abx@aux@cite{0}{weng2024_longvlm}
\abx@aux@segm{0}{0}{weng2024_longvlm}
\abx@aux@cite{0}{weng2024_longvlm}
\abx@aux@segm{0}{0}{weng2024_longvlm}
\abx@aux@cite{0}{weng2024_longvlm}
\abx@aux@segm{0}{0}{weng2024_longvlm}
\abx@aux@cite{0}{weng2024_longvlm}
\abx@aux@segm{0}{0}{weng2024_longvlm}
\abx@aux@cite{0}{weng2024_longvlm}
\abx@aux@segm{0}{0}{weng2024_longvlm}
\abx@aux@cite{0}{weng2024_longvlm}
\abx@aux@segm{0}{0}{weng2024_longvlm}
\abx@aux@cite{0}{li2024_videochat}
\abx@aux@segm{0}{0}{li2024_videochat}
\abx@aux@cite{0}{gao2023_llama_adapter_v2}
\abx@aux@segm{0}{0}{gao2023_llama_adapter_v2}
\abx@aux@cite{0}{zhang2023_videollama}
\abx@aux@segm{0}{0}{zhang2023_videollama}
\abx@aux@cite{0}{maaz2024_video_chatgpt}
\abx@aux@segm{0}{0}{maaz2024_video_chatgpt}
\abx@aux@cite{0}{luo2025_valley}
\abx@aux@segm{0}{0}{luo2025_valley}
\abx@aux@cite{0}{liu2023_bt_adapter}
\abx@aux@segm{0}{0}{liu2023_bt_adapter}
\abx@aux@cite{0}{liu2023_bt_adapter}
\abx@aux@segm{0}{0}{liu2023_bt_adapter}
\abx@aux@cite{0}{weng2024_longvlm}
\abx@aux@segm{0}{0}{weng2024_longvlm}
\abx@aux@cite{0}{weng2024_longvlm}
\abx@aux@segm{0}{0}{weng2024_longvlm}
\abx@aux@cite{0}{weng2024_longvlm}
\abx@aux@segm{0}{0}{weng2024_longvlm}
\abx@aux@cite{0}{yang2022_frozenbilm}
\abx@aux@segm{0}{0}{yang2022_frozenbilm}
\abx@aux@cite{0}{li2024_videochat}
\abx@aux@segm{0}{0}{li2024_videochat}
\abx@aux@cite{0}{gao2023_llama_adapter_v2}
\abx@aux@segm{0}{0}{gao2023_llama_adapter_v2}
\abx@aux@cite{0}{zhang2023_videollama}
\abx@aux@segm{0}{0}{zhang2023_videollama}
\abx@aux@cite{0}{maaz2024_video_chatgpt}
\abx@aux@segm{0}{0}{maaz2024_video_chatgpt}
\abx@aux@cite{0}{luo2025_valley}
\abx@aux@segm{0}{0}{luo2025_valley}
\abx@aux@cite{0}{liu2023_bt_adapter}
\abx@aux@segm{0}{0}{liu2023_bt_adapter}
\abx@aux@cite{0}{weng2024_longvlm}
\abx@aux@segm{0}{0}{weng2024_longvlm}
\@writefile{toc}{\contentsline {subsubsection}{Architecture \& Implementation Details}{1882}{section*.4400}\protected@file@percent }
\newlabel{enr:subsubsec_chapter24_longvlm_impl}{{24.10.2}{1882}{Architecture \& Implementation Details}{section*.4400}{}}
\abx@aux@backref{3882}{weng2024_longvlm}{0}{1882}{1882}
\abx@aux@backref{3883}{weng2024_longvlm}{0}{1882}{1882}
\abx@aux@backref{3884}{weng2024_longvlm}{0}{1882}{1882}
\abx@aux@backref{3885}{weng2024_longvlm}{0}{1882}{1882}
\abx@aux@backref{3886}{weng2024_longvlm}{0}{1882}{1882}
\abx@aux@backref{3887}{weng2024_longvlm}{0}{1882}{1882}
\@writefile{toc}{\contentsline {subsubsection}{Experiments and Ablations}{1882}{section*.4401}\protected@file@percent }
\newlabel{enr:subsubsec_chapter24_longvlm_exps}{{24.10.2}{1882}{Experiments and Ablations}{section*.4401}{}}
\@writefile{toc}{\contentsline {paragraph}{Benchmarks and metrics}{1882}{section*.4402}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {24.66}{\ignorespaces Comparison on the Video-ChatGPT benchmark (higher is better). Mean is the average over CI/DO/CU/TU/C. Data sizes follow the original papers. Numbers are from \blx@tocontentsinit {0}\cite [Tab.~1]{weng2024_longvlm}.}}{1882}{table.caption.4403}\protected@file@percent }
\abx@aux@backref{3889}{weng2024_longvlm}{0}{1882}{1882}
\newlabel{tab:chapter24_longvlm_main_vcgpt}{{24.66}{1882}{Comparison on the Video-ChatGPT benchmark (higher is better). Mean is the average over CI/DO/CU/TU/C. Data sizes follow the original papers. Numbers are from \cite [Tab.~1]{weng2024_longvlm}}{table.caption.4403}{}}
\abx@aux@backref{3890}{li2024_videochat}{0}{1882}{1882}
\abx@aux@backref{3891}{gao2023_llama_adapter_v2}{0}{1882}{1882}
\abx@aux@backref{3892}{zhang2023_videollama}{0}{1882}{1882}
\abx@aux@backref{3893}{maaz2024_video_chatgpt}{0}{1882}{1882}
\abx@aux@backref{3894}{luo2025_valley}{0}{1882}{1882}
\abx@aux@backref{3895}{liu2023_bt_adapter}{0}{1882}{1882}
\abx@aux@backref{3896}{liu2023_bt_adapter}{0}{1882}{1882}
\abx@aux@backref{3897}{weng2024_longvlm}{0}{1882}{1882}
\abx@aux@cite{0}{weng2024_longvlm}
\abx@aux@segm{0}{0}{weng2024_longvlm}
\abx@aux@cite{0}{weng2024_longvlm}
\abx@aux@segm{0}{0}{weng2024_longvlm}
\@writefile{lot}{\contentsline {table}{\numberline {24.67}{\ignorespaces Zero-shot QA results (higher is better). Accuracy (\%) and quality \emph  {Score} with data sources, reproduced from \blx@tocontentsinit {0}\cite [Tab.~2]{weng2024_longvlm}.}}{1883}{table.caption.4404}\protected@file@percent }
\abx@aux@backref{3899}{weng2024_longvlm}{0}{1883}{1883}
\newlabel{tab:chapter24_longvlm_main_qa}{{24.67}{1883}{Zero-shot QA results (higher is better). Accuracy (\%) and quality \emph {Score} with data sources, reproduced from \cite [Tab.~2]{weng2024_longvlm}}{table.caption.4404}{}}
\abx@aux@backref{3900}{yang2022_frozenbilm}{0}{1883}{1883}
\abx@aux@backref{3901}{li2024_videochat}{0}{1883}{1883}
\abx@aux@backref{3902}{gao2023_llama_adapter_v2}{0}{1883}{1883}
\abx@aux@backref{3903}{zhang2023_videollama}{0}{1883}{1883}
\abx@aux@backref{3904}{maaz2024_video_chatgpt}{0}{1883}{1883}
\abx@aux@backref{3905}{luo2025_valley}{0}{1883}{1883}
\abx@aux@backref{3906}{liu2023_bt_adapter}{0}{1883}{1883}
\abx@aux@backref{3907}{weng2024_longvlm}{0}{1883}{1883}
\@writefile{lof}{\contentsline {figure}{\numberline {24.123}{\ignorespaces Quantitative and qualitative results. Left: LongVLM is consistently on the outer envelope across aspects (CI/DO/CU/TU/C) and QA tasks. Right: A multi-turn conversation over a 3m46s video shows temporal awareness, fine detail tracking (e.g., apparel color), and plausible reasoning grounded in content. Adapted from \blx@tocontentsinit {0}\cite {weng2024_longvlm}.}}{1883}{figure.caption.4405}\protected@file@percent }
\abx@aux@backref{3909}{weng2024_longvlm}{0}{1883}{1883}
\newlabel{fig:chapter24_longvlm_results}{{24.123}{1883}{Quantitative and qualitative results. Left: LongVLM is consistently on the outer envelope across aspects (CI/DO/CU/TU/C) and QA tasks. Right: A multi-turn conversation over a 3m46s video shows temporal awareness, fine detail tracking (e.g., apparel color), and plausible reasoning grounded in content. Adapted from \cite {weng2024_longvlm}}{figure.caption.4405}{}}
\abx@aux@cite{0}{weng2024_longvlm}
\abx@aux@segm{0}{0}{weng2024_longvlm}
\abx@aux@cite{0}{weng2024_longvlm}
\abx@aux@segm{0}{0}{weng2024_longvlm}
\abx@aux@cite{0}{weng2024_longvlm}
\abx@aux@segm{0}{0}{weng2024_longvlm}
\abx@aux@cite{0}{weng2024_longvlm}
\abx@aux@segm{0}{0}{weng2024_longvlm}
\abx@aux@cite{0}{weng2024_longvlm}
\abx@aux@segm{0}{0}{weng2024_longvlm}
\abx@aux@cite{0}{weng2024_longvlm}
\abx@aux@segm{0}{0}{weng2024_longvlm}
\abx@aux@cite{0}{weng2024_longvlm}
\abx@aux@segm{0}{0}{weng2024_longvlm}
\abx@aux@cite{0}{weng2024_longvlm}
\abx@aux@segm{0}{0}{weng2024_longvlm}
\abx@aux@cite{0}{weng2024_longvlm}
\abx@aux@segm{0}{0}{weng2024_longvlm}
\@writefile{toc}{\contentsline {paragraph}{Ablations}{1884}{section*.4406}\protected@file@percent }
\abx@aux@backref{3910}{weng2024_longvlm}{0}{1884}{1884}
\@writefile{lot}{\contentsline {table}{\numberline {24.68}{\ignorespaces Local vs.\ global aggregation on Video-ChatGPT (higher is better). Pooling uses 3D average pooling within each short segment; Merging uses the proposed hierarchical token merging; {[L,\,G]} concatenates Local then Global tokens, while {[G,\,L]} prepends Global before Local. Numbers from \blx@tocontentsinit {0}\cite [Tab.~3]{weng2024_longvlm}.}}{1884}{table.caption.4407}\protected@file@percent }
\abx@aux@backref{3912}{weng2024_longvlm}{0}{1884}{1884}
\newlabel{tab:chapter24_longvlm_ablate_lg}{{24.68}{1884}{Local vs.\ global aggregation on Video-ChatGPT (higher is better). Pooling uses 3D average pooling within each short segment; Merging uses the proposed hierarchical token merging; {[L,\,G]} concatenates Local then Global tokens, while {[G,\,L]} prepends Global before Local. Numbers from \cite [Tab.~3]{weng2024_longvlm}}{table.caption.4407}{}}
\abx@aux@backref{3913}{weng2024_longvlm}{0}{1884}{1884}
\abx@aux@backref{3914}{weng2024_longvlm}{0}{1884}{1884}
\@writefile{toc}{\contentsline {paragraph}{Qualitative analyses}{1884}{section*.4408}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24.124}{\ignorespaces Ablation evidence: local-only vs.\ local+global. Left: Without global context, a local-only model mistakes a long jump for hurdles; adding global semantics recovers the correct event. Right: Local-only confuses an axe with a bat; global context plus local details yields the correct interpretation. Adapted from \blx@tocontentsinit {0}\cite {weng2024_longvlm}.}}{1884}{figure.caption.4409}\protected@file@percent }
\abx@aux@backref{3916}{weng2024_longvlm}{0}{1884}{1884}
\newlabel{fig:chapter24_longvlm_vcgpt_qual}{{24.124}{1884}{Ablation evidence: local-only vs.\ local+global. Left: Without global context, a local-only model mistakes a long jump for hurdles; adding global semantics recovers the correct event. Right: Local-only confuses an axe with a bat; global context plus local details yields the correct interpretation. Adapted from \cite {weng2024_longvlm}}{figure.caption.4409}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24.125}{\ignorespaces Additional generations from the Video-ChatGPT benchmark illustrating temporal grounding, fine-grained details (e.g., color, specific actions), and coherent scene understanding across diverse videos. Adapted from \blx@tocontentsinit {0}\cite {weng2024_longvlm}.}}{1885}{figure.caption.4410}\protected@file@percent }
\abx@aux@backref{3918}{weng2024_longvlm}{0}{1885}{1885}
\newlabel{fig:chapter24_longvlm_more}{{24.125}{1885}{Additional generations from the Video-ChatGPT benchmark illustrating temporal grounding, fine-grained details (e.g., color, specific actions), and coherent scene understanding across diverse videos. Adapted from \cite {weng2024_longvlm}}{figure.caption.4410}{}}
\@writefile{toc}{\contentsline {paragraph}{Limitations and Future Work}{1885}{section*.4411}\protected@file@percent }
\newlabel{enr:subsubsec_chapter24_longvlm_limits}{{24.10.2}{1885}{Limitations and Future Work}{section*.4411}{}}
\BKM@entry{id=979,dest={73656374696F6E2A2E34343132},srcline={6884}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030345C3030302E5C303030315C303030305C3030302E5C303030335C3030303A5C3030305C3034305C3030304C5C303030575C3030304D5C3030303A5C3030305C3034305C303030425C3030306C5C3030306F5C303030635C3030306B5C303030775C303030695C303030735C303030655C3030305C3034305C303030525C303030695C3030306E5C303030675C303030415C303030745C303030745C303030655C3030306E5C303030745C303030695C3030306F5C3030306E5C3030305C3034305C303030665C3030306F5C303030725C3030305C3034305C3030304D5C303030695C3030306C5C3030306C5C303030695C3030306F5C3030306E5C3030302D5C303030545C3030306F5C3030306B5C303030655C3030306E5C3030305C3034305C303030435C3030306F5C3030306E5C303030745C303030655C303030785C303030745C30303073}
\abx@aux@cite{0}{liu2025_lwm}
\abx@aux@segm{0}{0}{liu2025_lwm}
\abx@aux@cite{0}{liu2025_lwm}
\abx@aux@segm{0}{0}{liu2025_lwm}
\abx@aux@cite{0}{liu2025_lwm}
\abx@aux@segm{0}{0}{liu2025_lwm}
\abx@aux@cite{0}{liu2025_lwm}
\abx@aux@segm{0}{0}{liu2025_lwm}
\abx@aux@cite{0}{liu2025_lwm}
\abx@aux@segm{0}{0}{liu2025_lwm}
\abx@aux@cite{0}{liu2025_lwm}
\abx@aux@segm{0}{0}{liu2025_lwm}
\@writefile{toc}{\contentsline {subsection}{Enrichment 24.10.3: LWM: Blockwise RingAttention for Million-Token Contexts}{1887}{section*.4412}\protected@file@percent }
\newlabel{enr:subsec_chapter24_lwm}{{24.10.3}{1887}{\color {ocre}Enrichment \thesubsection : LWM: Blockwise RingAttention for Million-Token Contexts}{section*.4412}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation}{1887}{section*.4413}\protected@file@percent }
\abx@aux@backref{3919}{liu2025_lwm}{0}{1887}{1887}
\@writefile{lof}{\contentsline {figure}{\numberline {24.126}{\ignorespaces Context-size comparison across LLMs. LWM attains a one-million-token context window and is positioned at the frontier alongside large-context systems such as Gemini 1.5, substantially exceeding earlier 128K/100K or smaller context models. The context window is the effective short-term memory: larger windows allow whole books, long codebases, or hour-long videos to be processed in a single pass. Adapted from \blx@tocontentsinit {0}\cite {liu2025_lwm}.}}{1887}{figure.caption.4414}\protected@file@percent }
\abx@aux@backref{3921}{liu2025_lwm}{0}{1887}{1887}
\newlabel{fig:chapter24_lwm_context}{{24.126}{1887}{Context-size comparison across LLMs. LWM attains a one-million-token context window and is positioned at the frontier alongside large-context systems such as Gemini 1.5, substantially exceeding earlier 128K/100K or smaller context models. The context window is the effective short-term memory: larger windows allow whole books, long codebases, or hour-long videos to be processed in a single pass. Adapted from \cite {liu2025_lwm}}{figure.caption.4414}{}}
\@writefile{toc}{\contentsline {paragraph}{Method}{1887}{section*.4415}\protected@file@percent }
\newlabel{enr:subsubsec_chapter24_lwm_method}{{24.10.3}{1887}{Method}{section*.4415}{}}
\abx@aux@backref{3922}{liu2025_lwm}{0}{1887}{1887}
\abx@aux@backref{3924}{liu2025_lwm}{0}{1887}{1887}
\abx@aux@backref{3923}{liu2025_lwm}{0}{1887}{1887}
\abx@aux@cite{0}{liu2025_lwm}
\abx@aux@segm{0}{0}{liu2025_lwm}
\@writefile{toc}{\contentsline {paragraph}{Training Curriculum}{1888}{section*.4416}\protected@file@percent }
\newlabel{enr:subsubsec_chapter24_lwm_training}{{24.10.3}{1888}{Training Curriculum}{section*.4416}{}}
\abx@aux@backref{3925}{liu2025_lwm}{0}{1888}{1888}
\abx@aux@cite{0}{liu2025_lwm}
\abx@aux@segm{0}{0}{liu2025_lwm}
\abx@aux@cite{0}{liu2025_lwm}
\abx@aux@segm{0}{0}{liu2025_lwm}
\abx@aux@cite{0}{liu2025_lwm}
\abx@aux@segm{0}{0}{liu2025_lwm}
\abx@aux@cite{0}{liu2025_lwm}
\abx@aux@segm{0}{0}{liu2025_lwm}
\abx@aux@backref{3926}{liu2025_lwm}{0}{1891}{1891}
\abx@aux@backref{3927}{liu2025_lwm}{0}{1891}{1891}
\@writefile{lof}{\contentsline {figure}{\numberline {24.127}{\ignorespaces Architecture overview. LWM is a single autoregressive Transformer over a \emph  {unified} token stream comprising BPE text and VQGAN vision codes (256 tokens per frame). Modality delimiters \texttt  {<vision>}...\texttt  {</vision>} and \texttt  {<eof>}/\texttt  {<eov>} mark boundaries; the model predicts the next token regardless of modality. Adapted from \blx@tocontentsinit {0}\cite {liu2025_lwm}.}}{1891}{figure.caption.4417}\protected@file@percent }
\abx@aux@backref{3929}{liu2025_lwm}{0}{1891}{1891}
\newlabel{fig:chapter24_lwm_arch}{{24.127}{1891}{Architecture overview. LWM is a single autoregressive Transformer over a \emph {unified} token stream comprising BPE text and VQGAN vision codes (256 tokens per frame). Modality delimiters \texttt {<vision>}...\texttt {</vision>} and \texttt {<eof>}/\texttt {<eov>} mark boundaries; the model predicts the next token regardless of modality. Adapted from \cite {liu2025_lwm}}{figure.caption.4417}{}}
\abx@aux@cite{0}{liu2025_lwm}
\abx@aux@segm{0}{0}{liu2025_lwm}
\abx@aux@cite{0}{liu2025_lwm}
\abx@aux@segm{0}{0}{liu2025_lwm}
\abx@aux@cite{0}{liu2025_lwm}
\abx@aux@segm{0}{0}{liu2025_lwm}
\abx@aux@cite{0}{esser2021_vqgan}
\abx@aux@segm{0}{0}{esser2021_vqgan}
\abx@aux@cite{0}{liu2025_lwm}
\abx@aux@segm{0}{0}{liu2025_lwm}
\abx@aux@cite{0}{liu2025_lwm}
\abx@aux@segm{0}{0}{liu2025_lwm}
\abx@aux@backref{3930}{liu2025_lwm}{0}{1892}{1892}
\@writefile{toc}{\contentsline {subsubsection}{Architecture \& Implementation Details}{1892}{section*.4418}\protected@file@percent }
\newlabel{enr:subsubsec_chapter24_lwm_impl}{{24.10.3}{1892}{Architecture \& Implementation Details}{section*.4418}{}}
\abx@aux@backref{3931}{liu2025_lwm}{0}{1892}{1892}
\abx@aux@backref{3932}{liu2025_lwm}{0}{1892}{1892}
\abx@aux@backref{3933}{esser2021_vqgan}{0}{1892}{1892}
\abx@aux@backref{3934}{liu2025_lwm}{0}{1892}{1892}
\abx@aux@backref{3935}{liu2025_lwm}{0}{1892}{1892}
\abx@aux@cite{0}{liu2025_lwm}
\abx@aux@segm{0}{0}{liu2025_lwm}
\abx@aux@cite{0}{liu2025_lwm}
\abx@aux@segm{0}{0}{liu2025_lwm}
\abx@aux@cite{0}{liu2025_lwm}
\abx@aux@segm{0}{0}{liu2025_lwm}
\abx@aux@cite{0}{liu2025_lwm}
\abx@aux@segm{0}{0}{liu2025_lwm}
\abx@aux@cite{0}{liu2025_lwm}
\abx@aux@segm{0}{0}{liu2025_lwm}
\abx@aux@backref{3936}{liu2025_lwm}{0}{1893}{1893}
\abx@aux@backref{3937}{liu2025_lwm}{0}{1893}{1893}
\abx@aux@backref{3938}{liu2025_lwm}{0}{1893}{1893}
\abx@aux@cite{0}{liu2025_lwm}
\abx@aux@segm{0}{0}{liu2025_lwm}
\abx@aux@cite{0}{liu2025_lwm}
\abx@aux@segm{0}{0}{liu2025_lwm}
\@writefile{lof}{\contentsline {figure}{\numberline {24.128}{\ignorespaces Progressive data curation and training. Stage I extends language context using long books; Stage II integrates vision–language with a curriculum from images to short clips, Q\&A-style instruction data, and progressively longer videos. Pie charts show that images and short-frame videos dominate visual tokens, while mid-length documents dominate text tokens. Adapted from \blx@tocontentsinit {0}\cite {liu2025_lwm}.}}{1894}{figure.caption.4419}\protected@file@percent }
\abx@aux@backref{3940}{liu2025_lwm}{0}{1894}{1894}
\newlabel{fig:chapter24_lwm_data}{{24.128}{1894}{Progressive data curation and training. Stage I extends language context using long books; Stage II integrates vision–language with a curriculum from images to short clips, Q\&A-style instruction data, and progressively longer videos. Pie charts show that images and short-frame videos dominate visual tokens, while mid-length documents dominate text tokens. Adapted from \cite {liu2025_lwm}}{figure.caption.4419}{}}
\@writefile{toc}{\contentsline {subsubsection}{Experiments and Ablations}{1894}{section*.4420}\protected@file@percent }
\newlabel{enr:subsubsec_chapter24_lwm_exps}{{24.10.3}{1894}{Experiments and Ablations}{section*.4420}{}}
\@writefile{toc}{\contentsline {paragraph}{Long-context retrieval (needle and multi-needle)}{1894}{section*.4421}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24.129}{\ignorespaces Needle retrieval across context positions. LWM sustains high retrieval accuracy across positions and scales the context to $1$M tokens, while baselines are limited to shorter contexts. The x-axis is log (0–128K) then linear (128K–1M). Adapted from \blx@tocontentsinit {0}\cite {liu2025_lwm}.}}{1895}{figure.caption.4422}\protected@file@percent }
\abx@aux@backref{3942}{liu2025_lwm}{0}{1895}{1895}
\newlabel{fig:chapter24_lwm_needle}{{24.129}{1895}{Needle retrieval across context positions. LWM sustains high retrieval accuracy across positions and scales the context to $1$M tokens, while baselines are limited to shorter contexts. The x-axis is log (0–128K) then linear (128K–1M). Adapted from \cite {liu2025_lwm}}{figure.caption.4422}{}}
\@writefile{toc}{\contentsline {paragraph}{Language tasks at short context}{1895}{section*.4423}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{LOFT benchmarks (512K)}{1895}{section*.4424}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {24.69}{\ignorespaces LOFT at 512K context: LWM vs.\ strong baselines (selected). Higher is better.}}{1895}{table.caption.4425}\protected@file@percent }
\newlabel{tab:chapter24_lwm_loft}{{24.69}{1895}{LOFT at 512K context: LWM vs.\ strong baselines (selected). Higher is better}{table.caption.4425}{}}
\abx@aux@cite{0}{liu2025_lwm}
\abx@aux@segm{0}{0}{liu2025_lwm}
\abx@aux@cite{0}{liu2025_lwm}
\abx@aux@segm{0}{0}{liu2025_lwm}
\@writefile{toc}{\contentsline {paragraph}{Long-video understanding}{1896}{section*.4426}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24.130}{\ignorespaces One-hour YouTube compilation QA. LWM-Chat-1M retrieves fine-grained details across hundreds of clips within one sequence, succeeding where several proprietary and open-source models either refuse, miss, or hallucinate. Adapted from \blx@tocontentsinit {0}\cite {liu2025_lwm}.}}{1896}{figure.caption.4427}\protected@file@percent }
\abx@aux@backref{3944}{liu2025_lwm}{0}{1896}{1896}
\newlabel{fig:chapter24_lwm_1hour}{{24.130}{1896}{One-hour YouTube compilation QA. LWM-Chat-1M retrieves fine-grained details across hundreds of clips within one sequence, succeeding where several proprietary and open-source models either refuse, miss, or hallucinate. Adapted from \cite {liu2025_lwm}}{figure.caption.4427}{}}
\abx@aux@cite{0}{liu2025_lwm}
\abx@aux@segm{0}{0}{liu2025_lwm}
\abx@aux@cite{0}{liu2025_lwm}
\abx@aux@segm{0}{0}{liu2025_lwm}
\@writefile{toc}{\contentsline {paragraph}{Generation}{1897}{section*.4428}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24.131}{\ignorespaces Text-to-image and text-to-video generation. Top: image generation; bottom: short video sequences showing simple temporal dynamics captured by autoregressive decoding over visual codes. Adapted from \blx@tocontentsinit {0}\cite {liu2025_lwm}.}}{1897}{figure.caption.4429}\protected@file@percent }
\abx@aux@backref{3946}{liu2025_lwm}{0}{1897}{1897}
\newlabel{fig:chapter24_lwm_gen}{{24.131}{1897}{Text-to-image and text-to-video generation. Top: image generation; bottom: short video sequences showing simple temporal dynamics captured by autoregressive decoding over visual codes. Adapted from \cite {liu2025_lwm}}{figure.caption.4429}{}}
\@writefile{toc}{\contentsline {subsubsection}{Limitations and Future Work}{1897}{section*.4430}\protected@file@percent }
\newlabel{enr:subsubsec_chapter24_lwm_limits}{{24.10.3}{1897}{Limitations and Future Work}{section*.4430}{}}
\BKM@entry{id=980,dest={73656374696F6E2A2E34343331},srcline={7155}}{5C3337365C3337375C303030455C3030306E5C303030725C303030695C303030635C303030685C3030306D5C303030655C3030306E5C303030745C3030305C3034305C303030325C303030345C3030302E5C303030315C303030315C3030303A5C3030305C3034305C303030535C303030705C303030655C303030635C303030695C303030615C3030306C5C303030695C3030307A5C303030655C303030645C3030305C3034305C303030445C303030695C303030725C303030655C303030635C303030745C303030695C3030306F5C3030306E5C30303073}
\abx@aux@cite{0}{blattmann2023_vdm}
\abx@aux@segm{0}{0}{blattmann2023_vdm}
\abx@aux@cite{0}{xu2023_languagebind}
\abx@aux@segm{0}{0}{xu2023_languagebind}
\abx@aux@cite{0}{lu2021_omnimatte}
\abx@aux@segm{0}{0}{lu2021_omnimatte}
\abx@aux@cite{0}{lu2023_omnimatterf}
\abx@aux@segm{0}{0}{lu2023_omnimatterf}
\abx@aux@cite{0}{li2022_uniformerv2}
\abx@aux@segm{0}{0}{li2022_uniformerv2}
\abx@aux@cite{0}{wang2023_mvd}
\abx@aux@segm{0}{0}{wang2023_mvd}
\@writefile{toc}{\contentsline {section}{Enrichment 24.11: Specialized Directions}{1898}{section*.4431}\protected@file@percent }
\newlabel{enr:sec_chapter24_specialized}{{24.11}{1898}{\color {ocre}Enrichment \thesection : Specialized Directions}{section*.4431}{}}
\abx@aux@backref{3947}{blattmann2023_vdm}{0}{1898}{1898}
\abx@aux@backref{3948}{xu2023_languagebind}{0}{1898}{1898}
\abx@aux@backref{3949}{lu2021_omnimatte}{0}{1898}{1898}
\abx@aux@backref{3950}{lu2023_omnimatterf}{0}{1898}{1898}
\abx@aux@backref{3951}{li2022_uniformerv2}{0}{1898}{1898}
\abx@aux@backref{3952}{wang2023_mvd}{0}{1898}{1898}
\pgfsyspdfmark {pgfid148}{0}{52099153}
\pgfsyspdfmark {pgfid147}{5966969}{45620378}
\ttl@finishall
\abx@aux@read@bbl@mdfivesum{0BAEA021A1DD5A75E058F73A003B0685}
\abx@aux@defaultrefcontext{0}{yu2023_ldit}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{aanaes2016_mvs}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ahmadyan2021_objectron}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{alaluf2022_stylegan3editing}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{jalammar2018_illustrated}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{flamingo2022_fewshot}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{alexe2012_objectness}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{alger2019_data}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{aliev2020_npbg}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{an2025_llavaonevision15}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{karpathy_convnetjs}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{appen_road_annotation}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{arandjelovic2017_look_listen}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{medium_lstm_vanishing}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{arjovsky2017_wgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{armandpour2021_pgmg}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{arnab2021_vivit}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{athar2023_burst}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhihu2023_classifierfreeguidance}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{awadalla2023_openflamingo}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ba2015_attention}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{baccouche2011_seqdl}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bahdanau2016_neural}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bai2023_qwenvl2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bai2023_qwenvl}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bai2023_basictad}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{baker2022_ssim_floating_point}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ballas2016_delving}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bardes2022_vicreg}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{barron2022_mipnerf360}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{barron2021_mipnerf}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{barron2023_zipnerf}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bau2017_network_dissection}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bekuzarov2022_contrastive_loss}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bekuzarov2023_xmempp}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bello2021_revisitingresnets}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bengio1994_learning}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bengio2013_representation}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{paepper2023_sdembeddingviz}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bergstra2012_randomsearch}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{berman2019_multigrain}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bertasius2021_timesformer}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{beyer2023_flexivit}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bian2023_nopenerf}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{binkowski2018_demystifying}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{blattmann2023_vdm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bochkovskiy2020_yolov4}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{bossard2014_food101}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lake2015_human}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{brock2019_biggan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{brock2021_highperformance}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{brock2021_nfnet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{brock2017_introspective}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{brooks1979_modelbased}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{brooks2023_instructpix2pix}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{brown2020_language}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{brown2020_gpt3}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{buolamwini2018_gendershades}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{cai2019_proxylessnas}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{canny1986_edgedetection}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{carion2020_detr}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{carlini2017_defensive_distillation}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{carlini2017_towards}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{carlini2023_universal_llm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{caron2021_selfsupervised}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{caron2018_deepcluster}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{dino2021_selfsupervised}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{caron2020_swav}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{caron2019_deepercluster}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{carreira2017_i3d}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chan2016_listenattend}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chang2015_shapenet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chang2022_maskgit}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chao2018_tal}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chen2021_mvsnerf}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chen2022_tensorf}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chen2020_hopskipjump}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chen2017_deeplab}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chen2020_gpt_pixels}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chen2020_imagegpt}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chen2023_fantasia3d}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chen2023_riemannianfm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chen2019_neuralode}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chen2022_cyclemlp}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chen2020_simclr}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chen2020_simclrv2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chen2021_corp}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chen2021_simsiam}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chen2021_empiricalstudy}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chen2021_mocov3}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chen2020_improved}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chen2020_uniter}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chen2023_dbarf}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{cheng2022_mask2former}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{cheng2024_cutie}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{cheng2014_bing}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{cheng2024_videollama2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chollet2017_xception}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{cho2014_gru}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{cciccek2016_3dunet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{clark2019_videogan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{clevert2015_fast}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{clevert2016_fast_and_accurate}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{cohen2018_distributionmatching}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{cubuk2020_randaugment}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{dai2017_scannet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{dai2023_instructblip}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{team2023_gemini}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{dehghani2023_navit}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{deitke2023_objaverse}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{imagenet2009_hierarchicaldatabase}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{devlin2015_imagetocaption}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{devlin2019_bert}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{devries2017_cutout}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{dhariwal2021_beats}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{dhariwal2021_diffusion}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ding2023_mose}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ding2024_sam2long}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{dinh2017_realnvp}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{doersch2015_context}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{donahue2019_bigbigan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{donahue2015_ltrcnn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{levy2016_medicalimaging}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{vit2020_transformers}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{driess2023_palme}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{duc2022_selfkd}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{dumoulin2017_cbn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{dupont2020_enr}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{dwibedi2021_nnclr}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hf_gs_blog}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{efros1999_texture}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{eigen2014_depth}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{eldan2016_power}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{erdem2020_RoIAlign}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ericsson2021_selfsup}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ermolov2021_twist}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{eslami2018_dgqn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{esposito2022_kiloneus}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{esser2021_vqgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{esser2024_scalingrectifiedflow}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pascal2010_visualchallenge}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{fan2021_mvit}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{fan2017_pointset}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{feichtenhofer2020_x3d}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{feichtenhofer2018_deepreps}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{feichtenhofer2021_r2plus1d_ssl}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{feichtenhofer2019_deepinsights}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{feichtenhofer2022_mae_st}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{feichtenhofer2019_slowfast}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{fischler1973_pictorialstructures}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{fridovichkeil2022_plenoxels}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{fu2021_violet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{Fu_2024_COLMAPFreeGS}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{fukushima1980_neocognitron}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{gadre2023_datacomp}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{gao2022_nerfdiff}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{gao2017_charadessta}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{gao2023_llama_adapter_v2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{gat2024_discreteflowmatching}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{gatys2016_stylization}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{gatys2015_texture}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{gidaris2018_unsupervised}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{girdhar2023_imagebind}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{girdhar2023_omnimae}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{girshick2015_fastrcnn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{girshick2014_rcnn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{gkioxari2020_meshrcnn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{glorot2010_understanding}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{goh2021_multimodal}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{goodfellow2014_adversarial}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{goodfellow2015_explaining}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{gou2020_kd}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{graham2015_fractionalmaxpool}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{grathwohl2019_ffjord}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{grauman2022_ego4d}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{gregor2015_draw}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{grill2020_byol}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{gropp2020implicit}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mamba2023_selective}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{gu2018_ava}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{gu2018_nonautoregressive}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{gulrajani2017_improvedwgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{gupta2018_socialgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hadsell2006_dimreduction}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{han2020_memdpc}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{haque2023_instructnerf2nerf}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{harris1988_combined}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hayou2024_loraplus}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{he2018_rethinkingimagenet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{he2016_resnet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{he2015_delving}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{he2016identity}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{he2017_maskrcnn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{he2022_mae}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{he2020_moco}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{he2018_resnetd}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hedman2018_deepblending}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{henaff2020_cpcv2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hendrycks2016_gelu}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{heo2024_rotarype}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hertz2022_prompt2prompt}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{heusel2017_fid}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hinton2015_distillation}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hlav2023_kaiming}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hlav2023_xavier}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ho2020_ddpm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ho2022_classifierfree}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ho2021_cascaded}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hoang2018_mgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hochreiter1997_lstm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{holderrieth2024_gm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{howard2017_mobilenets}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{howard2019_mobilenetv3}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{howard2018_universal}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hu2021_lora}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hu2018_senet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hu2021_adco}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hu2023_trimiprf}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hu2024_diffusest}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hu2023_promptcap}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{huang2016_stochasticdepth}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{huang2018_densenet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{huang2024_allora}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{huang2023_audiogpt}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{huang2020_tfixup}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{huang2017_adain}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hubel1959_receptivefields}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hui2020_styleganblog}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{becominghuman2018_allaboutnorm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{groundedsam2_repo}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ioffe2015_batchnorm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{isola2017_pix2pix}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{jabri2020_stc}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{jacot2018_ntk}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{jain2022_dreamfields}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{jain2023_oneformer}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{jang2023_nerfshop}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{jang2017_gumbel}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{jensen2014_dtu}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ji2010_3dcnn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{jia2021_align}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{jiang2014_thumos14}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{johari2022_geonerf}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{johnson2016_perceptual}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{johnson2015_densecap}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{johnson2017_infering}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kalajdzievski2023_rs}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kang2023_gigagan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{karpathy2015_visualsemantic}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{karpathy2015_visualizing_rnns}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{karpathy2014_videocnn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{karpathy2014_largevideo}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{karras2019_stylegan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{karras2021_stylegan3}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{karras2020_stylegan2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{karras2018_progrowing}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kay2017_kinetics}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kazemnejad2019_pencoding}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ke2021rethinking_position}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ke2023_hqsam}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{Kerbl_2024_GaussianSurfels}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kerbl2023_3dgaussiansplatting}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kerr2023_lerf}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{keskar2017_flatminima}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{khrulkov2018_geometry}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kilcher2022_flowmatchingyt}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kingma2014_autoencoding}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kingma2017_adam}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kingma2018_glow}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kirillov2020_pointrend}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sam2023_segmentation}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kirillov2023_sam}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{klambauer2017_selu}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{knapitsch2017_tanks}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{koch2019_abc}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kohler2019_exponentialbn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kornilov2024_ofm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{krause2013_stanfordcars}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{krishna2017_activitynet_captions}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{krishnamoorthi2018_quantizing}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{krizhevsky2009_learning}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{krizhevsky2012_alexnet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kulkarni2015_dc_ign}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kumar2022_finetuning}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kundu2022_contrastive_v7labs}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kwon2023_diffusiongan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lai2020_mast}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{law2019_cornernet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lazova2023_controlnerf}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lebanoff2018_pixelrnn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lecun1998_lenet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ledig2017_srgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lee2022_rqtransformer}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lee2021_cbyol}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lee2019_bts}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lee2023_styleganT}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lei2021_clipbert}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lei2021_qvhighlight}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lewis2020_bart}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{li2024_llavaonevision}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{li2022_lseg}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2022_dino}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{li2022_dn_detr}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{liu2024_llava_next}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{li2022_maskdino}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{li2018_visualizing}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{li2021_albef}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{li2023_blip2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{li2022_blip}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{li2024_videochat}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{li2024_mvbench}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{li2024_umt}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{li2021_tanet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{li2022_understanding}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{li2020_oscar}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{li2020_tea}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{li2021_learnable_fourier}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{li2021_improved_mvit}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{li2022_uniformerv2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{li2020_relationalmaps}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{li2021_teinet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{li2022_uniformer}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{li2021_neural}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{li2017_lwf}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lialin2024_scaling}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lin2021_barf}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lin2023_magic3d}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lin2019_tsm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lin2019_bmn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lin2021_salient_boundary}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lin2014microsoft}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lin2017_fpn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lin2018_focalloss}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lin2018_pacgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lipman2024_flowmatchingguidecode}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lipman2022_flowmatching}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{litjens2017_medicalcnn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{liu2021_pay_attention_to_mlps}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{liu2025_lwm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{liu2024_llava_next_video}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{liu2023_llava}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{nerf_survey2023}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{liu2020_nsvf}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{liu2021_tam}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{liu2023_bt_adapter}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{liu2024_dora}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{liu2022_dab_detr}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{liu2023_groundingdino}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{liu2023_neudf}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{liu2017_sphereface}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{liu2019_roberta}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{liu2022_ts2net}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{liu2022_neuray}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{liu2022_tadtr}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{liu2022_videoswin}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{liu2022_swinv2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{liu2021_swin}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{li2020_tanet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lombardi2019_neuralvolumes}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{long2015_fcn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lorensen1987_marchingcubes}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{loshchilov2019_adamw}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lowe1987_objectrecognition}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lowe1999_sift}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lu2022_dpm_solver}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lu2021_omnimatte}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lu2023_omnimatterf}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lu2023_chameleon}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lu2022_scienceqa}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lucic2018_ganstudy}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{lucic2019_selfgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{luo2022_diffusiontutorial}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{luo2022_clip4clip}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{luo2025_valley}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{luo2017_understanding_receptive_field}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ma2022_xclip}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ma2018_shufflenetv2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{maaten2008_tsne}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{maaz2024_video_chatgpt}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{madry2018_towards}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{Mahadi2024_GRU_Plasmonic}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{turki2022_meganerf}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mangalam2023_egoschema}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{maninis2025_tips}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mao2017_lsgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{marr1982_vision}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{martinbrualla2021_nerfw}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mccann1997_convexity}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{anonymous2021_nt_xent}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mei2016_listenwalk}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{meng2022_sde}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{meng2025_pissa}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mescheder2018_r1regularization}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mescheder2019_occupancy}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hedman2018_nrwinwild}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{metzer2022_latentnerf}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mildenhall2019_llff}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mildenhall2020_nerf}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mildenhall2022_rawnerf}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{minderer2022_owlvit}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{minsky1969_perceptrons}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mirza2014_cgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{misra2019_pirl}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mitrovic2020_relic}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{miyato2018_spectralnorm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mo2019_partnet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{moosavi2017_universal}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mordvintsev2015_deepdream}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mou2023_t2iadapter}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mueller2022_instantngp}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{naeem2023_silc}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{nakkiran2020_deep_double_descent}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{neimark2021_vtn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{nguyen2016_multifaceted}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{nguyen2023_boss}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{nichol2021_improvedddpm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{nichol2022_glide}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{niemeyer2020_dvr}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{niemeyer2022_regnerf}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{nilsson2020_understanding_ssim}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{noh2015_deconvnet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{oechsle2021_unisurf}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{oh2019_stm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{oktay2018_attentionunet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{oord2016_pixernn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{oord2019_representation}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{oord2018_neural_discrete}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{oord2018_vqvae}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{openai2023b_gpt4v}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{oquab2023_dinov2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{vinyals2015_captioning}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pan2021_acar}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pan2021_videomoco}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{park2019_deepsdf}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{park2021_hypernerf}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{park2021_nerfies}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{park2019_spade}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pascanu2013_difficulty}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pathak2016_context}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{patnaik2020_roi_pool}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{patraucean2023_perceptiontest}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{patrick2021_motionformer}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{patrick2021_mformer}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pearl2009_causality}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{peebles2023_dit}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{perez2017_film}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{perko2013_differential}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pinaya2021_pixelcnn_blindspot}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pizzi2022_sscd}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{podell2023_sdxl}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{poggio2017_theory}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{polyak1992_averagegradient}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ponttuset2017_davis}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pooladian2023_msfm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{poole2022_dreamfusion}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{pumarola2021_dnerf}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{qi2017_pointnet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{qi2017_pointnetplusplus}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{qian2022_pointnext}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{qian2021_cvrl}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{qwen2025_qwen25technicalreport}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{radford2016_dcgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{radford2019_language}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{clip2021_multimodal}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{radford2021_clip}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{radosavovic2020_regnet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{radosavovic2020_dnds}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{raffel2020_t5}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{rahaman2019_spectralbias}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ramachandran2017_searching}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ramachandran2017_swish}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ramasinghe2022_coordmlp}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{dalle2021_texttoimage}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ramesh2022_dalle2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ramesh2021_dalle}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ranftl2021_dpt}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ranftl2022_midas}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ravi2024_sam2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{razavi2019_vqvae2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{recasens2021_broaden}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{redmon2017_yolo9000}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{redmon2018_yolov3}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{redmon2016_yolo}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{reed2016_ganintcls}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{reed2016_gawnn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{reizenstein2021_co3d}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ren2015_fasterrcnn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ren2016_fasterrcnn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ren2024_groundedsam}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{revaud2019_aploss}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{rezatofighi2019_giou}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{richemond2020_byol_no_batch}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{roberts1963_3dsolids}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{rojas2024_sassl}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{rombach2022_ldm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ronneberger2015_unet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{rosenblatt1958_perceptron}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{Rosinol_2024_SplaTAM}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{rother2004_grabcut}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{rudin1976_real}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ruiz2023_dreambooth}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{rumelhart1986_backpropagation}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sage2018_logogan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{saharia2022_imagen}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sajjadi2018_precision}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{salimans2022_progressive}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{salimans2016_improved}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{salimans2017_pixelcnnpp}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sam2_repo}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{dieleman2014_galaxycnn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sandler2018_mobilenetv2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sanghi2022_clipmesh}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{santurkar2018_howdoesbatchnormhelp}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{santurkar2019_howdoesbatchnormhelp}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sauer2022_styleganxl}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{saxe2014_exact}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{schonberger2016_structure}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{schroff2015_facenet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{schuhmann2021_laion}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{schuhmann2022_laion5b}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{blog2023_separable_convolutions}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{seitz2006_multiview}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{selvaraju2017_gradcam}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{shao2023_answer_heuristics}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{shaw2018selfrelative_pos}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{shi1997_normalizedcuts}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{shi2016_espcn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{shin2018_pixelsvoxelsviews}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{shuttleworth2024_loraillusion}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ksimek2013_intrinsic}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{simonyan2014_deepinside}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{simonyan2014_twostream}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{simonyan2014_vgg}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sitzmann2019_srn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{smith2019_geometric}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{smith2017_clr}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{smith2018_superconvergence}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{snavely2008_photo}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sohl2015_diffusion}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{solai2023_backpropconv}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{song2020_ddim}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{song2023_consistency}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{cvpr2022_diffusion_tutorial}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{song2021_sde}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{springenberg2015_allconv}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{srivastava2014_dropout}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{srivastava2015_training}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sd2022_variations}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sd2022_unclip}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{steiner2021_how_to_train_vit}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{janestreet_l2_bn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sun2022_directvoxelgrid}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sun2022_directvoxelgridv2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sun2018_pix3d}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sun2023_nms_strikes_back}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sutskever2014_seq2seq}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{szegedy2015_googlenet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{taigman2014_deepface}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tan2021_vimpac}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tan2021_efficientnetv2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tan2019_efficientnet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tan2019_mnasnet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tancik2022_blocknerf}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tancik2020_fourier}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tancik2020_fourierfeatures}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tancik2023_nerfacto}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tang2023_dreamfields}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{Tang_2023_DreamGaussian}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tao2022_dfgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tatarchenko2017_ogn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tatarchenko2015_tco}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tatarchenko2019_singleview}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{deepfloyd2023_if}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tensorflow2020_efficientnetlite}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{telgarsky2016_benefits}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tian2024_var}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tian2021_understanding}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tian2019_fcos}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tokmakov2022_vost}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tolstikhin2021_mlpmixer}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tomasev2022_relicv2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tong2020_otflow}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tong2022_videomae}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{toshev2014pose_estimation}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{touvron2021_deit}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{touvron2022_deitiii}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{touvron2019_fixres}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{touvron2021_resmlp}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{carlini2020_adaptive}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tran2019_ipcsn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tran2018_closer}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tran2015_c3d}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{trevithick2021_grf}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{truong2023_sparf}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{sh-tsang2018_groupnorm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tsang2022byol_review}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tschannen2025_siglip2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{udandarao2025_acid}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{uijlings2013_selective}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ulyanov2017_instance}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{valipour2022_dylora}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{vantai2022_flowmatchingyt}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{vantai2023_trainingflows}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{vaswani2017_attention}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{villani2008_optimal}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{vincent2011_dsm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{vinyals2015_showtell}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{viola2001_boosteddetection}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{voita2019_analyzing_heads}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wah2011_cub}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wan2024_locca}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wan2013_dropconnect}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wang2021_ibrnet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wang2018_amsoftmax}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wang2020_corrnet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wang2022_allinone}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wang2022_git}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wang2022_bevt}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wang2022_omnivl}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wang2021_tdn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wang2023_videomaev2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wang2018_pixel2mesh}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wang2023_badnerf}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wang2021_neus}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wang2024_qwen2vl}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wang2023_mvd}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wang2021_uvo}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wang2022_internvideo}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wang2022_beit3}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{xiao2021_clsa}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wang2018_videograph}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wang2018_nonlocal_nn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wang2018_nonlocal}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wang2024_internvid}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wang2024_internvideo2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wang2025internvideo2_5}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{Wang_2024_GSIR}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wang2023_neus2}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wang2022_hf_neus}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wang2023_pet_neus}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wang2019_dgcnn}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wang2023_stylediffusion}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wang2021b_simvlm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wei2022_maskfeat}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wei2000_texture}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wen2019_pixel2meshplusplus}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{weng2024_longvlm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wiki_Aliasing}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wiki_sine_cosine}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{williams1992_simple}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wolf2019_proganblog}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{woo2018_cbam}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wu2022_memvit}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wu2020_msf}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wu2022_point}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{Wu_2023_4DGS}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wu2022_aiapp}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wu2015_shapenets}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{guo2014_atari}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{xie2019_unsupervised}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{xie2017_aggregated}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{xu2016_askattend}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{xu2015_showattend}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{xu2021_tcanet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{xu2020_rtdnet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{xu2018_youtubevos}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{xu2022_pointnerf}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{xu2018_attngan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{xu2023_promptfreediffusion}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{xu2024_versatile}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{xu2018_swf}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{xu2023_languagebind}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{yan2022_mtv}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{yan2023_videococa}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{yang2022_frozenbilm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{yang2024_samurai}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{yang2024_consistencyfm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{yang2019_youtube_vis}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{yao2022_filip}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{yao2022_improving}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{yariv2020_idr}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{yariv2021_volsdf}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ye2023_ipadapter}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{ye2024_mplug_owl}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{yenigun_overfitting}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{yi2019_gancyclegan_survey}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{yosinski2015_deepviz}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{you2017_lars}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{yu2020_pixelnerf}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{yu2021_plenoctrees}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{yu2022_coca}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{yu2022_parti}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{yu2022_vitvq}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{yu2024_gsdf}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{yu2022_s2mlp}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{yuan2021_florence}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{yue2020_v4d}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{yun2019_cutmix}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zakka2016_batchnorm}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zbontar2021_barlow}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zeiler2014_visualizing}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zellers2022_merlotreserve}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zellers2021_merlot}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zeng2021_dcan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhai2019_largescale}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhai2023_siglip}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2025_videollama3}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2022_actionformer}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2022_tallformer}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2023_adalora}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2019_sagan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2019_self_attention_gan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2017_stackgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2018_stackganpp}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2023_videollama}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2020_resnest}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2019_fixup}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2018_mixup}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2020_nerfplusplus}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2023_controlnet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2021_vinvl}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2023_llama_adapter}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2019_blurpool}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2016_colorful}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2018_lpips}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2015_mvs_illum}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2018_shufflenet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{Zhang_2024_SuGaR}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2023_inst}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2018_resunet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2001_shape_motion}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhang2024_mcot}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhao2022_tuber}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhao2019_hacs}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhao2021_point}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhao2025_videoprism}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhao2023_unicontrolnet}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhao2022_lavila}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhou2016_cam}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhou2017_places}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhou2024_transfusion}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhou2022_ibot}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhou2022_prompt}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhou2019_unifiedvqa}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhou2018_unetpp}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhu2023_minigpt4}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhu2023_re_detr}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhu2017_cyclegan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhu2019_dmgan}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhu2021_daotad}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhu2023_transfusion}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zhu2024_chameleon}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zitnick2014_edgeboxes}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zoph2017_nas}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zoph2018_learning}{nty/global//global/global/global}
\abx@aux@defaultrefcontext{0}{zou2022_xdecoder}{nty/global//global/global/global}
\xdef \mintedoldcachechecksum{\detokenize{D19A8BA9AB41230E0F47C5453CA0B786:85}}
\gdef \@abspage@last{1951}
