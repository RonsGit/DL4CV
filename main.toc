\babel@toc {english}{}\relax 
\contentsline {chapter}{Preface}{28}{chapter*.2}%
\contentsline {section}{\numberline {0.1}Getting Started: About the Project and How to Navigate It}{28}{section.0.1}%
\contentsline {subsection}{\numberline {0.1.1}Why This Document?}{28}{subsection.0.1.1}%
\contentsline {subsection}{\numberline {0.1.2}Your Feedback Matters}{29}{subsection.0.1.2}%
\contentsline {subsection}{\numberline {0.1.3}How to Use This Document Effectively}{29}{subsection.0.1.3}%
\contentsline {subsection}{\numberline {0.1.4}Staying Updated in the Field}{30}{subsection.0.1.4}%
\contentsline {subsection}{\numberline {0.1.5}Dependency Tree}{31}{subsection.0.1.5}%
\contentsline {subsection}{\numberline {0.1.6}Contributors}{32}{subsection.0.1.6}%
\contentsline {paragraph}{Named contributors}{32}{section*.4}%
\contentsline {paragraph}{Community feedback}{32}{section*.5}%
\contentsline {subsection}{\numberline {0.1.7}The Importance of Practice}{33}{subsection.0.1.7}%
\contentsline {subsection}{\numberline {0.1.8}Final Remarks}{33}{subsection.0.1.8}%
\contentsline {chapter}{\numberline {1}Lecture 1: Course Introduction}{34}{chapter.1}%
\contentsline {section}{\numberline {1.1}Core Terms in the Field}{34}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}Artificial Intelligence (AI)}{34}{subsection.1.1.1}%
\contentsline {subsection}{\numberline {1.1.2}Machine Learning (ML)}{34}{subsection.1.1.2}%
\contentsline {subsection}{\numberline {1.1.3}Deep Learning (DL)}{35}{subsection.1.1.3}%
\contentsline {subsection}{\numberline {1.1.4}Computer Vision (CV)}{36}{subsection.1.1.4}%
\contentsline {subsection}{\numberline {1.1.5}Connecting the Dots}{36}{subsection.1.1.5}%
\contentsline {section}{\numberline {1.2}Why Study Deep Learning for Computer Vision?}{36}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}Motivation for Deep Learning in Computer Vision}{37}{subsection.1.2.1}%
\contentsline {section}{\numberline {1.3}Historical Milestones}{37}{section.1.3}%
\contentsline {subsection}{\numberline {1.3.1}Hubel and Wiesel (1959): How Vision Works?}{38}{subsection.1.3.1}%
\contentsline {subsection}{\numberline {1.3.2}Larry Roberts (1963): From Edges to Keypoints}{38}{subsection.1.3.2}%
\contentsline {subsection}{\numberline {1.3.3}David Marr (1970s): From Features to a 3D Model}{39}{subsection.1.3.3}%
\contentsline {subsection}{\numberline {1.3.4}Recognition via Parts (1970s)}{39}{subsection.1.3.4}%
\contentsline {subsection}{\numberline {1.3.5}Recognition via Edge Detection (1980s)}{40}{subsection.1.3.5}%
\contentsline {subsection}{\numberline {1.3.6}Recognition via Grouping (1990s)}{41}{subsection.1.3.6}%
\contentsline {subsection}{\numberline {1.3.7}Recognition via Matching and Benchmarking (2000s)}{42}{subsection.1.3.7}%
\contentsline {subsection}{\numberline {1.3.8}The ImageNet Dataset and Classification Challenge}{44}{subsection.1.3.8}%
\contentsline {subsection}{\numberline {1.3.9}AlexNet: A Revolution in Computer Vision (2012)}{45}{subsection.1.3.9}%
\contentsline {subsubsection}{Building on AlexNet: Evolution of CNNs and Beyond}{45}{section*.19}%
\contentsline {section}{\numberline {1.4}Milestones in the Evolution of Learning in Computer Vision}{47}{section.1.4}%
\contentsline {subsection}{\numberline {1.4.1}The Perceptron (1958)}{47}{subsection.1.4.1}%
\contentsline {subsection}{\numberline {1.4.2}The AI Winter and Multilayer Perceptrons (1969)}{48}{subsection.1.4.2}%
\contentsline {subsection}{\numberline {1.4.3}The Neocognitron (1980)}{49}{subsection.1.4.3}%
\contentsline {subsection}{\numberline {1.4.4}Backpropagation and the Revival of Neural Networks (1986)}{49}{subsection.1.4.4}%
\contentsline {subsection}{\numberline {1.4.5}LeNet and the Emergence of Convolutional Networks (1998)}{50}{subsection.1.4.5}%
\contentsline {subsection}{\numberline {1.4.6}The 2000s: The Era of Deep Learning}{50}{subsection.1.4.6}%
\contentsline {subsection}{\numberline {1.4.7}Deep Learning Explosion (2007-2020)}{51}{subsection.1.4.7}%
\contentsline {subsection}{\numberline {1.4.8}2012 to Present: Deep Learning is Everywhere}{51}{subsection.1.4.8}%
\contentsline {subsubsection}{Core Vision Tasks}{51}{section*.27}%
\contentsline {subsubsection}{Video and Temporal Analysis}{51}{section*.28}%
\contentsline {subsubsection}{Generative and Multimodal Models}{52}{section*.29}%
\contentsline {subsubsection}{Specialized Domains}{52}{section*.30}%
\contentsline {subsubsection}{State-of-the-Art Foundation Models}{52}{section*.31}%
\contentsline {subsubsection}{Computation is Cheaper: More GFLOPs per Dollar}{53}{section*.34}%
\contentsline {section}{\numberline {1.5}Key Challenges in CV and Future Directions}{55}{section.1.5}%
\contentsline {chapter}{\numberline {2}Lecture 2: Image Classification}{57}{chapter.2}%
\contentsline {section}{\numberline {2.1}Introduction to Image Classification}{57}{section.2.1}%
\contentsline {section}{\numberline {2.2}Image Classification Challenges}{58}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}The Semantic Gap}{58}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}Robustness to Camera Movement}{58}{subsection.2.2.2}%
\contentsline {subsection}{\numberline {2.2.3}Intra-Class Variation}{59}{subsection.2.2.3}%
\contentsline {subsection}{\numberline {2.2.4}Fine-Grained Classification}{59}{subsection.2.2.4}%
\contentsline {subsection}{\numberline {2.2.5}Background Clutter}{60}{subsection.2.2.5}%
\contentsline {subsection}{\numberline {2.2.6}Illumination Changes}{60}{subsection.2.2.6}%
\contentsline {subsection}{\numberline {2.2.7}Deformation and Object Scale}{61}{subsection.2.2.7}%
\contentsline {subsection}{\numberline {2.2.8}Occlusions}{61}{subsection.2.2.8}%
\contentsline {subsection}{\numberline {2.2.9}Summary of Challenges}{62}{subsection.2.2.9}%
\contentsline {section}{\numberline {2.3}Image Classification as a Building Block for Other Tasks}{62}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Object Detection}{63}{subsection.2.3.1}%
\contentsline {subsection}{\numberline {2.3.2}Image Captioning}{64}{subsection.2.3.2}%
\contentsline {subsection}{\numberline {2.3.3}Decision-Making in Board Games}{65}{subsection.2.3.3}%
\contentsline {subsection}{\numberline {2.3.4}Summary: Leveraging Image Classification}{66}{subsection.2.3.4}%
\contentsline {section}{\numberline {2.4}Constructing an Image Classifier}{66}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1}Feature-Based Image Classification: The Classical Approach}{66}{subsection.2.4.1}%
\contentsline {subsection}{\numberline {2.4.2}From Hand-Crafted Rules to Data-Driven Learning}{67}{subsection.2.4.2}%
\contentsline {subsection}{\numberline {2.4.3}Programming with Data: The Modern Paradigm}{68}{subsection.2.4.3}%
\contentsline {subsection}{\numberline {2.4.4}Data-Driven Machine Learning: The New Frontier}{68}{subsection.2.4.4}%
\contentsline {section}{\numberline {2.5}Datasets in Image Classification}{69}{section.2.5}%
\contentsline {subsection}{\numberline {2.5.1}MNIST: The Toy Dataset}{69}{subsection.2.5.1}%
\contentsline {subsection}{\numberline {2.5.2}CIFAR: Real-World Object Recognition}{69}{subsection.2.5.2}%
\contentsline {subsection}{\numberline {2.5.3}ImageNet: The Gold Standard}{71}{subsection.2.5.3}%
\contentsline {subsection}{\numberline {2.5.4}MIT Places: Scene Recognition}{72}{subsection.2.5.4}%
\contentsline {subsection}{\numberline {2.5.5}Comparing Dataset Sizes}{72}{subsection.2.5.5}%
\contentsline {subsection}{\numberline {2.5.6}Omniglot: Few-Shot Learning}{73}{subsection.2.5.6}%
\contentsline {subsection}{\numberline {2.5.7}Conclusion: Datasets Driving Progress}{73}{subsection.2.5.7}%
\contentsline {section}{\numberline {2.6}Nearest Neighbor Classifier: A Gateway to Understanding Classification}{74}{section.2.6}%
\contentsline {subsection}{\numberline {2.6.1}Why Begin with Nearest Neighbor?}{74}{subsection.2.6.1}%
\contentsline {subsection}{\numberline {2.6.2}Setting the Stage: From Pixels to Predictions}{74}{subsection.2.6.2}%
\contentsline {subsection}{\numberline {2.6.3}Algorithm Description}{74}{subsection.2.6.3}%
\contentsline {subsection}{\numberline {2.6.4}Distance Metrics: The Core of Nearest Neighbor}{75}{subsection.2.6.4}%
\contentsline {subsection}{\numberline {2.6.5}Extending Nearest Neighbor: Applications Beyond Images}{78}{subsection.2.6.5}%
\contentsline {subsubsection}{Using Nearest Neighbor for Non-Image Data}{78}{section*.70}%
\contentsline {subsubsection}{Academic Paper Recommendation Example}{78}{section*.71}%
\contentsline {subsubsection}{Key Insights}{79}{section*.73}%
\contentsline {subsection}{\numberline {2.6.6}Hyperparameters in Nearest Neighbor}{79}{subsection.2.6.6}%
\contentsline {subsection}{\numberline {2.6.7}Cross-Validation}{80}{subsection.2.6.7}%
\contentsline {subsection}{\numberline {2.6.8}Implementation and Complexity}{82}{subsection.2.6.8}%
\contentsline {subsection}{\numberline {2.6.9}Visualization of Decision Boundaries}{83}{subsection.2.6.9}%
\contentsline {subsection}{\numberline {2.6.10}Improvements: k-Nearest Neighbors}{83}{subsection.2.6.10}%
\contentsline {subsection}{\numberline {2.6.11}Limitations and Universal Approximation}{84}{subsection.2.6.11}%
\contentsline {subsection}{\numberline {2.6.12}Using CNN Features for Nearest Neighbor Classification}{85}{subsection.2.6.12}%
\contentsline {subsection}{\numberline {2.6.13}Conclusion: From Nearest Neighbor to Advanced ML Frontiers}{87}{subsection.2.6.13}%
\contentsline {chapter}{\numberline {3}Lecture 3: Linear Classifiers}{88}{chapter.3}%
\contentsline {section}{\numberline {3.1}Linear Classifiers: A Foundation for Neural Networks}{88}{section.3.1}%
\contentsline {subsection}{Enrichment 3.1.1: Understanding the Role of Bias in Linear Classifiers}{91}{section*.87}%
\contentsline {paragraph}{Without Bias (\(b=0\)):}{91}{section*.88}%
\contentsline {paragraph}{With Bias (\(b = 3\)):}{91}{section*.89}%
\contentsline {subsection}{\numberline {3.1.2}A Toy Example: Grayscale Cat Image}{92}{subsection.3.1.2}%
\contentsline {subsection}{\numberline {3.1.3}The Bias Trick}{93}{subsection.3.1.3}%
\contentsline {section}{\numberline {3.2}Linear Classifiers: The Algebraic Viewpoint}{95}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Scaling Properties and Insights}{95}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2}From Algebra to Visual Interpretability}{96}{subsection.3.2.2}%
\contentsline {section}{\numberline {3.3}Linear Classifiers: The Visual Viewpoint}{96}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}Template Matching Perspective}{97}{subsection.3.3.1}%
\contentsline {subsection}{\numberline {3.3.2}Interpreting Templates}{98}{subsection.3.3.2}%
\contentsline {subsection}{\numberline {3.3.3}Python Code Example: Visualizing Learned Templates}{98}{subsection.3.3.3}%
\contentsline {subsection}{\numberline {3.3.4}Template Limitations: Multiple Modes}{99}{subsection.3.3.4}%
\contentsline {subsection}{\numberline {3.3.5}Looking Ahead}{99}{subsection.3.3.5}%
\contentsline {section}{\numberline {3.4}Linear Classifiers: The Geometric Viewpoint}{99}{section.3.4}%
\contentsline {subsection}{\numberline {3.4.1}Images as High-Dimensional Points}{99}{subsection.3.4.1}%
\contentsline {subsection}{\numberline {3.4.2}Limitations of Linear Classifiers}{100}{subsection.3.4.2}%
\contentsline {subsection}{\numberline {3.4.3}Historical Context: The Perceptron and XOR Limitation}{101}{subsection.3.4.3}%
\contentsline {subsection}{\numberline {3.4.4}Challenges of High-Dimensional Geometry}{101}{subsection.3.4.4}%
\contentsline {section}{\numberline {3.5}Summary: Shortcomings of Linear Classifiers}{102}{section.3.5}%
\contentsline {subsection}{\numberline {3.5.1}Algebraic Viewpoint}{102}{subsection.3.5.1}%
\contentsline {subsection}{\numberline {3.5.2}Visual Viewpoint}{102}{subsection.3.5.2}%
\contentsline {subsection}{\numberline {3.5.3}Geometric Viewpoint}{102}{subsection.3.5.3}%
\contentsline {subsection}{\numberline {3.5.4}Conclusion: Linear Classifiers Aren't Enough}{102}{subsection.3.5.4}%
\contentsline {subsection}{\numberline {3.5.5}Choosing the Weights for Linear Classifiers}{102}{subsection.3.5.5}%
\contentsline {section}{\numberline {3.6}Loss Functions}{103}{section.3.6}%
\contentsline {subsection}{\numberline {3.6.1}Core Requirements for Loss Functions}{103}{subsection.3.6.1}%
\contentsline {subsection}{\numberline {3.6.2}Desirable Properties (Depending on the Task)}{103}{subsection.3.6.2}%
\contentsline {subsection}{\numberline {3.6.3}Cross-Entropy Loss}{104}{subsection.3.6.3}%
\contentsline {subsubsection}{Softmax Function}{104}{section*.100}%
\contentsline {paragraph}{Advanced Note: Boltzmann Perspective.}{104}{section*.101}%
\contentsline {subsubsection}{Loss Computation}{104}{section*.102}%
\contentsline {paragraph}{Example: CIFAR-10 Image Classification}{104}{section*.103}%
\contentsline {paragraph}{Properties of Cross-Entropy Loss}{105}{section*.105}%
\contentsline {subsubsection}{Why "Cross-Entropy"?}{105}{section*.106}%
\contentsline {subsubsection}{Enrichment 3.6.3.1: Why Cross-Entropy Uses Logarithms, Not Squared Errors}{106}{section*.107}%
\contentsline {subsection}{\numberline {3.6.4}Multiclass SVM Loss}{109}{subsection.3.6.4}%
\contentsline {subsubsection}{Loss Definition}{109}{section*.108}%
\contentsline {subsubsection}{Example Computation}{109}{section*.109}%
\contentsline {paragraph}{Loss for the Cat Image}{110}{section*.110}%
\contentsline {paragraph}{Loss for the Car Image}{110}{section*.112}%
\contentsline {paragraph}{Loss for the Frog Image}{111}{section*.114}%
\contentsline {paragraph}{Total Loss}{112}{section*.116}%
\contentsline {subsubsection}{Key Questions and Insights}{112}{section*.118}%
\contentsline {subsection}{\numberline {3.6.5}Comparison of Cross-Entropy and Multiclass SVM Losses}{112}{subsection.3.6.5}%
\contentsline {subsubsection}{Debugging with Initial Loss Values}{113}{section*.120}%
\contentsline {subsubsection}{Conclusion: SVM, Cross Entropy, and the Evolving Landscape of Loss Functions}{114}{section*.121}%
\contentsline {subsection}{Enrichment 3.6.6: Additive Margin Softmax (AM-Softmax)}{115}{section*.122}%
\contentsline {subsubsection}{Motivation}{115}{section*.123}%
\contentsline {paragraph}{Why angles on a hypersphere}{115}{section*.124}%
\contentsline {paragraph}{A-Softmax (SphereFace): multiplicative angular margin and its pitfalls}{115}{section*.125}%
\contentsline {paragraph}{A-Softmax in detail: multiplicative transform $m$ and blending $\lambda $}{116}{section*.126}%
\contentsline {subsubsection}{Method}{117}{section*.128}%
\contentsline {paragraph}{Normalization and rescaling}{117}{section*.129}%
\contentsline {paragraph}{AM-Softmax in detail: subtracting a fixed cosine margin $\boldsymbol {m}$}{117}{section*.130}%
\contentsline {paragraph}{A-Softmax versus AM-Softmax at a glance}{118}{section*.131}%
\contentsline {paragraph}{Why it helps and how it looks in feature space}{118}{section*.133}%
\contentsline {paragraph}{Gradient behavior and the role of normalization}{119}{section*.135}%
\contentsline {paragraph}{Implementation pattern}{119}{section*.137}%
\contentsline {paragraph}{Benchmarks and observations}{120}{section*.139}%
\contentsline {paragraph}{Good fit}{121}{section*.144}%
\contentsline {paragraph}{Limitations and cautions}{121}{section*.145}%
\contentsline {paragraph}{Practical notes on tuning $s$ and $m$}{122}{section*.146}%
\contentsline {chapter}{\numberline {4}Lecture 4: Regularization \& Optimization}{123}{chapter.4}%
\contentsline {section}{\numberline {4.1}Introduction to Regularization}{123}{section.4.1}%
\contentsline {subsection}{\numberline {4.1.1}How Regularization is Used?}{124}{subsection.4.1.1}%
\contentsline {subsection}{\numberline {4.1.2}Regularization: Simpler Models Are Preferred}{124}{subsection.4.1.2}%
\contentsline {section}{\numberline {4.2}Types of Regularization: L1 and L2}{125}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}L1 Regularization (Lasso)}{125}{subsection.4.2.1}%
\contentsline {subsection}{\numberline {4.2.2}L2 Regularization (Ridge)}{125}{subsection.4.2.2}%
\contentsline {subsection}{\numberline {4.2.3}Choosing Between L1 and L2 Regularization}{126}{subsection.4.2.3}%
\contentsline {subsection}{Enrichment 4.2.4: Can We Combine L1 and L2 Regularization?}{126}{section*.148}%
\contentsline {paragraph}{When to Use Elastic Net?}{127}{section*.149}%
\contentsline {paragraph}{When Not to Use Elastic Net?}{127}{section*.150}%
\contentsline {paragraph}{Summary:}{127}{section*.151}%
\contentsline {subsection}{\numberline {4.2.5}Expressing Preferences Through Regularization}{127}{subsection.4.2.5}%
\contentsline {section}{\numberline {4.3}Impact of Feature Scaling on Regularization}{128}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1}Practical Implication}{128}{subsection.4.3.1}%
\contentsline {subsection}{\numberline {4.3.2}Example: Rescaling and Lasso Regression.}{128}{subsection.4.3.2}%
\contentsline {section}{\numberline {4.4}Regularization as a Catalyst for Better Optimization}{128}{section.4.4}%
\contentsline {subsection}{\numberline {4.4.1}Regularization as Part of Optimization}{128}{subsection.4.4.1}%
\contentsline {subsection}{\numberline {4.4.2}Augmenting the Loss Surface with Curvature}{128}{subsection.4.4.2}%
\contentsline {subsection}{\numberline {4.4.3}Mitigating Instability in High Dimensions}{129}{subsection.4.4.3}%
\contentsline {subsection}{\numberline {4.4.4}Improving Conditioning for Faster Convergence}{129}{subsection.4.4.4}%
\contentsline {section}{\numberline {4.5}Optimization: Traversing the Loss Landscape}{129}{section.4.5}%
\contentsline {subsection}{\numberline {4.5.1}The Loss Landscape Intuition}{130}{subsection.4.5.1}%
\contentsline {subsection}{Enrichment 4.5.2: Why Explicit Analytical Solutions Are Often Impractical}{130}{section*.154}%
\contentsline {subsubsection}{Enrichment 4.5.2.1: High Dimensionality}{130}{section*.155}%
\contentsline {subsubsection}{Enrichment 4.5.2.2: Non-Convexity of the Loss Landscape}{130}{section*.156}%
\contentsline {subsubsection}{Enrichment 4.5.2.3: Complexity of Regularization Terms}{131}{section*.157}%
\contentsline {subsubsection}{Enrichment 4.5.2.4: Lack of Generalizability and Flexibility}{131}{section*.158}%
\contentsline {subsubsection}{Enrichment 4.5.2.5: Memory and Computational Cost}{131}{section*.159}%
\contentsline {subsection}{\numberline {4.5.3}Optimization Idea \#1: Random Search}{132}{subsection.4.5.3}%
\contentsline {subsection}{\numberline {4.5.4}Optimization Idea \#2: Following the Slope}{132}{subsection.4.5.4}%
\contentsline {subsection}{\numberline {4.5.5}Gradients: The Mathematical Basis}{133}{subsection.4.5.5}%
\contentsline {subsubsection}{Why Does the Gradient Point to the Steepest Ascent?}{133}{section*.162}%
\contentsline {subsubsection}{Why Does the Negative Gradient Indicate the Steepest Descent?}{134}{section*.163}%
\contentsline {section}{\numberline {4.6}From Gradient Computation to Gradient Descent}{135}{section.4.6}%
\contentsline {subsection}{\numberline {4.6.1}Gradient Computation Methods}{135}{subsection.4.6.1}%
\contentsline {subsubsection}{Numerical Gradient: Approximating Gradients via Finite Differences}{135}{section*.165}%
\contentsline {paragraph}{Process:}{135}{section*.166}%
\contentsline {paragraph}{Advantages:}{136}{section*.168}%
\contentsline {paragraph}{Disadvantages:}{136}{section*.169}%
\contentsline {subsubsection}{Analytical Gradient: Exact Gradients via Calculus}{136}{section*.170}%
\contentsline {paragraph}{Advantages:}{136}{section*.172}%
\contentsline {paragraph}{Relation to Gradient Descent:}{136}{section*.173}%
\contentsline {subsection}{\numberline {4.6.2}Gradient Descent: The Iterative Optimization Algorithm}{137}{subsection.4.6.2}%
\contentsline {subsubsection}{Motivation and Concept}{137}{section*.174}%
\contentsline {paragraph}{Steps of Gradient Descent:}{137}{section*.175}%
\contentsline {subsubsection}{Hyperparameters of Gradient Descent}{137}{section*.177}%
\contentsline {paragraph}{1. Learning Rate (\( \eta \)):}{137}{section*.178}%
\contentsline {paragraph}{2. Weight Initialization:}{137}{section*.179}%
\contentsline {paragraph}{3. Stopping Criterion:}{138}{section*.180}%
\contentsline {section}{\numberline {4.7}Visualizing Gradient Descent}{138}{section.4.7}%
\contentsline {subsection}{\numberline {4.7.1}Understanding Gradient Descent Through Visualization}{138}{subsection.4.7.1}%
\contentsline {subsection}{\numberline {4.7.2}Properties of Gradient Descent}{138}{subsection.4.7.2}%
\contentsline {subsubsection}{Curved Paths Toward the Minimum}{138}{section*.182}%
\contentsline {subsubsection}{Slowing Down Near the Minimum}{139}{section*.183}%
\contentsline {subsection}{\numberline {4.7.3}Why Gradient Descent Moves \emph {All} Parameters Together}{139}{subsection.4.7.3}%
\contentsline {paragraph}{The gradient is one \( d \)-dimensional arrow}{139}{section*.184}%
\contentsline {paragraph}{Axis-aligned moves may crawl or diverge}{139}{section*.185}%
\contentsline {paragraph}{When does coordinate descent shine?}{140}{section*.186}%
\contentsline {paragraph}{Take-away}{140}{section*.187}%
\contentsline {subsection}{\numberline {4.7.4}Batch Gradient Descent}{140}{subsection.4.7.4}%
\contentsline {section}{\numberline {4.8}Stochastic Gradient Descent (SGD)}{140}{section.4.8}%
\contentsline {subsection}{\numberline {4.8.1}Introduction to Stochastic Gradient Descent}{140}{subsection.4.8.1}%
\contentsline {subsubsection}{Minibatch Gradient Computation}{141}{section*.188}%
\contentsline {subsubsection}{Data Sampling and Epochs}{141}{section*.190}%
\contentsline {subsubsection}{Why "Stochastic"?}{142}{section*.191}%
\contentsline {subsection}{\numberline {4.8.2}Advantages and Challenges of SGD}{142}{subsection.4.8.2}%
\contentsline {subsubsection}{Advantages}{142}{section*.193}%
\contentsline {subsubsection}{Challenges of SGD}{142}{section*.194}%
\contentsline {paragraph}{High Condition Numbers}{142}{section*.195}%
\contentsline {paragraph}{Saddle Points and Local Minima}{143}{section*.197}%
\contentsline {paragraph}{Noisy Gradients}{143}{section*.199}%
\contentsline {subsection}{\numberline {4.8.3}Looking Ahead: Improving SGD}{144}{subsection.4.8.3}%
\contentsline {section}{\numberline {4.9}SGD with Momentum}{144}{section.4.9}%
\contentsline {subsection}{\numberline {4.9.1}Motivation}{144}{subsection.4.9.1}%
\contentsline {subsection}{\numberline {4.9.2}How SGD with Momentum Works}{144}{subsection.4.9.2}%
\contentsline {subsubsection}{Update Equations}{145}{section*.201}%
\contentsline {subsection}{\numberline {4.9.3}Intuition Behind Momentum}{145}{subsection.4.9.3}%
\contentsline {subsection}{\numberline {4.9.4}Benefits of Momentum}{146}{subsection.4.9.4}%
\contentsline {subsection}{\numberline {4.9.5}Downsides of Momentum}{147}{subsection.4.9.5}%
\contentsline {subsection}{\numberline {4.9.6}Nesterov Momentum: A Look-Ahead Strategy}{147}{subsection.4.9.6}%
\contentsline {subsubsection}{Overview}{147}{section*.205}%
\contentsline {subsubsection}{Mathematical Formulation}{147}{section*.206}%
\contentsline {subsubsection}{Motivation and Advantages}{148}{section*.208}%
\contentsline {subsubsection}{Reformulation for Practical Implementation}{148}{section*.209}%
\contentsline {subsubsection}{Comparison with SGD and SGD+Momentum}{148}{section*.210}%
\contentsline {subsubsection}{Limitations of Nesterov Momentum and the Need for Adaptivity}{149}{section*.211}%
\contentsline {paragraph}{Motivation for a Better Optimizer: AdaGrad}{150}{section*.212}%
\contentsline {section}{\numberline {4.10}AdaGrad: Adaptive Gradient Algorithm}{150}{section.4.10}%
\contentsline {subsection}{\numberline {4.10.1}How AdaGrad Works}{150}{subsection.4.10.1}%
\contentsline {paragraph}{Updating the Weight Matrix Components}{151}{section*.214}%
\contentsline {paragraph}{Why Does This Work?}{151}{section*.215}%
\contentsline {subsection}{\numberline {4.10.2}Advantages of AdaGrad}{151}{subsection.4.10.2}%
\contentsline {subsection}{\numberline {4.10.3}Disadvantages of AdaGrad}{151}{subsection.4.10.3}%
\contentsline {section}{\numberline {4.11}RMSProp: Root Mean Square Propagation}{152}{section.4.11}%
\contentsline {subsection}{\numberline {4.11.1}Motivation for RMSProp}{152}{subsection.4.11.1}%
\contentsline {subsection}{\numberline {4.11.2}How RMSProp Works}{152}{subsection.4.11.2}%
\contentsline {subsection}{\numberline {4.11.3}Updating the Weight Matrix Components}{152}{subsection.4.11.3}%
\contentsline {subsection}{\numberline {4.11.4}Advantages of RMSProp}{153}{subsection.4.11.4}%
\contentsline {subsection}{\numberline {4.11.5}Downsides of RMSProp}{153}{subsection.4.11.5}%
\contentsline {subsubsection}{No Momentum Carry-Over}{153}{section*.217}%
\contentsline {subsubsection}{Bias in Early Updates}{154}{section*.218}%
\contentsline {subsubsection}{Sensitivity to Hyperparameters}{154}{section*.219}%
\contentsline {subsection}{\numberline {4.11.6}Motivation for Adam, a SOTA Optimizer}{154}{subsection.4.11.6}%
\contentsline {section}{\numberline {4.12}Adam: Adaptive Moment Estimation}{155}{section.4.12}%
\contentsline {subsection}{\numberline {4.12.1}Motivation for Adam}{155}{subsection.4.12.1}%
\contentsline {subsection}{\numberline {4.12.2}How Adam Works}{155}{subsection.4.12.2}%
\contentsline {subsection}{\numberline {4.12.3}Bias Correction}{156}{subsection.4.12.3}%
\contentsline {subsection}{\numberline {4.12.4}Why Adam Works Well in Practice}{157}{subsection.4.12.4}%
\contentsline {subsection}{\numberline {4.12.5}Comparison with Other Optimizers}{158}{subsection.4.12.5}%
\contentsline {subsection}{\numberline {4.12.6}Advantages of Adam}{158}{subsection.4.12.6}%
\contentsline {subsection}{\numberline {4.12.7}Limitations of Adam}{158}{subsection.4.12.7}%
\contentsline {paragraph}{Looking Ahead}{158}{section*.224}%
\contentsline {section}{\numberline {4.13}AdamW: Decoupling Weight Decay from L2 Regularization}{159}{section.4.13}%
\contentsline {subsection}{\numberline {4.13.1}Motivation for AdamW}{159}{subsection.4.13.1}%
\contentsline {subsection}{\numberline {4.13.2}How AdamW Works}{159}{subsection.4.13.2}%
\contentsline {subsection}{\numberline {4.13.3}Note on Weight Decay in AdamW}{160}{subsection.4.13.3}%
\contentsline {subsection}{\numberline {4.13.4}The AdamW Improvement}{160}{subsection.4.13.4}%
\contentsline {subsection}{\numberline {4.13.5}Advantages of AdamW}{161}{subsection.4.13.5}%
\contentsline {subsection}{\numberline {4.13.6}Why AdamW is the Default Optimizer}{161}{subsection.4.13.6}%
\contentsline {subsection}{\numberline {4.13.7}Limitations of AdamW}{161}{subsection.4.13.7}%
\contentsline {section}{\numberline {4.14}Second-Order Optimization}{161}{section.4.14}%
\contentsline {subsection}{\numberline {4.14.1}Overview of Second-Order Optimization}{161}{subsection.4.14.1}%
\contentsline {subsection}{\numberline {4.14.2}Quadratic Approximation Using the Hessian}{162}{subsection.4.14.2}%
\contentsline {subsection}{\numberline {4.14.3}Practical Challenges of Second-Order Methods}{162}{subsection.4.14.3}%
\contentsline {subsection}{\numberline {4.14.4}First-Order Methods Approximating Second-Order Behavior}{163}{subsection.4.14.4}%
\contentsline {subsection}{\numberline {4.14.5}Improving Second-Order Optimization: BFGS and L-BFGS}{163}{subsection.4.14.5}%
\contentsline {subsubsection}{BFGS: An Approximation of the Hessian Matrix}{164}{section*.230}%
\contentsline {subsubsection}{L-BFGS: Reducing Memory Requirements}{164}{section*.231}%
\contentsline {subsubsection}{Advantages and Limitations of BFGS and L-BFGS}{165}{section*.232}%
\contentsline {paragraph}{Advantages:}{165}{section*.233}%
\contentsline {paragraph}{Limitations:}{165}{section*.234}%
\contentsline {subsubsection}{Applications of L-BFGS}{165}{section*.235}%
\contentsline {subsection}{\numberline {4.14.6}Summary of Second-Order Optimization Approaches}{165}{subsection.4.14.6}%
\contentsline {chapter}{\numberline {5}Lecture 5: Neural Networks}{166}{chapter.5}%
\contentsline {section}{\numberline {5.1}Introduction to Neural Networks}{166}{section.5.1}%
\contentsline {subsection}{\numberline {5.1.1}Limitations of Linear Classifiers}{166}{subsection.5.1.1}%
\contentsline {subsection}{\numberline {5.1.2}Feature Transforms as a Solution}{166}{subsection.5.1.2}%
\contentsline {subsubsection}{Feature Transforms in Action}{166}{section*.236}%
\contentsline {subsection}{\numberline {5.1.3}Challenges in Manual Feature Transformations}{168}{subsection.5.1.3}%
\contentsline {subsection}{\numberline {5.1.4}Data-Driven Feature Transformations}{168}{subsection.5.1.4}%
\contentsline {subsection}{\numberline {5.1.5}Combining Multiple Representations}{169}{subsection.5.1.5}%
\contentsline {subsection}{\numberline {5.1.6}Real-World Example: 2011 ImageNet Winner}{169}{subsection.5.1.6}%
\contentsline {section}{\numberline {5.2}Neural Networks: The Basics}{170}{section.5.2}%
\contentsline {subsection}{\numberline {5.2.1}Motivation for Neural Networks}{170}{subsection.5.2.1}%
\contentsline {subsection}{\numberline {5.2.2}Fully Connected Networks}{171}{subsection.5.2.2}%
\contentsline {subsection}{\numberline {5.2.3}Interpreting Neural Networks}{172}{subsection.5.2.3}%
\contentsline {paragraph}{Network Pruning: Pruning Redundant Representations}{173}{section*.247}%
\contentsline {section}{\numberline {5.3}Building Neural Networks}{173}{section.5.3}%
\contentsline {subsection}{\numberline {5.3.1}Activation Functions: Non-Linear Bridges Between Layers}{174}{subsection.5.3.1}%
\contentsline {paragraph}{Why Non-Linearity Matters.}{174}{section*.250}%
\contentsline {subsection}{\numberline {5.3.2}A Simple Neural Network in 20 Lines}{175}{subsection.5.3.2}%
\contentsline {section}{\numberline {5.4}Biological Inspiration}{176}{section.5.4}%
\contentsline {subsection}{\numberline {5.4.1}Biological Analogy: a Loose Analogy}{177}{subsection.5.4.1}%
\contentsline {section}{\numberline {5.5}Space Warping: Another Motivation for Artificial Neural Networks}{177}{section.5.5}%
\contentsline {subsection}{\numberline {5.5.1}Linear Transformations and Their Limitations}{177}{subsection.5.5.1}%
\contentsline {subsection}{\numberline {5.5.2}Introducing Non-Linearity with ReLU}{178}{subsection.5.5.2}%
\contentsline {subsection}{\numberline {5.5.3}Making Data Linearly Separable}{179}{subsection.5.5.3}%
\contentsline {subsection}{\numberline {5.5.4}Scaling Up: Increasing Representation Power}{179}{subsection.5.5.4}%
\contentsline {subsection}{\numberline {5.5.5}Regularizing Neural Networks}{180}{subsection.5.5.5}%
\contentsline {section}{\numberline {5.6}Universal Approximation Theorem}{180}{section.5.6}%
\contentsline {subsection}{\numberline {5.6.1}Practical Context: The Bump Function}{180}{subsection.5.6.1}%
\contentsline {subsection}{\numberline {5.6.2}Questions for Further Exploration}{181}{subsection.5.6.2}%
\contentsline {subsection}{\numberline {5.6.3}Reality Check: Universal Approximation is Not Enough}{181}{subsection.5.6.3}%
\contentsline {subsection}{Enrichment 5.6.4: Deep Networks vs Shallow Networks}{182}{section*.263}%
\contentsline {subsubsection}{Enrichment 5.6.4.1: Why Not Just Use a Very Deep and Wide Network?}{183}{section*.264}%
\contentsline {section}{\numberline {5.7}Convex Functions: A Special Case}{183}{section.5.7}%
\contentsline {subsection}{\numberline {5.7.1}Non-Convex Functions}{184}{subsection.5.7.1}%
\contentsline {subsection}{\numberline {5.7.2}Convex Optimization in Linear Classifiers}{184}{subsection.5.7.2}%
\contentsline {subsection}{\numberline {5.7.3}Challenges with Neural Networks}{185}{subsection.5.7.3}%
\contentsline {subsection}{\numberline {5.7.4}Bridging to Backpropagation: Efficient Gradient Computation}{185}{subsection.5.7.4}%
\contentsline {chapter}{\numberline {6}Lecture 6: Backpropagation}{186}{chapter.6}%
\contentsline {section}{\numberline {6.1}Introduction: The Challenge of Computing Gradients}{186}{section.6.1}%
\contentsline {subsection}{\numberline {6.1.1}A Bad Idea: Manually Deriving Gradients}{187}{subsection.6.1.1}%
\contentsline {subsection}{\numberline {6.1.2}A Better Idea: Utilizing Computational Graphs (Backpropagation)}{187}{subsection.6.1.2}%
\contentsline {paragraph}{Why Use Computational Graphs?}{188}{section*.270}%
\contentsline {section}{\numberline {6.2}Toy Example of Backpropagation: $f(x,y,z) = (x + y)\,z$}{188}{section.6.2}%
\contentsline {subsection}{\numberline {6.2.1}Forward Pass}{189}{subsection.6.2.1}%
\contentsline {subsection}{\numberline {6.2.2}Backward Pass: Computing Gradients}{189}{subsection.6.2.2}%
\contentsline {section}{\numberline {6.3}Why Backpropagation?}{189}{section.6.3}%
\contentsline {subsection}{\numberline {6.3.1}Local \& Scalable Gradients Computation}{189}{subsection.6.3.1}%
\contentsline {subsection}{\numberline {6.3.2}Pairing Backpropagation Gradients \& Optimizers is Easy}{190}{subsection.6.3.2}%
\contentsline {subsection}{\numberline {6.3.3}Modularity and Custom Nodes}{191}{subsection.6.3.3}%
\contentsline {subsection}{\numberline {6.3.4}Utilizing Patterns in Gradient Flow}{191}{subsection.6.3.4}%
\contentsline {subsection}{\numberline {6.3.5}Addition Gate: The Gradient Distributor}{191}{subsection.6.3.5}%
\contentsline {subsection}{\numberline {6.3.6}Copy Gate: The Gradient Adder}{192}{subsection.6.3.6}%
\contentsline {subsection}{\numberline {6.3.7}Multiplication Gate: The Gradient Swapper}{192}{subsection.6.3.7}%
\contentsline {subsection}{\numberline {6.3.8}Max Gate: The Gradient Router}{192}{subsection.6.3.8}%
\contentsline {section}{\numberline {6.4}Implementing Backpropagation in Code}{193}{section.6.4}%
\contentsline {subsection}{\numberline {6.4.1}Flat Backpropagation: A Direct Approach}{194}{subsection.6.4.1}%
\contentsline {paragraph}{Why Flat Backpropagation Works Well.}{194}{section*.277}%
\contentsline {section}{\numberline {6.5}A More Modular Approach: Computational Graphs in Practice}{194}{section.6.5}%
\contentsline {subsection}{\numberline {6.5.1}Topological Ordering in Computational Graphs}{195}{subsection.6.5.1}%
\contentsline {subsection}{\numberline {6.5.2}The API: Forward and Backward Methods}{195}{subsection.6.5.2}%
\contentsline {subsection}{\numberline {6.5.3}Advantages of a Modular Computational Graph}{195}{subsection.6.5.3}%
\contentsline {section}{\numberline {6.6}Implementing Backpropagation with PyTorch Autograd}{196}{section.6.6}%
\contentsline {subsection}{\numberline {6.6.1}Example: Multiplication Gate in Autograd}{196}{subsection.6.6.1}%
\contentsline {subsection}{\numberline {6.6.2}Extending Autograd for Custom Functions}{196}{subsection.6.6.2}%
\contentsline {section}{\numberline {6.7}Beyond Scalars: Backpropagation for Vectors and Tensors}{197}{section.6.7}%
\contentsline {subsection}{\numberline {6.7.1}Gradients vs.\ Jacobians}{198}{subsection.6.7.1}%
\contentsline {subsection}{\numberline {6.7.2}Extending Backpropagation to Vectors}{198}{subsection.6.7.2}%
\contentsline {subsection}{\numberline {6.7.3}Example: Backpropagation for Elementwise ReLU}{199}{subsection.6.7.3}%
\contentsline {subsection}{\numberline {6.7.4}Efficient Computation via Local Gradient Slices}{201}{subsection.6.7.4}%
\contentsline {subsection}{\numberline {6.7.5}Backpropagation with Matrices: A Concrete Example}{201}{subsection.6.7.5}%
\contentsline {paragraph}{Numerical Setup.}{201}{section*.285}%
\contentsline {paragraph}{Slice Logic for One Input Element.}{202}{section*.287}%
\contentsline {paragraph}{Another Example: \(\mathbf {X}_{2,3}\).}{202}{section*.289}%
\contentsline {subsection}{\numberline {6.7.6}Implicit Multiplication for the Entire Gradient}{203}{subsection.6.7.6}%
\contentsline {paragraph}{Why Slices Are the Solution.}{204}{section*.292}%
\contentsline {subsection}{\numberline {6.7.7}A Chain View of Backpropagation}{204}{subsection.6.7.7}%
\contentsline {subsubsection}{Reverse-Mode Automatic Differentiation}{204}{section*.293}%
\contentsline {subsubsection}{Forward-Mode Automatic Differentiation}{205}{section*.295}%
\contentsline {paragraph}{When Is Forward-Mode Automatic Differentiation is Useful?}{205}{section*.297}%
\contentsline {subsection}{\numberline {6.7.8}Computing Higher-Order Derivatives with Backpropagation}{206}{subsection.6.7.8}%
\contentsline {paragraph}{Why Compute Hessians?}{206}{section*.299}%
\contentsline {paragraph}{Efficient Hessian Computation: Hessian-Vector Products}{206}{section*.300}%
\contentsline {subsection}{\numberline {6.7.9}Application: Gradient-Norm Regularization}{207}{subsection.6.7.9}%
\contentsline {subsection}{\numberline {6.7.10}Automatic Differentiation: Summary of Key Insights}{207}{subsection.6.7.10}%
\contentsline {chapter}{\numberline {7}Lecture 7: Convolutional Networks}{208}{chapter.7}%
\contentsline {section}{\numberline {7.1}Introduction: The Limitations of Fully-Connected Networks}{208}{section.7.1}%
\contentsline {section}{\numberline {7.2}Components of Convolutional Neural Networks}{209}{section.7.2}%
\contentsline {section}{\numberline {7.3}Convolutional Layers: Preserving Spatial Structure}{209}{section.7.3}%
\contentsline {subsection}{\numberline {7.3.1}Input and Output Dimensions}{210}{subsection.7.3.1}%
\contentsline {paragraph}{Common Filter Sizes}{210}{section*.305}%
\contentsline {paragraph}{Why Are Kernel Sizes Typically Odd?}{211}{section*.306}%
\contentsline {subsection}{\numberline {7.3.2}Filter Application and Output Calculation}{211}{subsection.7.3.2}%
\contentsline {subsection}{Enrichment 7.3.3: Understanding Convolution Through the Sobel Operator}{212}{section*.308}%
\contentsline {subsubsection}{Enrichment 7.3.3.1: Using the Sobel Kernel for Edge Detection}{212}{section*.310}%
\contentsline {paragraph}{Approximating Image Gradients with the Sobel Operator}{212}{section*.311}%
\contentsline {paragraph}{Basic Difference Operators}{213}{section*.312}%
\contentsline {paragraph}{The Sobel Filters: Adding Robustness}{213}{section*.313}%
\contentsline {subsubsection}{Enrichment 7.3.3.2: Why Does the Sobel Filter Use These Weights?}{213}{section*.314}%
\contentsline {subsubsection}{Enrichment 7.3.3.3: Computing the Gradient Magnitude}{213}{section*.315}%
\contentsline {paragraph}{Hands-On Exploration}{215}{section*.320}%
\contentsline {section}{Enrichment 7.4: Convolutional Layers with Multi-Channel Filters}{216}{section*.321}%
\contentsline {subsection}{Enrichment 7.4.1: Extending Convolution to Multi-Channel Inputs}{217}{section*.323}%
\contentsline {paragraph}{Multi-Channel Convolution Process}{218}{section*.326}%
\contentsline {paragraph}{Sliding the Filter Across the Image}{218}{section*.328}%
\contentsline {paragraph}{From Single Filters to Complete Convolutional Layers}{219}{section*.330}%
\contentsline {paragraph}{What Our Example Missed: Padding and Stride}{219}{section*.331}%
\contentsline {paragraph}{Are Kernel Values Restricted?}{219}{section*.332}%
\contentsline {paragraph}{Negative and Large Output Values}{219}{section*.333}%
\contentsline {subsection}{\numberline {7.4.2}Multiple Filters and Output Channels}{220}{subsection.7.4.2}%
\contentsline {subsection}{\numberline {7.4.3}Two Interpretations of Convolutional Outputs}{220}{subsection.7.4.3}%
\contentsline {subsection}{\numberline {7.4.4}Batch Processing with Convolutional Layers}{220}{subsection.7.4.4}%
\contentsline {section}{\numberline {7.5}Building Convolutional Neural Networks}{221}{section.7.5}%
\contentsline {subsection}{\numberline {7.5.1}Stacking Convolutional Layers}{221}{subsection.7.5.1}%
\contentsline {subsection}{\numberline {7.5.2}Adding Fully Connected Layers for Classification}{222}{subsection.7.5.2}%
\contentsline {subsection}{\numberline {7.5.3}The Need for Non-Linearity}{222}{subsection.7.5.3}%
\contentsline {subsection}{\numberline {7.5.4}Summary}{223}{subsection.7.5.4}%
\contentsline {section}{\numberline {7.6}Controlling Spatial Dimensions in Convolutional Layers}{223}{section.7.6}%
\contentsline {subsection}{\numberline {7.6.1}How Convolution Affects Spatial Size}{223}{subsection.7.6.1}%
\contentsline {subsection}{\numberline {7.6.2}Mitigating Shrinking Feature Maps: Padding}{224}{subsection.7.6.2}%
\contentsline {paragraph}{Choosing the Padding Size}{224}{section*.339}%
\contentsline {paragraph}{Preserving Border Information with Padding}{224}{section*.340}%
\contentsline {subsection}{\numberline {7.6.3}Receptive Fields: Understanding What Each Pixel Sees}{225}{subsection.7.6.3}%
\contentsline {paragraph}{The Problem of Limited Receptive Field Growth}{226}{section*.342}%
\contentsline {subsection}{\numberline {7.6.4}Controlling Spatial Reduction with Strides}{227}{subsection.7.6.4}%
\contentsline {section}{\numberline {7.7}Understanding What Convolutional Filters Learn}{227}{section.7.7}%
\contentsline {subsection}{\numberline {7.7.1}MLPs vs. CNNs: Learning Spatial Structure}{227}{subsection.7.7.1}%
\contentsline {subsection}{\numberline {7.7.2}Learning Local Features: The First Layer}{227}{subsection.7.7.2}%
\contentsline {subsection}{\numberline {7.7.3}Building More Complex Patterns in Deeper Layers}{228}{subsection.7.7.3}%
\contentsline {paragraph}{Hierarchical Learning via Composition}{228}{section*.346}%
\contentsline {section}{\numberline {7.8}Parameters and Computational Complexity in Convolutional Networks}{228}{section.7.8}%
\contentsline {subsection}{\numberline {7.8.1}Example: Convolutional Layer Setup}{229}{subsection.7.8.1}%
\contentsline {subsection}{\numberline {7.8.2}Output Volume Calculation}{229}{subsection.7.8.2}%
\contentsline {subsection}{\numberline {7.8.3}Number of Learnable Parameters}{229}{subsection.7.8.3}%
\contentsline {subsection}{\numberline {7.8.4}Multiply-Accumulate Operations (MACs)}{230}{subsection.7.8.4}%
\contentsline {paragraph}{MACs Calculation:}{230}{section*.348}%
\contentsline {subsection}{\numberline {7.8.5}MACs and FLOPs}{230}{subsection.7.8.5}%
\contentsline {subsection}{\numberline {7.8.6}Why Multiply-Add Operations (MACs) Matter}{230}{subsection.7.8.6}%
\contentsline {subsection}{Enrichment 7.8.7: Backpropagation for Convolutional Neural Networks}{230}{section*.349}%
\contentsline {paragraph}{Key Idea: Convolution as a Graph Node}{230}{section*.350}%
\contentsline {paragraph}{Computing \(\tfrac {dO}{dF}\)}{231}{section*.352}%
\contentsline {paragraph}{Computing \(\tfrac {dL}{dX}\)}{232}{section*.353}%
\contentsline {section}{Enrichment 7.9: Parameter Sharing in Convolutional Neural Networks}{233}{section*.355}%
\contentsline {subsection}{Enrichment 7.9.1: Parameter Sharing in CNNs vs. MLPs}{233}{section*.356}%
\contentsline {subsection}{Enrichment 7.9.2: Motivation for Parameter Sharing}{233}{section*.357}%
\contentsline {subsection}{Enrichment 7.9.3: How Parameter Sharing Works}{233}{section*.358}%
\contentsline {subsection}{Enrichment 7.9.4: When Does Parameter Sharing Not Make Complete Sense?}{233}{section*.359}%
\contentsline {subsection}{Enrichment 7.9.5: Alternative Approaches When Parameter Sharing Fails}{234}{section*.360}%
\contentsline {subsubsection}{Enrichment 7.9.5.1: Locally-Connected Layers}{234}{section*.361}%
\contentsline {subsubsection}{Enrichment 7.9.5.2: Understanding Locally-Connected Layers}{234}{section*.362}%
\contentsline {subsubsection}{Enrichment 7.9.5.3: Limitations of Locally-Connected Layers}{234}{section*.363}%
\contentsline {subsubsection}{Enrichment 7.9.5.4: Hybrid Approaches}{235}{section*.364}%
\contentsline {subsubsection}{Enrichment 7.9.5.5: A Glimpse at Attention Mechanisms}{235}{section*.365}%
\contentsline {section}{\numberline {7.10}Special Types of Convolutions: 1x1, 1D, and 3D Convolutions}{236}{section.7.10}%
\contentsline {subsection}{\numberline {7.10.1}1x1 Convolutions}{236}{subsection.7.10.1}%
\contentsline {subsubsection}{Dimensionality Reduction and Feature Selection}{236}{section*.366}%
\contentsline {subsubsection}{Efficiency of 1x1 Convolutions as a Bottleneck}{236}{section*.368}%
\contentsline {paragraph}{Example: Transforming 256 Channels to 256 Channels with a 3x3 Kernel.}{237}{section*.369}%
\contentsline {paragraph}{Parameter and FLOP Savings.}{237}{section*.370}%
\contentsline {subsection}{\numberline {7.10.2}1D Convolutions}{237}{subsection.7.10.2}%
\contentsline {paragraph}{Numerical Example: 1D Convolution on Multichannel Time Series Data}{237}{section*.371}%
\contentsline {paragraph}{Computing the Output}{238}{section*.372}%
\contentsline {paragraph}{Applications of 1D Convolutions}{238}{section*.373}%
\contentsline {subsection}{\numberline {7.10.3}3D Convolutions}{239}{subsection.7.10.3}%
\contentsline {paragraph}{Numerical Example: 3D Convolution on Volumetric Data}{239}{section*.375}%
\contentsline {paragraph}{3D Convolution Formula}{240}{section*.376}%
\contentsline {paragraph}{Computing the Output}{240}{section*.377}%
\contentsline {paragraph}{Final Output Tensor}{241}{section*.378}%
\contentsline {paragraph}{Applications of 3D Convolutions}{241}{section*.379}%
\contentsline {paragraph}{Advantages of 3D Convolutions}{241}{section*.380}%
\contentsline {paragraph}{Challenges of 3D Convolutions}{241}{section*.381}%
\contentsline {subsection}{\numberline {7.10.4}Efficient Convolutions for Mobile and Embedded Systems}{241}{subsection.7.10.4}%
\contentsline {subsection}{\numberline {7.10.5}Spatial Separable Convolutions}{241}{subsection.7.10.5}%
\contentsline {paragraph}{Concept and Intuition}{241}{section*.382}%
\contentsline {paragraph}{Limitations and Transition to Depthwise Separable Convolutions}{242}{section*.383}%
\contentsline {subsection}{\numberline {7.10.6}Depthwise Separable Convolutions}{242}{subsection.7.10.6}%
\contentsline {paragraph}{Concept and Motivation}{242}{section*.384}%
\contentsline {paragraph}{Computational Efficiency}{243}{section*.385}%
\contentsline {paragraph}{Standard \((K \times K)\) Convolution}{243}{section*.386}%
\contentsline {paragraph}{Depthwise Separable Convolution}{243}{section*.387}%
\contentsline {subsubsection}{Example: \((K=3,\tmspace +\thickmuskip {.2777em}C_{\mathrm {in}}=128,\tmspace +\thickmuskip {.2777em}C_{\mathrm {out}}=256,\tmspace +\thickmuskip {.2777em}H=W=32)\)}{243}{section*.388}%
\contentsline {paragraph}{Reduction Factor}{245}{section*.390}%
\contentsline {paragraph}{Practical Usage and Examples}{245}{section*.391}%
\contentsline {paragraph}{Trade-Offs}{245}{section*.392}%
\contentsline {subsection}{\numberline {7.10.7}Summary of Specialized Convolutions}{245}{subsection.7.10.7}%
\contentsline {section}{\numberline {7.11}Pooling Layers}{247}{section.7.11}%
\contentsline {subsection}{\numberline {7.11.1}Types of Pooling}{247}{subsection.7.11.1}%
\contentsline {paragraph}{Pooling Methods}{247}{section*.395}%
\contentsline {subsection}{\numberline {7.11.2}Effect of Pooling}{247}{subsection.7.11.2}%
\contentsline {subsection}{Enrichment 7.11.3: Pooling Layers in Backpropagation}{248}{section*.398}%
\contentsline {subsubsection}{Forward Pass of Pooling Layers}{248}{section*.399}%
\contentsline {paragraph}{Example of Forward Pass}{248}{section*.400}%
\contentsline {subsubsection}{Backpropagation Through Pooling Layers}{249}{section*.401}%
\contentsline {paragraph}{Max Pooling Backpropagation}{249}{section*.402}%
\contentsline {paragraph}{Impact on Gradient Flow}{249}{section*.403}%
\contentsline {paragraph}{Mitigation Strategies}{249}{section*.404}%
\contentsline {paragraph}{Average Pooling Backpropagation}{250}{section*.405}%
\contentsline {subsubsection}{Generalization of Backpropagation for Pooling}{250}{section*.406}%
\contentsline {subsection}{\numberline {7.11.4}Global Pooling Layers}{250}{subsection.7.11.4}%
\contentsline {subsubsection}{General Advantages}{250}{section*.407}%
\contentsline {subsubsection}{Global Average Pooling (GAP)}{250}{section*.408}%
\contentsline {paragraph}{Operation}{250}{section*.409}%
\contentsline {paragraph}{Upsides}{251}{section*.410}%
\contentsline {paragraph}{Downsides}{251}{section*.411}%
\contentsline {paragraph}{Backpropagation}{251}{section*.412}%
\contentsline {subsubsection}{Global Max Pooling (GMP)}{251}{section*.413}%
\contentsline {paragraph}{Operation.}{251}{section*.414}%
\contentsline {paragraph}{Upsides}{251}{section*.415}%
\contentsline {paragraph}{Downsides}{251}{section*.416}%
\contentsline {paragraph}{Backpropagation}{251}{section*.417}%
\contentsline {subsubsection}{Comparison of GAP and GMP}{251}{section*.418}%
\contentsline {subsubsection}{Contrasting with Regular Pooling}{252}{section*.419}%
\contentsline {paragraph}{Window Size}{252}{section*.420}%
\contentsline {paragraph}{When to Use Global Pooling}{252}{section*.421}%
\contentsline {paragraph}{When to Use Regular Pooling}{252}{section*.422}%
\contentsline {section}{\numberline {7.12}Classical CNN Architectures}{253}{section.7.12}%
\contentsline {subsection}{\numberline {7.12.1}LeNet-5 Architecture}{253}{subsection.7.12.1}%
\contentsline {subsubsection}{Detailed Layer Breakdown}{254}{section*.424}%
\contentsline {subsubsection}{Summary of LeNet-5}{255}{section*.425}%
\contentsline {subsubsection}{Key Architectural Trends in CNNs, Illustrated by LeNet-5}{255}{section*.426}%
\contentsline {paragraph}{Hierarchical Feature Learning}{255}{section*.427}%
\contentsline {paragraph}{Alternating Convolution and Pooling}{255}{section*.428}%
\contentsline {paragraph}{Transition to Fully Connected (FC) Layers}{255}{section*.429}%
\contentsline {subsection}{\numberline {7.12.2}How Are CNN Architectures Designed?}{255}{subsection.7.12.2}%
\contentsline {section}{Enrichment 7.13: Vanishing \& Exploding Gradients: A Barrier to DL}{256}{section*.430}%
\contentsline {paragraph}{Context}{256}{section*.431}%
\contentsline {subsection}{Enrichment 7.13.1: Understanding the Problem}{256}{section*.432}%
\contentsline {subsubsection}{The Role of Gradients in Deep Networks}{256}{section*.433}%
\contentsline {subsubsection}{Gradient Computation in Deep Networks}{256}{section*.434}%
\contentsline {paragraph}{Key Components of Gradient Propagation}{256}{section*.435}%
\contentsline {subsubsection}{Impact of Depth in Neural Networks}{257}{section*.436}%
\contentsline {subsubsection}{Practical Example: Vanishing Gradients with Sigmoid Activation}{259}{section*.437}%
\contentsline {subsubsection}{Effect of Activation Gradients}{260}{section*.439}%
\contentsline {subsubsection}{Effect of Weight Multiplications}{260}{section*.440}%
\contentsline {subsubsection}{Conclusion: Vanishing Gradients}{260}{section*.441}%
\contentsline {section}{\numberline {7.14}Batch Normalization}{262}{section.7.14}%
\contentsline {subsection}{\numberline {7.14.1}Understanding Mean, Variance, and Normalization}{262}{subsection.7.14.1}%
\contentsline {paragraph}{Mean:}{262}{section*.442}%
\contentsline {paragraph}{Variance:}{262}{section*.443}%
\contentsline {paragraph}{Standard Deviation:}{262}{section*.444}%
\contentsline {paragraph}{Effect of Normalization:}{262}{section*.445}%
\contentsline {subsection}{\numberline {7.14.2}Internal Covariate Shift and Batch Normalizations Role}{263}{subsection.7.14.2}%
\contentsline {paragraph}{What is Covariate Shift?}{263}{section*.446}%
\contentsline {paragraph}{What is Internal Covariate Shift?}{263}{section*.447}%
\contentsline {subsection}{\numberline {7.14.3}Batch Normalization Process}{263}{subsection.7.14.3}%
\contentsline {paragraph}{Why is this flexibility useful?}{264}{section*.449}%
\contentsline {subsubsection}{Batch Normalization for Convolutional Neural Networks (CNNs)}{265}{section*.450}%
\contentsline {subsection}{\numberline {7.14.4}Batch Normalization and Optimization}{266}{subsection.7.14.4}%
\contentsline {subsubsection}{Beyond Covariate Shift: Why Does BatchNorm Improve Training?}{266}{section*.452}%
\contentsline {subsubsection}{Why Does BatchNorm Smooth the Loss Surface?}{267}{section*.455}%
\contentsline {paragraph}{1. Hessian Eigenvalues and Loss Surface Curvature}{267}{section*.456}%
\contentsline {paragraph}{Computing Eigenvalues}{267}{section*.457}%
\contentsline {paragraph}{Interpretation of Eigenvalues}{268}{section*.458}%
\contentsline {paragraph}{2. Reducing the Lipschitz Constant}{268}{section*.459}%
\contentsline {paragraph}{3. Implicit Regularization via Mini-Batch Noise}{268}{section*.460}%
\contentsline {paragraph}{4. Decoupling Weight Norm from Direction: A Geometric Reparameterization}{269}{section*.461}%
\contentsline {paragraph}{5. Stabilizing Deep Networks and Preventing Dead Activations}{269}{section*.462}%
\contentsline {subsubsection}{Conclusion: Why BatchNorm HelpsWith Caution}{270}{section*.463}%
\contentsline {subsubsection}{Batch Normalization in Test Time}{271}{section*.464}%
\contentsline {subsubsection}{Limitations of BatchNorm}{271}{section*.466}%
\contentsline {subsection}{Enrichment 7.14.5: Batch Normalization Placement}{272}{section*.467}%
\contentsline {subsubsection}{Batch Normalization Placement: Typical Ordering}{272}{section*.468}%
\contentsline {paragraph}{Mathematical Rationale}{272}{section*.469}%
\contentsline {subsection}{\numberline {7.14.6}Alternative Normalization Methods (LN, IN, GN, ...)}{273}{subsection.7.14.6}%
\contentsline {subsubsection}{Layer Normalization (LN)}{273}{section*.470}%
\contentsline {paragraph}{Core Idea}{273}{section*.471}%
\contentsline {paragraph}{Definition (Fully Connected Layers)}{273}{section*.473}%
\contentsline {paragraph}{Extension to Convolutional Layers}{274}{section*.474}%
\contentsline {paragraph}{Interpretation}{274}{section*.476}%
\contentsline {paragraph}{Advantages of Layer Normalization}{274}{section*.477}%
\contentsline {subsubsection}{Instance Normalization (IN)}{275}{section*.478}%
\contentsline {paragraph}{Interpretation}{275}{section*.480}%
\contentsline {paragraph}{Advantages of Instance Normalization}{275}{section*.481}%
\contentsline {subsubsection}{Group Normalization (GN)}{276}{section*.482}%
\contentsline {paragraph}{Interpretation}{276}{section*.484}%
\contentsline {paragraph}{Advantages of Group Normalization}{276}{section*.485}%
\contentsline {subsubsection}{Why Do IN, LN, and GN Improve Optimization?}{277}{section*.486}%
\contentsline {paragraph}{Common Benefits Across IN, LN, and GN}{277}{section*.487}%
\contentsline {paragraph}{Summary: How These Methods Enhance Training}{277}{section*.488}%
\contentsline {subsection}{Enrichment 7.14.7: Backpropagation for Batch Normalization}{277}{section*.489}%
\contentsline {paragraph}{Chain Rule in the Graph}{278}{section*.490}%
\contentsline {subparagraph}{Gradients w.r.t.\ \(\gamma \) and \(\beta \)}{278}{subparagraph*.491}%
\contentsline {subparagraph}{Gradient w.r.t.\ \(\hat {x}_i\)}{278}{subparagraph*.492}%
\contentsline {paragraph}{Gradients Involving \(\mu \) and \(\sigma ^2\)}{278}{section*.493}%
\contentsline {paragraph}{Final: Gradients w.r.t.\ Each \(x_i\)}{279}{section*.494}%
\contentsline {paragraph}{Computational Efficiency}{279}{section*.495}%
\contentsline {paragraph}{Extension to LN, IN, GN}{279}{section*.496}%
\contentsline {paragraph}{Conclusion}{279}{section*.497}%
\contentsline {subsection}{Enrichment 7.14.8: Batch Normalization \& \(\ell _2\) Regularization}{280}{section*.498}%
\contentsline {paragraph}{Context and References}{280}{section*.499}%
\contentsline {paragraph}{1. \(\ell _2\) Regularization Without BatchNorm}{280}{section*.500}%
\contentsline {paragraph}{2. BN Cancels Weight Norm in the Forward Pass}{280}{section*.501}%
\contentsline {paragraph}{3. Why \(\ell _2\) Still Matters: Learning Dynamics Perspective}{281}{section*.502}%
\contentsline {paragraph}{4. Coexisting With Learning Rate Schedules}{282}{section*.503}%
\contentsline {paragraph}{5. Behavior of BNs \(\gamma , \beta \)}{282}{section*.504}%
\contentsline {paragraph}{6. Recommendations}{282}{section*.505}%
\contentsline {paragraph}{7. Conclusion: BN \& L2 Are Complementary, Not Contradictory}{283}{section*.506}%
\contentsline {chapter}{\numberline {8}Lecture 8: CNN Architectures I}{284}{chapter.8}%
\contentsline {section}{\numberline {8.1}Introduction: From Building Blocks to SOTA CNNs}{284}{section.8.1}%
\contentsline {section}{\numberline {8.2}AlexNet}{284}{section.8.2}%
\contentsline {subsection}{\numberline {8.2.1}Architecture Details}{285}{subsection.8.2.1}%
\contentsline {paragraph}{First Convolutional Layer (Conv1)}{285}{section*.507}%
\contentsline {paragraph}{Memory Requirements}{285}{section*.508}%
\contentsline {paragraph}{Number of Learnable Parameters}{285}{section*.509}%
\contentsline {paragraph}{Computational Cost}{285}{section*.510}%
\contentsline {paragraph}{Max Pooling Layer}{285}{section*.511}%
\contentsline {paragraph}{Memory and Computational Cost}{286}{section*.512}%
\contentsline {subsection}{\numberline {8.2.2}Final Fully Connected Layers}{286}{subsection.8.2.2}%
\contentsline {paragraph}{Computational Cost}{286}{section*.513}%
\contentsline {subsection}{\numberline {8.2.3}Key Takeaways from AlexNet}{287}{subsection.8.2.3}%
\contentsline {subsection}{\numberline {8.2.4}ZFNet: An Improvement on AlexNet}{287}{subsection.8.2.4}%
\contentsline {subsubsection}{Key Modifications in ZFNet}{288}{section*.517}%
\contentsline {section}{\numberline {8.3}VGG: A Principled CNN Architecture}{288}{section.8.3}%
\contentsline {paragraph}{Historical Context.}{288}{section*.518}%
\contentsline {paragraph}{Core Design Principles.}{288}{section*.520}%
\contentsline {subsection}{\numberline {8.3.1}Network Structure}{288}{subsection.8.3.1}%
\contentsline {subsection}{\numberline {8.3.2}Key Architectural Insights}{289}{subsection.8.3.2}%
\contentsline {subsubsection}{Small-Kernel Convolutions (\(3\times 3\))}{289}{section*.522}%
\contentsline {subsubsection}{Pooling \(\,2\times 2\), Stride=2, No Padding}{289}{section*.523}%
\contentsline {subsubsection}{Doubling Channels After Each Pool}{289}{section*.524}%
\contentsline {subsection}{\numberline {8.3.3}Why This Strategy Works}{290}{subsection.8.3.3}%
\contentsline {paragraph}{Balanced Computation.}{290}{section*.525}%
\contentsline {paragraph}{Influence on Later Architectures.}{290}{section*.526}%
\contentsline {subsection}{\numberline {8.3.4}Practical Observations}{290}{subsection.8.3.4}%
\contentsline {subsection}{\numberline {8.3.5}Training Very Deep Networks: The VGG Approach}{290}{subsection.8.3.5}%
\contentsline {subsubsection}{Incremental Training Strategy}{290}{section*.527}%
\contentsline {subsubsection}{Optimization and Training Details}{291}{section*.528}%
\contentsline {subsubsection}{Effectiveness of the Approach}{291}{section*.529}%
\contentsline {section}{\numberline {8.4}GoogLeNet: Efficiency and Parallelism}{291}{section.8.4}%
\contentsline {subsection}{\numberline {8.4.1}Stem Network: Efficient Early Downsampling}{292}{subsection.8.4.1}%
\contentsline {subsection}{\numberline {8.4.2}The Inception Module: Parallel Feature Extraction}{292}{subsection.8.4.2}%
\contentsline {subsubsection}{Why Does the Inception Module Improve Gradient Flow?}{293}{section*.533}%
\contentsline {paragraph}{Structure of the Inception Module}{294}{section*.534}%
\contentsline {subsection}{\numberline {8.4.3}Global Average Pooling (GAP)}{294}{subsection.8.4.3}%
\contentsline {subsection}{\numberline {8.4.4}Auxiliary Classifiers: A Workaround for Vanishing Gradients}{295}{subsection.8.4.4}%
\contentsline {paragraph}{Why Were Auxiliary Classifiers Needed?}{295}{section*.536}%
\contentsline {paragraph}{How Do They Help?}{295}{section*.537}%
\contentsline {paragraph}{Auxiliary Classifier Design}{295}{section*.538}%
\contentsline {paragraph}{Gradient Flow and Regularization}{296}{section*.540}%
\contentsline {paragraph}{Relevance Today}{296}{section*.541}%
\contentsline {paragraph}{Conclusion}{296}{section*.542}%
\contentsline {section}{\numberline {8.5}The Rise of Residual Networks (ResNets)}{297}{section.8.5}%
\contentsline {subsection}{\numberline {8.5.1}Challenges in Training Deep Neural Networks}{297}{subsection.8.5.1}%
\contentsline {subsection}{\numberline {8.5.2}The Need for Residual Connections}{297}{subsection.8.5.2}%
\contentsline {subsection}{\numberline {8.5.3}Introducing Residual Blocks}{298}{subsection.8.5.3}%
\contentsline {paragraph}{Intuition Behind Residual Connections}{299}{section*.546}%
\contentsline {subsection}{\numberline {8.5.4}Architectural Design of ResNets}{299}{subsection.8.5.4}%
\contentsline {subsection}{\numberline {8.5.5}Bottleneck Blocks for Deeper Networks}{300}{subsection.8.5.5}%
\contentsline {subsection}{\numberline {8.5.6}ResNet Winning Streak and Continued Influence}{301}{subsection.8.5.6}%
\contentsline {subsection}{\numberline {8.5.7}Further Improvements: Pre-Activation Blocks}{301}{subsection.8.5.7}%
\contentsline {subsection}{\numberline {8.5.8}Architectural Comparisons and Evolution Beyond ResNet}{302}{subsection.8.5.8}%
\contentsline {subsubsection}{The 2016 ImageNet Challenge: Lack of Novelty}{302}{section*.551}%
\contentsline {subsubsection}{Comparing Model Complexity and Efficiency}{302}{section*.552}%
\contentsline {subsubsection}{Beyond ResNets: Refinements and Lightweight Models}{303}{section*.554}%
\contentsline {chapter}{\numberline {9}Lecture 9: Training Neural Networks I}{304}{chapter.9}%
\contentsline {section}{\numberline {9.1}Introduction to Training Neural Networks}{304}{section.9.1}%
\contentsline {subsection}{\numberline {9.1.1}Categories of Practical Training Subjects}{304}{subsection.9.1.1}%
\contentsline {section}{\numberline {9.2}Activation Functions}{305}{section.9.2}%
\contentsline {subsection}{\numberline {9.2.1}Sigmoid Activation Function}{305}{subsection.9.2.1}%
\contentsline {subsubsection}{Issues with the Sigmoid Function}{305}{section*.555}%
\contentsline {subsubsection}{The Tanh Activation Function}{307}{section*.558}%
\contentsline {subsection}{\numberline {9.2.2}Rectified Linear Units (ReLU) and Its Variants}{308}{subsection.9.2.2}%
\contentsline {subsubsection}{Issues with ReLU}{308}{section*.560}%
\contentsline {subsubsection}{Mitigation Strategies for ReLU Issues}{309}{section*.562}%
\contentsline {subsubsection}{Leaky ReLU and Parametric ReLU (PReLU)}{310}{section*.563}%
\contentsline {subsubsection}{Exponential Linear Unit (ELU)}{310}{section*.565}%
\contentsline {subsubsection}{Scaled Exponential Linear Unit (SELU)}{311}{section*.567}%
\contentsline {paragraph}{Definition and Self-Normalization Properties}{312}{section*.568}%
\contentsline {paragraph}{Derivation of \(\alpha \) and \(\lambda \)}{312}{section*.569}%
\contentsline {paragraph}{Weight Initialization and Network Architecture Considerations}{312}{section*.570}%
\contentsline {paragraph}{Practical Considerations and Limitations}{312}{section*.571}%
\contentsline {subsubsection}{Gaussian Error Linear Unit (GELU)}{313}{section*.573}%
\contentsline {paragraph}{Definition}{313}{section*.574}%
\contentsline {paragraph}{Advantages of GELU}{314}{section*.576}%
\contentsline {paragraph}{Comparisons with ReLU and ELU}{314}{section*.577}%
\contentsline {paragraph}{Computational Considerations}{315}{section*.578}%
\contentsline {subsection}{Enrichment 9.2.3: Swish: A Self-Gated Activation Function}{315}{section*.579}%
\contentsline {subsubsection}{Advantages of Swish}{316}{section*.581}%
\contentsline {subsubsection}{Disadvantages of Swish}{316}{section*.582}%
\contentsline {subsubsection}{Comparison to Other Top-Tier Activations}{316}{section*.583}%
\contentsline {subsubsection}{Conclusion}{317}{section*.584}%
\contentsline {subsection}{\numberline {9.2.4}Choosing the Right Activation Function}{317}{subsection.9.2.4}%
\contentsline {subsubsection}{General Guidelines for Choosing an Activation Function}{317}{section*.586}%
\contentsline {section}{\numberline {9.3}Data Pre-Processing}{318}{section.9.3}%
\contentsline {subsection}{\numberline {9.3.1}Why Pre-Processing Matters}{318}{subsection.9.3.1}%
\contentsline {subsection}{\numberline {9.3.2}Avoiding Poor Training Dynamics}{318}{subsection.9.3.2}%
\contentsline {subsection}{\numberline {9.3.3}Common Pre-Processing Techniques}{319}{subsection.9.3.3}%
\contentsline {subsection}{\numberline {9.3.4}Normalization for Robust Optimization}{320}{subsection.9.3.4}%
\contentsline {subsection}{\numberline {9.3.5}Maintaining Consistency During Inference}{320}{subsection.9.3.5}%
\contentsline {subsection}{\numberline {9.3.6}Pre-Processing in Well-Known Architectures}{320}{subsection.9.3.6}%
\contentsline {section}{\numberline {9.4}Weight Initialization}{321}{section.9.4}%
\contentsline {subsection}{\numberline {9.4.1}Constant Initialization}{321}{subsection.9.4.1}%
\contentsline {subsubsection}{Zero Initialization}{321}{section*.590}%
\contentsline {subsubsection}{Nonzero Constant Initialization}{322}{section*.591}%
\contentsline {paragraph}{Forward Pass Analysis}{322}{section*.592}%
\contentsline {paragraph}{Backpropagation and Gradient Symmetry}{322}{section*.593}%
\contentsline {paragraph}{Implications and Conclusion}{323}{section*.594}%
\contentsline {subsection}{\numberline {9.4.2}Breaking Symmetry: Random Initialization}{323}{subsection.9.4.2}%
\contentsline {subsection}{\numberline {9.4.3}Variance-Based Initialization: Ensuring Stable Information Flow}{323}{subsection.9.4.3}%
\contentsline {paragraph}{Key Requirements for Stable Propagation}{324}{section*.595}%
\contentsline {paragraph}{Forward Pass Analysis}{324}{section*.596}%
\contentsline {paragraph}{Why Is This Important?}{324}{section*.597}%
\contentsline {paragraph}{Why Does Forward Signal Variance Also Matter?}{324}{section*.598}%
\contentsline {paragraph}{Challenges in Achieving Stable Variance}{325}{section*.599}%
\contentsline {subsection}{\numberline {9.4.4}Xavier Initialization}{325}{subsection.9.4.4}%
\contentsline {subsubsection}{Motivation}{325}{section*.600}%
\contentsline {subsubsection}{Mathematical Formulation}{326}{section*.602}%
\contentsline {subsubsection}{Assumptions}{326}{section*.603}%
\contentsline {subsubsection}{Derivation of Xavier Initialization}{327}{section*.604}%
\contentsline {paragraph}{Forward Pass: Maintaining Activation Variance}{327}{section*.605}%
\contentsline {paragraph}{Backward Pass: Maintaining Gradient Variance}{327}{section*.606}%
\contentsline {paragraph}{Balancing Forward and Backward Variance}{328}{section*.607}%
\contentsline {subsubsection}{Final Xavier Initialization Formulation}{329}{section*.608}%
\contentsline {subsubsection}{Limitations of Xavier Initialization}{329}{section*.609}%
\contentsline {subsection}{\numberline {9.4.5}Kaiming He Initialization}{330}{subsection.9.4.5}%
\contentsline {subsubsection}{Motivation}{330}{section*.610}%
\contentsline {paragraph}{Mathematical Notation}{331}{section*.613}%
\contentsline {subsubsection}{Assumptions}{331}{section*.614}%
\contentsline {subsubsection}{Forward and Backward Pass Derivation}{332}{section*.615}%
\contentsline {paragraph}{Forward Pass Analysis}{332}{section*.616}%
\contentsline {paragraph}{Backward Pass Analysis}{333}{section*.617}%
\contentsline {subsubsection}{Final Kaiming Initialization Formulation}{333}{section*.618}%
\contentsline {subsubsection}{Implementation in Deep Learning Frameworks}{333}{section*.619}%
\contentsline {subsubsection}{Initialization in Residual Networks (ResNets)}{334}{section*.620}%
\contentsline {paragraph}{Why Doesn't Kaiming Initialization Work for ResNets?}{334}{section*.621}%
\contentsline {paragraph}{Fixup Initialization}{334}{section*.622}%
\contentsline {subsection}{\numberline {9.4.6}Conclusion: Choosing the Right Initialization Strategy}{335}{subsection.9.4.6}%
\contentsline {subsubsection}{Ongoing Research and Open Questions}{335}{section*.624}%
\contentsline {section}{\numberline {9.5}Regularization Techniques}{336}{section.9.5}%
\contentsline {subsection}{\numberline {9.5.1}Dropout}{336}{subsection.9.5.1}%
\contentsline {subsubsection}{Why Does Dropout Work?}{337}{section*.627}%
\contentsline {subsubsection}{Dropout at Test Time}{338}{section*.629}%
\contentsline {subsubsection}{Inverted Dropout}{340}{section*.633}%
\contentsline {subsubsection}{Where is Dropout Used in CNNs?}{341}{section*.635}%
\contentsline {subsection}{Enrichment 9.5.2: Ordering of Dropout and Batch Normalization}{341}{section*.637}%
\contentsline {subsubsection}{Enrichment 9.5.2.1: Impact of Dropout Placement on BN}{342}{section*.638}%
\contentsline {subsubsection}{Enrichment 9.5.2.2: Why BN Before Dropout is Preferred}{342}{section*.639}%
\contentsline {subsection}{\numberline {9.5.3}Other Regularization Techniques}{343}{subsection.9.5.3}%
\contentsline {subsubsection}{Data Augmentation as Implicit Regularization}{343}{section*.640}%
\contentsline {subsubsection}{DropConnect}{345}{section*.644}%
\contentsline {paragraph}{Comparison Between Dropout and DropConnect}{345}{section*.646}%
\contentsline {paragraph}{Effectiveness and Use Cases}{346}{section*.647}%
\contentsline {paragraph}{Summary}{346}{section*.648}%
\contentsline {subsubsection}{Fractional Max Pooling}{346}{section*.649}%
\contentsline {subsubsection}{Stochastic Depth}{347}{section*.651}%
\contentsline {subsubsection}{CutOut}{347}{section*.653}%
\contentsline {subsubsection}{MixUp}{348}{section*.655}%
\contentsline {subsubsection}{Summary and Regularization Guidelines}{349}{section*.657}%
\contentsline {chapter}{\numberline {10}Lecture 10: Training Neural Networks II}{350}{chapter.10}%
\contentsline {section}{\numberline {10.1}Learning Rate Schedules}{350}{section.10.1}%
\contentsline {subsection}{\numberline {10.1.1}The Importance of Learning Rate Selection}{350}{subsection.10.1.1}%
\contentsline {subsection}{\numberline {10.1.2}Step Learning Rate Schedule}{351}{subsection.10.1.2}%
\contentsline {subsubsection}{Practical Considerations}{352}{section*.660}%
\contentsline {subsection}{\numberline {10.1.3}Cosine Learning Rate Decay}{352}{subsection.10.1.3}%
\contentsline {subsection}{\numberline {10.1.4}Linear Learning Rate Decay}{353}{subsection.10.1.4}%
\contentsline {subsection}{\numberline {10.1.5}Inverse Square Root Decay}{354}{subsection.10.1.5}%
\contentsline {subsection}{\numberline {10.1.6}Constant Learning Rate}{355}{subsection.10.1.6}%
\contentsline {subsection}{\numberline {10.1.7}Adaptive Learning Rate Mechanisms}{356}{subsection.10.1.7}%
\contentsline {subsection}{\numberline {10.1.8}Early Stopping}{356}{subsection.10.1.8}%
\contentsline {subsection}{Enrichment 10.1.9: Super-Convergence and OneCycle}{357}{section*.666}%
\contentsline {paragraph}{Motivation}{357}{section*.667}%
\contentsline {subsubsection}{Method: schedule and inverse momentum coupling}{357}{section*.668}%
\contentsline {paragraph}{What we are scheduling (symbols at a glance)}{357}{section*.669}%
\contentsline {paragraph}{Notation and helper ramps (defined before use)}{357}{section*.670}%
\contentsline {paragraph}{Why this shape? (intuition)}{357}{section*.671}%
\contentsline {paragraph}{Step-by-step construction of the schedules}{358}{section*.672}%
\contentsline {paragraph}{Parameterization and defaults}{358}{section*.673}%
\contentsline {paragraph}{What each hyper-parameter controls}{358}{section*.674}%
\contentsline {subsubsection}{Diagnostics: the LR range test}{359}{section*.675}%
\contentsline {subsubsection}{Empirical picture: CIFAR-10 and ImageNet}{359}{section*.677}%
\contentsline {subsubsection}{Intuition and comparisons}{360}{section*.680}%
\contentsline {subsubsection}{When to useand caveats}{361}{section*.681}%
\contentsline {paragraph}{Practical tuning guide}{362}{section*.682}%
\contentsline {section}{\numberline {10.2}Hyperparameter Selection}{363}{section.10.2}%
\contentsline {subsection}{\numberline {10.2.1}Grid Search}{363}{subsection.10.2.1}%
\contentsline {subsection}{\numberline {10.2.2}Random Search}{363}{subsection.10.2.2}%
\contentsline {subsection}{\numberline {10.2.3}Steps for Hyperparameter Tuning}{364}{subsection.10.2.3}%
\contentsline {subsection}{\numberline {10.2.4}Interpreting Learning Curves}{366}{subsection.10.2.4}%
\contentsline {subsection}{\numberline {10.2.5}Model Ensembles and Averaging Techniques}{370}{subsection.10.2.5}%
\contentsline {subsection}{\numberline {10.2.6}Exponential Moving Average (EMA) and Polyak Averaging}{371}{subsection.10.2.6}%
\contentsline {section}{\numberline {10.3}Transfer Learning}{371}{section.10.3}%
\contentsline {subsection}{\numberline {10.3.1}How to Perform Transfer Learning with CNNs?}{375}{subsection.10.3.1}%
\contentsline {subsection}{\numberline {10.3.2}Transfer Learning Beyond Classification}{376}{subsection.10.3.2}%
\contentsline {subsection}{\numberline {10.3.3}Does Transfer Learning Always Win?}{377}{subsection.10.3.3}%
\contentsline {subsection}{Enrichment 10.3.4: Regularization in the Era of Finetuning}{378}{section*.703}%
\contentsline {paragraph}{1. Freezing Most of the Backbone}{378}{section*.704}%
\contentsline {paragraph}{2. Regularizing Small Trainable Heads: Caution With Dropout}{378}{section*.705}%
\contentsline {paragraph}{3. Training From Scratch on Large Datasets}{378}{section*.706}%
\contentsline {paragraph}{4. Implicit and Soft Regularization Prevail}{378}{section*.707}%
\contentsline {paragraph}{5. Summary}{378}{section*.708}%
\contentsline {chapter}{\numberline {11}Lecture 11: CNN Architectures II}{379}{chapter.11}%
\contentsline {section}{\numberline {11.1}Post-ResNet Architectures}{379}{section.11.1}%
\contentsline {section}{\numberline {11.2}Grouped Convolutions}{380}{section.11.2}%
\contentsline {subsection}{\numberline {11.2.1}Grouped Convolutions in PyTorch}{385}{subsection.11.2.1}%
\contentsline {subsubsection}{Key Observations}{386}{section*.720}%
\contentsline {subsubsection}{When to Use Grouped Convolutions?}{386}{section*.721}%
\contentsline {section}{\numberline {11.3}ResNeXt: Next-Generation Residual Networks}{386}{section.11.3}%
\contentsline {subsection}{\numberline {11.3.1}Motivation: Why ResNeXt?}{387}{subsection.11.3.1}%
\contentsline {subsection}{\numberline {11.3.2}Key Innovation: Aggregated Transformations}{387}{subsection.11.3.2}%
\contentsline {subsection}{\numberline {11.3.3}ResNeXt and Grouped Convolutions}{388}{subsection.11.3.3}%
\contentsline {subsection}{\numberline {11.3.4}Advantages of ResNeXt Over ResNet}{388}{subsection.11.3.4}%
\contentsline {subsection}{\numberline {11.3.5}ResNeXt Model Naming Convention}{389}{subsection.11.3.5}%
\contentsline {section}{\numberline {11.4}Squeeze-and-Excitation Networks (SENet)}{389}{section.11.4}%
\contentsline {subsection}{\numberline {11.4.1}Squeeze-and-Excitation (SE) Block}{390}{subsection.11.4.1}%
\contentsline {subsubsection}{Squeeze: Global Information Embedding}{390}{section*.725}%
\contentsline {subsubsection}{Excitation: Adaptive Recalibration}{390}{section*.726}%
\contentsline {subsubsection}{Channel Recalibration}{391}{section*.727}%
\contentsline {subsubsection}{How SE Blocks Enhance ResNet Bottleneck Blocks}{391}{section*.729}%
\contentsline {subsubsection}{Why Does SE Improve Performance?}{392}{section*.730}%
\contentsline {subsubsection}{Performance Gains, Scalability, and Integration of SE Blocks}{392}{section*.731}%
\contentsline {subsubsection}{Impact on Various Tasks}{392}{section*.733}%
\contentsline {subsubsection}{Practical Applications and Widespread Adoption}{393}{section*.734}%
\contentsline {subsection}{\numberline {11.4.2}SE Blocks and the End of the ImageNet Classification Challenge}{393}{subsection.11.4.2}%
\contentsline {subsection}{\numberline {11.4.3}Challenges and Solutions for SE Networks}{394}{subsection.11.4.3}%
\contentsline {subsubsection}{Challenges of SE Networks}{394}{section*.736}%
\contentsline {subsubsection}{Solutions to SE Network Challenges}{394}{section*.737}%
\contentsline {subsubsection}{Shifting Research Directions: Efficiency and Mobile Deployability}{395}{section*.738}%
\contentsline {subsubsection}{What Comes Next?}{395}{section*.739}%
\contentsline {section}{\numberline {11.5}Efficient Architectures for Edge Devices}{395}{section.11.5}%
\contentsline {subsection}{\numberline {11.5.1}MobileNet: Depthwise Separable Convolutions}{396}{subsection.11.5.1}%
\contentsline {subsubsection}{Width Multiplier: Thinner Models}{397}{section*.742}%
\contentsline {subsubsection}{Resolution Multiplier: Reduced Representations}{397}{section*.743}%
\contentsline {subsubsection}{Computational Cost of Depthwise Separable Convolutions}{397}{section*.744}%
\contentsline {paragraph}{Summary of Multipliers}{398}{section*.745}%
\contentsline {subsubsection}{MobileNetV1 vs. Traditional Architectures}{398}{section*.746}%
\contentsline {subsubsection}{Depthwise Separable vs. Standard Convolutions in MobileNet}{398}{section*.748}%
\contentsline {subsubsection}{Summary and Next Steps}{398}{section*.750}%
\contentsline {subsection}{\numberline {11.5.2}ShuffleNet: Efficient Channel Mixing via Grouped Convolutions}{399}{subsection.11.5.2}%
\contentsline {subsubsection}{The ShuffleNet Unit}{400}{section*.753}%
\contentsline {paragraph}{Core Design Features}{400}{section*.754}%
\contentsline {paragraph}{Structure of a ShuffleNet Unit}{400}{section*.755}%
\contentsline {paragraph}{Stride-2 Modification}{401}{section*.757}%
\contentsline {subsubsection}{ShuffleNet Architecture}{401}{section*.758}%
\contentsline {paragraph}{Stage-wise Construction:}{401}{section*.759}%
\contentsline {paragraph}{Scaling Factor}{402}{section*.760}%
\contentsline {paragraph}{Design Rationale}{402}{section*.761}%
\contentsline {subsubsection}{Computational Efficiency of ShuffleNet}{402}{section*.762}%
\contentsline {subsubsection}{Inference Speed and Practical Performance}{402}{section*.763}%
\contentsline {subsubsection}{Performance Comparison: ShuffleNet vs. MobileNet}{403}{section*.764}%
\contentsline {subsubsection}{Beyond ShuffleNet: Evolution of Efficient CNN Architectures}{403}{section*.766}%
\contentsline {subsection}{\numberline {11.5.3}MobileNetV2: Inverted Bottleneck and Linear Residual}{403}{subsection.11.5.3}%
\contentsline {paragraph}{Understanding Feature Representations and Manifolds}{403}{section*.767}%
\contentsline {paragraph}{ReLU and Information Collapse}{403}{section*.768}%
\contentsline {subsubsection}{The MobileNetV2 Block: Inverted Residuals and Linear Bottleneck}{404}{section*.770}%
\contentsline {paragraph}{Detailed Block Architecture}{404}{section*.771}%
\contentsline {subsubsection}{Why is the Inverted Block Fitting to Efficient Networks?}{405}{section*.773}%
\contentsline {paragraph}{1. Depthwise Convolutions Maintain Low Computational Cost}{405}{section*.774}%
\contentsline {paragraph}{2. Moderate Expansion Factor \((t)\) Balances Efficiency}{405}{section*.775}%
\contentsline {paragraph}{3. Comparison to MobileNetV1}{405}{section*.776}%
\contentsline {paragraph}{4. Comparison to ResNet Bottleneck Blocks}{406}{section*.777}%
\contentsline {paragraph}{5. Linear Bottleneck Preserves Subtle Features}{406}{section*.778}%
\contentsline {paragraph}{Summary}{406}{section*.779}%
\contentsline {subsubsection}{ReLU6 and Its Role in Low-Precision Inference}{406}{section*.780}%
\contentsline {paragraph}{Practical Observations and Alternatives}{407}{section*.781}%
\contentsline {subsubsection}{MobileNetV2 Architecture and Performance}{407}{section*.783}%
\contentsline {subsubsection}{Comparison to MobileNetV1, ShuffleNet, and NASNet}{408}{section*.785}%
\contentsline {subsection}{\numberline {11.5.4}Neural Architecture Search (NAS) and MobileNetV3}{409}{subsection.11.5.4}%
\contentsline {subsubsection}{How NAS Works? Policy Gradient Optimization}{409}{section*.787}%
\contentsline {paragraph}{What is a Policy Gradient?}{409}{section*.788}%
\contentsline {paragraph}{Updating the Controller Using Policy Gradients}{409}{section*.789}%
\contentsline {paragraph}{Searching for Reusable Block Designs}{410}{section*.791}%
\contentsline {subsubsection}{MobileNetV3: NAS-Optimized Mobile Network}{411}{section*.793}%
\contentsline {subsubsection}{The MobileNetV3 Block Architecture and Refinements}{411}{section*.794}%
\contentsline {paragraph}{Structure of the MobileNetV3 Block}{411}{section*.795}%
\contentsline {paragraph}{Differences from Previous MobileNet Blocks}{411}{section*.796}%
\contentsline {subsubsection}{Why is MobileNetV3 More Efficient?}{412}{section*.797}%
\contentsline {paragraph}{Key Optimizations That Improve Efficiency}{412}{section*.798}%
\contentsline {paragraph}{Empirical Comparison of MobileNetV3}{412}{section*.799}%
\contentsline {subsubsection}{The Computational Cost of NAS and Its Limitations}{413}{section*.802}%
\contentsline {paragraph}{Why is NAS Expensive?}{413}{section*.803}%
\contentsline {subsubsection}{ShuffleNetV2 and Practical Design Rules}{414}{section*.805}%
\contentsline {paragraph}{Why ShuffleNetV2?}{414}{section*.806}%
\contentsline {paragraph}{Four Key Guidelines for Practical Efficiency}{414}{section*.807}%
\contentsline {paragraph}{From ShuffleNetV1 to ShuffleNetV2}{415}{section*.808}%
\contentsline {paragraph}{Performance vs.\ MobileNetV3}{415}{section*.809}%
\contentsline {subsubsection}{The Need for Model Scaling and EfficientNets}{415}{section*.810}%
\contentsline {paragraph}{Beyond Hand-Designed and NAS-Optimized Models}{415}{section*.811}%
\contentsline {paragraph}{Introducing EfficientNet}{415}{section*.812}%
\contentsline {section}{\numberline {11.6}EfficientNet Compound Model Scaling}{416}{section.11.6}%
\contentsline {subsection}{\numberline {11.6.1}How Should We Scale a Model}{416}{subsection.11.6.1}%
\contentsline {paragraph}{The Problem with Independent Scaling}{416}{section*.814}%
\contentsline {subsection}{\numberline {11.6.2}How EfficientNet Works}{417}{subsection.11.6.2}%
\contentsline {paragraph}{Step 1: Designing a Baseline Architecture}{417}{section*.816}%
\contentsline {paragraph}{EfficientNet-B0 Architecture}{418}{section*.817}%
\contentsline {paragraph}{Step 2: Finding Optimal Scaling Factors}{418}{section*.819}%
\contentsline {paragraph}{Step 3: Scaling to Different Model Sizes}{418}{section*.820}%
\contentsline {subsection}{\numberline {11.6.3}Why is EfficientNet More Effective}{418}{subsection.11.6.3}%
\contentsline {paragraph}{Balanced Scaling Improves Efficiency}{418}{section*.821}%
\contentsline {paragraph}{Comparison with MobileNetV3}{419}{section*.822}%
\contentsline {paragraph}{Comparison with Other Networks}{419}{section*.823}%
\contentsline {subsection}{\numberline {11.6.4}Limitations of EfficientNet}{419}{subsection.11.6.4}%
\contentsline {paragraph}{Whats Next? EfficientNetV2 and Beyond}{420}{section*.825}%
\contentsline {paragraph}{Conclusion}{420}{section*.826}%
\contentsline {section}{\numberline {11.7}EfficientNet-Lite Optimizing EfficientNet for Edge Devices}{420}{section.11.7}%
\contentsline {subsection}{\numberline {11.7.1}Motivation for EfficientNet-Lite}{420}{subsection.11.7.1}%
\contentsline {subsection}{\numberline {11.7.2}EfficientNet-Lite Architecture}{420}{subsection.11.7.2}%
\contentsline {subsection}{\numberline {11.7.3}Performance and Comparison with Other Models}{420}{subsection.11.7.3}%
\contentsline {paragraph}{Model Size vs. Accuracy Trade-off}{421}{section*.828}%
\contentsline {section}{\numberline {11.8}EfficientNetV2: Faster Training and Improved Efficiency}{422}{section.11.8}%
\contentsline {subsection}{\numberline {11.8.1}Motivation for EfficientNetV2}{422}{subsection.11.8.1}%
\contentsline {subsection}{\numberline {11.8.2}Fused-MBConv: Improving Early Layers}{422}{subsection.11.8.2}%
\contentsline {subsection}{\numberline {11.8.3}Progressive Learning: Efficient Training with Smaller Images}{423}{subsection.11.8.3}%
\contentsline {subsection}{\numberline {11.8.4}FixRes: Addressing Train-Test Resolution Discrepancy}{423}{subsection.11.8.4}%
\contentsline {paragraph}{The Problem: Region of Classification (RoC) Mismatch}{423}{section*.831}%
\contentsline {paragraph}{FixRes Solution}{424}{section*.832}%
\contentsline {paragraph}{Implementation in EfficientNetV2}{424}{section*.834}%
\contentsline {subsection}{\numberline {11.8.5}Non-Uniform Scaling for Improved Efficiency}{424}{subsection.11.8.5}%
\contentsline {subsection}{\numberline {11.8.6}EfficientNetV2 Architecture}{425}{subsection.11.8.6}%
\contentsline {subsection}{\numberline {11.8.7}EfficientNetV2 vs. EfficientNetV1}{425}{subsection.11.8.7}%
\contentsline {subsection}{\numberline {11.8.8}EfficientNetV2 vs.\ Other Models}{426}{subsection.11.8.8}%
\contentsline {paragraph}{Training Speed and Efficiency}{427}{section*.837}%
\contentsline {paragraph}{Key Takeaways}{427}{section*.839}%
\contentsline {section}{\numberline {11.9}NFNets: Normalizer-Free ResNets}{428}{section.11.9}%
\contentsline {subsection}{\numberline {11.9.1}Motivation: Why Do We Need NFNets?}{428}{subsection.11.9.1}%
\contentsline {subsection}{\numberline {11.9.2}Variance Explosion Without BatchNorm}{428}{subsection.11.9.2}%
\contentsline {paragraph}{Variance Scaling in Residual Networks}{428}{section*.840}%
\contentsline {paragraph}{Role of Weight Initialization}{428}{section*.841}%
\contentsline {subsection}{\numberline {11.9.3}Why Not Rescale the Residual Branch?}{428}{subsection.11.9.3}%
\contentsline {subsection}{\numberline {11.9.4}NFNets: Weight Normalization Instead of BN}{429}{subsection.11.9.4}%
\contentsline {paragraph}{Why This Works}{429}{section*.842}%
\contentsline {paragraph}{Relation to Earlier Weight Standardization}{429}{section*.843}%
\contentsline {subsection}{\numberline {11.9.5}NFNets Architecture and ResNet-D}{430}{subsection.11.9.5}%
\contentsline {subsection}{\numberline {11.9.6}Comparison Across Diverse Architectures}{430}{subsection.11.9.6}%
\contentsline {paragraph}{Key Takeaways}{430}{section*.845}%
\contentsline {subsection}{\numberline {11.9.7}Further Reading and Resources}{431}{subsection.11.9.7}%
\contentsline {section}{\numberline {11.10}Revisiting ResNets: Improved Training and Scaling Strategies}{432}{section.11.10}%
\contentsline {subsection}{\numberline {11.10.1}Training Enhancements for ResNets}{432}{subsection.11.10.1}%
\contentsline {paragraph}{Key Enhancements}{432}{section*.847}%
\contentsline {subsection}{\numberline {11.10.2}Scaling ResNets for Efficient Training}{432}{subsection.11.10.2}%
\contentsline {subsection}{\numberline {11.10.3}ResNet-RS vs. EfficientNet: A Re-Evaluation}{433}{subsection.11.10.3}%
\contentsline {paragraph}{Comparison of ResNet-RS and EfficientNet}{433}{section*.849}%
\contentsline {paragraph}{Key Observations}{433}{section*.851}%
\contentsline {paragraph}{Conclusion}{434}{section*.852}%
\contentsline {section}{\numberline {11.11}RegNets: Network Design Spaces}{434}{section.11.11}%
\contentsline {subsection}{\numberline {11.11.1}RegNet Architecture}{434}{subsection.11.11.1}%
\contentsline {paragraph}{Block Design: Generalizing ResNeXt}{435}{section*.854}%
\contentsline {subsection}{\numberline {11.11.2}Optimizing the Design Space}{435}{subsection.11.11.2}%
\contentsline {paragraph}{Random Sampling and Performance Trends}{435}{section*.856}%
\contentsline {paragraph}{Reducing the Design Space}{436}{section*.857}%
\contentsline {paragraph}{Final Six Parameters}{436}{section*.858}%
\contentsline {paragraph}{Why This Works}{436}{section*.860}%
\contentsline {paragraph}{Conclusion}{437}{section*.861}%
\contentsline {subsection}{\numberline {11.11.3}Performance and Applications}{437}{subsection.11.11.3}%
\contentsline {paragraph}{Key Takeaways}{438}{section*.864}%
\contentsline {paragraph}{Conclusion}{438}{section*.865}%
\contentsline {section}{\numberline {11.12}Summary of Efficient Network Architectures}{439}{section.11.12}%
\contentsline {subsubsection}{Grouped Convolutions and ResNeXt}{439}{section*.866}%
\contentsline {subsubsection}{Squeeze-and-Excitation (SE) Blocks}{439}{section*.867}%
\contentsline {subsubsection}{MobileNet and ShuffleNet: Depthwise Separable Convolutions and Channel Mixing}{439}{section*.868}%
\contentsline {subsubsection}{MobileNetV2: Inverted Residual Blocks}{439}{section*.869}%
\contentsline {subsubsection}{NAS and MobileNetV3, ShuffleNetV2 Insights}{439}{section*.870}%
\contentsline {subsubsection}{EfficientNet: Compound Scaling}{439}{section*.871}%
\contentsline {subsubsection}{EfficientNet-Lite and EfficientNetV2}{440}{section*.872}%
\contentsline {subsubsection}{NFNets: BN-Free Training}{440}{section*.873}%
\contentsline {subsubsection}{Revisiting ResNets: Scaling and Training Recipes}{440}{section*.874}%
\contentsline {subsubsection}{RegNets: Optimizing the Design Space}{440}{section*.875}%
\contentsline {subsubsection}{Key Takeaways}{440}{section*.876}%
\contentsline {chapter}{\numberline {12}Lecture 12: Deep Learning Software}{441}{chapter.12}%
\contentsline {section}{\numberline {12.1}Deep Learning Frameworks: Evolution and Landscape}{441}{section.12.1}%
\contentsline {subsection}{\numberline {12.1.1}The Purpose of Deep Learning Frameworks}{442}{subsection.12.1.1}%
\contentsline {subsection}{\numberline {12.1.2}Recall: Computational Graphs}{442}{subsection.12.1.2}%
\contentsline {section}{\numberline {12.2}PyTorch: Fundamental Concepts}{443}{section.12.2}%
\contentsline {subsection}{\numberline {12.2.1}Tensors and Basic Computation}{443}{subsection.12.2.1}%
\contentsline {subsection}{\numberline {12.2.2}Autograd: Automatic Differentiation}{444}{subsection.12.2.2}%
\contentsline {subsection}{\numberline {12.2.3}Computational Graphs and Modular Computation}{445}{subsection.12.2.3}%
\contentsline {subsubsection}{Building the Computational Graph}{445}{section*.879}%
\contentsline {subsubsection}{Loss Computation and Backpropagation}{446}{section*.883}%
\contentsline {subsubsection}{Extending Computational Graphs with Python Functions}{447}{section*.885}%
\contentsline {subsubsection}{Custom Autograd Functions}{448}{section*.886}%
\contentsline {subsubsection}{Summary: Backpropagation and Graph Optimization}{449}{section*.888}%
\contentsline {subsection}{\numberline {12.2.4}High-Level Abstractions in PyTorch: \texttt {torch.nn} and Optimizers}{449}{subsection.12.2.4}%
\contentsline {subsubsection}{Using \texttt {torch.nn.Sequential}}{449}{section*.889}%
\contentsline {subsubsection}{Using Optimizers: Automating Gradient Descent}{450}{section*.890}%
\contentsline {subsubsection}{Defining Custom \texttt {nn.Module} Subclasses}{451}{section*.891}%
\contentsline {subsubsection}{Key Takeaways}{451}{section*.892}%
\contentsline {subsection}{\numberline {12.2.5}Combining Custom Modules with Sequential Models}{451}{subsection.12.2.5}%
\contentsline {subsubsection}{Example: Parallel Block}{452}{section*.893}%
\contentsline {subsection}{\numberline {12.2.6}Efficient Data Loading with \texttt {torch.utils.data}}{453}{subsection.12.2.6}%
\contentsline {subsubsection}{Example: Using \texttt {DataLoader} for Mini-batching}{453}{section*.896}%
\contentsline {subsection}{\numberline {12.2.7}Using Pretrained Models with TorchVision}{454}{subsection.12.2.7}%
\contentsline {subsubsection}{Key Takeaways}{454}{section*.897}%
\contentsline {section}{\numberline {12.3}Dynamic vs. Static Computational Graphs in PyTorch}{455}{section.12.3}%
\contentsline {subsubsection}{Example: Dynamic Graph Construction}{455}{section*.898}%
\contentsline {subsection}{\numberline {12.3.1}Static Graphs and Just-in-Time (JIT) Compilation}{455}{subsection.12.3.1}%
\contentsline {subsection}{\numberline {12.3.2}Using JIT to Create Static Graphs}{456}{subsection.12.3.2}%
\contentsline {subsection}{\numberline {12.3.3}Handling Conditionals in Static Graphs}{456}{subsection.12.3.3}%
\contentsline {subsection}{\numberline {12.3.4}Optimizing Computation Graphs with JIT}{457}{subsection.12.3.4}%
\contentsline {subsection}{\numberline {12.3.5}Benefits and Limitations of Static Graphs}{458}{subsection.12.3.5}%
\contentsline {subsection}{\numberline {12.3.6}When Are Dynamic Graphs Necessary?}{458}{subsection.12.3.6}%
\contentsline {section}{\numberline {12.4}TensorFlow: Dynamic and Static Computational Graphs}{458}{section.12.4}%
\contentsline {subsection}{\numberline {12.4.1}Defining Computational Graphs in TensorFlow 2.0}{458}{subsection.12.4.1}%
\contentsline {subsection}{\numberline {12.4.2}Static Graphs with \texttt {tf.function}}{459}{subsection.12.4.2}%
\contentsline {section}{\numberline {12.5}Keras: High-Level API for TensorFlow}{459}{section.12.5}%
\contentsline {section}{\numberline {12.6}TensorBoard: Visualizing Training Metrics}{460}{section.12.6}%
\contentsline {section}{\numberline {12.7}Comparison: PyTorch vs. TensorFlow}{461}{section.12.7}%
\contentsline {paragraph}{Conclusion}{461}{section*.904}%
\contentsline {chapter}{\numberline {13}Lecture 13: Object Detection}{462}{chapter.13}%
\contentsline {section}{\numberline {13.1}Object Detection: Introduction}{462}{section.13.1}%
\contentsline {subsection}{\numberline {13.1.1}Computer Vision Tasks: Beyond Classification}{462}{subsection.13.1.1}%
\contentsline {subsection}{\numberline {13.1.2}What is Object Detection?}{463}{subsection.13.1.2}%
\contentsline {subsection}{\numberline {13.1.3}Challenges in Object Detection}{463}{subsection.13.1.3}%
\contentsline {subsection}{\numberline {13.1.4}Bounding Boxes and Intersection over Union (IoU)}{463}{subsection.13.1.4}%
\contentsline {subsection}{\numberline {13.1.5}Evaluating Bounding Boxes: Intersection over Union (IoU)}{464}{subsection.13.1.5}%
\contentsline {subsection}{\numberline {13.1.6}Multitask Loss: Classification and Regression}{465}{subsection.13.1.6}%
\contentsline {section}{\numberline {13.2}From Single-Object to Multi-Object Detection}{465}{section.13.2}%
\contentsline {subsection}{\numberline {13.2.1}Challenges in Detecting Multiple Objects}{466}{subsection.13.2.1}%
\contentsline {subsection}{\numberline {13.2.2}Sliding Window Approach}{466}{subsection.13.2.2}%
\contentsline {subsection}{\numberline {13.2.3}Region Proposal Methods}{467}{subsection.13.2.3}%
\contentsline {section}{\numberline {13.3}Naive Solution: Region-Based CNN (R-CNN)}{468}{section.13.3}%
\contentsline {subsection}{\numberline {13.3.1}Bounding Box Regression: Refining Object Localization}{469}{subsection.13.3.1}%
\contentsline {paragraph}{Why a Logarithmic Transformation?}{470}{section*.916}%
\contentsline {subsection}{\numberline {13.3.2}Training R-CNN}{471}{subsection.13.3.2}%
\contentsline {paragraph}{1) Collecting Positive and Negative Examples}{471}{section*.917}%
\contentsline {paragraph}{2) Fine-Tuning the CNN on Region Proposals (Classification Only)}{471}{section*.919}%
\contentsline {paragraph}{3) Training the Bounding Box Regressors}{472}{section*.920}%
\contentsline {paragraph}{4) Forming the Final Detector}{473}{section*.921}%
\contentsline {subsubsection}{Training Considerations for Object Detection}{473}{section*.922}%
\contentsline {subsection}{\numberline {13.3.3}Selecting Final Predictions for Object Detection}{474}{subsection.13.3.3}%
\contentsline {section}{\numberline {13.4}Non-Maximum Suppression (NMS)}{474}{section.13.4}%
\contentsline {subsection}{\numberline {13.4.1}Motivation: The Need for NMS}{474}{subsection.13.4.1}%
\contentsline {subsection}{\numberline {13.4.2}NMS Algorithm}{475}{subsection.13.4.2}%
\contentsline {subsection}{\numberline {13.4.3}Example: Step-by-Step Execution}{475}{subsection.13.4.3}%
\contentsline {subsection}{\numberline {13.4.4}Limitations of NMS}{476}{subsection.13.4.4}%
\contentsline {subsection}{\numberline {13.4.5}Refining NMS for Overlapping Objects}{476}{subsection.13.4.5}%
\contentsline {section}{\numberline {13.5}Evaluating Object Detectors: Mean Average Precision (mAP)}{476}{section.13.5}%
\contentsline {subsection}{\numberline {13.5.1}Key Evaluation Metrics}{477}{subsection.13.5.1}%
\contentsline {paragraph}{Precision and Recall}{477}{section*.926}%
\contentsline {paragraph}{\textbf {Trade-offs Between Precision and Recall}}{477}{section*.927}%
\contentsline {paragraph}{\textbf {Isn't F1 Score Suffice?}}{477}{section*.928}%
\contentsline {paragraph}{\textbf {Precision-Recall (PR) Curve and Average Precision (AP)}}{478}{section*.929}%
\contentsline {paragraph}{\textbf {Why the 0.5 IoU Threshold?}}{478}{section*.930}%
\contentsline {paragraph}{\textbf {Why AP is Preferable to the F1 Score:}}{478}{section*.931}%
\contentsline {subsection}{\numberline {13.5.2}Step-by-Step Example: Computing AP for a Single Class}{479}{subsection.13.5.2}%
\contentsline {subsection}{\numberline {13.5.3}Mean Average Precision (mAP)}{482}{subsection.13.5.3}%
\contentsline {subsection}{\numberline {13.5.4}COCO mAP: A Stricter Measure}{482}{subsection.13.5.4}%
\contentsline {subsubsection}{COCO mAP for Different Object Sizes}{482}{section*.938}%
\contentsline {paragraph}{When and Why Object Size-Specific mAP Matters}{483}{section*.940}%
\contentsline {subsubsection}{Prioritizing Specific Classes in Object Detection}{483}{section*.941}%
\contentsline {subsection}{\numberline {13.5.5}Evaluating Object Detectors: Key Takeaways}{484}{subsection.13.5.5}%
\contentsline {subsection}{Enrichment 13.5.6: Mosaic Augmentation for Object Detection}{485}{section*.942}%
\contentsline {paragraph}{Motivation and Advantages}{485}{section*.943}%
\contentsline {paragraph}{Implementation Considerations}{485}{section*.945}%
\contentsline {paragraph}{Domain-Dependent Utility}{486}{section*.947}%
\contentsline {paragraph}{Conclusion}{486}{section*.948}%
\contentsline {chapter}{\numberline {14}Lecture 14: Object Detectors}{487}{chapter.14}%
\contentsline {section}{\numberline {14.1}Beyond R-CNN: Advancing Object Detection}{487}{section.14.1}%
\contentsline {subsection}{\numberline {14.1.1}Looking Ahead: Beyond CNN-Based Object Detectors}{487}{subsection.14.1.1}%
\contentsline {section}{\numberline {14.2}Fast R-CNN: Accelerating Object Detection}{488}{section.14.2}%
\contentsline {subsection}{\numberline {14.2.1}Key Idea: Shared Feature Extraction}{488}{subsection.14.2.1}%
\contentsline {subsection}{\numberline {14.2.2}Using Fully Convolutional Deep Backbones for Feature Extraction}{489}{subsection.14.2.2}%
\contentsline {subsection}{\numberline {14.2.3}Region of Interest (RoI) Pooling}{490}{subsection.14.2.3}%
\contentsline {paragraph}{Mapping Region Proposals onto the Feature Map}{490}{section*.952}%
\contentsline {paragraph}{Dividing the Region into Fixed Bins}{490}{section*.953}%
\contentsline {paragraph}{Max Pooling within Each Bin}{490}{section*.954}%
\contentsline {paragraph}{Summary: Key Steps in RoI Pooling}{491}{section*.956}%
\contentsline {paragraph}{Limitations of RoI Pooling}{491}{section*.957}%
\contentsline {subsection}{\numberline {14.2.4}RoIAlign}{492}{subsection.14.2.4}%
\contentsline {subsubsection}{RoIAlign: A Visual Example}{493}{section*.959}%
\contentsline {paragraph}{Step 1: Projection of Region Proposal onto the Feature Map}{493}{section*.960}%
\contentsline {paragraph}{Step 2: Selecting Interpolation Points in Each Bin}{493}{section*.962}%
\contentsline {subparagraph}{Why Choose 0.25 and 0.75 for Sampling?}{494}{subparagraph*.964}%
\contentsline {paragraph}{Step 3: Mapping Sampled Points onto the Feature Grid}{495}{section*.965}%
\contentsline {paragraph}{Step 4: Computing Bilinear Interpolation Weights}{496}{section*.967}%
\contentsline {subparagraph}{Normalization Constant and Its Interpretation}{496}{subparagraph*.968}%
\contentsline {subparagraph}{Weight Computation for Each Corner}{496}{subparagraph*.969}%
\contentsline {paragraph}{Step 5: Computing the Interpolated Feature Value}{499}{section*.974}%
\contentsline {subparagraph}{\textbf {Example Computation}}{499}{subparagraph*.975}%
\contentsline {paragraph}{Step 6: Aggregating Interpolated Values}{500}{section*.976}%
\contentsline {subparagraph}{\textbf {Final Output}}{500}{subparagraph*.977}%
\contentsline {paragraph}{Key Takeaways}{500}{section*.979}%
\contentsline {paragraph}{RoIAlign Important Implementation Parts in PyTorch}{501}{section*.980}%
\contentsline {section}{\numberline {14.3}Faster R-CNN: Faster Proposals Using RPNs}{504}{section.14.3}%
\contentsline {subsection}{\numberline {14.3.1}Fast R-CNN Bottleneck: Region Proposal Computation}{504}{subsection.14.3.1}%
\contentsline {subsection}{\numberline {14.3.2}Towards Faster Region Proposals: Learning Proposals with CNNs}{504}{subsection.14.3.2}%
\contentsline {subsection}{\numberline {14.3.3}Region Proposal Networks (RPNs)}{505}{subsection.14.3.3}%
\contentsline {paragraph}{\textbf {How RPNs Work}}{505}{section*.982}%
\contentsline {paragraph}{\textbf {Anchor Boxes: Handling Scale and Aspect Ratio Variations}}{505}{section*.983}%
\contentsline {paragraph}{\textbf {Bounding Box Refinement: Aligning Anchors to Objects}}{507}{section*.987}%
\contentsline {paragraph}{\textbf {Training RPNs: Assigning Labels to Anchors}}{507}{section*.989}%
\contentsline {paragraph}{\textbf {Loss Function for RPN Training}}{508}{section*.990}%
\contentsline {subparagraph}{\textbf {Assigning Ground-Truth Bounding Boxes to Anchors}}{508}{subparagraph*.991}%
\contentsline {paragraph}{\textbf {Smooth \( L_1 \) Loss for Bounding Box Regression}}{508}{section*.992}%
\contentsline {paragraph}{\textbf {Why Use Negative Anchors?}}{509}{section*.993}%
\contentsline {subsubsection}{Enrichment 14.3.3.1: Training Region Proposal Networks (RPNs)}{509}{section*.994}%
\contentsline {paragraph}{1. Input Feature Map}{509}{section*.995}%
\contentsline {paragraph}{2. Sliding Window: Shared 3\(\times \)3 Conv}{509}{section*.996}%
\contentsline {paragraph}{3. RPN Heads: Anchor-wise Classification and Regression}{509}{section*.997}%
\contentsline {paragraph}{4. Anchor Labeling and Ground Truth Assignment}{510}{section*.998}%
\contentsline {paragraph}{5. Bounding-Box Regression Targets}{510}{section*.999}%
\contentsline {paragraph}{6. Loss Computation}{510}{section*.1000}%
\contentsline {paragraph}{\textbf {Inference: Generating Region Proposals}}{511}{section*.1002}%
\contentsline {paragraph}{\textbf {RPNs Improve Region Proposal Generation}}{511}{section*.1003}%
\contentsline {subsection}{\numberline {14.3.4}Faster R-CNN Loss in Practice: Joint Training with Four Losses}{512}{subsection.14.3.4}%
\contentsline {paragraph}{\textbf {Joint Training in Faster R-CNN}}{512}{section*.1004}%
\contentsline {paragraph}{\textbf {How RPN Improves Inference Speed}}{512}{section*.1005}%
\contentsline {subsection}{\numberline {14.3.5}Feature Pyramid Networks (FPNs): Multi-Scale Feature Learning}{513}{subsection.14.3.5}%
\contentsline {subsubsection}{Feature Pyramid Networks: A More Efficient Approach}{513}{section*.1008}%
\contentsline {subsubsection}{Enhancing Low-Level Features with High-Level Semantics}{514}{section*.1010}%
\contentsline {paragraph}{How Upsampling Works in FPNs}{515}{section*.1012}%
\contentsline {subsubsection}{Combining Results from Multiple Feature Levels}{515}{section*.1013}%
\contentsline {paragraph}{Advantages of FPNs}{515}{section*.1014}%
\contentsline {paragraph}{\textbf {The Two-Stage Object Detection Pipeline}}{516}{section*.1015}%
\contentsline {section}{\numberline {14.4}RetinaNet: A Breakthrough in Single-Stage Object Detection}{517}{section.14.4}%
\contentsline {subsection}{\numberline {14.4.1}Why Single-Stage Detectors Can Be Faster}{517}{subsection.14.4.1}%
\contentsline {subsection}{\numberline {14.4.2}The Class Imbalance Problem in Dense Detection}{517}{subsection.14.4.2}%
\contentsline {subsection}{\numberline {14.4.3}Focal Loss: Addressing Class Imbalance}{518}{subsection.14.4.3}%
\contentsline {subsection}{\numberline {14.4.4}RetinaNet Architecture and Pipeline}{520}{subsection.14.4.4}%
\contentsline {section}{\numberline {14.5}FCOS: An Anchor-Free, Fully Convolutional Detector}{521}{section.14.5}%
\contentsline {subsection}{\numberline {14.5.1}Core Pipeline and Feature Map Interpretation}{521}{subsection.14.5.1}%
\contentsline {subsection}{\numberline {14.5.2}Bounding Box Regression}{523}{subsection.14.5.2}%
\contentsline {subsection}{\numberline {14.5.3}Centerness: Filtering Low-Quality Predictions}{523}{subsection.14.5.3}%
\contentsline {subsection}{\numberline {14.5.4}Multi-Level Feature Prediction with FPN}{524}{subsection.14.5.4}%
\contentsline {subsection}{\numberline {14.5.5}Loss Function: Focal Loss and IoU Loss}{525}{subsection.14.5.5}%
\contentsline {subsection}{\numberline {14.5.6}Inference: Selecting Final Detections}{525}{subsection.14.5.6}%
\contentsline {subsection}{\numberline {14.5.7}Advantages of FCOS}{525}{subsection.14.5.7}%
\contentsline {section}{Enrichment 14.6: YOLO - You Only Look Once}{526}{section*.1024}%
\contentsline {subsection}{Enrichment 14.6.1: Background}{526}{section*.1025}%
\contentsline {subsection}{Enrichment 14.6.2: Step-by-Step: How YOLOv1 Processes an Input Image}{526}{section*.1026}%
\contentsline {paragraph}{1. Input Image and Preprocessing}{526}{section*.1027}%
\contentsline {paragraph}{2. Feature Extraction (DarkNet + Additional Convolution Layers)}{526}{section*.1028}%
\contentsline {paragraph}{3. Flattening and Fully Connected Layers}{526}{section*.1029}%
\contentsline {paragraph}{4. Understanding the Output Format}{527}{section*.1030}%
\contentsline {paragraph}{5. Why a Sigmoid?}{527}{section*.1031}%
\contentsline {paragraph}{6. Converting Predictions to Actual Bounding Boxes}{527}{section*.1032}%
\contentsline {paragraph}{7. Loss and Training (High Level)}{528}{section*.1033}%
\contentsline {paragraph}{8. Why It Works (and Its Trade-offs)}{528}{section*.1034}%
\contentsline {paragraph}{9. Final Detections and NMS}{528}{section*.1035}%
\contentsline {paragraph}{Summary}{529}{section*.1036}%
\contentsline {subsection}{Enrichment 14.6.3: Evolution of YOLO}{529}{section*.1038}%
\contentsline {section}{\numberline {14.7}Conclusion: The Evolution of Object Detection}{530}{section.14.7}%
\contentsline {paragraph}{From R-CNN to Faster R-CNN: Learning Region Proposals}{530}{section*.1039}%
\contentsline {paragraph}{Improving Multi-Scale Detection: Feature Pyramid Networks (FPN)}{530}{section*.1040}%
\contentsline {paragraph}{RetinaNet: A Breakthrough for One-Stage Detectors}{530}{section*.1041}%
\contentsline {paragraph}{FCOS: Moving Toward Anchor-Free Detection}{530}{section*.1042}%
\contentsline {paragraph}{YOLO: A Widely Used Real-Time Detector}{531}{section*.1043}%
\contentsline {paragraph}{Looking Ahead: Transformers and SOTA Detectors}{531}{section*.1044}%
\contentsline {paragraph}{Summary}{531}{section*.1045}%
\contentsline {chapter}{\numberline {15}Lecture 15: Image Segmentation}{532}{chapter.15}%
\contentsline {section}{\numberline {15.1}From Object Detection to Segmentation}{532}{section.15.1}%
\contentsline {section}{Enrichment 15.2: Why is Object Detection Not Enough?}{533}{section*.1047}%
\contentsline {section}{\numberline {15.3}Advancements in Semantic Segmentation}{534}{section.15.3}%
\contentsline {subsection}{\numberline {15.3.1}Early Approaches: Sliding Window Method}{534}{subsection.15.3.1}%
\contentsline {subsection}{\numberline {15.3.2}Fully Convolutional Networks (FCNs)}{534}{subsection.15.3.2}%
\contentsline {subsection}{\numberline {15.3.3}Challenges in FCNs for Semantic Segmentation}{535}{subsection.15.3.3}%
\contentsline {subsection}{\numberline {15.3.4}Encoder-Decoder Architectures}{535}{subsection.15.3.4}%
\contentsline {section}{\numberline {15.4}Upsampling and Unpooling}{536}{section.15.4}%
\contentsline {subsection}{\numberline {15.4.1}Bed of Nails Unpooling}{537}{subsection.15.4.1}%
\contentsline {paragraph}{Limitations of Bed of Nails Unpooling}{537}{section*.1052}%
\contentsline {subsection}{\numberline {15.4.2}Nearest-Neighbor Unpooling}{538}{subsection.15.4.2}%
\contentsline {subsection}{\numberline {15.4.3}Bilinear Interpolation for Upsampling}{539}{subsection.15.4.3}%
\contentsline {subsubsection}{Bilinear Interpolation: Generalized Case}{539}{section*.1055}%
\contentsline {subsubsection}{Advantages of Bilinear Interpolation}{541}{section*.1057}%
\contentsline {subsubsection}{Limitations and Transition to Bicubic Interpolation}{541}{section*.1058}%
\contentsline {subsection}{\numberline {15.4.4}Bicubic Interpolation for Upsampling}{541}{subsection.15.4.4}%
\contentsline {subsubsection}{Why Bicubic Interpolation?}{541}{section*.1059}%
\contentsline {subsubsection}{Mathematical Reasoning}{541}{section*.1060}%
\contentsline {subsubsection}{Bicubic Interpolation: Generalized Case}{542}{section*.1061}%
\contentsline {subsubsection}{Advantages and Limitations}{543}{section*.1063}%
\contentsline {subsection}{\numberline {15.4.5}Max Unpooling}{543}{subsection.15.4.5}%
\contentsline {subsubsection}{Max Unpooling in the Context of Noh et al. (ICCV 2015)}{543}{section*.1064}%
\contentsline {subsubsection}{Why Max Unpooling is More Effective Than Bed of Nails Unpooling}{544}{section*.1066}%
\contentsline {subsubsection}{Bridging to Transposed Convolution}{544}{section*.1067}%
\contentsline {subsection}{\numberline {15.4.6}Transposed Convolution}{545}{subsection.15.4.6}%
\contentsline {subsubsection}{Understanding the Similarity to Standard Convolution}{545}{section*.1068}%
\contentsline {subsubsection}{Step-by-Step Process of Transposed Convolution}{545}{section*.1069}%
\contentsline {subsubsection}{1D Transposed Convolution}{547}{section*.1073}%
\contentsline {subsubsection}{Advantages of Transposed Convolution}{548}{section*.1075}%
\contentsline {subsubsection}{Connection to Standard Convolution}{548}{section*.1076}%
\contentsline {subsection}{\numberline {15.4.7}Convolution and Transposed Convolution as Matrix Multiplication}{548}{subsection.15.4.7}%
\contentsline {subsubsection}{Convolution via Matrix Multiplication}{548}{section*.1077}%
\contentsline {subsubsection}{Transposed Convolution via Matrix Multiplication (Stride = 1)}{549}{section*.1079}%
\contentsline {subsubsection}{Transposed Convolution and Gradient Derivation}{550}{section*.1081}%
\contentsline {subsubsection}{Advantages of Transposed Convolution}{551}{section*.1082}%
\contentsline {subsubsection}{Challenges and Considerations}{551}{section*.1083}%
\contentsline {subsection}{\numberline {15.4.8}Conclusion: Choosing the Right Upsampling Method}{551}{subsection.15.4.8}%
\contentsline {subsubsection}{Guidelines for Choosing an Upsampling Method}{551}{section*.1085}%
\contentsline {subsubsection}{Final Thoughts}{552}{section*.1086}%
\contentsline {section}{\numberline {15.5}Instance Segmentation}{552}{section.15.5}%
\contentsline {subsection}{\numberline {15.5.1}Mask R-CNN: A Two-Stage Framework for Instance Segmentation}{553}{subsection.15.5.1}%
\contentsline {subsubsection}{Faster R-CNN Backbone}{553}{section*.1087}%
\contentsline {subsubsection}{Key Additions in Mask R-CNN}{553}{section*.1088}%
\contentsline {subsubsection}{Segmentation Mask Prediction: Fixed Size Output}{553}{section*.1089}%
\contentsline {subsubsection}{Training Mask R-CNN and Loss Functions}{553}{section*.1090}%
\contentsline {subsubsection}{Bilinear Interpolation vs. Bicubic Interpolation}{554}{section*.1091}%
\contentsline {subsubsection}{Class-Aware Mask Selection}{554}{section*.1092}%
\contentsline {subsubsection}{Gradient Flow in Mask R-CNN}{554}{section*.1093}%
\contentsline {subsubsection}{Summary}{555}{section*.1094}%
\contentsline {subsection}{\numberline {15.5.2}Extending the Object Detection Paradigm}{555}{subsection.15.5.2}%
\contentsline {section}{Enrichment 15.6: U-Net: A Fully Conv Architecture for Segmentation}{558}{section*.1099}%
\contentsline {subsection}{Enrichment 15.6.1: Overview}{558}{section*.1100}%
\contentsline {subsection}{Enrichment 15.6.2: U-Net Architecture}{558}{section*.1101}%
\contentsline {subsection}{Enrichment 15.6.3: Skip Connections and Concatenation}{559}{section*.1103}%
\contentsline {subsection}{Enrichment 15.6.4: Training U-Net}{559}{section*.1104}%
\contentsline {subsection}{Enrichment 15.6.5: Comparison with Mask R-CNN}{559}{section*.1105}%
\contentsline {subsection}{Enrichment 15.6.6: Impact and Evolution of U-Net}{560}{section*.1106}%
\contentsline {section}{Enrichment 15.7: Striding Towards SOTA Image Segmentation}{561}{section*.1107}%
\contentsline {subsection}{Enrichment 15.7.1: SAM: Segment Anything Model}{562}{section*.1108}%
\contentsline {paragraph}{Background}{562}{section*.1109}%
\contentsline {paragraph}{Contribution and innovation}{562}{section*.1111}%
\contentsline {paragraph}{Zero-shot segmentation by prompting (and ambiguity-aware decoding)}{563}{section*.1112}%
\contentsline {subsubsection}{Method}{566}{section*.1115}%
\contentsline {paragraph}{Model overview and data flow}{566}{section*.1116}%
\contentsline {paragraph}{Image encoder}{566}{section*.1118}%
\contentsline {paragraph}{Prompt encoder}{566}{section*.1119}%
\contentsline {paragraph}{Positional encodings for 2D prompts}{567}{section*.1120}%
\contentsline {paragraph}{Mask decoder (two-way attention and dynamic heads)}{571}{section*.1129}%
\contentsline {paragraph}{Training objective and loss}{572}{section*.1131}%
\contentsline {paragraph}{Pseudo-code for interactive inference}{574}{section*.1133}%
\contentsline {subsubsection}{Data engine and SA-1B}{575}{section*.1134}%
\contentsline {paragraph}{Dataset properties and diversity}{575}{section*.1136}%
\contentsline {subsubsection}{Experiments and ablations}{576}{section*.1140}%
\contentsline {paragraph}{Zero-shot samples across domains}{576}{section*.1141}%
\contentsline {paragraph}{Interactive point-to-mask evaluation}{577}{section*.1143}%
\contentsline {paragraph}{Ablations (highlights)}{577}{section*.1145}%
\contentsline {subsubsection}{Limitations and future directions}{578}{section*.1146}%
\contentsline {subsection}{Enrichment 15.7.2: SAM 2: Segment Anything in Images and Videos}{579}{section*.1147}%
\contentsline {subsubsection}{Motivation}{579}{section*.1149}%
\contentsline {subsubsection}{Method}{580}{section*.1151}%
\contentsline {paragraph}{Problem setup}{580}{section*.1152}%
\contentsline {paragraph}{What is new compared to SAM}{580}{section*.1153}%
\contentsline {paragraph}{Why streaming memory}{580}{section*.1154}%
\contentsline {paragraph}{High-level data flow}{580}{section*.1155}%
\contentsline {paragraph}{Streaming memory mechanics}{581}{section*.1156}%
\contentsline {paragraph}{Prompt encoder}{582}{section*.1157}%
\contentsline {paragraph}{Mask decoder with memory conditioning}{582}{section*.1158}%
\contentsline {paragraph}{Training objective and supervision}{582}{section*.1159}%
\contentsline {paragraph}{Pseudo-code for streaming interactive inference}{583}{section*.1160}%
\contentsline {subsubsection}{Architecture \& Implementation Details}{583}{section*.1161}%
\contentsline {subsubsection}{Experiments and Ablations}{583}{section*.1163}%
\contentsline {paragraph}{SA-V dataset and data engine}{583}{section*.1164}%
\contentsline {paragraph}{Zero-shot semi-supervised VOS}{585}{section*.1168}%
\contentsline {paragraph}{Segment Anything across 37 datasets}{585}{section*.1170}%
\contentsline {paragraph}{Ablations}{586}{section*.1172}%
\contentsline {subsubsection}{Limitations and Future Directions}{587}{section*.1173}%
\contentsline {chapter}{\numberline {16}Lecture 16: Recurrent Networks}{588}{chapter.16}%
\contentsline {section}{\numberline {16.1}Introduction to Recurrent Neural Networks (RNNs)}{588}{section.16.1}%
\contentsline {subsection}{\numberline {16.1.1}Why Study Sequential Models?}{588}{subsection.16.1.1}%
\contentsline {subsection}{\numberline {16.1.2}RNNs as a General-Purpose Sequence Model}{589}{subsection.16.1.2}%
\contentsline {subsection}{\numberline {16.1.3}RNNs for Visual Attention and Image Generation}{589}{subsection.16.1.3}%
\contentsline {subsubsection}{Visual Attention: Sequential Image Processing}{589}{section*.1175}%
\contentsline {subsubsection}{Autoregressive Image Generation with RNNs}{590}{section*.1176}%
\contentsline {subsection}{\numberline {16.1.4}Limitations of Traditional Neural Networks for Sequential Data}{590}{subsection.16.1.4}%
\contentsline {subsection}{\numberline {16.1.5}Overview of Recurrent Neural Networks (RNNs) and Their Evolution}{590}{subsection.16.1.5}%
\contentsline {subsubsection}{RNN Progression: From Vanilla to Gated Units}{590}{section*.1178}%
\contentsline {paragraph}{Vanilla RNNs}{590}{section*.1179}%
\contentsline {paragraph}{Long Short-Term Memory (LSTM)}{590}{section*.1180}%
\contentsline {paragraph}{Gated Recurrent Units (GRUs)}{591}{section*.1181}%
\contentsline {paragraph}{Bidirectional RNNs}{591}{section*.1182}%
\contentsline {subsubsection}{Motivation Toward Transformers}{591}{section*.1183}%
\contentsline {subsubsection}{Bridging to Detailed Explanations}{591}{section*.1184}%
\contentsline {section}{\numberline {16.2}Recurrent Neural Networks (RNNs) - How They Work}{592}{section.16.2}%
\contentsline {subsection}{\numberline {16.2.1}RNN Computational Graph}{592}{subsection.16.2.1}%
\contentsline {subsubsection}{Many-to-Many}{592}{section*.1185}%
\contentsline {subsubsection}{Many-to-One}{593}{section*.1187}%
\contentsline {subsubsection}{One-to-Many}{594}{section*.1189}%
\contentsline {subsection}{\numberline {16.2.2}Seq2Seq: Sequence-to-Sequence Learning}{594}{subsection.16.2.2}%
\contentsline {subsubsection}{Significance of Seq2Seq Models}{596}{section*.1192}%
\contentsline {section}{\numberline {16.3}Example Usage of Seq2Seq: Language Modeling}{596}{section.16.3}%
\contentsline {subsection}{\numberline {16.3.1}Formulating the Problem}{596}{subsection.16.3.1}%
\contentsline {subsection}{\numberline {16.3.2}One-Hot Encoding of Input Characters}{597}{subsection.16.3.2}%
\contentsline {subsubsection}{Advantages of One-Hot Encoding}{597}{section*.1193}%
\contentsline {subsection}{\numberline {16.3.3}Processing the First Character}{597}{subsection.16.3.3}%
\contentsline {subsection}{\numberline {16.3.4}Computing Loss Across Time Steps}{598}{subsection.16.3.4}%
\contentsline {subsection}{\numberline {16.3.5}Generating Text with a Trained RNN}{598}{subsection.16.3.5}%
\contentsline {subsection}{\numberline {16.3.6}Using an Embedding Layer for Character Inputs}{599}{subsection.16.3.6}%
\contentsline {subsection}{\numberline {16.3.7}Conclusion and Next Steps}{600}{subsection.16.3.7}%
\contentsline {section}{\numberline {16.4}Backpropagation Through Time (BPTT)}{600}{section.16.4}%
\contentsline {subsection}{\numberline {16.4.1}Mathematical Formulation of BPTT and Memory Constraints}{600}{subsection.16.4.1}%
\contentsline {subsection}{\numberline {16.4.2}Truncated Backpropagation Through Time}{601}{subsection.16.4.2}%
\contentsline {subsubsection}{Loss Processing in Truncated BPTT}{601}{section*.1197}%
\contentsline {subsection}{\numberline {16.4.3}Why BPTT Fails for Long Sequences}{602}{subsection.16.4.3}%
\contentsline {section}{\numberline {16.5}Why RNNs Use \textit {tanh} Instead of ReLU}{602}{section.16.5}%
\contentsline {subsection}{\numberline {16.5.1}Recurrent Computation and Gradient Behavior}{602}{subsection.16.5.1}%
\contentsline {subsubsection}{Repeated Multiplication and the Hidden State}{602}{section*.1198}%
\contentsline {paragraph}{Spectral Properties of \(\mathbf {W}_{hh}\).}{603}{section*.1199}%
\contentsline {subsubsection}{How Large or Small States Affect Gradients}{603}{section*.1200}%
\contentsline {paragraph}{Effect of Activation Function}{603}{section*.1201}%
\contentsline {subsection}{\numberline {16.5.2}Mathematical Rationale for \textit {tanh} in RNNs}{603}{subsection.16.5.2}%
\contentsline {subsubsection}{How \textit {tanh} Curbs Exploding Gradients}{604}{section*.1202}%
\contentsline {paragraph}{1. Bounded Outputs:}{604}{section*.1203}%
\contentsline {paragraph}{2. Derivative Control:}{604}{section*.1204}%
\contentsline {paragraph}{3. Zero-Centered Activation:}{604}{section*.1205}%
\contentsline {paragraph}{A Caveat: Vanishing Gradients Still Remain}{604}{section*.1206}%
\contentsline {subsection}{\numberline {16.5.3}Why ReLU6 or Leaky ReLU Are Not a Full Remedy}{605}{subsection.16.5.3}%
\contentsline {paragraph}{ReLU6: The Saturation Issue}{605}{section*.1207}%
\contentsline {paragraph}{Leaky ReLU: A Partial Fix with Remaining Instability}{605}{section*.1208}%
\contentsline {subsection}{\numberline {16.5.4}Why Gradient Clipping Alone is Insufficient}{605}{subsection.16.5.4}%
\contentsline {paragraph}{Clipping Does Not Prevent Hidden State Growth}{606}{section*.1209}%
\contentsline {section}{\numberline {16.6}Example Usages of Recurrent Neural Networks}{607}{section.16.6}%
\contentsline {subsection}{\numberline {16.6.1}RNNs for Text-Based Tasks}{607}{subsection.16.6.1}%
\contentsline {subsubsection}{Generating Text with RNNs}{607}{section*.1210}%
\contentsline {subsection}{\numberline {16.6.2}Understanding What RNNs Learn}{607}{subsection.16.6.2}%
\contentsline {paragraph}{Visualization of Hidden State Activations}{607}{section*.1211}%
\contentsline {subsubsection}{Interpretable Hidden Units}{608}{section*.1213}%
\contentsline {paragraph}{Quote Detection Cell}{608}{section*.1214}%
\contentsline {paragraph}{Line Length Tracking Cell}{609}{section*.1216}%
\contentsline {paragraph}{Other Interpretable Hidden Units}{609}{section*.1218}%
\contentsline {subsubsection}{Key Takeaways from Interpretable Units}{609}{section*.1219}%
\contentsline {subsection}{\numberline {16.6.3}Image Captioning}{610}{subsection.16.6.3}%
\contentsline {subsection}{\numberline {16.6.4}Image Captioning Results}{611}{subsection.16.6.4}%
\contentsline {subsection}{\numberline {16.6.5}Failure Cases in Image Captioning}{611}{subsection.16.6.5}%
\contentsline {subsection}{\numberline {16.6.6}Bridging to LSTMs and GRUs: The Need for Gated Memory}{612}{subsection.16.6.6}%
\contentsline {section}{\numberline {16.7}Long Short-Term Memory (LSTM) Overview}{613}{section.16.7}%
\contentsline {subsection}{\numberline {16.7.1}LSTM Gating Mechanism}{613}{subsection.16.7.1}%
\contentsline {subsection}{\numberline {16.7.2}LSTM Gate Computation}{613}{subsection.16.7.2}%
\contentsline {subsection}{\numberline {16.7.3}LSTM State Updates}{614}{subsection.16.7.3}%
\contentsline {subsection}{\numberline {16.7.4}Gradient Flow in LSTMs}{615}{subsection.16.7.4}%
\contentsline {subsubsection}{Why the Cell State $\mathbf {c}_t$ Preserves Long-Term Information}{615}{section*.1224}%
\contentsline {subsubsection}{Why $\mathbf {f}_t$ Prevents Vanishing Gradients}{616}{section*.1225}%
\contentsline {subsubsection}{Why $\mathbf {f}_t$ Can Be Learned to Stay Near 1}{616}{section*.1226}%
\contentsline {subsubsection}{Why Forget Gates Do Not Cause Exponential Decay Like RNN Activations}{617}{section*.1227}%
\contentsline {subsubsection}{How Hidden-State Gradients Differ}{617}{section*.1228}%
\contentsline {paragraph}{Consequence for Training}{617}{section*.1229}%
\contentsline {subsubsection}{Why Weight-Gradient Vanishing Is Less Critical}{617}{section*.1230}%
\contentsline {subsubsection}{Mitigating Exploding Gradients}{618}{section*.1231}%
\contentsline {section}{\numberline {16.8}Resemblance of LSTMs to Highway Networks and ResNets}{618}{section.16.8}%
\contentsline {subsection}{\numberline {16.8.1}Highway Networks and LSTMs}{618}{subsection.16.8.1}%
\contentsline {subsection}{\numberline {16.8.2}ResNets and LSTMs}{619}{subsection.16.8.2}%
\contentsline {paragraph}{Differences Between ResNets and Highway Networks}{619}{section*.1233}%
\contentsline {subsection}{\numberline {16.8.3}Summary of LSTM, Highway, and ResNet Connections}{620}{subsection.16.8.3}%
\contentsline {section}{\numberline {16.9}Stacking Layers in RNNs and LSTMs}{620}{section.16.9}%
\contentsline {subsection}{\numberline {16.9.1}Architecture of Stacked RNNs and LSTMs}{620}{subsection.16.9.1}%
\contentsline {subsection}{\numberline {16.9.2}Practical Limitations of Deep RNN Architectures}{621}{subsection.16.9.2}%
\contentsline {subsection}{\numberline {16.9.3}Deep RNNs: Balancing Depth and Efficiency}{621}{subsection.16.9.3}%
\contentsline {section}{Enrichment 16.10: Other RNN Variants: GRU}{622}{section*.1237}%
\contentsline {subsection}{Enrichment 16.10.1: GRU Architecture}{622}{section*.1238}%
\contentsline {paragraph}{Key Observations:}{623}{section*.1240}%
\contentsline {subsection}{Enrichment 16.10.2: Gradient Flow in GRUs}{623}{section*.1241}%
\contentsline {subsection}{Enrichment 16.10.3: Advantages of GRUs over LSTMs}{624}{section*.1245}%
\contentsline {subsection}{Enrichment 16.10.4: Limitations of GRUs}{624}{section*.1246}%
\contentsline {subsection}{Enrichment 16.10.5: Comparison with LSTMs}{624}{section*.1247}%
\contentsline {subsection}{Enrichment 16.10.6: Bridging to Advanced Architectures}{625}{section*.1248}%
\contentsline {section}{\numberline {16.11}Summary and Future Directions}{625}{section.16.11}%
\contentsline {subsection}{\numberline {16.11.1}Neural Architecture Search for Improved RNNs}{625}{subsection.16.11.1}%
\contentsline {subsection}{\numberline {16.11.2}Summary of RNN Architectures}{626}{subsection.16.11.2}%
\contentsline {subsection}{\numberline {16.11.3}Beyond RNNs: From Recurrence to Attention}{626}{subsection.16.11.3}%
\contentsline {chapter}{\numberline {17}Lecture 17: Attention}{627}{chapter.17}%
\contentsline {section}{\numberline {17.1}Limitations of Sequence-to-Sequence with RNNs}{627}{section.17.1}%
\contentsline {section}{\numberline {17.2}Introducing the Attention Mechanism}{628}{section.17.2}%
\contentsline {subsubsection}{Intuition Behind Attention}{630}{section*.1255}%
\contentsline {subsection}{\numberline {17.2.1}Benefits of Attention}{630}{subsection.17.2.1}%
\contentsline {subsection}{\numberline {17.2.2}Attention Interpretability}{630}{subsection.17.2.2}%
\contentsline {subsubsection}{Attention Maps: Visualizing Model Decisions}{630}{section*.1256}%
\contentsline {subsubsection}{Understanding Attention Patterns}{631}{section*.1258}%
\contentsline {subsubsection}{Why Attention Interpretability Matters}{632}{section*.1259}%
\contentsline {section}{\numberline {17.3}Applying Attention to Image Captioning}{632}{section.17.3}%
\contentsline {subsection}{\numberline {17.3.1}Feature Representation and Attention Computation}{632}{subsection.17.3.1}%
\contentsline {subsection}{\numberline {17.3.2}Generalizing to Any Timestep \( t \)}{634}{subsection.17.3.2}%
\contentsline {subsection}{\numberline {17.3.3}Example: Captioning an Image of a Cat}{634}{subsection.17.3.3}%
\contentsline {subsection}{\numberline {17.3.4}Visualizing Attention in Image Captioning}{635}{subsection.17.3.4}%
\contentsline {subsubsection}{Hard vs. Soft Attention}{635}{section*.1262}%
\contentsline {subsection}{\numberline {17.3.5}Biological Inspiration: Saccades in Human Vision}{636}{subsection.17.3.5}%
\contentsline {subsection}{\numberline {17.3.6}Beyond Captioning: Generalizing Attention Mechanisms}{637}{subsection.17.3.6}%
\contentsline {section}{\numberline {17.4}Attention Layer}{638}{section.17.4}%
\contentsline {subsection}{\numberline {17.4.1}Scaled Dot-Product Attention}{638}{subsection.17.4.1}%
\contentsline {paragraph}{Why Scale by \(\sqrt {D_Q}\)?}{639}{section*.1265}%
\contentsline {paragraph}{Scaling and Softmax Temperature}{639}{section*.1266}%
\contentsline {paragraph}{Why Dot Product?}{640}{section*.1267}%
\contentsline {subsection}{\numberline {17.4.2}Extending to Multiple Query Vectors}{640}{subsection.17.4.2}%
\contentsline {paragraph}{Benefits of Multiple Queries:}{640}{section*.1268}%
\contentsline {subsection}{\numberline {17.4.3}Introducing Key and Value Vectors}{641}{subsection.17.4.3}%
\contentsline {paragraph}{Why Separate Keys and Values?}{641}{section*.1269}%
\contentsline {subsection}{\numberline {17.4.4}An Analogy: Search Engines}{641}{subsection.17.4.4}%
\contentsline {subsubsection}{Empire State Building Example}{641}{section*.1270}%
\contentsline {subsubsection}{Why This Separation Matters}{642}{section*.1271}%
\contentsline {subsection}{\numberline {17.4.5}Bridging to Visualization and Further Understanding}{642}{subsection.17.4.5}%
\contentsline {subsubsection}{Overview of the Attention Layer Steps}{642}{section*.1272}%
\contentsline {subsection}{\numberline {17.4.6}Towards Self-Attention}{644}{subsection.17.4.6}%
\contentsline {section}{\numberline {17.5}Self-Attention}{645}{section.17.5}%
\contentsline {subsection}{\numberline {17.5.1}Mathematical Formulation of Self-Attention}{645}{subsection.17.5.1}%
\contentsline {subsection}{\numberline {17.5.2}Non-Linearity in Self-Attention}{646}{subsection.17.5.2}%
\contentsline {subsection}{\numberline {17.5.3}Permutation Equivariance in Self-Attention}{647}{subsection.17.5.3}%
\contentsline {subsubsection}{When is Permutation Equivariance a Problem?}{647}{section*.1276}%
\contentsline {subsection}{\numberline {17.5.4}Positional Encodings: Introduction}{648}{subsection.17.5.4}%
\contentsline {paragraph}{Why Not Use Simple Positional Indices?}{648}{section*.1277}%
\contentsline {subsection}{\numberline {17.5.5}Sinusoidal Positional Encoding}{648}{subsection.17.5.5}%
\contentsline {subsubsection}{Mathematical Definition}{649}{section*.1278}%
\contentsline {paragraph}{Intuition: Multiple Frequencies for Local \& Global Positioning}{649}{section*.1279}%
\contentsline {subsubsection}{Removing Ambiguity with Sine and Cosine}{649}{section*.1281}%
\contentsline {subsubsection}{Why Use \(\displaystyle 10000\) in the Denominator?}{650}{section*.1283}%
\contentsline {subsubsection}{Frequency Variation and Intuition}{650}{section*.1284}%
\contentsline {paragraph}{Concrete Example:}{652}{section*.1286}%
\contentsline {subsubsection}{How Relative Position Awareness Emerges}{652}{section*.1287}%
\contentsline {paragraph}{Why This Matters for Relative Positioning}{652}{section*.1288}%
\contentsline {subsubsection}{Does Positional Information Vanish in Deeper Layers?}{653}{section*.1290}%
\contentsline {subsubsection}{Why Sinusoidal Encoding Solves Previous Limitations}{653}{section*.1291}%
\contentsline {subsubsection}{Conclusion on Sinusoidal Positional Encoding}{653}{section*.1292}%
\contentsline {subsection}{\numberline {17.5.6}Learned Positional Encodings: An Alternative Approach}{654}{subsection.17.5.6}%
\contentsline {paragraph}{Definition and Mechanics}{654}{section*.1293}%
\contentsline {paragraph}{Examples of Learned Positional Encodings}{654}{section*.1294}%
\contentsline {subparagraph}{Highlighting Crucial Positions or Transitions}{654}{subparagraph*.1295}%
\contentsline {paragraph}{Why Task-Specific Optimization of Position is Useful}{654}{section*.1296}%
\contentsline {subsubsection}{Pros \& Cons of Learned Positional Embeddings}{655}{section*.1297}%
\contentsline {paragraph}{Conclusion on Learned Positional Embeddings}{656}{section*.1298}%
\contentsline {subsection}{\numberline {17.5.7}Masked Self-Attention Layer}{657}{subsection.17.5.7}%
\contentsline {subsubsection}{Why Do We Need Masking?}{657}{section*.1299}%
\contentsline {subsubsection}{Applying the Mask in Attention Computation}{657}{section*.1300}%
\contentsline {subsubsection}{How Masking Affects the Attention Weights}{657}{section*.1301}%
\contentsline {subsubsection}{Example of Masking in a Short Sequence}{658}{section*.1303}%
\contentsline {subsubsection}{Handling Batches with Variable-Length Sequences}{658}{section*.1304}%
\contentsline {paragraph}{Why is Padding Necessary?}{658}{section*.1305}%
\contentsline {subsubsection}{Moving on to Input Processing with Self-Attention}{659}{section*.1306}%
\contentsline {subsection}{\numberline {17.5.8}Processing Inputs with Self-Attention}{660}{subsection.17.5.8}%
\contentsline {subsubsection}{Parallelization in Self-Attention}{660}{section*.1307}%
\contentsline {subsubsection}{Handling Batches of Sequences with Different Lengths}{661}{section*.1308}%
\contentsline {subsubsection}{Why is Self-Attention Parallelizable?}{662}{section*.1309}%
\contentsline {subsubsection}{Computational Complexity of Self-Attention, RNNs, and Convolutions}{662}{section*.1310}%
\contentsline {subsubsection}{When is Self-Attention Computationally Efficient?}{662}{section*.1311}%
\contentsline {subsubsection}{Conclusion: When to Use Self-Attention?}{663}{section*.1312}%
\contentsline {subsection}{\numberline {17.5.9}Multi-Head Self-Attention Layer}{664}{subsection.17.5.9}%
\contentsline {subsubsection}{Motivation}{664}{section*.1313}%
\contentsline {paragraph}{Analogy with Convolutional Kernels}{664}{section*.1314}%
\contentsline {paragraph}{Diversity in Attention Patterns}{664}{section*.1315}%
\contentsline {subsubsection}{How Multi-Head Attention Works}{664}{section*.1316}%
\contentsline {paragraph}{Splitting Dimensions}{664}{section*.1317}%
\contentsline {paragraph}{Computing Multi-Head Attention}{665}{section*.1318}%
\contentsline {paragraph}{Concatenation and Output Projection}{665}{section*.1319}%
\contentsline {subsubsection}{Optimized Implementation and Linear Projection}{666}{section*.1321}%
\contentsline {subsubsection}{PyTorch Implementation of Multi-Head Attention}{667}{section*.1322}%
\contentsline {subsubsection}{Stepping Stone to Transformers and Vision Applications}{667}{section*.1323}%
\contentsline {subsection}{\numberline {17.5.10}Self-Attention for Vision Applications}{668}{subsection.17.5.10}%
\contentsline {paragraph}{Generating Queries, Keys, and Values}{668}{section*.1324}%
\contentsline {paragraph}{Reshaping for Attention Computation}{668}{section*.1325}%
\contentsline {paragraph}{Computing Attention Scores}{668}{section*.1326}%
\contentsline {paragraph}{Normalizing Attention Weights}{669}{section*.1327}%
\contentsline {paragraph}{Computing the Attention Output}{669}{section*.1328}%
\contentsline {paragraph}{Reshaping, Final Projection, and Residual Connection}{669}{section*.1329}%
\contentsline {paragraph}{Summary}{670}{section*.1331}%
\contentsline {subsubsection}{Bridging Towards Transformers}{670}{section*.1332}%
\contentsline {section}{\numberline {17.6}Transformer}{671}{section.17.6}%
\contentsline {subsection}{\numberline {17.6.1}Motivation and Introduction}{671}{subsection.17.6.1}%
\contentsline {subsubsection}{Three Ways of Processing Sequences}{671}{section*.1333}%
\contentsline {paragraph}{Recurrent Neural Networks (RNNs)}{671}{section*.1334}%
\contentsline {paragraph}{1D Convolution for Sequence Processing}{671}{section*.1335}%
\contentsline {paragraph}{Self-Attention Mechanism}{672}{section*.1336}%
\contentsline {subsection}{\numberline {17.6.2}Why the Transformer?}{672}{subsection.17.6.2}%
\contentsline {subsection}{\numberline {17.6.3}Seq2Seq Original Transformer Workflow}{674}{subsection.17.6.3}%
\contentsline {subsubsection}{Transformer Encoder Block: Structure and Reasoning}{676}{section*.1340}%
\contentsline {subsubsection}{Transformer Decoder Block: Structure and Reasoning}{677}{section*.1341}%
\contentsline {subsubsection}{Transitioning to Unified Transformer Blocks}{678}{section*.1342}%
\contentsline {subsection}{\numberline {17.6.4}The Modern Transformer Block}{679}{subsection.17.6.4}%
\contentsline {subsubsection}{Structure of the Modern Transformer Block}{679}{section*.1344}%
\contentsline {subsubsection}{PyTorch Implementation}{680}{section*.1345}%
\contentsline {subsubsection}{Why the Modern Transformer Block?}{681}{section*.1346}%
\contentsline {subsubsection}{Key Benefits of the Modern Transformer Block}{681}{section*.1348}%
\contentsline {subsubsection}{Further Reading and Resources}{683}{section*.1351}%
\contentsline {subsubsection}{Bridging Towards Vision Transformers}{683}{section*.1352}%
\contentsline {chapter}{\numberline {18}Lecture 18: Vision Transformers}{684}{chapter.18}%
\contentsline {section}{\numberline {18.1}Bringing Transformers to Vision Tasks}{684}{section.18.1}%
\contentsline {section}{\numberline {18.2}Integrating Attention into Convolutional Neural Networks (CNNs)}{685}{section.18.2}%
\contentsline {subsection}{\numberline {18.2.1}How Does It Work?}{685}{subsection.18.2.1}%
\contentsline {subsection}{\numberline {18.2.2}Limitations of Adding Attention to CNNs}{685}{subsection.18.2.2}%
\contentsline {section}{\numberline {18.3}Replacing Convolution with Local Attention Mechanisms}{687}{section.18.3}%
\contentsline {subsection}{\numberline {18.3.1}How Does Local Attention Work?}{687}{subsection.18.3.1}%
\contentsline {subsection}{\numberline {18.3.2}Why is Local Attention More Flexible than Convolutions?}{688}{subsection.18.3.2}%
\contentsline {subsection}{\numberline {18.3.3}Computational Complexity Comparison: Local Attention vs.\ Convolution}{688}{subsection.18.3.3}%
\contentsline {paragraph}{Convolutional Complexity}{688}{section*.1355}%
\contentsline {paragraph}{Local Attention Complexity}{688}{section*.1356}%
\contentsline {paragraph}{Why is Local Attention More Expensive?}{689}{section*.1357}%
\contentsline {paragraph}{Summary}{689}{section*.1358}%
\contentsline {paragraph}{From Local Attention to ViTs}{689}{section*.1359}%
\contentsline {section}{\numberline {18.4}Vision Transformers (ViTs): From Pixels to Patches}{690}{section.18.4}%
\contentsline {subsection}{\numberline {18.4.1}Splitting an Image into Patches}{690}{subsection.18.4.1}%
\contentsline {subsection}{\numberline {18.4.2}Class Token and Positional Encoding}{691}{subsection.18.4.2}%
\contentsline {subsection}{\numberline {18.4.3}Final Processing: From Context Token to Classification}{692}{subsection.18.4.3}%
\contentsline {subsection}{\numberline {18.4.4}Vision Transformer: Process Summary and Implementation}{692}{subsection.18.4.4}%
\contentsline {subsubsection}{Vision Transformer Processing Steps}{692}{section*.1362}%
\contentsline {subsubsection}{PyTorch Implementation of a Vision Transformer}{693}{section*.1363}%
\contentsline {subsection}{\numberline {18.4.5}Computational Complexity: ViT vs.\ Pixel-Level Self-Attention}{697}{subsection.18.4.5}%
\contentsline {subsubsection}{Pixel-Level Self-Attention}{697}{section*.1364}%
\contentsline {subsubsection}{Patch-Based Self-Attention (ViT)}{697}{section*.1365}%
\contentsline {subsubsection}{Key Takeaways}{697}{section*.1366}%
\contentsline {subsection}{\numberline {18.4.6}Limitations and Data Requirements of Vision Transformers}{698}{subsection.18.4.6}%
\contentsline {subsubsection}{Large-Scale Pretraining is Critical}{698}{section*.1367}%
\contentsline {subsubsection}{Why Do ViTs Require More Data?}{698}{section*.1369}%
\contentsline {paragraph}{1. No Built-in Locality or Weight Sharing}{699}{section*.1370}%
\contentsline {paragraph}{2. Higher Parameter Count and Capacity}{699}{section*.1371}%
\contentsline {paragraph}{3. Less Implicit Regularization}{699}{section*.1372}%
\contentsline {paragraph}{4. Absence of Hierarchical Representations}{699}{section*.1373}%
\contentsline {paragraph}{A Note on Inductive Bias}{700}{section*.1374}%
\contentsline {subsection}{\numberline {18.4.7}Understanding ViT Model Variants}{700}{subsection.18.4.7}%
\contentsline {subsubsection}{Model Configurations}{700}{section*.1375}%
\contentsline {subsubsection}{Transfer Performance Across Datasets}{701}{section*.1377}%
\contentsline {subsection}{\numberline {18.4.8}Improving ViT Training Efficiency}{701}{subsection.18.4.8}%
\contentsline {paragraph}{Regularization Techniques:}{702}{section*.1379}%
\contentsline {paragraph}{Data Augmentation Strategies:}{702}{section*.1380}%
\contentsline {subsubsection}{Towards Data-Efficient Vision Transformers: Introducing DeiT}{702}{section*.1381}%
\contentsline {section}{\numberline {18.5}Data-Efficient Image Transformers (DeiTs)}{703}{section.18.5}%
\contentsline {subsection}{\numberline {18.5.1}Cross-Entropy and KL Divergence: Theory, Intuition, and Role in Distillation}{703}{subsection.18.5.1}%
\contentsline {paragraph}{Cross-Entropy Loss}{703}{section*.1382}%
\contentsline {paragraph}{KL Divergence: Full Distribution Matching}{704}{section*.1384}%
\contentsline {paragraph}{Illustrative Example: CE vs KL}{704}{section*.1385}%
\contentsline {paragraph}{Hard vs.\ Soft Distillation: Choosing the Right Signal}{705}{section*.1386}%
\contentsline {subsection}{\numberline {18.5.2}DeiT Distillation Token and Training Strategy}{705}{subsection.18.5.2}%
\contentsline {subsubsection}{Distillation via Tokens: Setup}{705}{section*.1387}%
\contentsline {paragraph}{Hard Distillation in Practice.}{706}{section*.1388}%
\contentsline {subsubsection}{Soft Distillation: Temperature and KL Loss}{706}{section*.1390}%
\contentsline {subsubsection}{Why Use a CNN Teacher?}{707}{section*.1391}%
\contentsline {subsubsection}{Learned Token Behavior}{707}{section*.1392}%
\contentsline {subsubsection}{Fine-Tuning: High Resolution and Distillation Retention}{708}{section*.1394}%
\contentsline {paragraph}{Two-Phase Training Rationale}{708}{section*.1395}%
\contentsline {paragraph}{Why Higher Resolution Helps}{708}{section*.1396}%
\contentsline {paragraph}{Upscaling and L2-Norm Preservation}{708}{section*.1397}%
\contentsline {paragraph}{Teacher Adaptation with FixRes}{708}{section*.1398}%
\contentsline {paragraph}{Dual Supervision in Fine-Tuning}{708}{section*.1399}%
\contentsline {subsubsection}{Why This Works in Data-Limited Settings}{708}{section*.1400}%
\contentsline {subsection}{\numberline {18.5.3}Model Variants}{709}{subsection.18.5.3}%
\contentsline {subsection}{\numberline {18.5.4}Conclusion and Outlook: From DeiT to DeiT III and Beyond}{709}{subsection.18.5.4}%
\contentsline {subsubsection}{DeiT III: Revenge of the ViT}{710}{section*.1403}%
\contentsline {paragraph}{Open Questions Raised by DeiT}{710}{section*.1404}%
\contentsline {subsubsection}{Toward Hierarchical Vision Transformers}{710}{section*.1405}%
\contentsline {section}{\numberline {18.6}Swin Transformer: Hierarchical Vision Transformers with Shifted Windows}{712}{section.18.6}%
\contentsline {subsection}{\numberline {18.6.1}How Swin Works}{712}{subsection.18.6.1}%
\contentsline {paragraph}{Patch Tokenization}{712}{section*.1407}%
\contentsline {subsection}{\numberline {18.6.2}Window-Based Self-Attention (W-MSA)}{713}{subsection.18.6.2}%
\contentsline {subsection}{\numberline {18.6.3}Limitation: No Cross-Window Communication}{714}{subsection.18.6.3}%
\contentsline {subsection}{\numberline {18.6.4}Solution: Shifted Windows (SW-MSA)}{714}{subsection.18.6.4}%
\contentsline {paragraph}{How it works}{714}{section*.1410}%
\contentsline {paragraph}{Benefits of SW-MSA}{714}{section*.1411}%
\contentsline {paragraph}{Challenges Introduced by Shifted Windows}{715}{section*.1413}%
\contentsline {subsection}{\numberline {18.6.5}Cyclic Shifted Window-Masked Self Attention (Cyclic SW-MSA)}{717}{subsection.18.6.5}%
\contentsline {subsubsection}{Masking in SW-MSA}{717}{section*.1418}%
\contentsline {paragraph}{Step-by-Step Construction of the Mask}{717}{section*.1419}%
\contentsline {paragraph}{Why Use \(-100.0\) in the Mask?}{718}{section*.1420}%
\contentsline {paragraph}{Expanded Receptive Fields}{719}{section*.1421}%
\contentsline {subsection}{\numberline {18.6.6}Patch Merging in Swin Transformers}{720}{subsection.18.6.6}%
\contentsline {subsection}{\numberline {18.6.7}Positional Encoding in Swin Transformers}{721}{subsection.18.6.7}%
\contentsline {paragraph}{Relative Position Bias in Swin Transformers}{721}{section*.1428}%
\contentsline {paragraph}{Hierarchical Windows and Relative Offsets}{721}{section*.1429}%
\contentsline {paragraph}{Why Relative Position Bias for Hierarchical Transformers?}{722}{section*.1430}%
\contentsline {paragraph}{Implementation Detail}{722}{section*.1431}%
\contentsline {paragraph}{Practical Benefits}{722}{section*.1432}%
\contentsline {subsection}{\numberline {18.6.8}Conclusion: The Swin Transformer Architecture and Variants}{723}{subsection.18.6.8}%
\contentsline {section}{\numberline {18.7}Extensions and Successors to Swin}{724}{section.18.7}%
\contentsline {subsection}{\numberline {18.7.1}Swin Evolution: Swin Transformer V2}{724}{subsection.18.7.1}%
\contentsline {paragraph}{1) Scaled Cosine Attention}{725}{section*.1438}%
\contentsline {paragraph}{2) Log-Spaced Continuous Position Bias (Log-CPB)}{725}{section*.1439}%
\contentsline {paragraph}{3) Residual Post-Norm}{726}{section*.1440}%
\contentsline {paragraph}{Implications and Results}{726}{section*.1442}%
\contentsline {subsection}{\numberline {18.7.2}Multiscale Vision Transformer (MViT)}{727}{subsection.18.7.2}%
\contentsline {paragraph}{1.\ Pooling Attention (MHPA)}{727}{section*.1443}%
\contentsline {paragraph}{How Does Pooling Work?}{728}{section*.1445}%
\contentsline {paragraph}{Multiscale Hierarchy via Pooling}{728}{section*.1446}%
\contentsline {paragraph}{2.\ Hierarchical Token Downsampling}{728}{section*.1447}%
\contentsline {paragraph}{3.\ Global Attention vs.\ Local Windows}{728}{section*.1448}%
\contentsline {paragraph}{Originally Designed for Video, Effective for Images}{729}{section*.1449}%
\contentsline {paragraph}{Empirical Strengths}{729}{section*.1450}%
\contentsline {subsection}{\numberline {18.7.3}Improved Multiscale Vision Transformers: MViTv2}{729}{subsection.18.7.3}%
\contentsline {subsubsection}{Decomposed Relative Positional Embeddings}{729}{section*.1451}%
\contentsline {paragraph}{Motivation}{729}{section*.1452}%
\contentsline {paragraph}{Decomposed Formulation}{729}{section*.1453}%
\contentsline {paragraph}{Integration into Attention}{730}{section*.1454}%
\contentsline {subsubsection}{Residual Pooling Connections}{730}{section*.1455}%
\contentsline {paragraph}{Problem}{730}{section*.1456}%
\contentsline {paragraph}{Solution.}{730}{section*.1457}%
\contentsline {paragraph}{Empirical Impact}{730}{section*.1458}%
\contentsline {subsubsection}{Performance Benefits}{730}{section*.1459}%
\contentsline {subsubsection}{Summary}{731}{section*.1460}%
\contentsline {paragraph}{Looking Ahead}{731}{section*.1461}%
\contentsline {section}{\numberline {18.8}MLP-Mixer: All-MLP Vision Architecture}{732}{section.18.8}%
\contentsline {subsection}{\numberline {18.8.1}The MLP-Mixer Architecture}{732}{subsection.18.8.1}%
\contentsline {paragraph}{Token-Mixing and Channel-Mixing Blocks}{732}{section*.1463}%
\contentsline {paragraph}{CNN Equivalence}{733}{section*.1464}%
\contentsline {subsection}{\numberline {18.8.2}Results and Limitations}{733}{subsection.18.8.2}%
\contentsline {subsection}{\numberline {18.8.3}Looking Ahead: Applying Transformers to Object Detection}{733}{subsection.18.8.3}%
\contentsline {section}{\numberline {18.9}Detection Transformer (DeTR)}{734}{section.18.9}%
\contentsline {paragraph}{Architecture Overview}{734}{section*.1466}%
\contentsline {paragraph}{Why Transformers for Detection?}{734}{section*.1467}%
\contentsline {subsection}{\numberline {18.9.1}Matching Predictions and Ground Truth with No-Object Padding}{735}{subsection.18.9.1}%
\contentsline {paragraph}{Challenge:}{735}{section*.1468}%
\contentsline {paragraph}{Solution: No-Object Padding}{735}{section*.1469}%
\contentsline {paragraph}{Hungarian Matching:}{735}{section*.1471}%
\contentsline {paragraph}{Implementation Snippet:}{736}{section*.1472}%
\contentsline {paragraph}{Why This Matters:}{736}{section*.1473}%
\contentsline {subsection}{\numberline {18.9.2}Hungarian Matching Loss and Bounding Box Optimization}{736}{subsection.18.9.2}%
\contentsline {paragraph}{Step 1: Optimal Bipartite Matching}{736}{section*.1474}%
\contentsline {paragraph}{Step 2: Matching Cost Definition}{737}{section*.1475}%
\contentsline {paragraph}{Step 3: Final Loss Computation}{737}{section*.1476}%
\contentsline {paragraph}{Bounding Box Loss: Smooth L1 and GIoU Components}{737}{section*.1477}%
\contentsline {subparagraph}{1. Smooth L1 Loss (Huber Variant)}{737}{subparagraph*.1478}%
\contentsline {subparagraph}{2. Generalized IoU (GIoU) Loss}{738}{subparagraph*.1479}%
\contentsline {subparagraph}{3. Combining Smooth L1 and GIoU}{739}{subparagraph*.1481}%
\contentsline {paragraph}{Conclusion}{739}{section*.1482}%
\contentsline {subsection}{\numberline {18.9.3}Architecture Overview: CNN Backbone + Transformer Decoder}{739}{subsection.18.9.3}%
\contentsline {paragraph}{1.\ CNN Backbone}{739}{section*.1483}%
\contentsline {paragraph}{2.\ Transformer Encoder}{739}{section*.1484}%
\contentsline {paragraph}{3.\ Learned Object Queries and Transformer Decoder}{740}{section*.1486}%
\contentsline {paragraph}{4.\ Interpreting Object Queries}{741}{section*.1488}%
\contentsline {paragraph}{5.\ Why Attention is a Natural Fit}{741}{section*.1490}%
\contentsline {subsection}{\numberline {18.9.4}DeTR Results, Impact, and Follow-Up Work}{742}{subsection.18.9.4}%
\contentsline {paragraph}{From Detection to Segmentation}{742}{section*.1492}%
\contentsline {paragraph}{Real-World Usage: HuggingFace Implementation}{743}{section*.1495}%
\contentsline {paragraph}{Follow-Up Works and Extensions}{743}{section*.1496}%
\contentsline {paragraph}{Broader Impact}{743}{section*.1497}%
\contentsline {paragraph}{Conclusion}{743}{section*.1498}%
\contentsline {chapter}{\numberline {19}Lecture 19: Generative Models I}{744}{chapter.19}%
\contentsline {section}{\numberline {19.1}Supervised vs.\ Unsupervised Learning}{744}{section.19.1}%
\contentsline {subsection}{\numberline {19.1.1}Supervised Learning}{744}{subsection.19.1.1}%
\contentsline {subsection}{\numberline {19.1.2}Unsupervised Learning}{744}{subsection.19.1.2}%
\contentsline {section}{\numberline {19.2}Discriminative vs.\ Generative Models}{745}{section.19.2}%
\contentsline {subsection}{\numberline {19.2.1}Discriminative Models}{746}{subsection.19.2.1}%
\contentsline {subsection}{\numberline {19.2.2}Generative Models}{746}{subsection.19.2.2}%
\contentsline {subsection}{\numberline {19.2.3}Conditional Generative Models}{747}{subsection.19.2.3}%
\contentsline {subsection}{\numberline {19.2.4}Model Relationships via Bayes' Rule}{747}{subsection.19.2.4}%
\contentsline {subsection}{\numberline {19.2.5}Summary of Generative Model Taxonomy}{748}{subsection.19.2.5}%
\contentsline {section}{\numberline {19.3}Autoregressive Models and Explicit Density Estimation}{749}{section.19.3}%
\contentsline {subsection}{\numberline {19.3.1}Maximum Likelihood Estimation}{749}{subsection.19.3.1}%
\contentsline {subsection}{\numberline {19.3.2}Autoregressive Factorization}{750}{subsection.19.3.2}%
\contentsline {subsection}{\numberline {19.3.3}Recurrent Pixel Networks: Overview and Motivation}{750}{subsection.19.3.3}%
\contentsline {paragraph}{Autoregressive Architectures in PixelRNN}{750}{section*.1506}%
\contentsline {subsubsection}{PixelCNN}{751}{section*.1508}%
\contentsline {paragraph}{Image Generation as Sequential Prediction}{752}{section*.1509}%
\contentsline {paragraph}{Autoregressive Generation Process}{752}{section*.1510}%
\contentsline {paragraph}{Masked Convolution for Feature Extraction}{753}{section*.1512}%
\contentsline {paragraph}{Red Channel: Feature Processing and Softmax}{754}{section*.1513}%
\contentsline {paragraph}{Green Channel: Conditioning on Red}{754}{section*.1514}%
\contentsline {paragraph}{Blue Channel: Conditioning on Red and Green}{755}{section*.1516}%
\contentsline {paragraph}{Moving to the Next Pixel}{756}{section*.1517}%
\contentsline {paragraph}{Training PixelCNNs Efficiently}{756}{section*.1519}%
\contentsline {paragraph}{Why Move Beyond PixelCNN? Blind Spots, Receptive Fields, and Inference Latency}{757}{section*.1520}%
\contentsline {subparagraph}{1. Receptive Field Growth is Local and Incremental}{757}{subparagraph*.1521}%
\contentsline {subparagraph}{2. Blind Spots: Missing Valid Context Pixels}{757}{subparagraph*.1522}%
\contentsline {subparagraph}{3. Inference Time: Slow Sequential Generation}{757}{subparagraph*.1523}%
\contentsline {subparagraph}{Motivation for Recurrent Alternatives}{758}{subparagraph*.1524}%
\contentsline {subsubsection}{Row LSTM}{759}{section*.1525}%
\contentsline {paragraph}{From Convolution Stacks to Convolutional Recurrence}{759}{section*.1526}%
\contentsline {paragraph}{What is a Convolutional LSTM?}{759}{section*.1527}%
\contentsline {paragraph}{Triangular Receptive Field}{760}{section*.1528}%
\contentsline {paragraph}{Looking Ahead}{760}{section*.1530}%
\contentsline {subsubsection}{Diagonal BiLSTM}{761}{section*.1531}%
\contentsline {paragraph}{Skewing the Input for Diagonal Convolutions}{761}{section*.1533}%
\contentsline {paragraph}{Causal Correction for Bidirectionality}{762}{section*.1535}%
\contentsline {paragraph}{Convolutional LSTM Logic}{762}{section*.1537}%
\contentsline {paragraph}{Why Diagonal BiLSTM is the Most Expressive Variant}{763}{section*.1539}%
\contentsline {paragraph}{Residual Connections in PixelRNNs}{764}{section*.1540}%
\contentsline {paragraph}{Looking Ahead}{764}{section*.1542}%
\contentsline {subsubsection}{Multi-Scale PixelRNN}{765}{section*.1543}%
\contentsline {paragraph}{Two-Stage Architecture}{765}{section*.1544}%
\contentsline {paragraph}{Conditioning via Upsampling and Biasing}{765}{section*.1545}%
\contentsline {paragraph}{Why Multi-Scale Helps}{766}{section*.1547}%
\contentsline {paragraph}{Trade-Offs and Usage}{767}{section*.1548}%
\contentsline {subsubsection}{Results and Qualitative Samples}{767}{section*.1549}%
\contentsline {subsubsection}{Enrichment 19.3.3.1: Beyond PixelRNN: Advanced Autoregressive Variants}{768}{section*.1551}%
\contentsline {paragraph}{Gated PixelCNN}{768}{section*.1552}%
\contentsline {paragraph}{PixelCNN++}{768}{section*.1553}%
\contentsline {paragraph}{ImageGPT}{768}{section*.1554}%
\contentsline {paragraph}{Looking Ahead: From Autoregressive Models to VAEs}{769}{section*.1555}%
\contentsline {section}{\numberline {19.4}Variational Autoencoders (VAEs)}{770}{section.19.4}%
\contentsline {subsection}{\numberline {19.4.1}Regular (Non-Variational) Autoencoders}{770}{subsection.19.4.1}%
\contentsline {paragraph}{Usage in Transfer Learning}{770}{section*.1557}%
\contentsline {paragraph}{Architecture Patterns}{771}{section*.1559}%
\contentsline {paragraph}{Limitations of Vanilla Autoencoders}{771}{section*.1560}%
\contentsline {subsection}{\numberline {19.4.2}Introducing the VAE}{772}{subsection.19.4.2}%
\contentsline {paragraph}{Core Goals}{772}{section*.1561}%
\contentsline {paragraph}{Why a Latent Variable Model?}{772}{section*.1562}%
\contentsline {paragraph}{Probabilistic Decoder}{773}{section*.1564}%
\contentsline {paragraph}{Why Not a Full Covariance Matrix?}{774}{section*.1566}%
\contentsline {paragraph}{Diagonal Assumption and Trade-Offs}{774}{section*.1567}%
\contentsline {paragraph}{Marginal Likelihood: What We Want to Optimize}{774}{section*.1568}%
\contentsline {subsubsection}{Training VAEs and Developing the ELBO}{775}{section*.1569}%
\contentsline {paragraph}{The Role of Bayes' Rule}{775}{section*.1570}%
\contentsline {paragraph}{Why This Matters}{775}{section*.1571}%
\contentsline {paragraph}{Switching Objectives: Approximating the Posterior}{775}{section*.1572}%
\contentsline {paragraph}{Rewriting the Log Likelihood}{775}{section*.1573}%
\contentsline {paragraph}{Interpreting the ELBO}{777}{section*.1574}%
\contentsline {chapter}{\numberline {20}Lecture 20: Generative Models II}{778}{chapter.20}%
\contentsline {section}{\numberline {20.1}VAE Training and Data Generation}{778}{section.20.1}%
\contentsline {subsection}{\numberline {20.1.1}Encoder and Decoder Architecture: MNIST Example}{778}{subsection.20.1.1}%
\contentsline {subsection}{\numberline {20.1.2}Training Pipeline: Step-by-Step}{779}{subsection.20.1.2}%
\contentsline {paragraph}{The ELBO Objective}{779}{section*.1577}%
\contentsline {subsubsection}{Why a Diagonal Gaussian Prior?}{781}{section*.1579}%
\contentsline {subsection}{\numberline {20.1.3}How Can We Generate Data Using VAEs?}{782}{subsection.20.1.3}%
\contentsline {paragraph}{Sampling Procedure}{782}{section*.1580}%
\contentsline {section}{\numberline {20.2}Results and Applications of VAEs}{783}{section.20.2}%
\contentsline {subsection}{\numberline {20.2.1}Qualitative Generation Results}{783}{subsection.20.2.1}%
\contentsline {subsection}{\numberline {20.2.2}Latent Space Traversals}{783}{subsection.20.2.2}%
\contentsline {paragraph}{Editing with VAEs via Latent Traversals}{784}{section*.1584}%
\contentsline {paragraph}{Takeaway}{786}{section*.1588}%
\contentsline {section}{\numberline {20.3}Summary \& Examples: Variational Autoencoders}{786}{section.20.3}%
\contentsline {paragraph}{Pros:}{786}{section*.1589}%
\contentsline {paragraph}{Cons:}{786}{section*.1590}%
\contentsline {paragraph}{Active Research Directions:}{786}{section*.1591}%
\contentsline {subsection}{\numberline {20.3.1}VQ-VAE-2: Combining VAEs with Autoregressive Models}{787}{subsection.20.3.1}%
\contentsline {paragraph}{Motivation}{787}{section*.1593}%
\contentsline {paragraph}{Architecture Overview}{787}{section*.1594}%
\contentsline {paragraph}{How does autoregressive sampling begin?}{788}{section*.1595}%
\contentsline {paragraph}{How does this enable generation?}{789}{section*.1596}%
\contentsline {paragraph}{Summary Table: Dimensional Flow and Index Usage}{789}{section*.1597}%
\contentsline {paragraph}{Next: Training and Inference Flow}{789}{section*.1599}%
\contentsline {subsubsection}{Training the VQ-VAE-2 Autoencoder}{790}{section*.1601}%
\contentsline {paragraph}{Objective Overview}{790}{section*.1602}%
\contentsline {paragraph}{1. Reconstruction Loss (\( \mathcal {L}_{\text {recon}} \))}{790}{section*.1603}%
\contentsline {paragraph}{2. Codebook Update (\( \mathcal {L}_{\text {codebook}} \))}{790}{section*.1604}%
\contentsline {subparagraph}{(a) Gradient-Based Codebook Loss (as in the original paper)}{791}{subparagraph*.1605}%
\contentsline {subparagraph}{(b) EMA-Based Codebook Update (Used in Practice)}{791}{subparagraph*.1606}%
\contentsline {subparagraph}{Summary of Update Strategies}{791}{subparagraph*.1607}%
\contentsline {paragraph}{3. Commitment Loss (\( \mathcal {L}_{\text {commit}} \))}{792}{section*.1608}%
\contentsline {paragraph}{Why Two Losses with Stop-Gradients Are Needed}{792}{section*.1609}%
\contentsline {paragraph}{Compact Notation for Vector Quantization Loss}{792}{section*.1610}%
\contentsline {paragraph}{Training Summary}{792}{section*.1611}%
\contentsline {paragraph}{Training Summary with EMA Codebook Updates}{793}{section*.1612}%
\contentsline {subsubsection}{Training the Autoregressive Priors}{793}{section*.1613}%
\contentsline {paragraph}{Motivation}{793}{section*.1614}%
\contentsline {paragraph}{Hierarchical Modeling}{793}{section*.1615}%
\contentsline {paragraph}{Overall Training Details}{794}{section*.1616}%
\contentsline {paragraph}{Sampling Procedure}{794}{section*.1617}%
\contentsline {paragraph}{Initialization Note}{794}{section*.1618}%
\contentsline {paragraph}{Advantages of VQ-VAE-2 with Autoregressive Priors}{794}{section*.1619}%
\contentsline {paragraph}{Results \& Summary}{795}{section*.1620}%
\contentsline {section}{\numberline {20.4}Generative Adversarial Networks (GANs)}{796}{section.20.4}%
\contentsline {paragraph}{Bridging from Autoregressive Models, VAEs to GANs}{796}{section*.1623}%
\contentsline {paragraph}{Enter GANs}{796}{section*.1624}%
\contentsline {subsection}{\numberline {20.4.1}Setup: Implicit Generation via Adversarial Learning}{796}{subsection.20.4.1}%
\contentsline {paragraph}{Sampling from the True Distribution}{796}{section*.1625}%
\contentsline {paragraph}{Discriminator as a Learned Judge}{797}{section*.1626}%
\contentsline {paragraph}{Adversarial Training Dynamics}{797}{section*.1627}%
\contentsline {paragraph}{Core Intuition}{798}{section*.1629}%
\contentsline {subsection}{\numberline {20.4.2}GAN Training Objective}{798}{subsection.20.4.2}%
\contentsline {paragraph}{Difficulties in Optimization}{799}{section*.1631}%
\contentsline {paragraph}{Modified Generator Loss (Non-Saturating Trick)}{799}{section*.1633}%
\contentsline {paragraph}{Solution: Switch the Objective}{800}{section*.1634}%
\contentsline {paragraph}{Looking Ahead: Why This Objective?}{800}{section*.1636}%
\contentsline {subsection}{\numberline {20.4.3}Why the GAN Training Objective Is Optimal}{801}{subsection.20.4.3}%
\contentsline {paragraph}{Step-by-Step Derivation}{801}{section*.1637}%
\contentsline {paragraph}{Justification of the Mathematical Transformations}{801}{section*.1638}%
\contentsline {paragraph}{Solving the Inner Maximization (Discriminator)}{801}{section*.1639}%
\contentsline {paragraph}{Plugging the Optimal Discriminator into the Objective}{802}{section*.1640}%
\contentsline {paragraph}{Rewriting as KL Divergences}{802}{section*.1641}%
\contentsline {paragraph}{Introducing the JensenShannon Divergence (JSD)}{803}{section*.1642}%
\contentsline {paragraph}{Final Result: Objective Minimizes JSD}{803}{section*.1643}%
\contentsline {paragraph}{Summary}{803}{section*.1644}%
\contentsline {paragraph}{Important Caveats and Limitations of the Theoretical Result}{803}{section*.1645}%
\contentsline {section}{\numberline {20.5}GANs in Practice: From Early Milestones to Modern Advances}{804}{section.20.5}%
\contentsline {subsection}{\numberline {20.5.1}The Original GAN (2014)}{804}{subsection.20.5.1}%
\contentsline {subsection}{\numberline {20.5.2}Deep Convolutional GAN (DCGAN)}{804}{subsection.20.5.2}%
\contentsline {paragraph}{Architectural Innovations and Design Principles}{804}{section*.1647}%
\contentsline {paragraph}{Why it Works}{806}{section*.1649}%
\contentsline {paragraph}{Latent Space Interpolation}{806}{section*.1651}%
\contentsline {subsubsection}{Latent Vector Arithmetic}{807}{section*.1653}%
\contentsline {subsection}{\numberline {20.5.3}Evaluating Generative Adversarial Networks (GANs)}{808}{subsection.20.5.3}%
\contentsline {paragraph}{The Core Challenge}{808}{section*.1656}%
\contentsline {paragraph}{Manual Inspection and Preference Ranking}{808}{section*.1658}%
\contentsline {paragraph}{Nearest Neighbor Retrieval}{808}{section*.1659}%
\contentsline {paragraph}{Inception Score (IS)}{808}{section*.1661}%
\contentsline {paragraph}{Fr\'echet Inception Distance (FID)}{809}{section*.1662}%
\contentsline {paragraph}{What Does the Formula Measure?}{809}{section*.1663}%
\contentsline {paragraph}{Theoretical Background: 2-Wasserstein Distance}{809}{section*.1664}%
\contentsline {paragraph}{How to Interpret FID Scores}{809}{section*.1665}%
\contentsline {paragraph}{Why FID Is Preferred}{810}{section*.1666}%
\contentsline {paragraph}{FID Limitations}{810}{section*.1667}%
\contentsline {paragraph}{FID Summary}{810}{section*.1668}%
\contentsline {paragraph}{Other Quantitative Metrics}{810}{section*.1669}%
\contentsline {paragraph}{Summary}{810}{section*.1671}%
\contentsline {subsection}{\numberline {20.5.4}GAN Explosion}{811}{subsection.20.5.4}%
\contentsline {paragraph}{Next Steps: Improving GANs}{811}{section*.1673}%
\contentsline {subsection}{\numberline {20.5.5}Wasserstein GAN (WGAN): Earth Movers Distance}{811}{subsection.20.5.5}%
\contentsline {paragraph}{Supports and Low-Dimensional Manifolds}{812}{section*.1674}%
\contentsline {paragraph}{Why the JS Divergence Fails in High Dimensions}{812}{section*.1675}%
\contentsline {paragraph}{Why Non-Saturating GANs Still Suffer}{812}{section*.1676}%
\contentsline {paragraph}{The Need for a Better Distance Metric}{813}{section*.1677}%
\contentsline {paragraph}{Wasserstein-1 Distance: Transporting Mass}{813}{section*.1678}%
\contentsline {paragraph}{Example: Optimal Transport Plans as Joint Tables}{813}{section*.1679}%
\contentsline {paragraph}{Why This Matters}{814}{section*.1680}%
\contentsline {paragraph}{From Intractable Transport to Practical Training}{815}{section*.1682}%
\contentsline {paragraph}{What These Expectations Mean in Practice}{815}{section*.1683}%
\contentsline {paragraph}{How the Training Works}{815}{section*.1684}%
\contentsline {paragraph}{Why This Makes Sense  Even if Samples Differ Sharply}{815}{section*.1685}%
\contentsline {paragraph}{Summary}{816}{section*.1686}%
\contentsline {paragraph}{Side-by-Side: Standard GAN vs.\ WGAN}{816}{section*.1687}%
\contentsline {paragraph}{Whats Missing: Enforcing the 1-Lipschitz Constraint}{816}{section*.1689}%
\contentsline {paragraph}{Weight Clipping: A Crude Approximation}{817}{section*.1690}%
\contentsline {paragraph}{Benefits of WGAN}{817}{section*.1691}%
\contentsline {paragraph}{Limitations of Weight Clipping in Practice}{818}{section*.1694}%
\contentsline {subsection}{\numberline {20.5.6}WGAN-GP: Gradient Penalty for Stable Lipschitz Enforcement}{820}{subsection.20.5.6}%
\contentsline {paragraph}{Theoretical Motivation: Lipschitz Continuity and Gradient Norms}{820}{section*.1695}%
\contentsline {paragraph}{The WGAN-GP Loss Function}{820}{section*.1696}%
\contentsline {subparagraph}{Why Interpolated Points? Intuition and Implementation in WGAN-GP}{820}{subparagraph*.1697}%
\contentsline {subparagraph}{Conceptual Motivation: \emph {Where} Should Lipschitz Matter?}{820}{subparagraph*.1698}%
\contentsline {subparagraph}{Why This Avoids Over-Regularization}{821}{subparagraph*.1699}%
\contentsline {subparagraph}{Code Walkthrough: Penalty Computation for a Single Critic Update}{821}{subparagraph*.1700}%
\contentsline {subparagraph}{Resulting Dynamics \& Why It Helps}{822}{subparagraph*.1701}%
\contentsline {subparagraph}{Interpreting the Loss Components}{822}{subparagraph*.1702}%
\contentsline {subparagraph}{Key Benefits of the Gradient Penalty vs.\ Weight Clipping}{822}{subparagraph*.1703}%
\contentsline {paragraph}{Architectural Robustness}{823}{section*.1705}%
\contentsline {paragraph}{State-of-the-Art Results on CIFAR-10}{824}{section*.1707}%
\contentsline {paragraph}{Conclusion}{824}{section*.1708}%
\contentsline {section}{Enrichment 20.6: The StyleGAN Family}{825}{section*.1709}%
\contentsline {subsection}{Enrichment 20.6.1: ProGAN Overview: A Stability-Oriented Design}{825}{section*.1710}%
\contentsline {paragraph}{Training Strategy}{825}{section*.1711}%
\contentsline {paragraph}{Why This Works}{826}{section*.1712}%
\contentsline {subsubsection}{Enrichment 20.6.1.1: Limitations of ProGAN: Toward Style-Based Generators}{828}{section*.1714}%
\contentsline {subsection}{Enrichment 20.6.2: StyleGAN: Style-Based Synthesis via Latent Modulation}{828}{section*.1715}%
\contentsline {paragraph}{(1) Mapping Network (\(\mathcal {Z} \to \mathcal {W}\)):}{829}{section*.1718}%
\contentsline {paragraph}{Why Not Just Increase the Dimensionality of \( z \)?}{829}{section*.1719}%
\contentsline {paragraph}{(2) Modulating Each Layer via AdaIN (Block A):}{830}{section*.1720}%
\contentsline {paragraph}{(3) Fixed Learned Input (Constant Tensor):}{831}{section*.1721}%
\contentsline {paragraph}{(4) Stochastic Detail Injection (Block B):}{831}{section*.1722}%
\contentsline {paragraph}{(5) Style Mixing Regularization: Breaking Co-Adaptation Across Layers}{831}{section*.1723}%
\contentsline {paragraph}{(6) Perceptual Path Length (PPL): Quantifying Disentanglement in Latent Space}{832}{section*.1724}%
\contentsline {paragraph}{What Is LPIPS?}{832}{section*.1725}%
\contentsline {paragraph}{Why PPL Matters  and How It Relates to Training}{833}{section*.1726}%
\contentsline {paragraph}{(7) Loss Functions: From WGAN-GP to Non-Saturating GAN + R\textsubscript {1}}{833}{section*.1727}%
\contentsline {paragraph}{Summary and Additional Contributions}{834}{section*.1728}%
\contentsline {subsection}{Enrichment 20.6.3: StyleGAN2: Eliminating Artifacts, Improving Training Stability}{835}{section*.1731}%
\contentsline {subsubsection}{Enrichment 20.6.3.1: Background: From StyleGAN1 to StyleGAN2}{835}{section*.1732}%
\contentsline {subsubsection}{Enrichment 20.6.3.2: Weight Demodulation: A Principled Replacement for AdaIN}{836}{section*.1734}%
\contentsline {subsubsection}{Enrichment 20.6.3.3: Noise Injection Relocation: Separating Style and Stochasticity}{837}{section*.1736}%
\contentsline {subsubsection}{Enrichment 20.6.3.4: Path Length Regularization: Smoother Latent Traversals}{838}{section*.1737}%
\contentsline {subsubsection}{Enrichment 20.6.3.5: Lazy R\textsubscript {1} Regularization and Evolved Loss Strategy}{839}{section*.1738}%
\contentsline {paragraph}{Discriminator Loss:}{839}{section*.1739}%
\contentsline {paragraph}{Generator Loss:}{839}{section*.1740}%
\contentsline {paragraph}{Joint Optimization Logic:}{839}{section*.1741}%
\contentsline {subsubsection}{Enrichment 20.6.3.6: No Progressive Growing}{840}{section*.1742}%
\contentsline {paragraph}{1. Multi-Scale Skip Connections in the Generator}{840}{section*.1744}%
\contentsline {paragraph}{2. Residual Blocks in the Discriminator}{840}{section*.1745}%
\contentsline {paragraph}{3. Tracking Per-Resolution Contributions}{841}{section*.1746}%
\contentsline {subsubsection}{Enrichment 20.6.3.7: StyleGAN3: Eliminating Texture Sticking}{842}{section*.1748}%
\contentsline {paragraph}{Why Does Texture Sticking Occur?}{842}{section*.1750}%
\contentsline {paragraph}{How StyleGAN3 Fixes It: Core Innovations}{842}{section*.1751}%
\contentsline {paragraph}{Training Changes and Equivariance Goals}{843}{section*.1752}%
\contentsline {paragraph}{Latent and Spatial Disentanglement}{843}{section*.1753}%
\contentsline {paragraph}{Impact in Practice}{844}{section*.1754}%
\contentsline {paragraph}{Takeaway}{844}{section*.1755}%
\contentsline {section}{Enrichment 20.7: Conditional GANs: Label-Aware Image Synthesis}{845}{section*.1756}%
\contentsline {subsection}{Enrichment 20.7.1: Conditional Batch Normalization (CBN)}{845}{section*.1758}%
\contentsline {paragraph}{Motivation}{845}{section*.1759}%
\contentsline {paragraph}{How CBN Works}{846}{section*.1760}%
\contentsline {paragraph}{CBN in the Generator}{846}{section*.1762}%
\contentsline {subsubsection}{Enrichment 20.7.1.1: Projection-Based Conditioning in Discriminators}{847}{section*.1763}%
\contentsline {paragraph}{Advantages of Projection-Based Conditioning:}{847}{section*.1764}%
\contentsline {subsubsection}{Enrichment 20.7.1.2: Training Conditional GANs with CBN}{847}{section*.1765}%
\contentsline {paragraph}{Generator \( G(z, y) \): Label-Aware Synthesis}{847}{section*.1766}%
\contentsline {paragraph}{Discriminator \( D(x, y) \): Realness and Label Consistency}{848}{section*.1767}%
\contentsline {paragraph}{Training Pipeline with CBN Conditioning:}{848}{section*.1768}%
\contentsline {paragraph}{Log-Loss Intuition:}{849}{section*.1769}%
\contentsline {paragraph}{Limitations of CBN-Only Conditioning}{849}{section*.1770}%
\contentsline {subsection}{Enrichment 20.7.2: Spectral Normalization for Stable GAN Training}{850}{section*.1771}%
\contentsline {subsubsection}{Enrichment 20.7.2.1: Spectral Normalization - Mathematical Background}{850}{section*.1772}%
\contentsline {paragraph}{Eigenvalues and Eigenvectors: Invariant Directions in Linear Maps}{850}{section*.1773}%
\contentsline {paragraph}{Singular Value Decomposition (SVD): Structure and Signal in Data}{852}{section*.1774}%
\contentsline {paragraph}{SVD: Structure, Meaning, and Application to Real-World Data}{853}{section*.1775}%
\contentsline {paragraph}{Spectral Structure via \( X^\top X \) and \( XX^\top \)}{855}{section*.1776}%
\contentsline {paragraph}{Economy (or Truncated) SVD}{855}{section*.1777}%
\contentsline {paragraph}{How is SVD Computed in Practice?}{856}{section*.1778}%
\contentsline {paragraph}{Spectral Norm of a Weight Matrix}{858}{section*.1779}%
\contentsline {paragraph}{Fast SpectralNorm Estimation via Power Iteration}{858}{section*.1780}%
\contentsline {paragraph}{Alternative Loss: Hinge Loss Formulation}{860}{section*.1782}%
\contentsline {paragraph}{Interpretation and Benefits}{861}{section*.1783}%
\contentsline {subsection}{Enrichment 20.7.3: Self-Attention GANs (SAGAN)}{862}{section*.1784}%
\contentsline {paragraph}{Architecture Overview}{862}{section*.1786}%
\contentsline {paragraph}{Why It Helps}{862}{section*.1787}%
\contentsline {paragraph}{Training Details and Stabilization}{863}{section*.1788}%
\contentsline {paragraph}{Loss Function}{863}{section*.1789}%
\contentsline {paragraph}{Quantitative Results}{863}{section*.1790}%
\contentsline {paragraph}{Summary}{863}{section*.1791}%
\contentsline {subsection}{Enrichment 20.7.4: BigGANs: Scaling Up GANs}{863}{section*.1792}%
\contentsline {paragraph}{Key Innovations and Techniques}{863}{section*.1793}%
\contentsline {subsubsection}{Enrichment 20.7.4.1: Skip-\( z \) Connections: Hierarchical Latent Injection}{865}{section*.1795}%
\contentsline {paragraph}{Mechanism:}{865}{section*.1796}%
\contentsline {paragraph}{Comparison to Standard CBN:}{866}{section*.1797}%
\contentsline {paragraph}{BigGAN-deep Simplification:}{866}{section*.1798}%
\contentsline {subsubsection}{Enrichment 20.7.4.2: Residual Architecture: Deep and Stable Generators}{866}{section*.1799}%
\contentsline {paragraph}{Motivation and Design:}{866}{section*.1800}%
\contentsline {paragraph}{BigGAN vs. BigGAN-deep:}{866}{section*.1801}%
\contentsline {subsubsection}{Enrichment 20.7.4.3: Truncation Trick in BigGAN: Quality vs. Diversity}{869}{section*.1804}%
\contentsline {paragraph}{Truncated Normal Distributions in Latent Space}{869}{section*.1805}%
\contentsline {paragraph}{Why Truncate?}{869}{section*.1806}%
\contentsline {paragraph}{How Is \( \tau \) Chosen?}{869}{section*.1807}%
\contentsline {paragraph}{Implementation in Practice}{869}{section*.1808}%
\contentsline {paragraph}{Tradeoffs and Limitations}{870}{section*.1809}%
\contentsline {paragraph}{When Truncation Fails}{870}{section*.1810}%
\contentsline {paragraph}{How to Make Truncation Work Reliably}{870}{section*.1811}%
\contentsline {subsubsection}{Enrichment 20.7.4.4: Orthogonal Regularization: A Smoothness Prior for Truncated Latents}{870}{section*.1812}%
\contentsline {subsubsection}{Enrichment 20.7.4.5: Exponential Moving Average (EMA) of Generator Weights}{871}{section*.1813}%
\contentsline {subsubsection}{Enrichment 20.7.4.6: Discriminator-to-Generator Update Ratio}{872}{section*.1814}%
\contentsline {paragraph}{Results and Legacy}{873}{section*.1815}%
\contentsline {subsection}{Enrichment 20.7.5: StackGAN: Two-Stage Text-to-Image Synthesis}{874}{section*.1816}%
\contentsline {paragraph}{From Overview to Components:}{876}{section*.1819}%
\contentsline {subsubsection}{Enrichment 20.7.5.1: Conditioning Augmentation (CA)}{877}{section*.1820}%
\contentsline {paragraph}{Solution: Learn a Distribution Over Conditioning Vectors}{877}{section*.1821}%
\contentsline {paragraph}{Sampling via Reparameterization Trick}{877}{section*.1822}%
\contentsline {paragraph}{KL Divergence Regularization}{877}{section*.1823}%
\contentsline {paragraph}{Benefits of Conditioning Augmentation}{877}{section*.1824}%
\contentsline {paragraph}{Summary Table: Conditioning Augmentation}{878}{section*.1825}%
\contentsline {subsubsection}{Enrichment 20.7.5.2: Stage-I Generator: Coarse Sketching from Noise and Caption}{878}{section*.1826}%
\contentsline {paragraph}{Motivation: Why Two Stages?}{878}{section*.1827}%
\contentsline {paragraph}{Architecture of Stage-I Generator}{878}{section*.1828}%
\contentsline {paragraph}{Output Normalization: Why Tanh?}{879}{section*.1829}%
\contentsline {paragraph}{From Latent Tensor to Displayable Image}{879}{section*.1830}%
\contentsline {paragraph}{How Channel Reduction Works in Upsampling Blocks}{879}{section*.1831}%
\contentsline {paragraph}{Summary of Stage-I Generator}{879}{section*.1832}%
\contentsline {subsubsection}{Enrichment 20.7.5.3: Stage-II Generator: Refinement with Residual Conditioning}{880}{section*.1833}%
\contentsline {paragraph}{Why Two Stages Are Beneficial}{880}{section*.1834}%
\contentsline {paragraph}{Inputs to Stage-II Generator}{880}{section*.1835}%
\contentsline {paragraph}{Network Structure and Residual Design}{880}{section*.1836}%
\contentsline {paragraph}{Semantic Reinforcement via Dual Conditioning}{880}{section*.1837}%
\contentsline {paragraph}{Discriminator in Stage-II}{881}{section*.1838}%
\contentsline {paragraph}{Overall Effect of Stage-II}{881}{section*.1839}%
\contentsline {paragraph}{Summary of Stage-II Generator}{881}{section*.1840}%
\contentsline {subsubsection}{Enrichment 20.7.5.4: Training Procedure and Multi-Stage Objectives}{881}{section*.1841}%
\contentsline {subsubsection}{Enrichment 20.7.5.5: Legacy and Extensions: StackGAN++ and Beyond}{882}{section*.1842}%
\contentsline {subsection}{Enrichment 20.7.6: VQ-GAN: Taming Transformers for High-Res Image Synthesis}{883}{section*.1843}%
\contentsline {subsubsection}{Enrichment 20.7.6.1: VQ-GAN: Overview and Motivation}{883}{section*.1844}%
\contentsline {subsubsection}{Enrichment 20.7.6.2: Training Objectives and Losses in VQ-GAN}{884}{section*.1846}%
\contentsline {paragraph}{Total Loss}{885}{section*.1847}%
\contentsline {paragraph}{1. Perceptual Reconstruction Loss \( \mathcal {L}_{\text {rec}} \)}{885}{section*.1848}%
\contentsline {paragraph}{2. Adversarial Patch Loss \( \mathcal {L}_{\text {GAN}} \)}{885}{section*.1849}%
\contentsline {paragraph}{3. Vector Quantization Commitment and Codebook Loss \( \mathcal {L}_{\text {VQ}} \)}{885}{section*.1850}%
\contentsline {paragraph}{Combined Optimization Strategy}{885}{section*.1851}%
\contentsline {paragraph}{Why This Loss Works}{885}{section*.1852}%
\contentsline {paragraph}{Training Summary}{886}{section*.1853}%
\contentsline {subsubsection}{Enrichment 20.7.6.3: Discrete Codebooks and Token Quantization}{886}{section*.1854}%
\contentsline {paragraph}{Latent Grid and Codebook Structure}{886}{section*.1855}%
\contentsline {paragraph}{Nearest-Neighbor Quantization}{886}{section*.1856}%
\contentsline {paragraph}{Gradient Flow via Stop-Gradient and Codebook Updates}{886}{section*.1857}%
\contentsline {paragraph}{Codebook Capacity and Token Usage}{887}{section*.1858}%
\contentsline {paragraph}{Spatial Token Grid as Transformer Input}{887}{section*.1859}%
\contentsline {paragraph}{Comparison to VQ-VAE-2}{887}{section*.1860}%
\contentsline {paragraph}{Summary}{887}{section*.1861}%
\contentsline {subsubsection}{Enrichment 20.7.6.4: Autoregressive Transformer for Token Modeling}{887}{section*.1862}%
\contentsline {paragraph}{Token Sequence Construction}{887}{section*.1863}%
\contentsline {paragraph}{Autoregressive Training Objective}{887}{section*.1864}%
\contentsline {paragraph}{Positional Encoding and Embedding Table}{888}{section*.1865}%
\contentsline {paragraph}{Sampling for Image Generation}{888}{section*.1866}%
\contentsline {paragraph}{Windowed Attention for Long Sequences}{888}{section*.1867}%
\contentsline {paragraph}{Comparison with Pixel-Level Modeling}{888}{section*.1868}%
\contentsline {subsubsection}{Transformer Variants: Decoder-Only and EncoderDecoder}{888}{section*.1869}%
\contentsline {paragraph}{Training Setup}{889}{section*.1870}%
\contentsline {paragraph}{Summary}{889}{section*.1871}%
\contentsline {subsubsection}{Enrichment 20.7.6.5: Token Sampling and Grid Resolution}{889}{section*.1872}%
\contentsline {paragraph}{Autoregressive Sampling Pipeline}{890}{section*.1873}%
\contentsline {paragraph}{Impact of Latent Grid Resolution}{890}{section*.1874}%
\contentsline {paragraph}{Sliding Window Attention (Optional Variant)}{890}{section*.1875}%
\contentsline {paragraph}{Summary}{890}{section*.1876}%
\contentsline {subsubsection}{Enrichment 20.7.6.6: VQ-GAN: Summary and Outlook}{891}{section*.1877}%
\contentsline {paragraph}{Why VQ-GAN Works}{891}{section*.1878}%
\contentsline {paragraph}{Future Directions and Influence}{891}{section*.1879}%
\contentsline {section}{Enrichment 20.8: Additional Important GAN Works}{892}{section*.1880}%
\contentsline {subsection}{Enrichment 20.8.1: SRGAN: Photo-Realistic Super-Resolution}{892}{section*.1881}%
\contentsline {paragraph}{Motivation and Limitations of Pixel-Wise Supervision}{892}{section*.1882}%
\contentsline {paragraph}{Why Use VGG-Based Perceptual Loss?}{892}{section*.1883}%
\contentsline {paragraph}{Architecture Overview}{893}{section*.1884}%
\contentsline {paragraph}{Upsampling Strategy: Sub-Pixel Convolution Blocks}{893}{section*.1885}%
\contentsline {paragraph}{Discriminator Design}{894}{section*.1886}%
\contentsline {paragraph}{Perceptual Loss Function}{895}{section*.1889}%
\contentsline {paragraph}{Training Strategy}{895}{section*.1890}%
\contentsline {paragraph}{Quantitative and Perceptual Results}{895}{section*.1891}%
\contentsline {subsection}{Enrichment 20.8.2: pix2pix: Paired Image-to-Image Translation with cGANs}{896}{section*.1892}%
\contentsline {paragraph}{Motivation and Formulation}{896}{section*.1893}%
\contentsline {subsubsection}{Enrichment 20.8.2.1: Generator Architecture and L1 Loss}{897}{section*.1895}%
\contentsline {paragraph}{Generator Architecture: U-Net with Skip Connections}{897}{section*.1896}%
\contentsline {paragraph}{The Role of L1 Loss}{897}{section*.1897}%
\contentsline {paragraph}{Why Not WGAN or WGAN-GP?}{897}{section*.1898}%
\contentsline {subsubsection}{Enrichment 20.8.2.2: Discriminator Design and PatchGAN}{898}{section*.1899}%
\contentsline {paragraph}{Discriminator Design and Patch-Level Realism (PatchGAN)}{898}{section*.1900}%
\contentsline {subsubsection}{Enrichment 20.8.2.3: Full Training Objective and Optimization}{899}{section*.1901}%
\contentsline {paragraph}{Generator Loss: Combining Adversarial and Reconstruction Objectives}{899}{section*.1902}%
\contentsline {subsubsection}{Enrichment 20.8.2.4: Summary and Generalization Across Tasks}{900}{section*.1903}%
\contentsline {subsection}{Enrichment 20.8.3: CycleGAN: Unpaired Image-to-Image Translation}{900}{section*.1904}%
\contentsline {subsubsection}{Enrichment 20.8.3.1: Motivation: Beyond Paired Supervision in Image Translation}{900}{section*.1905}%
\contentsline {subsubsection}{Enrichment 20.8.3.2: Typical Use Cases}{901}{section*.1907}%
\contentsline {subsubsection}{Enrichment 20.8.3.3: CycleGAN Architecture: Dual Generators and Discriminators}{902}{section*.1909}%
\contentsline {subsubsection}{Enrichment 20.8.3.4: CycleGAN: Loss Functions and Training Objectives}{902}{section*.1910}%
\contentsline {subsubsection}{Enrichment 20.8.3.5: Network Architecture and Practical Training Considerations}{905}{section*.1912}%
\contentsline {subsubsection}{Enrichment 20.8.3.6: Ablation Study: Impact of Loss Components in CycleGAN}{906}{section*.1913}%
\contentsline {paragraph}{Effect of Removing Loss Components}{906}{section*.1914}%
\contentsline {paragraph}{Quantitative Results (from the CycleGAN Paper)}{906}{section*.1915}%
\contentsline {paragraph}{Qualitative Analysis}{907}{section*.1918}%
\contentsline {paragraph}{Summary}{907}{section*.1920}%
\contentsline {subsubsection}{Enrichment 20.8.3.7: Summary and Transition to Additional Generative Approaches}{907}{section*.1921}%
\contentsline {section}{Enrichment 20.9: Diffusion Models: Modern Generative Modeling}{908}{section*.1922}%
\contentsline {subsubsection}{Enrichment 20.9.0.1: Motivation: Limitations of Previous Generative Models}{908}{section*.1923}%
\contentsline {paragraph}{Autoregressive Models (PixelCNN, PixelRNN, ...)}{908}{section*.1924}%
\contentsline {paragraph}{Variational Autoencoders (VAEs)}{908}{section*.1925}%
\contentsline {paragraph}{Generative Adversarial Networks (GANs)}{908}{section*.1926}%
\contentsline {paragraph}{Hybrid Approaches (VQ-VAE, VQ-GAN)}{908}{section*.1927}%
\contentsline {paragraph}{The Case for Diffusion Models}{908}{section*.1928}%
\contentsline {subsection}{Enrichment 20.9.1: Introduction to Diffusion Models}{909}{section*.1929}%
\contentsline {paragraph}{Mathematical Foundation and Dual Processes}{909}{section*.1930}%
\contentsline {paragraph}{Noise Schedules: How Fast Should the Data Be Destroyed?}{909}{section*.1931}%
\contentsline {paragraph}{Why Is the Process Markovian?}{910}{section*.1932}%
\contentsline {paragraph}{Coupled Roles of Signal Attenuation and Noise Injection}{910}{section*.1933}%
\contentsline {paragraph}{Why Diagonal Covariance?}{911}{section*.1934}%
\contentsline {paragraph}{Closed-Form Marginals of the Forward Process}{911}{section*.1935}%
\contentsline {paragraph}{Why Many Small Steps?}{912}{section*.1936}%
\contentsline {paragraph}{Preparing for the Reverse Process}{913}{section*.1938}%
\contentsline {paragraph}{A Tractable Alternative: Conditioning on \( \mathbf {x}_0 \)}{914}{section*.1939}%
\contentsline {paragraph}{Visual Intuition}{914}{section*.1940}%
\contentsline {paragraph}{Step 1: Define the joint distribution}{914}{section*.1942}%
\contentsline {paragraph}{Step 2: Apply Gaussian conditioning}{915}{section*.1943}%
\contentsline {paragraph}{Step 3: Simplifying the Posterior Mean and Variance}{915}{section*.1944}%
\contentsline {paragraph}{Interpretation:}{916}{section*.1945}%
\contentsline {paragraph}{Final Result}{916}{section*.1946}%
\contentsline {paragraph}{Why This Posterior Is Useful for Training}{916}{section*.1947}%
\contentsline {paragraph}{Why \( q(\mathbf {x}_{t-1} \mid \mathbf {x}_t, \mathbf {x}_0) \) Is Not Used at Inference}{917}{section*.1948}%
\contentsline {paragraph}{Intuition for the Denoising Process}{917}{section*.1949}%
\contentsline {paragraph}{Building a Principled Loss Function}{917}{section*.1950}%
\contentsline {subsection}{Enrichment 20.9.2: Denoising Diffusion Probabilistic Models (DDPM)}{919}{section*.1951}%
\contentsline {subsubsection}{Enrichment 20.9.2.1: Summary of Core Variables in Diffusion Models}{919}{section*.1952}%
\contentsline {paragraph}{Forward Noise Schedule and Signal Retention}{919}{section*.1953}%
\contentsline {paragraph}{Reverse Posterior and Posterior Parameters}{920}{section*.1954}%
\contentsline {paragraph}{Learned Reverse Mean and Sampling Parameterization}{922}{section*.1955}%
\contentsline {subsubsection}{Enrichment 20.9.2.2: ELBO Formulation and Loss Decomposition}{923}{section*.1956}%
\contentsline {paragraph}{Maximum Likelihood in Latent-Variable Generative Models}{923}{section*.1957}%
\contentsline {paragraph}{Introducing a Tractable Proposal Distribution}{923}{section*.1959}%
\contentsline {paragraph}{Why the Importance Ratio Is Well-Defined}{924}{section*.1961}%
\contentsline {paragraph}{From Integral to Expectation: Importance Sampling Identity}{924}{section*.1962}%
\contentsline {paragraph}{Applying Jensens Inequality: A Lower Bound for Optimization}{925}{section*.1963}%
\contentsline {paragraph}{Factorization of the Model and Variational Distributions}{925}{section*.1965}%
\contentsline {paragraph}{Inserting a Tractable Posterior into the ELBO}{925}{section*.1966}%
\contentsline {paragraph}{Decomposing the Log-Ratio}{926}{section*.1967}%
\contentsline {paragraph}{ELBO in KL-Compatible Form}{927}{section*.1968}%
\contentsline {paragraph}{Rewriting as KL Expectations}{927}{section*.1969}%
\contentsline {paragraph}{Final KL-Based ELBO for Diffusion Models}{928}{section*.1970}%
\contentsline {paragraph}{Interpreting the ELBO Components}{928}{section*.1971}%
\contentsline {paragraph}{Why the KL Divergence Is Tractable and Useful for Training}{929}{section*.1972}%
\contentsline {subsubsection}{Enrichment 20.9.2.3: Noise Prediction Objective and Simplification}{930}{section*.1974}%
\contentsline {paragraph}{From ELBO to Mean Prediction}{930}{section*.1975}%
\contentsline {paragraph}{Fixing the Variance}{930}{section*.1976}%
\contentsline {paragraph}{Rewriting the Mean via Noise Prediction}{930}{section*.1977}%
\contentsline {subsubsection}{Enrichment 20.9.2.4: Training and Inference in DDPMs}{933}{section*.1978}%
\contentsline {paragraph}{Connection to the Model Distribution \boldmath \( p_\theta (x_{t-1} \mid x_t) \).}{934}{section*.1979}%
\contentsline {paragraph}{Interpreting the Update.}{934}{section*.1980}%
\contentsline {paragraph}{Stochasticity and Sample Diversity.}{934}{section*.1981}%
\contentsline {paragraph}{Final Step Refinement.}{934}{section*.1982}%
\contentsline {subsubsection}{Enrichment 20.9.2.5: Architecture, Datasets, and Implementation Details}{935}{section*.1983}%
\contentsline {paragraph}{Backbone Architecture: Why U-Net Fits Denoising in Diffusion Models}{935}{section*.1984}%
\contentsline {subparagraph}{Why an EncoderDecoder?}{935}{subparagraph*.1985}%
\contentsline {subparagraph}{Multiscale Hierarchy and Architectural Intuition}{935}{subparagraph*.1986}%
\contentsline {subparagraph}{Walkthrough: Layer-by-Layer Data Flow}{936}{subparagraph*.1987}%
\contentsline {subparagraph}{Why U-Net Matches the Diffusion Objective}{936}{subparagraph*.1988}%
\contentsline {paragraph}{Resolution and Depth Scaling}{937}{section*.1989}%
\contentsline {paragraph}{Time Embedding via Sinusoidal Positional Encoding}{937}{section*.1990}%
\contentsline {paragraph}{How the Time Embedding is Used}{937}{section*.1991}%
\contentsline {paragraph}{Why Not Simpler Alternatives?}{937}{section*.1992}%
\contentsline {paragraph}{Model Scale and Dataset Diversity}{939}{section*.1993}%
\contentsline {paragraph}{Summary}{939}{section*.1994}%
\contentsline {subsubsection}{Enrichment 20.9.2.6: Empirical Evaluation and Latent-Space Behavior}{940}{section*.1995}%
\contentsline {paragraph}{Noise Prediction Yields Stable Training and Best Sample Quality}{940}{section*.1996}%
\contentsline {paragraph}{Image Interpolation in Latent Space}{940}{section*.1997}%
\contentsline {paragraph}{Coarse-to-Fine Interpolation and Structural Completion}{941}{section*.1999}%
\contentsline {paragraph}{Progressive Lossy Compression via Reverse Denoising}{942}{section*.2001}%
\contentsline {subsection}{Enrichment 20.9.3: Denoising Diffusion Implicit Models (DDIM)}{943}{section*.2003}%
\contentsline {paragraph}{Motivation}{943}{section*.2004}%
\contentsline {paragraph}{From DDPM Sampling to DDIM Inversion}{943}{section*.2005}%
\contentsline {paragraph}{1. From Forward Diffusion to Inversion}{943}{section*.2006}%
\contentsline {paragraph}{2. Reverse Step to Arbitrary \( s < t \)}{944}{section*.2007}%
\contentsline {paragraph}{3.\ Why the singlenoise picture is still correct}{946}{section*.2010}%
\contentsline {paragraph}{4. Optional Stochastic Extension}{947}{section*.2011}%
\contentsline {paragraph}{5. Advantages of DDIM Sampling}{948}{section*.2012}%
\contentsline {subsection}{Enrichment 20.9.4: Guidance Techniques in Diffusion Models}{949}{section*.2013}%
\contentsline {paragraph}{Training Procedure}{951}{section*.2016}%
\contentsline {paragraph}{Sampling with Classifier-Free Guidance}{952}{section*.2017}%
\contentsline {paragraph}{Why Classifier-Free Guidance Works: A Score-Based and Intuitive View}{953}{section*.2018}%
\contentsline {paragraph}{Interpretation}{954}{section*.2019}%
\contentsline {paragraph}{Typical Settings}{955}{section*.2020}%
\contentsline {paragraph}{Advantages}{955}{section*.2022}%
\contentsline {paragraph}{Adoption in Large-Scale Models}{955}{section*.2023}%
\contentsline {subsection}{Enrichment 20.9.5: Cascaded Diffusion Models}{956}{section*.2024}%
\contentsline {paragraph}{Motivation and Overview}{956}{section*.2025}%
\contentsline {paragraph}{Architecture: U-Net Design for Cascaded Diffusion Models}{957}{section*.2027}%
\contentsline {paragraph}{Empirical Performance of CDMs}{959}{section*.2029}%
\contentsline {subsection}{Enrichment 20.9.6: Progressive Distillation for Fast Sampling}{960}{section*.2030}%
\contentsline {paragraph}{Motivation}{960}{section*.2031}%
\contentsline {paragraph}{Pseudocode: Progressive Distillation Loop}{961}{section*.2033}%
\contentsline {paragraph}{Prerequisites Required to Understand The Progressive Distillation Loop}{962}{section*.2034}%
\contentsline {paragraph}{What Is SNR and Why Use It?}{963}{section*.2035}%
\contentsline {paragraph}{Cosine Schedule and Angular Construction}{963}{section*.2036}%
\contentsline {paragraph}{Teacher Trajectory Construction via Two DDIM Steps}{963}{section*.2037}%
\contentsline {paragraph}{Empirical Results and Sample Quality}{967}{section*.2038}%
\contentsline {paragraph}{Conclusion}{968}{section*.2040}%
\contentsline {subsection}{Enrichment 20.9.7: Velocity-Space Sampling: Learning Denoising Trajectories}{969}{section*.2041}%
\contentsline {section}{Enrichment 20.10: Flow Matching: Beating Diffusion Using Flows}{971}{section*.2042}%
\contentsline {paragraph}{Further Reading}{973}{section*.2043}%
\contentsline {subsection}{Enrichment 20.10.1: Generative Flows: Learning by Trajectory Integration}{973}{section*.2044}%
\contentsline {paragraph}{Motivation: From Mapping to Likelihood.}{973}{section*.2045}%
\contentsline {paragraph}{From KL to Log-Likelihood}{973}{section*.2046}%
\contentsline {paragraph}{How Does \( p_1 \) Arise from a Flow?}{973}{section*.2047}%
\contentsline {paragraph}{The Role of the Continuity Equation}{975}{section*.2049}%
\contentsline {paragraph}{Flux: Constructing \( p_t(x) v_t(x) \)}{975}{section*.2050}%
\contentsline {paragraph}{Divergence: Understanding \( \nabla \cdot (p_t v_t) \)}{976}{section*.2051}%
\contentsline {paragraph}{Putting the Continuity Equation in Plain English}{976}{section*.2052}%
\contentsline {paragraph}{Broader Implications for Continuous-Time Generative Models}{977}{section*.2053}%
\contentsline {paragraph}{Interpretation}{978}{section*.2055}%
\contentsline {paragraph}{Why Pure CNFLikelihood Training Is Not Scalable?}{979}{section*.2056}%
\contentsline {paragraph}{Flow Matching: A New Approach}{980}{section*.2057}%
\contentsline {subsection}{Enrichment 20.10.2: Development of the Flow Matching Objective}{981}{section*.2058}%
\contentsline {paragraph}{From Density Path to Vector Field}{981}{section*.2059}%
\contentsline {paragraph}{The Naive Flow Matching Objective}{981}{section*.2060}%
\contentsline {paragraph}{Why the Naive Objective Is Intractable}{982}{section*.2062}%
\contentsline {paragraph}{A Local Solution via Conditional Paths}{982}{section*.2063}%
\contentsline {paragraph}{Recovering the Marginal Vector Field}{983}{section*.2064}%
\contentsline {paragraph}{Why This Identity Is Valid}{983}{section*.2065}%
\contentsline {paragraph}{From Validity to Practicality: The Need for a Tractable Objective}{985}{section*.2066}%
\contentsline {paragraph}{Conditional Flow Matching (CFM): A Sample-Based Reformulation}{985}{section*.2067}%
\contentsline {paragraph}{Why This Is Powerful}{986}{section*.2068}%
\contentsline {subsection}{Enrichment 20.10.3: Conditional Probability Paths and Vector Fields}{986}{section*.2069}%
\contentsline {paragraph}{Motivation}{986}{section*.2070}%
\contentsline {paragraph}{Canonical Gaussian Conditional Paths}{986}{section*.2071}%
\contentsline {paragraph}{Deriving the Velocity Field from the Continuity Equation}{987}{section*.2072}%
\contentsline {paragraph}{General Gaussian Conditional Paths and Affine Flow Maps}{987}{section*.2073}%
\contentsline {paragraph}{The Canonical Affine Flow and Induced Velocity Field}{987}{section*.2074}%
\contentsline {paragraph}{The Conditional Flow Matching Loss}{988}{section*.2076}%
\contentsline {paragraph}{From Theory to Practice: Training with Conditional Flow Matching}{989}{section*.2078}%
\contentsline {paragraph}{Implementation Notes}{989}{section*.2080}%
\contentsline {paragraph}{Summary}{990}{section*.2081}%
\contentsline {subsection}{Enrichment 20.10.4: Choosing Conditional Paths - Diffusion vs OT}{991}{section*.2082}%
\contentsline {subsubsection}{Choosing Conditional Paths  Diffusion vs OT}{991}{section*.2083}%
\contentsline {paragraph}{Variance Exploding (VE) Conditional Paths}{991}{section*.2084}%
\contentsline {paragraph}{Variance Preserving (VP) Conditional Paths}{991}{section*.2085}%
\contentsline {paragraph}{Limitations of Diffusion-Based Conditional Paths}{992}{section*.2086}%
\contentsline {subsubsection}{Optimal Transport Conditional Probability Paths}{992}{section*.2087}%
\contentsline {paragraph}{What Is Optimal Transport?}{992}{section*.2088}%
\contentsline {paragraph}{Affine OT Flow Between Gaussians}{993}{section*.2089}%
\contentsline {paragraph}{The OT Vector Field}{993}{section*.2090}%
\contentsline {paragraph}{The Corresponding Flow Map and CFM Loss}{993}{section*.2091}%
\contentsline {paragraph}{Vector Field Geometry: Diffusion vs. Optimal Transport}{993}{section*.2092}%
\contentsline {paragraph}{Why Optimal Transport Defines a Superior Learning Signal}{995}{section*.2094}%
\contentsline {paragraph}{OT-based Conditional Flow Matching Inference}{996}{section*.2096}%
\contentsline {paragraph}{Takeaway}{996}{section*.2097}%
\contentsline {subsection}{Enrichment 20.10.5: Implementation, Experiments, and Related Work}{997}{section*.2098}%
\contentsline {paragraph}{Implementation Details}{997}{section*.2099}%
\contentsline {paragraph}{Empirical Results: OT vs.\ Diffusion}{997}{section*.2100}%
\contentsline {paragraph}{Quantitative Benchmarks}{997}{section*.2102}%
\contentsline {paragraph}{Additional Comparisons}{998}{section*.2104}%
\contentsline {paragraph}{Related Work and Positioning}{998}{section*.2105}%
\contentsline {paragraph}{Outlook}{999}{section*.2106}%
\contentsline {section}{Enrichment 20.11: Additional Pioneering Works in Generative AI}{1000}{section*.2107}%
\contentsline {subsection}{Enrichment 20.11.1: GLIDE: Text-Guided Diffusion with Classifier-Free Guidance}{1000}{section*.2108}%
\contentsline {paragraph}{Model Architecture and Conditioning Mechanism}{1000}{section*.2109}%
\contentsline {paragraph}{Super-Resolution Modules in \textsc {GLIDE}}{1004}{section*.2111}%
\contentsline {paragraph}{Relationship to Cascaded Diffusion Models (CDMs)}{1004}{section*.2112}%
\contentsline {paragraph}{Full Generation Pipeline of \textsc {GLIDE}}{1005}{section*.2113}%
\contentsline {paragraph}{ADM U-Net Architecture in \textsc {GLIDE}}{1005}{section*.2114}%
\contentsline {paragraph}{Summary of the GLIDE System}{1006}{section*.2115}%
\contentsline {paragraph}{Text-Guided Editing and Inpainting Capabilities}{1007}{section*.2116}%
\contentsline {paragraph}{Sketch-Based Conditional Editing with SDEdit}{1009}{section*.2119}%
\contentsline {paragraph}{Classifier-Free Guidance vs.\ CLIP Guidance}{1010}{section*.2121}%
\contentsline {paragraph}{Failure Cases and Architectural Limitations}{1012}{section*.2124}%
\contentsline {subsection}{Enrichment 20.11.2: DALLE 1: Discrete Tokens for Text-to-Image Generation}{1013}{section*.2126}%
\contentsline {paragraph}{Motivation: Turning Images into Token Sequences for GPT-Style Modelling}{1013}{section*.2127}%
\contentsline {paragraph}{How VQ-VAE Enables Discrete Tokenization}{1015}{section*.2129}%
\contentsline {paragraph}{Clarifying Terminology: dVAE vs. VQ-VAE}{1020}{section*.2132}%
\contentsline {paragraph}{Training Datasets and Sample Generation Pipeline}{1021}{section*.2133}%
\contentsline {paragraph}{Experimental Results and Motivation for DALL$\cdot $E~2}{1023}{section*.2135}%
\contentsline {subsection}{Enrichment 20.11.3: DALL$\cdot $E~2: Diffusion Priors over CLIP Embeddings}{1026}{section*.2139}%
\contentsline {paragraph}{System Overview and Architectural Shift}{1026}{section*.2140}%
\contentsline {paragraph}{Diffusion Prior: Bridging Text and Image Embeddings}{1027}{section*.2142}%
\contentsline {subparagraph}{Training Objective}{1028}{subparagraph*.2143}%
\contentsline {subparagraph}{Model Architecture}{1029}{subparagraph*.2144}%
\contentsline {paragraph}{Diffusion-Based Decoder}{1032}{section*.2146}%
\contentsline {paragraph}{Semantic Interpolation and Reconstruction in CLIP Latents}{1033}{section*.2147}%
\contentsline {paragraph}{Robustness and Generalization of the Decoder}{1037}{section*.2152}%
\contentsline {paragraph}{Dataset Construction and Semantic Pretraining}{1038}{section*.2154}%
\contentsline {paragraph}{Image Quality and Diversity: Qualitative and Quantitative Results}{1039}{section*.2155}%
\contentsline {paragraph}{Design Limitations and Architectural Tradeoffs}{1040}{section*.2157}%
\contentsline {paragraph}{Stepping Towards Latent Diffusion Models}{1040}{section*.2158}%
\contentsline {subsection}{Enrichment 20.11.4: Latent Diffusion Models (LDMs)}{1042}{section*.2159}%
\contentsline {paragraph}{Overview and Conceptual Shift}{1042}{section*.2160}%
\contentsline {paragraph}{Autoencoder Architecture and Training Objective}{1042}{section*.2161}%
\contentsline {paragraph}{Autoencoder Architecture and Latent Normalization}{1044}{section*.2163}%
\contentsline {subparagraph}{Encoder and Decoder Design}{1044}{subparagraph*.2164}%
\contentsline {subparagraph}{Latent Normalization for Diffusion Compatibility}{1044}{subparagraph*.2165}%
\contentsline {paragraph}{Denoising Diffusion in Latent Space}{1045}{section*.2166}%
\contentsline {subparagraph}{Architecture of the Denoising U-Net}{1045}{subparagraph*.2167}%
\contentsline {subsubsection}{Enrichment 20.11.4.1: Decoder Fidelity Without Explicit Text Conditioning}{1047}{section*.2168}%
\contentsline {paragraph}{Why It Still Works}{1047}{section*.2169}%
\contentsline {paragraph}{Trade-offs and Alternatives}{1047}{section*.2170}%
\contentsline {paragraph}{Conclusion}{1047}{section*.2171}%
\contentsline {paragraph}{Classifier-Free Guidance (CFG)}{1048}{section*.2172}%
\contentsline {paragraph}{Empirical Results and Ablations}{1048}{section*.2173}%
\contentsline {paragraph}{Limitations and Transition to Newer Works Like \emph {Imagen}}{1049}{section*.2175}%
\contentsline {subsection}{Enrichment 20.11.5: Imagen: Scaling Language Fidelity in Text2Img Models}{1050}{section*.2176}%
\contentsline {paragraph}{Motivation and Context}{1050}{section*.2177}%
\contentsline {subsubsection}{Cascaded Diffusion Pipeline}{1051}{section*.2178}%
\contentsline {subsubsection}{Classifier-Free Guidance and Dynamic Thresholding}{1052}{section*.2180}%
\contentsline {paragraph}{Problem: Oversaturation from Large Guidance}{1052}{section*.2181}%
\contentsline {paragraph}{Nave Solution: Static Thresholding}{1052}{section*.2182}%
\contentsline {paragraph}{Dynamic Thresholding: an Adaptive Alternative to Static Clipping}{1053}{section*.2183}%
\contentsline {subsubsection}{Experimental Findings and DrawBench Evaluation}{1054}{section*.2185}%
\contentsline {paragraph}{Scaling the Text Encoder}{1054}{section*.2186}%
\contentsline {paragraph}{DrawBench: A Diverse Prompt Evaluation Suite}{1055}{section*.2188}%
\contentsline {paragraph}{Qualitative Samples}{1056}{section*.2190}%
\contentsline {subsubsection}{Enrichment 20.11.5.1: Toward Fine-Grained Control and Editable Generation}{1056}{section*.2192}%
\contentsline {paragraph}{From Fidelity to Controllability}{1056}{section*.2193}%
\contentsline {paragraph}{Why Prompt-Aware Attention Control Is Needed}{1057}{section*.2194}%
\contentsline {paragraph}{Key Approaches and Innovations}{1057}{section*.2195}%
\contentsline {subsection}{Enrichment 20.11.6: Prompt-to-Prompt (P2P): Cross-Attention Editing in DMs}{1058}{section*.2196}%
\contentsline {paragraph}{Motivation and Core Insight}{1058}{section*.2197}%
\contentsline {subparagraph}{Cross-Attention as the Mechanism for Prompt Influence}{1059}{subparagraph*.2199}%
\contentsline {subparagraph}{Editing by Cross-Attention Injection}{1060}{subparagraph*.2201}%
\contentsline {subparagraph}{Use Case: Content Modifications via Prompt Edits}{1064}{subparagraph*.2203}%
\contentsline {subparagraph}{Use Case: Object Preservation Across Scene Changes}{1065}{subparagraph*.2205}%
\contentsline {subparagraph}{Use Case: Controlled Blending via Partial Attention Injection}{1067}{subparagraph*.2207}%
\contentsline {subparagraph}{Use Case: Emphasizing and De-emphasizing Concepts}{1068}{subparagraph*.2209}%
\contentsline {subparagraph}{Use Case: Text-Guided Stylization while Preserving Layout}{1069}{subparagraph*.2211}%
\contentsline {subparagraph}{Use Case: Editing Real Images via Inversion and Prompt-to-Prompt}{1070}{subparagraph*.2213}%
\contentsline {subparagraph}{Limitations and Transition to Personalized Editing}{1070}{subparagraph*.2215}%
\contentsline {subsection}{Enrichment 20.11.7: DreamBooth: Personalized Text-to-Image Generation}{1072}{section*.2216}%
\contentsline {paragraph}{Motivation and Core Insight}{1072}{section*.2217}%
\contentsline {subparagraph}{Model Setup and Identifier Creation}{1072}{subparagraph*.2219}%
\contentsline {subparagraph}{Training Objective and Prior Preservation}{1077}{subparagraph*.2224}%
\contentsline {paragraph}{Main Loss: Denoising Objective}{1077}{section*.2225}%
\contentsline {paragraph}{Preventing Overfitting: Prior Preservation Loss}{1077}{section*.2226}%
\contentsline {paragraph}{Effect and Interpretation}{1078}{section*.2228}%
\contentsline {subparagraph}{Subject-Driven Generation in New Contexts}{1079}{subparagraph*.2229}%
\contentsline {subsection}{Enrichment 20.11.8: ControlNet  Structured Conditioning for Diffusion Models}{1083}{section*.2235}%
\contentsline {subparagraph}{Motivation and Background}{1083}{subparagraph*.2236}%
\contentsline {subparagraph}{Block Injection and Architectural Motivation}{1084}{subparagraph*.2238}%
\contentsline {subsubsection}{Enrichment 20.11.8.1: ControlNet Architecture}{1084}{section*.2239}%
\contentsline {paragraph}{Injecting Spatial Conditioning into Frozen Networks}{1084}{section*.2240}%
\contentsline {paragraph}{ControlNet Architectural Design}{1085}{section*.2241}%
\contentsline {paragraph}{Motivation for Additive Injection: Why Not Inject \( c \) Directly?}{1085}{section*.2242}%
\contentsline {paragraph}{Component Breakdown}{1085}{section*.2243}%
\contentsline {paragraph}{How Can the Output Change If the U-Net Is Frozen? And Why Is Denoising Still Valid?}{1086}{section*.2244}%
\contentsline {paragraph}{Training Objective}{1087}{section*.2245}%
\contentsline {paragraph}{Why ControlNet Preserves Denoising Capability}{1087}{section*.2246}%
\contentsline {subsubsection}{Enrichment 20.11.8.2: Training Behavior and Sudden Convergence}{1090}{section*.2249}%
\contentsline {subparagraph}{Classifier-Free Guidance and Resolution-Aware Weighting}{1091}{subparagraph*.2251}%
\contentsline {subparagraph}{Resolution-Aware Weighting (CFG-RW)}{1091}{subparagraph*.2252}%
\contentsline {paragraph}{Why resolution matters}{1091}{section*.2253}%
\contentsline {paragraph}{Why It Works}{1092}{section*.2254}%
\contentsline {paragraph}{Training Intuition With CFG-RW}{1092}{section*.2255}%
\contentsline {subparagraph}{Limitations of ControlNet and the Need for Semantic Conditioning}{1093}{subparagraph*.2257}%
\contentsline {paragraph}{Preprocessing Dependency}{1093}{section*.2258}%
\contentsline {paragraph}{Lack of Semantic Awareness}{1093}{section*.2259}%
\contentsline {paragraph}{Limited Compositionality and Scalability}{1093}{section*.2260}%
\contentsline {subsection}{Enrichment 20.11.9: IP-Adapter  Semantic Image Prompting for DMs}{1094}{section*.2261}%
\contentsline {subparagraph}{Motivation and Background}{1094}{subparagraph*.2262}%
\contentsline {subparagraph}{Introducing IP-Adapter: A Lightweight and Compatible Solution}{1094}{subparagraph*.2263}%
\contentsline {paragraph}{Why IP-Adapter Works Without Compromising the Base Model}{1095}{section*.2264}%
\contentsline {subparagraph}{1. Image Guidance via Decoupled Cross-Attention in U-Net Blocks}{1095}{subparagraph*.2265}%
\contentsline {subparagraph}{2. The Base U-Net Remains Fully Frozen}{1095}{subparagraph*.2266}%
\contentsline {subparagraph}{3. Safe Integration via Additive Fusion}{1095}{subparagraph*.2267}%
\contentsline {subparagraph}{4. Denoising Logic is Preserved by Construction}{1095}{subparagraph*.2268}%
\contentsline {subparagraph}{5. \(\lambda \) Offers Explicit, Safe, Inference-Time Control}{1095}{subparagraph*.2269}%
\contentsline {subparagraph}{6. Summary: Why This Architecture is Effective and Non-Destructive}{1096}{subparagraph*.2270}%
\contentsline {paragraph}{ControlNet vs. IP-Adapter: Structural vs. Semantic Conditioning}{1096}{section*.2271}%
\contentsline {subparagraph}{ControlNet: Explicit Structural Conditioning}{1096}{subparagraph*.2272}%
\contentsline {subparagraph}{ControlNet \& Raw Images}{1096}{subparagraph*.2273}%
\contentsline {paragraph}{Key Architectural Components and Detailed Integration}{1099}{section*.2275}%
\contentsline {subparagraph}{Versatility and Generalization without Fine-Tuning}{1101}{subparagraph*.2277}%
\contentsline {paragraph}{Comparative Evaluation Across Structural Control Tasks}{1103}{section*.2279}%
\contentsline {paragraph}{Image-to-Image Translation, Inpainting, and Multimodal Prompting}{1104}{section*.2281}%
\contentsline {paragraph}{Ablation: Validating Architectural Design}{1106}{section*.2285}%
\contentsline {paragraph}{Looking Forward}{1109}{section*.2288}%
\contentsline {subsection}{Enrichment 20.11.10: Transfusion: Unified Multimodal Generation}{1110}{section*.2289}%
\contentsline {paragraph}{Motivation and Overview}{1110}{section*.2290}%
\contentsline {paragraph}{Architecture and Training Pipeline of Transfusion}{1112}{section*.2292}%
\contentsline {subparagraph}{Part 1: Image Tokenization Pipeline}{1112}{subparagraph*.2293}%
\contentsline {subparagraph}{Part 2: Text Tokenization Pipeline}{1113}{subparagraph*.2295}%
\contentsline {subparagraph}{Part 3: Multimodal Sequence Construction}{1114}{subparagraph*.2296}%
\contentsline {subparagraph}{Part 4: Transformer Processing with Hybrid Attention}{1114}{subparagraph*.2297}%
\contentsline {subparagraph}{Part 5: Training Objectives and Loss Functions}{1115}{subparagraph*.2299}%
\contentsline {subparagraph}{Part 6: Key Advantages of the Training Design}{1116}{subparagraph*.2300}%
\contentsline {paragraph}{Empirical Results and Qualitative Examples}{1117}{section*.2301}%
\contentsline {subparagraph}{Showcase: High-Quality Multi-Modal Generation}{1117}{subparagraph*.2302}%
\contentsline {subparagraph}{Zero-Shot Image Editing via Fine-Tuning}{1118}{subparagraph*.2304}%
\contentsline {paragraph}{Ablation Studies and Experimental Insights}{1119}{section*.2306}%
\contentsline {subparagraph}{Interpreting Evaluation Metrics}{1119}{subparagraph*.2307}%
\contentsline {subparagraph}{Attention Masking: Causal vs.\ Bidirectional}{1119}{subparagraph*.2308}%
\contentsline {subparagraph}{Patch Size Variations}{1119}{subparagraph*.2310}%
\contentsline {subparagraph}{Encoding Architecture: Linear vs. U-Net}{1120}{subparagraph*.2312}%
\contentsline {subparagraph}{Noise Scheduling in Image-to-Text Training}{1120}{subparagraph*.2314}%
\contentsline {subparagraph}{Comparison to Specialized Generative Models}{1120}{subparagraph*.2316}%
\contentsline {paragraph}{Summary}{1121}{section*.2318}%
\contentsline {subsection}{Enrichment 20.11.11: Visual Autoregressive Modeling (VAR)}{1122}{section*.2319}%
\contentsline {subparagraph}{Multi-Scale Architecture for Coarse-to-Fine Generation: How VAR Works}{1123}{subparagraph*.2321}%
\contentsline {paragraph}{Overview: A Two-Stage Pipeline for Image Generation}{1123}{section*.2322}%
\contentsline {paragraph}{Stage 1: Multi-Scale VQ-VAE for Hierarchical Tokenization}{1123}{section*.2323}%
\contentsline {subparagraph}{Hierarchical Token Encoding via Residual Refinement}{1124}{subparagraph*.2324}%
\contentsline {subparagraph}{Token Decoding and Image Reconstruction}{1124}{subparagraph*.2325}%
\contentsline {subparagraph}{Training Objective for the VQ-VAE}{1125}{subparagraph*.2326}%
\contentsline {paragraph}{Stage 2: Scale-Aware Autoregressive Transformer}{1125}{section*.2327}%
\contentsline {subparagraph}{From Tokens to Embeddings: Transformer Inputs}{1125}{subparagraph*.2328}%
\contentsline {subparagraph}{Why a Second Stage is Needed}{1126}{subparagraph*.2329}%
\contentsline {subparagraph}{Autoregressive Modeling Across Scales}{1126}{subparagraph*.2330}%
\contentsline {subparagraph}{Training Procedure}{1126}{subparagraph*.2331}%
\contentsline {subparagraph}{Inference and Generation}{1127}{subparagraph*.2332}%
\contentsline {subparagraph}{Final Decoding and Image Reconstruction}{1127}{subparagraph*.2333}%
\contentsline {paragraph}{Benefits of the VAR Design}{1128}{section*.2335}%
\contentsline {paragraph}{Experimental Results: High-Quality Generation and Editing}{1128}{section*.2336}%
\contentsline {subparagraph}{Comparison with Other Generative Paradigms}{1129}{subparagraph*.2338}%
\contentsline {paragraph}{Scaling Trends, Model Comparison, and Future Outlook}{1131}{section*.2340}%
\contentsline {subparagraph}{Scaling Efficiency and Sample Quality}{1131}{subparagraph*.2341}%
\contentsline {subparagraph}{Comparison to Diffusion and Autoregressive Models}{1132}{subparagraph*.2343}%
\contentsline {paragraph}{Qualitative Scaling Effects of VAR}{1132}{section*.2344}%
\contentsline {subparagraph}{Limitations and Future Directions}{1134}{subparagraph*.2346}%
\contentsline {subsection}{Enrichment 20.11.12: DiT: Diffusion Transformers}{1135}{section*.2347}%
\contentsline {paragraph}{Motivation and context}{1135}{section*.2348}%
\contentsline {paragraph}{High-level overview}{1135}{section*.2350}%
\contentsline {subparagraph}{Why transformers? Intuition.}{1135}{subparagraph*.2351}%
\contentsline {paragraph}{Method: architecture and components}{1136}{section*.2352}%
\contentsline {subparagraph}{Tokenization (patchify) of the latent.}{1136}{subparagraph*.2353}%
\contentsline {subparagraph}{High-level overview: DiT as a transformer backbone for diffusion}{1137}{subparagraph*.2355}%
\contentsline {subparagraph}{From AdaIN to adaLN: motivation and adaptation}{1137}{subparagraph*.2357}%
\contentsline {subparagraph}{DiT block: adaLN and the adaLN-Zero variant}{1137}{subparagraph*.2358}%
\contentsline {subparagraph}{Head and parameterization}{1138}{subparagraph*.2359}%
\contentsline {subparagraph}{Conditioning and guidance}{1139}{subparagraph*.2360}%
\contentsline {subparagraph}{Training objective and setup}{1139}{subparagraph*.2362}%
\contentsline {subsubsection}{Experiments and ablations}{1140}{section*.2363}%
\contentsline {subparagraph}{Scaling and SOTA comparisons.}{1140}{subparagraph*.2364}%
\contentsline {subparagraph}{Training-time scaling trends.}{1140}{subparagraph*.2366}%
\contentsline {subparagraph}{Qualitative scaling: more flops $\rightarrow $ better images.}{1141}{subparagraph*.2368}%
\contentsline {subparagraph}{Gflops predict FID.}{1142}{subparagraph*.2370}%
\contentsline {subparagraph}{Total training compute vs.\ FID.}{1142}{subparagraph*.2372}%
\contentsline {subparagraph}{Sampling compute cannot replace model compute.}{1143}{subparagraph*.2374}%
\contentsline {subparagraph}{Benchmark summary (ImageNet 256/512).}{1143}{subparagraph*.2376}%
\contentsline {paragraph}{What changed vs.\ Stable Diffusion and why it matters}{1143}{section*.2377}%
\contentsline {paragraph}{Relation to prior and follow-ups}{1144}{section*.2378}%
\contentsline {subsubsection}{Limitations and future work}{1144}{section*.2379}%
\contentsline {paragraph}{Practical recipe}{1144}{section*.2380}%
\contentsline {chapter}{\numberline {21}Lecture 21: Visualizing Models \& Generating Images}{1145}{chapter.21}%
\contentsline {section}{\numberline {21.1}Visualizing Layer Filters}{1145}{section.21.1}%
\contentsline {subsection}{\numberline {21.1.1}Visualizing First Layer Filters}{1145}{subsection.21.1.1}%
\contentsline {paragraph}{Architecture Comparison}{1145}{section*.2381}%
\contentsline {paragraph}{Interpretation and Limitations}{1146}{section*.2383}%
\contentsline {subsection}{\numberline {21.1.2}Visualizing Higher Layer Filters}{1146}{subsection.21.1.2}%
\contentsline {paragraph}{Example: ConvNetJS Visualization}{1147}{section*.2384}%
\contentsline {paragraph}{Interpretation and Motivation for Indirect Methods}{1147}{section*.2386}%
\contentsline {section}{\numberline {21.2}Last Layer Features: Nearest Neighbors, Dimensionality Reduction}{1148}{section.21.2}%
\contentsline {subsection}{\numberline {21.2.1}Semantic Similarity via Nearest Neighbors}{1148}{subsection.21.2.1}%
\contentsline {subsection}{\numberline {21.2.2}Dimensionality Reduction and Embedding Visualization}{1149}{subsection.21.2.2}%
\contentsline {paragraph}{Interpretation and Applications}{1150}{section*.2390}%
\contentsline {section}{\numberline {21.3}Visualizing Activations and Maximally Activating Patches}{1151}{section.21.3}%
\contentsline {paragraph}{How to Visualize Activations}{1151}{section*.2391}%
\contentsline {paragraph}{Why Do Activation Maps Reveal Spatial Information?}{1152}{section*.2393}%
\contentsline {paragraph}{What Do Activations Reveal?}{1153}{section*.2394}%
\contentsline {paragraph}{What Can We Do With Activation Maps?}{1153}{section*.2395}%
\contentsline {subsection}{\numberline {21.3.1}Maximally Activating Patches}{1154}{subsection.21.3.1}%
\contentsline {paragraph}{Methodology}{1154}{section*.2396}%
\contentsline {paragraph}{Intuition and Insights}{1155}{section*.2398}%
\contentsline {paragraph}{From What It Sees to What It Uses}{1155}{section*.2399}%
\contentsline {section}{\numberline {21.4}Saliency via Occlusion and Backpropagation}{1156}{section.21.4}%
\contentsline {subsection}{\numberline {21.4.1}Occlusion Sensitivity}{1156}{subsection.21.4.1}%
\contentsline {paragraph}{Methodology}{1156}{section*.2401}%
\contentsline {paragraph}{From Patch Scores to Pixel-Level Saliency}{1156}{section*.2402}%
\contentsline {paragraph}{Intuition and Interpretation}{1157}{section*.2403}%
\contentsline {subsubsection}{Enrichment 21.4.1.1: Advantages and Limitations of Occlusion Sensitivity}{1157}{section*.2404}%
\contentsline {subsection}{\numberline {21.4.2}Saliency via Gradient Backpropagation}{1157}{subsection.21.4.2}%
\contentsline {paragraph}{Interpretation and Use Cases}{1158}{section*.2406}%
\contentsline {paragraph}{Towards Unsupervised Segmentation}{1158}{section*.2407}%
\contentsline {section}{\numberline {21.5}Guided Backpropagation of Intermediate Features}{1159}{section.21.5}%
\contentsline {subsection}{\numberline {21.5.1}Backpropagation to Visualize Intermediate Neurons}{1159}{subsection.21.5.1}%
\contentsline {subsection}{\numberline {21.5.2}Guided Backpropagation: Cleaner Gradient Visualizations}{1159}{subsection.21.5.2}%
\contentsline {paragraph}{Why Does This Help? Intuition and Impact}{1160}{section*.2410}%
\contentsline {subsection}{\numberline {21.5.3}Visualizing Intermediate Feature Detectors}{1161}{subsection.21.5.3}%
\contentsline {paragraph}{From Saliency to Synthesis}{1161}{section*.2412}%
\contentsline {section}{\numberline {21.6}Gradient Ascent and Class Visualization}{1161}{section.21.6}%
\contentsline {paragraph}{Objective Function}{1162}{section*.2413}%
\contentsline {paragraph}{Optimization via Gradient Ascent}{1162}{section*.2414}%
\contentsline {subsection}{\numberline {21.6.1}Regularization: Making Images Look Natural}{1162}{subsection.21.6.1}%
\contentsline {paragraph}{Advanced Regularizers}{1163}{section*.2417}%
\contentsline {subsection}{\numberline {21.6.2}Visualizing Intermediate Features}{1164}{subsection.21.6.2}%
\contentsline {subsubsection}{Multifaceted Feature Visualization via Generative Models}{1164}{section*.2420}%
\contentsline {paragraph}{Realism vs. Fidelity}{1165}{section*.2423}%
\contentsline {section}{\numberline {21.7}Adversarial Examples: A Deep Dive into Model Vulnerability}{1166}{section.21.7}%
\contentsline {subsection}{\numberline {21.7.1}Fundamental Attack Mechanisms}{1166}{subsection.21.7.1}%
\contentsline {subsection}{\numberline {21.7.2}Taxonomy of Adversarial Attacks}{1167}{subsection.21.7.2}%
\contentsline {paragraph}{White-box attacks}{1167}{section*.2425}%
\contentsline {paragraph}{Black-box attacks}{1168}{section*.2426}%
\contentsline {subsection}{\numberline {21.7.3}Milestones in Robustness Evaluation}{1169}{subsection.21.7.3}%
\contentsline {subsection}{\numberline {21.7.4}Defense Toolbox and Its Limitations}{1169}{subsection.21.7.4}%
\contentsline {subsection}{\numberline {21.7.5}Real-World Relevance and Persistent Risks}{1170}{subsection.21.7.5}%
\contentsline {subsection}{\numberline {21.7.6}Open Challenges and Theoretical Connections}{1170}{subsection.21.7.6}%
\contentsline {section}{\numberline {21.8}Class Activation Mapping (CAM) and Grad-CAM}{1170}{section.21.8}%
\contentsline {paragraph}{Mechanism of CAM}{1170}{section*.2428}%
\contentsline {paragraph}{Limitations of CAM}{1172}{section*.2431}%
\contentsline {subsection}{\numberline {21.8.1}Generalization via Grad-CAM}{1172}{subsection.21.8.1}%
\contentsline {paragraph}{Comparative Visualization Examples}{1174}{section*.2433}%
\contentsline {subsection}{\numberline {21.8.2}Comparison Between CAM and Grad-CAM}{1175}{subsection.21.8.2}%
\contentsline {paragraph}{From Explanation to Synthesis: A Path Toward Feature Inversion}{1176}{section*.2437}%
\contentsline {section}{\numberline {21.9}Feature Inversion}{1176}{section.21.9}%
\contentsline {paragraph}{Problem Formulation}{1176}{section*.2438}%
\contentsline {paragraph}{Comparison to Gradient Ascent}{1176}{section*.2439}%
\contentsline {paragraph}{Effect of Layer Depth}{1177}{section*.2441}%
\contentsline {paragraph}{Interpretability Insights}{1178}{section*.2443}%
\contentsline {paragraph}{Applications}{1178}{section*.2444}%
\contentsline {paragraph}{Beyond Feature Inversion}{1178}{section*.2445}%
\contentsline {section}{\numberline {21.10}DeepDream: Amplifying Neural Perceptions}{1178}{section.21.10}%
\contentsline {paragraph}{Optimization Objective}{1179}{section*.2447}%
\contentsline {paragraph}{Amplifying Layer-wise Semantics}{1179}{section*.2448}%
\contentsline {paragraph}{Dreaming Deeper}{1181}{section*.2452}%
\contentsline {paragraph}{Interpretability Value}{1182}{section*.2455}%
\contentsline {section}{\numberline {21.11}Texture Synthesis}{1182}{section.21.11}%
\contentsline {subsection}{\numberline {21.11.1}Classical Approaches}{1183}{subsection.21.11.1}%
\contentsline {paragraph}{Limitations of Pixel Matching}{1184}{section*.2459}%
\contentsline {subsection}{\numberline {21.11.2}Neural Texture Synthesis via Gram Matrices}{1184}{subsection.21.11.2}%
\contentsline {paragraph}{Constructing the Gram Matrix}{1184}{section*.2460}%
\contentsline {paragraph}{Why Gram Matrices?}{1184}{section*.2462}%
\contentsline {paragraph}{Optimization Pipeline}{1185}{section*.2464}%
\contentsline {paragraph}{Effect of Matching Higher Layers}{1186}{section*.2466}%
\contentsline {paragraph}{Impact and Legacy}{1186}{section*.2468}%
\contentsline {section}{\numberline {21.12}Neural Style Transfer}{1187}{section.21.12}%
\contentsline {subsection}{\numberline {21.12.1}Neural Style Transfer: Content and Style Fusion}{1187}{subsection.21.12.1}%
\contentsline {paragraph}{Intuition}{1187}{section*.2469}%
\contentsline {paragraph}{Optimization Objective}{1187}{section*.2471}%
\contentsline {paragraph}{Optimization via Gradient Descent}{1188}{section*.2473}%
\contentsline {paragraph}{Stylization Results}{1190}{section*.2475}%
\contentsline {paragraph}{Controlling Style Intensity}{1191}{section*.2478}%
\contentsline {paragraph}{Effect of Style Image Scale}{1191}{section*.2480}%
\contentsline {paragraph}{Combining Styles}{1192}{section*.2482}%
\contentsline {paragraph}{Limitations}{1192}{section*.2484}%
\contentsline {subsection}{\numberline {21.12.2}Fast Neural Style Transfer}{1193}{subsection.21.12.2}%
\contentsline {paragraph}{Training Setup}{1193}{section*.2485}%
\contentsline {paragraph}{Key Insight}{1193}{section*.2487}%
\contentsline {paragraph}{Stylization Examples}{1194}{section*.2488}%
\contentsline {paragraph}{Instance Normalization}{1194}{section*.2490}%
\contentsline {paragraph}{Conditional Instance Normalization for Multi-Style Transfer}{1195}{section*.2492}%
\contentsline {paragraph}{Summary and Emerging Directions}{1195}{section*.2494}%
\contentsline {chapter}{\numberline {22}Lecture 22: Self-Supervised Learning}{1196}{chapter.22}%
\contentsline {section}{\numberline {22.1}Motivation and Definition}{1196}{section.22.1}%
\contentsline {subsection}{\numberline {22.1.1}What is Self-Supervised Learning (SSL)?}{1196}{subsection.22.1.1}%
\contentsline {paragraph}{Learning Representations Without Labels}{1196}{section*.2495}%
\contentsline {paragraph}{Pretraining Then Transferring}{1196}{section*.2496}%
\contentsline {paragraph}{Embedding Geometry and Semantic Similarity}{1197}{section*.2498}%
\contentsline {paragraph}{Why Pretext Tasks Work}{1197}{section*.2499}%
\contentsline {paragraph}{Categories of Pretext Tasks}{1197}{section*.2500}%
\contentsline {paragraph}{Backbones, Augmentations, and Losses}{1198}{section*.2501}%
\contentsline {paragraph}{Summary}{1198}{section*.2502}%
\contentsline {subsection}{\numberline {22.1.2}Why Self-Supervised Learning?}{1198}{subsection.22.1.2}%
\contentsline {paragraph}{Supervised Learning is Expensive}{1198}{section*.2503}%
\contentsline {paragraph}{But Unlabeled Data is Free (and Plentiful)}{1198}{section*.2504}%
\contentsline {paragraph}{Learning Like Humans}{1199}{section*.2505}%
\contentsline {paragraph}{SSL as the Backbone of Foundation Models}{1199}{section*.2506}%
\contentsline {subsection}{\numberline {22.1.3}LeCun's AI Cake: SSL as the Base Layer}{1199}{subsection.22.1.3}%
\contentsline {paragraph}{The Cake Analogy}{1199}{section*.2507}%
\contentsline {paragraph}{Practical Significance}{1199}{section*.2509}%
\contentsline {subsection}{\numberline {22.1.4}Practical Integration into Deep Learning Pipelines}{1200}{subsection.22.1.4}%
\contentsline {paragraph}{How SSL is Used in Practice}{1200}{section*.2510}%
\contentsline {paragraph}{Flexible Transfer and Modularity}{1200}{section*.2511}%
\contentsline {paragraph}{Strategic Impact and Adoption}{1200}{section*.2512}%
\contentsline {section}{\numberline {22.2}A Taxonomy of Self-Supervised Representation Learning Methods}{1201}{section.22.2}%
\contentsline {subsection}{\numberline {22.2.1}Contrastive Methods}{1201}{subsection.22.2.1}%
\contentsline {paragraph}{Discriminative Representations via Similarity and Dissimilarity}{1201}{section*.2513}%
\contentsline {paragraph}{Insight}{1201}{section*.2514}%
\contentsline {subsection}{\numberline {22.2.2}Distillation-Based Methods}{1201}{subsection.22.2.2}%
\contentsline {paragraph}{Teacher-Student Framework without Negatives}{1201}{section*.2515}%
\contentsline {paragraph}{Insight}{1201}{section*.2516}%
\contentsline {subsection}{\numberline {22.2.3}Feature Decorrelation Methods}{1202}{subsection.22.2.3}%
\contentsline {paragraph}{Promoting Redundancy Reduction}{1202}{section*.2517}%
\contentsline {paragraph}{Insight}{1202}{section*.2518}%
\contentsline {subsection}{\numberline {22.2.4}Clustering-Based Methods}{1202}{subsection.22.2.4}%
\contentsline {paragraph}{Learning via Group-Level Semantics}{1202}{section*.2519}%
\contentsline {paragraph}{Insight}{1202}{section*.2520}%
\contentsline {section}{\numberline {22.3}Contrastive Methods}{1203}{section.22.3}%
\contentsline {subsection}{\numberline {22.3.1}Motivation for Contrastive Learning}{1203}{subsection.22.3.1}%
\contentsline {paragraph}{Core Idea}{1203}{section*.2523}%
\contentsline {paragraph}{Instance Discrimination as a Pretext Task}{1203}{section*.2524}%
\contentsline {paragraph}{Avoiding Trivial Solutions}{1203}{section*.2525}%
\contentsline {paragraph}{Scalability and Generalization}{1204}{section*.2526}%
\contentsline {paragraph}{Key Advantages}{1204}{section*.2527}%
\contentsline {paragraph}{From Semantic Similarity to Objective Formulation}{1205}{section*.2529}%
\contentsline {paragraph}{Contrastive Learning as Mutual Information Maximization}{1205}{section*.2530}%
\contentsline {paragraph}{Towards a Unified Loss Function}{1205}{section*.2531}%
\contentsline {subsection}{\numberline {22.3.2}Origin and Intuition Behind Contrastive Loss}{1206}{subsection.22.3.2}%
\contentsline {paragraph}{From Dimensionality Reduction to Discriminative Embeddings}{1206}{section*.2532}%
\contentsline {paragraph}{Why the Margin Matters}{1206}{section*.2533}%
\contentsline {paragraph}{A Visual Summary of the Learning Objective}{1206}{section*.2535}%
\contentsline {paragraph}{Why Not Use \( \frac {1}{D_W} \)?}{1207}{section*.2537}%
\contentsline {paragraph}{From Supervision to Self-Supervision}{1207}{section*.2538}%
\contentsline {paragraph}{Triplet Setup: Anchor, Positive, Negative}{1208}{section*.2540}%
\contentsline {subsection}{\numberline {22.3.3}The NT-Xent Loss: Normalized Temperature-Scaled Cross-Entropy}{1209}{subsection.22.3.3}%
\contentsline {paragraph}{Overview and Purpose}{1209}{section*.2542}%
\contentsline {paragraph}{Pairwise Contrastive Loss: NT-Xent Formulation}{1209}{section*.2543}%
\contentsline {paragraph}{Batch Aggregation and the \( \frac {1}{2N} \) Factor}{1209}{section*.2544}%
\contentsline {paragraph}{The Role of Symmetry}{1210}{section*.2545}%
\contentsline {paragraph}{Illustration of the Loss Mechanism}{1210}{section*.2546}%
\contentsline {paragraph}{Role of the Projection Head}{1211}{section*.2548}%
\contentsline {paragraph}{Log-Softmax Intuition}{1211}{section*.2549}%
\contentsline {paragraph}{Summary}{1211}{section*.2551}%
\contentsline {subsection}{\numberline {22.3.4}SimCLR: A Simple Framework for Contrastive Learning}{1212}{subsection.22.3.4}%
\contentsline {paragraph}{Overview}{1212}{section*.2552}%
\contentsline {paragraph}{Architecture Components}{1212}{section*.2553}%
\contentsline {paragraph}{Design Principles Behind SimCLR}{1212}{section*.2554}%
\contentsline {paragraph}{Training Configuration and Stability}{1212}{section*.2555}%
\contentsline {paragraph}{Performance Benchmarks}{1213}{section*.2556}%
\contentsline {paragraph}{Visualization of SimCLR Pipeline}{1213}{section*.2557}%
\contentsline {paragraph}{Limitations and the Road to MoCo}{1214}{section*.2559}%
\contentsline {subsection}{\numberline {22.3.5}Momentum Contrast (MoCo)}{1214}{subsection.22.3.5}%
\contentsline {paragraph}{Motivation: Avoiding Large Batch Sizes}{1214}{section*.2560}%
\contentsline {paragraph}{Core Architecture}{1214}{section*.2561}%
\contentsline {paragraph}{Contrastive Loss in MoCo}{1214}{section*.2562}%
\contentsline {paragraph}{MoCo Training Pipeline}{1215}{section*.2563}%
\contentsline {paragraph}{Why MoCo Works: Scale, Stability, and Efficiency}{1215}{section*.2564}%
\contentsline {paragraph}{What the Queue Enables}{1215}{section*.2565}%
\contentsline {paragraph}{Momentum Hyperparameter Tuning and Ablation Results}{1216}{section*.2567}%
\contentsline {paragraph}{Other Key Ablations and Design Justifications}{1217}{section*.2569}%
\contentsline {paragraph}{Performance and Comparison with SimCLR}{1218}{section*.2571}%
\contentsline {paragraph}{From MoCo v1 to MoCo v2}{1218}{section*.2573}%
\contentsline {subsection}{\numberline {22.3.6}MoCo v2 and MoCo v3}{1219}{subsection.22.3.6}%
\contentsline {paragraph}{From MoCo v1 to v2: Architectural Refinements}{1219}{section*.2574}%
\contentsline {paragraph}{MoCo v3: Adapting Momentum Contrast to Vision Transformers}{1219}{section*.2577}%
\contentsline {paragraph}{Why Symmetric Loss?}{1221}{section*.2578}%
\contentsline {paragraph}{Explanation}{1221}{section*.2579}%
\contentsline {paragraph}{Performance Highlights}{1221}{section*.2580}%
\contentsline {paragraph}{Takeaway}{1222}{section*.2583}%
\contentsline {subsection}{\numberline {22.3.7}SimCLR v2: Scaling Contrastive Learning for Semi-Supervised Settings}{1222}{subsection.22.3.7}%
\contentsline {paragraph}{Motivation and Overview}{1222}{section*.2584}%
\contentsline {paragraph}{Three-Stage Training Framework}{1223}{section*.2586}%
\contentsline {paragraph}{Architectural Enhancements and Ablation Insights}{1223}{section*.2587}%
\contentsline {paragraph}{Why Distillation Works}{1223}{section*.2588}%
\contentsline {paragraph}{Quantitative Results and Analysis}{1224}{section*.2589}%
\contentsline {paragraph}{Conclusion}{1225}{section*.2593}%
\contentsline {subsection}{\numberline {22.3.8}ReLIC: Representation Learning via Invariant Causal Mechanisms}{1226}{subsection.22.3.8}%
\contentsline {paragraph}{Motivation and Causal Assumptions}{1226}{section*.2594}%
\contentsline {paragraph}{Learning via Invariant Proxy Prediction}{1226}{section*.2595}%
\contentsline {paragraph}{Summary}{1227}{section*.2597}%
\contentsline {paragraph}{From Proxy Tasks to Instance Discrimination}{1228}{section*.2598}%
\contentsline {paragraph}{ReLIC Architecture and Training Setup}{1228}{section*.2600}%
\contentsline {paragraph}{Contrastive and Distributional Loss Terms}{1229}{section*.2601}%
\contentsline {paragraph}{Loss Term 1: Instance-Level Contrastive Learning}{1230}{section*.2602}%
\contentsline {paragraph}{Loss Term 2: KL Regularization for Distributional Invariance}{1232}{section*.2604}%
\contentsline {paragraph}{From Causal Motivation to Loss Construction}{1233}{section*.2606}%
\contentsline {paragraph}{ReLIC Objective}{1234}{section*.2607}%
\contentsline {paragraph}{Architecture and Implementation Details}{1235}{section*.2608}%
\contentsline {paragraph}{Performance and Evaluation}{1235}{section*.2610}%
\contentsline {paragraph}{Summary and Outlook}{1235}{section*.2611}%
\contentsline {subsection}{\numberline {22.3.9}ReLICv2: Enhanced Invariant Representation Learning}{1236}{subsection.22.3.9}%
\contentsline {subsubsection}{Motivation: From View Invariance to Causal Robustness}{1236}{section*.2612}%
\contentsline {subsubsection}{Foreground Saliency Masking}{1236}{section*.2614}%
\contentsline {subsubsection}{Multi-View Learning with Large and Small Crops}{1237}{section*.2616}%
\contentsline {subsubsection}{ReLICv2 Objective}{1237}{section*.2617}%
\contentsline {paragraph}{Term 1: Contrastive Log-Likelihood (Large-to-Large)}{1238}{section*.2618}%
\contentsline {paragraph}{Term 2: KL Divergence (Large-to-Large)}{1239}{section*.2619}%
\contentsline {paragraph}{Small-to-Large View Consistency Terms}{1240}{section*.2620}%
\contentsline {paragraph}{Training Procedure}{1241}{section*.2621}%
\contentsline {paragraph}{Empirical Evaluation and Robustness Analysis}{1242}{section*.2622}%
\contentsline {paragraph}{Linear Evaluation Performance}{1242}{section*.2623}%
\contentsline {paragraph}{Robustness and Out-of-Distribution Generalization}{1243}{section*.2625}%
\contentsline {paragraph}{Semantic Clarity and Class-wise Consistency}{1243}{section*.2627}%
\contentsline {subsubsection}{Summary}{1244}{section*.2629}%
\contentsline {subsection}{\numberline {22.3.10}Further Contrastive Innovations}{1245}{subsection.22.3.10}%
\contentsline {paragraph}{Nearest-Neighbor Contrastive Learning (NNCLR)}{1245}{section*.2630}%
\contentsline {paragraph}{Adversarial Contrastive Learning (AdCo)}{1246}{section*.2632}%
\contentsline {paragraph}{Contrastive Learning with Stronger Augmentations (CLSA)}{1246}{section*.2633}%
\contentsline {subsubsection}{Enrichment 22.3.10.1: CLSA vs.\ ReLIC: KL Divergence in Perspective}{1247}{section*.2634}%
\contentsline {paragraph}{CLSA: Distributional Distillation Across Augmentation Strength}{1247}{section*.2635}%
\contentsline {paragraph}{ReLICv1: Invariant Prediction Across Augmentations}{1247}{section*.2636}%
\contentsline {paragraph}{Two KL Terms, Two Philosophies}{1247}{section*.2637}%
\contentsline {paragraph}{Summary (CLSA vs. ReLIC)}{1248}{section*.2638}%
\contentsline {paragraph}{Comparative Landscape and Emerging Trends}{1248}{section*.2639}%
\contentsline {paragraph}{A Transition Toward Natural Supervision}{1248}{section*.2641}%
\contentsline {subsection}{\numberline {22.3.11}CLIP: Learning Transferable Visual Models from Natural Language Supervision}{1249}{subsection.22.3.11}%
\contentsline {paragraph}{Motivation: Beyond Fixed Labels}{1249}{section*.2642}%
\contentsline {paragraph}{A Nave Approach: Caption Prediction}{1249}{section*.2644}%
\contentsline {paragraph}{Efficiency Comparison: Contrastive vs.\ Predictive Objectives}{1250}{section*.2645}%
\contentsline {paragraph}{Why Contrastive Learning Wins}{1251}{section*.2647}%
\contentsline {paragraph}{Key Insight}{1251}{section*.2648}%
\contentsline {subsubsection}{CLIPs Contrastive Training Approach and Loss}{1252}{section*.2649}%
\contentsline {paragraph}{Training Strategy: Paired Alignment at Scale}{1252}{section*.2650}%
\contentsline {paragraph}{Symmetric Contrastive Loss}{1252}{section*.2651}%
\contentsline {paragraph}{Interpretation and Scaling Advantages}{1253}{section*.2653}%
\contentsline {paragraph}{Efficient Large-Scale Training}{1253}{section*.2654}%
\contentsline {subsubsection}{CLIP Loss Pseudo Code \& Further Explanations}{1254}{section*.2655}%
\contentsline {paragraph}{Loss Pseudo Code}{1254}{section*.2656}%
\contentsline {paragraph}{Explanation}{1254}{section*.2657}%
\contentsline {paragraph}{Intuition Behind the BCE Terms}{1255}{section*.2658}%
\contentsline {subsubsection}{CLIP Experiments and Ablations}{1255}{section*.2659}%
\contentsline {paragraph}{Zero-Shot Performance vs.\ Supervised Models}{1255}{section*.2660}%
\contentsline {paragraph}{Robustness to Natural Distribution Shift}{1256}{section*.2662}%
\contentsline {paragraph}{Linear Probe Evaluation Across Models}{1257}{section*.2664}%
\contentsline {paragraph}{Tradeoffs in Dataset-Specific Adaptation}{1258}{section*.2666}%
\contentsline {paragraph}{Summary and Practical Takeaways}{1258}{section*.2668}%
\contentsline {section}{\numberline {22.4}Self-Distillation Methods}{1259}{section.22.4}%
\contentsline {subsection}{\numberline {22.4.1}Limitations of Contrastive Learning}{1259}{subsection.22.4.1}%
\contentsline {subsection}{\numberline {22.4.2}From Contrastive Methods to Self-Distillation}{1259}{subsection.22.4.2}%
\contentsline {paragraph}{Classical Knowledge Distillation}{1259}{section*.2669}%
\contentsline {paragraph}{From Classical KD to Self-Distillation}{1261}{section*.2671}%
\contentsline {paragraph}{Self-Distillation: Teacher-Free Prediction Alignment}{1261}{section*.2672}%
\contentsline {paragraph}{Cold Start and the Bootstrapping Feedback Loop}{1262}{section*.2673}%
\contentsline {paragraph}{Final Representation: What Do We Keep?}{1263}{section*.2675}%
\contentsline {paragraph}{Introduction Summary}{1263}{section*.2676}%
\contentsline {subsection}{\numberline {22.4.3}Bootstrap Your Own Latent (BYOL)}{1264}{subsection.22.4.3}%
\contentsline {paragraph}{Motivation: Learning Without Contrast}{1264}{section*.2677}%
\contentsline {paragraph}{Architectural Overview}{1264}{section*.2678}%
\contentsline {paragraph}{Mathematical Formulation and Training Objective}{1265}{section*.2680}%
\contentsline {paragraph}{Robustness and Empirical Performance}{1266}{section*.2681}%
\contentsline {paragraph}{Linear Evaluation on ImageNet}{1266}{section*.2683}%
\contentsline {paragraph}{Semi-Supervised Evaluation}{1267}{section*.2685}%
\contentsline {paragraph}{Transfer to Downstream Tasks}{1267}{section*.2687}%
\contentsline {paragraph}{Ablation Studies and Collapse Prevention}{1267}{section*.2689}%
\contentsline {paragraph}{Conclusion}{1268}{section*.2690}%
\contentsline {subsection}{\numberline {22.4.4}SimSiam: Self-Supervised Learning Without Negative Pairs or Momentum}{1269}{subsection.22.4.4}%
\contentsline {paragraph}{Motivation: Can Collapse Be Avoided Without Negatives or EMA?}{1269}{section*.2691}%
\contentsline {paragraph}{Architecture and Symmetric Learning Mechanism}{1269}{section*.2692}%
\contentsline {paragraph}{SimSiam Training Pseudocode}{1270}{section*.2694}%
\contentsline {paragraph}{Gradient Formula and Learning Signal}{1271}{section*.2695}%
\contentsline {paragraph}{EM-Like Interpretation of SimSiam Training}{1272}{section*.2696}%
\contentsline {paragraph}{Conclusion: Stop-Gradient as a Structural Inductive Bias}{1273}{section*.2697}%
\contentsline {paragraph}{Empirical Validation of the Stop-Gradient Mechanism}{1273}{section*.2698}%
\contentsline {paragraph}{Ablation Studies and Analysis}{1274}{section*.2700}%
\contentsline {paragraph}{Comparison to Other Self-Supervised Methods}{1276}{section*.2704}%
\contentsline {paragraph}{Paper Summary}{1276}{section*.2707}%
\contentsline {subsection}{\numberline {22.4.5}DINO: Self-Distillation with No Labels}{1277}{subsection.22.4.5}%
\contentsline {paragraph}{Motivation: From Invariance to Semantic Understanding}{1277}{section*.2708}%
\contentsline {paragraph}{Self-Distillation Without Labels}{1277}{section*.2710}%
\contentsline {paragraph}{Multi-Crop Strategy and View Asymmetry}{1278}{section*.2711}%
\contentsline {paragraph}{Architectural Backbone: Why Vision Transformers?}{1280}{section*.2714}%
\contentsline {paragraph}{Preventing Collapse with Centering and Sharpening}{1280}{section*.2715}%
\contentsline {paragraph}{Asymmetric Distillation Objective}{1281}{section*.2716}%
\contentsline {paragraph}{Why Use Softmax Without Labels?}{1282}{section*.2717}%
\contentsline {paragraph}{No Predictor: Functional Asymmetry Instead of Architectural Tricks}{1283}{section*.2718}%
\contentsline {paragraph}{PyTorch-Style Pseudocode and Explanation}{1284}{section*.2719}%
\contentsline {subsubsection}{Experimental Results and Ablations for DINO}{1285}{section*.2720}%
\contentsline {paragraph}{Linear and k-NN Evaluation on ImageNet}{1285}{section*.2721}%
\contentsline {paragraph}{Transfer to Retrieval and Segmentation Tasks}{1285}{section*.2723}%
\contentsline {paragraph}{Ablation: Emergent Object Segmentation via Self-Attention}{1286}{section*.2724}%
\contentsline {paragraph}{Ablation: Semantic Structure from Unlabeled Data}{1287}{section*.2726}%
\contentsline {paragraph}{Ablation: Teacher Update Strategies}{1288}{section*.2728}%
\contentsline {paragraph}{Ablation: Collapse Prevention via Centering and Sharpening}{1289}{section*.2730}%
\contentsline {paragraph}{Ablation: Patch Size and Inference Throughput}{1289}{section*.2732}%
\contentsline {paragraph}{Ablation: Batch Size Effects}{1290}{section*.2734}%
\contentsline {paragraph}{Ablation: Multi-Crop Augmentation and Resource Tradeoffs}{1290}{section*.2736}%
\contentsline {paragraph}{Paper Conclusion}{1290}{section*.2738}%
\contentsline {subsection}{\numberline {22.4.6}DINOv2: Learning Robust Visual Features Without Supervision}{1291}{subsection.22.4.6}%
\contentsline {paragraph}{Background and Motivation}{1291}{section*.2739}%
\contentsline {paragraph}{Emergent Semantic Structure Without Labels}{1291}{section*.2741}%
\contentsline {paragraph}{Scaling Training through Architectural and Data Efficiency}{1292}{section*.2742}%
\contentsline {paragraph}{Data Processing in DINOv2}{1292}{section*.2743}%
\contentsline {subsubsection}{SSCD: A Self-Supervised Descriptor for Image Copy Detection}{1293}{section*.2745}%
\contentsline {paragraph}{Motivation for Copy Detection in DINOv2}{1293}{section*.2746}%
\contentsline {paragraph}{Core Architecture and Augmentations}{1296}{section*.2748}%
\contentsline {paragraph}{Post-Processing via Whitening and Synergy with Training}{1296}{section*.2749}%
\contentsline {paragraph}{Augmentation Pipeline for Real-World Tampering}{1297}{section*.2750}%
\contentsline {paragraph}{Loss Formulation with Entropy and Mixed Positives}{1298}{section*.2751}%
\contentsline {paragraph}{Empirical Results and Impact}{1300}{section*.2753}%
\contentsline {subsubsection}{Masked Autoencoders (MAE): Scalable Vision Learners}{1301}{section*.2754}%
\contentsline {paragraph}{Asymmetric Architecture and High-Ratio Masking}{1302}{section*.2756}%
\contentsline {paragraph}{Empirical Results and Qualitative Analysis}{1304}{section*.2758}%
\contentsline {subsubsection}{iBOT: Masked Image Modeling with Self-Distillation}{1307}{section*.2764}%
\contentsline {paragraph}{iBOT Loss Function and Self-Distillation Objective}{1309}{section*.2766}%
\contentsline {paragraph}{iBOT Training Procedure}{1312}{section*.2767}%
\contentsline {paragraph}{Empirical Results and Evaluation}{1313}{section*.2769}%
\contentsline {paragraph}{Ablation Studies and Component Analysis}{1315}{section*.2774}%
\contentsline {paragraph}{iBOT vs. DINO: Paving the Way for DINOv2}{1316}{section*.2777}%
\contentsline {paragraph}{Sinkhorn--Knopp Centering in DINOv2}{1317}{section*.2778}%
\contentsline {paragraph}{Sinkhorn--Knopp Algorithm (NumPy Pseudocode)}{1319}{section*.2779}%
\contentsline {paragraph}{Explanation and DINOv2 Motivation}{1320}{section*.2780}%
\contentsline {paragraph}{Toy Example: The Fair Project Manager}{1320}{section*.2781}%
\contentsline {paragraph}{Connection to DINOv2}{1322}{section*.2782}%
\contentsline {paragraph}{Linear Evaluation on ImageNet and Comparison to Prior Work}{1323}{section*.2783}%
\contentsline {paragraph}{Ablation of Design Modifications from iBOT to DINOv2}{1324}{section*.2785}%
\contentsline {paragraph}{Ablation of Pretraining Data: LVD-142M vs ImageNet-22k}{1325}{section*.2787}%
\contentsline {paragraph}{Effectiveness of Knowledge Distillation from DINOv2}{1326}{section*.2789}%
\contentsline {paragraph}{Transfer to Diverse Visual Tasks}{1327}{section*.2791}%
\contentsline {subsection}{\numberline {22.4.7}DINOv3: Quick Overview}{1328}{subsection.22.4.7}%
\contentsline {paragraph}{Motivation}{1328}{section*.2792}%
\contentsline {paragraph}{What DINOv3 changes}{1328}{section*.2793}%
\contentsline {paragraph}{Position in the SSL landscape}{1329}{section*.2794}%
\contentsline {paragraph}{Practical gains}{1329}{section*.2795}%
\contentsline {subsubsection}{From Self-Distillation to Clustering-Based Objectives}{1329}{section*.2796}%
\contentsline {section}{\numberline {22.5}Clustering Methods}{1330}{section.22.5}%
\contentsline {subsection}{\numberline {22.5.1}SwAV: Online Clustering via Swapped Assignments}{1330}{subsection.22.5.1}%
\contentsline {paragraph}{From Contrastive Bottlenecks to Clustering-Based Self-Supervision}{1330}{section*.2797}%
\contentsline {subsubsection}{Architecture and Training Pipeline}{1331}{section*.2799}%
\contentsline {paragraph}{Multi-crop Augmentation and Swapped Prediction}{1331}{section*.2800}%
\contentsline {paragraph}{Training Objective and Prototype Updates}{1332}{section*.2801}%
\contentsline {paragraph}{Summary}{1333}{section*.2803}%
\contentsline {subsubsection}{Empirical Results and Key Findings}{1333}{section*.2804}%
\contentsline {paragraph}{Benchmarking on ImageNet}{1334}{section*.2805}%
\contentsline {paragraph}{Transfer to Downstream Tasks}{1334}{section*.2806}%
\contentsline {paragraph}{Training Efficiency and Accessibility}{1334}{section*.2807}%
\contentsline {paragraph}{Ablation Highlights}{1334}{section*.2808}%
\contentsline {paragraph}{Impact and Legacy}{1334}{section*.2809}%
\contentsline {section}{\numberline {22.6}Feature Decorrelation Methods}{1335}{section.22.6}%
\contentsline {subsection}{\numberline {22.6.1}Barlow Twins: Feature Decorrelation without Negatives}{1335}{subsection.22.6.1}%
\contentsline {paragraph}{Method Overview}{1335}{section*.2811}%
\contentsline {paragraph}{Redundancy Reduction Loss}{1336}{section*.2812}%
\contentsline {paragraph}{Practical Details}{1336}{section*.2814}%
\contentsline {subsubsection}{Empirical Results and Ablation Studies}{1337}{section*.2815}%
\contentsline {paragraph}{Linear Evaluation on ImageNet}{1337}{section*.2816}%
\contentsline {paragraph}{Transfer Learning Performance}{1337}{section*.2818}%
\contentsline {paragraph}{Ablation Studies}{1338}{section*.2820}%
\contentsline {paragraph}{Batch Size Robustness}{1338}{section*.2822}%
\contentsline {paragraph}{Effect of Projector Dimensionality}{1339}{section*.2824}%
\contentsline {paragraph}{Sensitivity to Augmentations}{1339}{section*.2826}%
\contentsline {paragraph}{Hyperparameter Stability}{1340}{section*.2828}%
\contentsline {paragraph}{Summary and Outlook}{1340}{section*.2830}%
\contentsline {subsection}{\numberline {22.6.2}VICReg: Variance-Invariance-Covariance Regularization}{1341}{subsection.22.6.2}%
\contentsline {subsubsection}{Invariance Term: Similarity Loss}{1342}{section*.2832}%
\contentsline {subsubsection}{Variance Term: Spread Preservation}{1343}{section*.2833}%
\contentsline {subsubsection}{Covariance Term: Redundancy Reduction}{1343}{section*.2834}%
\contentsline {subsubsection}{Implementation Details and Empirical Evaluation}{1345}{section*.2835}%
\contentsline {paragraph}{Training Setup}{1345}{section*.2836}%
\contentsline {paragraph}{Linear Evaluation on ImageNet}{1345}{section*.2837}%
\contentsline {paragraph}{Transfer Learning Performance}{1345}{section*.2839}%
\contentsline {paragraph}{Robustness to Batch Size}{1346}{section*.2841}%
\contentsline {paragraph}{Summary of Empirical Results}{1346}{section*.2842}%
\contentsline {subsubsection}{Ablation Studies and Objective Decomposition}{1346}{section*.2843}%
\contentsline {paragraph}{Effect of Removing Loss Terms}{1346}{section*.2844}%
\contentsline {paragraph}{Architectural Robustness}{1347}{section*.2846}%
\contentsline {paragraph}{Comparison with Whitening-Based Methods}{1347}{section*.2847}%
\contentsline {paragraph}{Ablation Summary}{1347}{section*.2848}%
\contentsline {section}{\numberline {22.7}Adapting SSL to Downstream Tasks}{1348}{section.22.7}%
\contentsline {subsection}{\numberline {22.7.1}Aligning Backbone Structure with Task Demands}{1348}{subsection.22.7.1}%
\contentsline {paragraph}{Masked Image Modeling: Prioritizing Spatial Detail}{1348}{section*.2849}%
\contentsline {paragraph}{Contrastive and Clustering Methods: Emphasizing Semantic Structure}{1348}{section*.2850}%
\contentsline {paragraph}{Hybrid Approaches: Balancing Spatial and Semantic Information}{1348}{section*.2851}%
\contentsline {paragraph}{Recommended Usage}{1349}{section*.2852}%
\contentsline {subsection}{\numberline {22.7.2}Data Distribution and Domain Shift Considerations}{1349}{subsection.22.7.2}%
\contentsline {paragraph}{Diagnosing Domain Shift}{1349}{section*.2853}%
\contentsline {paragraph}{Should We Try Multiple Backbones?}{1350}{section*.2854}%
\contentsline {paragraph}{Summary}{1350}{section*.2855}%
\contentsline {section}{\numberline {22.8}Fine-Tuning Self-Supervised Backbones}{1350}{section.22.8}%
\contentsline {subsection}{\numberline {22.8.1}Choosing an Adaptation Strategy: Data, Domain, and Cost}{1350}{subsection.22.8.1}%
\contentsline {subsubsection}{(1) Linear Probing and Lightweight Heads}{1350}{section*.2856}%
\contentsline {subsubsection}{(2) Parameter-Efficient Fine-Tuning (PEFT)}{1351}{section*.2857}%
\contentsline {paragraph}{Why Use PEFT?}{1351}{section*.2858}%
\contentsline {paragraph}{Common PEFT Strategies}{1351}{section*.2859}%
\contentsline {paragraph}{Practical Considerations}{1351}{section*.2861}%
\contentsline {subsubsection}{(3) Progressive Unfreezing and LP-FT}{1352}{section*.2862}%
\contentsline {paragraph}{Progressive Unfreezing}{1352}{section*.2863}%
\contentsline {paragraph}{Linear-Probe-Then-Fine-Tune (LP-FT)}{1352}{section*.2864}%
\contentsline {subsubsection}{(4) Full Fine-Tuning (FFT)}{1352}{section*.2865}%
\contentsline {subsubsection}{(5) Continued Self-Supervised Pretraining (C-SSL)}{1352}{section*.2866}%
\contentsline {paragraph}{Why Curation Beats Raw Scale}{1353}{section*.2867}%
\contentsline {paragraph}{Curation Workflow: Practical Steps}{1353}{section*.2868}%
\contentsline {paragraph}{Illustrative Case Study: Learning Artistic Style}{1354}{section*.2869}%
\contentsline {paragraph}{Challenge: Content-Biased Representations}{1354}{section*.2870}%
\contentsline {paragraph}{C-SSL Solution: Re-centering on Style}{1354}{section*.2871}%
\contentsline {paragraph}{Outcome: Efficient Style Recognition}{1354}{section*.2872}%
\contentsline {paragraph}{Summary: Fine-Tuning Strategies for Self-Supervised Models}{1355}{section*.2873}%
\contentsline {subsection}{\numberline {22.8.2}Linear Probing and MLP Head Adaptation}{1356}{subsection.22.8.2}%
\contentsline {paragraph}{Purpose and Motivation}{1356}{section*.2875}%
\contentsline {paragraph}{Application Procedure}{1356}{section*.2876}%
\contentsline {paragraph}{Hyperparameter Recommendations}{1357}{section*.2877}%
\contentsline {paragraph}{Practical Tips and Diagnostic Insights}{1357}{section*.2878}%
\contentsline {paragraph}{When to Escalate to LoRA or Other PEFT Techniques}{1357}{section*.2879}%
\contentsline {paragraph}{Best Practices}{1358}{section*.2880}%
\contentsline {paragraph}{Empirical Signal for Escalation}{1358}{section*.2881}%
\contentsline {subsection}{\numberline {22.8.3}Low-Rank Adaptation (LoRA) for Efficient Transfer}{1359}{subsection.22.8.3}%
\contentsline {paragraph}{Motivation and Intuition}{1359}{section*.2882}%
\contentsline {paragraph}{Mechanism}{1359}{section*.2883}%
\contentsline {paragraph}{Initialization and Forward Pass}{1359}{section*.2884}%
\contentsline {paragraph}{The Role of the Scaling Factor \( \alpha / r \)}{1359}{section*.2885}%
\contentsline {paragraph}{Tuning \( \alpha \) in Practice}{1359}{section*.2886}%
\contentsline {paragraph}{Empirical Findings and Low-Rank Capacity}{1360}{section*.2887}%
\contentsline {paragraph}{Inference-Time Behavior}{1360}{section*.2888}%
\contentsline {paragraph}{Advantages of LoRA}{1360}{section*.2889}%
\contentsline {paragraph}{Recommended Hyperparameters}{1360}{section*.2890}%
\contentsline {paragraph}{Example: PyTorch-style LoRA Setup}{1361}{section*.2891}%
\contentsline {paragraph}{When LoRA Is Not Enough}{1361}{section*.2892}%
\contentsline {paragraph}{Variants and Extensions}{1361}{section*.2893}%
\contentsline {paragraph}{Why They Matter}{1362}{section*.2894}%
\contentsline {paragraph}{Summary}{1362}{section*.2895}%
\contentsline {subsection}{\numberline {22.8.4}Progressive Unfreezing and LP-FT}{1363}{subsection.22.8.4}%
\contentsline {paragraph}{Motivation}{1363}{section*.2896}%
\contentsline {paragraph}{Progressive Unfreezing: Controlled Backbone Adaptation}{1363}{section*.2897}%
\contentsline {paragraph}{Example Schedule}{1363}{section*.2898}%
\contentsline {paragraph}{LP-FT: Linear Probing Followed by Full Fine-Tuning}{1363}{section*.2899}%
\contentsline {paragraph}{Best Use Cases}{1364}{section*.2900}%
\contentsline {paragraph}{Decision Guidelines}{1364}{section*.2901}%
\contentsline {paragraph}{Summary}{1364}{section*.2902}%
\contentsline {chapter}{\numberline {23}Lecture 23: 3D vision}{1365}{chapter.23}%
\contentsline {section}{\numberline {23.1}Introduction to 3D Perception from 2D Images}{1365}{section.23.1}%
\contentsline {subsection}{\numberline {23.1.1}Core Tasks in 3D Vision}{1366}{subsection.23.1.1}%
\contentsline {subsection}{\numberline {23.1.2}3D Representations}{1366}{subsection.23.1.2}%
\contentsline {section}{\numberline {23.2}Predicting Depth Maps from RGB Images}{1366}{section.23.2}%
\contentsline {paragraph}{Loss Function and the Limitations of Absolute Depth Regression}{1368}{section*.2905}%
\contentsline {paragraph}{Scale-Depth Ambiguity and the Need for Invariant Losses}{1368}{section*.2906}%
\contentsline {paragraph}{Scale-Invariant Log-Depth Loss}{1369}{section*.2908}%
\contentsline {paragraph}{Pairwise Interpretation}{1369}{section*.2909}%
\contentsline {paragraph}{Weighted Loss for Training}{1369}{section*.2910}%
\contentsline {paragraph}{Why a Single Global Scale Correction Suffices}{1370}{section*.2911}%
\contentsline {paragraph}{Scale and Shift-Invariant Losses in MiDaS and DPT}{1370}{section*.2912}%
\contentsline {paragraph}{Robust Trimmed MAE and Multi-Scale Gradient Losses}{1370}{section*.2913}%
\contentsline {paragraph}{Summary}{1371}{section*.2914}%
\contentsline {section}{\numberline {23.3}Surface Normals as a 3D Representation}{1372}{section.23.3}%
\contentsline {paragraph}{Visualizing Normals}{1372}{section*.2915}%
\contentsline {paragraph}{Learning Surface Normals}{1372}{section*.2917}%
\contentsline {paragraph}{Multi-Task Learning}{1373}{section*.2918}%
\contentsline {paragraph}{Limitations}{1373}{section*.2919}%
\contentsline {section}{\numberline {23.4}Voxel Grids}{1373}{section.23.4}%
\contentsline {paragraph}{Advantages}{1373}{section*.2921}%
\contentsline {paragraph}{Limitations}{1374}{section*.2922}%
\contentsline {paragraph}{3D Convolutional Processing}{1374}{section*.2923}%
\contentsline {paragraph}{Application Example: Image-to-Voxel Prediction}{1374}{section*.2925}%
\contentsline {subsection}{\numberline {23.4.1}Scaling Voxel Grids with Octrees}{1376}{subsection.23.4.1}%
\contentsline {paragraph}{Octrees: Intuition and Structure}{1376}{section*.2928}%
\contentsline {paragraph}{From Dense to Adaptive}{1376}{section*.2929}%
\contentsline {paragraph}{Octree Generating Networks (OGNs)}{1376}{section*.2930}%
\contentsline {paragraph}{Surface-Driven Efficiency}{1376}{section*.2931}%
\contentsline {paragraph}{Why and How Octrees Work}{1377}{section*.2933}%
\contentsline {paragraph}{Predicting Subdivision with Neural Networks}{1377}{section*.2934}%
\contentsline {paragraph}{Training with Supervised Supervision}{1378}{section*.2935}%
\contentsline {paragraph}{Why It Works}{1378}{section*.2936}%
\contentsline {paragraph}{Limitations and Motivation for Point-Based Methods}{1378}{section*.2937}%
\contentsline {section}{\numberline {23.5}Point Clouds}{1379}{section.23.5}%
\contentsline {paragraph}{Advantages}{1379}{section*.2939}%
\contentsline {paragraph}{Limitations}{1379}{section*.2940}%
\contentsline {paragraph}{Rendering}{1379}{section*.2941}%
\contentsline {paragraph}{Applications}{1380}{section*.2942}%
\contentsline {subsection}{\numberline {23.5.1}Point Cloud Generation from a Single Image}{1380}{subsection.23.5.1}%
\contentsline {paragraph}{Architecture Overview}{1380}{section*.2943}%
\contentsline {paragraph}{Architectural Motivation}{1381}{section*.2945}%
\contentsline {paragraph}{Loss Function: Chamfer Distance}{1382}{section*.2946}%
\contentsline {paragraph}{Intuition and Impact}{1383}{section*.2949}%
\contentsline {subsection}{\numberline {23.5.2}Learning on Point Clouds: PointNet and Variants}{1383}{subsection.23.5.2}%
\contentsline {paragraph}{Core Design: Set-Invariance via Shared MLP and Symmetric Pooling}{1383}{section*.2950}%
\contentsline {paragraph}{Pose Normalization via T-Net Modules}{1384}{section*.2952}%
\contentsline {paragraph}{Hierarchical Reasoning via Iterative Refinement}{1385}{section*.2953}%
\contentsline {paragraph}{Legacy and Evolution}{1385}{section*.2954}%
\contentsline {subsection}{\numberline {23.5.3}PointNet++: Hierarchical Feature Learning on Point Clouds}{1385}{subsection.23.5.3}%
\contentsline {paragraph}{Density-Adaptive Grouping and Robustness}{1386}{section*.2956}%
\contentsline {paragraph}{Feature Propagation for Dense Prediction}{1387}{section*.2958}%
\contentsline {paragraph}{Summary and Impact}{1388}{section*.2959}%
\contentsline {paragraph}{Extensions and Improvements}{1388}{section*.2960}%
\contentsline {paragraph}{Toward Structured Representations}{1388}{section*.2961}%
\contentsline {section}{\numberline {23.6}Triangle Meshes for 3D Shape Modeling}{1389}{section.23.6}%
\contentsline {paragraph}{Advantages of Triangle Meshes}{1389}{section*.2962}%
\contentsline {subsection}{\numberline {23.6.1}Pixel2Mesh: Predicting Triangle Meshes}{1390}{subsection.23.6.1}%
\contentsline {paragraph}{Pre-Pixel2Mesh Landscape}{1390}{section*.2964}%
\contentsline {paragraph}{Core Proposition}{1390}{section*.2965}%
\contentsline {paragraph}{Key Innovations}{1390}{section*.2966}%
\contentsline {paragraph}{High-Level Pipeline}{1391}{section*.2967}%
\contentsline {paragraph}{Graph-Based Feature Learning}{1392}{section*.2969}%
\contentsline {paragraph}{Predicting Vertex Positions via Graph Projection}{1393}{section*.2971}%
\contentsline {paragraph}{Edge-Based Graph Unpooling for Mesh Resolution Refinement}{1394}{section*.2972}%
\contentsline {paragraph}{Image-to-Mesh Feature Alignment}{1395}{section*.2974}%
\contentsline {paragraph}{Primary Objective: Chamfer Distance (Vertex-to-Vertex)}{1397}{section*.2978}%
\contentsline {paragraph}{Laplacian Smoothness Loss}{1397}{section*.2979}%
\contentsline {paragraph}{Edge Length Regularization}{1398}{section*.2980}%
\contentsline {paragraph}{Normal Consistency Loss}{1398}{section*.2981}%
\contentsline {paragraph}{Total Loss}{1398}{section*.2982}%
\contentsline {paragraph}{Limitations and Future Directions}{1399}{section*.2983}%
\contentsline {subsubsection}{Enrichment 23.6.1.1: Differentiable Surface Sampling in GEOMetrics}{1399}{section*.2984}%
\contentsline {paragraph}{Surface-to-Surface Comparison with Differentiable Sampling}{1399}{section*.2985}%
\contentsline {paragraph}{Point-to-Surface Loss and Fine-Tuning}{1399}{section*.2986}%
\contentsline {paragraph}{Differentiable Surface Sampling via Reparameterization}{1400}{section*.2987}%
\contentsline {paragraph}{Complete Loss Formulation in GEOMetrics}{1400}{section*.2989}%
\contentsline {paragraph}{Adaptive Mesh Refinement via Face Splitting}{1401}{section*.2990}%
\contentsline {paragraph}{Advantages Over Vertex-Based Supervision}{1401}{section*.2991}%
\contentsline {subsubsection}{Limitations of Pixel2Mesh and the Motivation for Successor Models}{1401}{section*.2992}%
\contentsline {paragraph}{Single-View Ambiguity and 2.5D Reconstruction}{1401}{section*.2993}%
\contentsline {paragraph}{Topological Rigidity and the Genus-0 Constraint}{1402}{section*.2994}%
\contentsline {paragraph}{Surface-Level Supervision and Over-Smoothing Limitations}{1402}{section*.2995}%
\contentsline {paragraph}{Domain Shift and Real-World Generalization}{1402}{section*.2996}%
\contentsline {paragraph}{Summary and Takeaways}{1403}{section*.2997}%
\contentsline {subsection}{\numberline {23.6.2}Mesh R-CNN: Topology-Aware Mesh Reconstruction from Real-World Images}{1404}{subsection.23.6.2}%
\contentsline {paragraph}{Motivation and Key Ideas}{1404}{section*.2998}%
\contentsline {paragraph}{Mask R-CNN as Backbone for 2D Instance Segmentation}{1405}{section*.3000}%
\contentsline {paragraph}{The Mesh Prediction Head: A Hybrid Voxel-to-Mesh Strategy}{1405}{section*.3001}%
\contentsline {subsubsection}{The Voxel Branch for Topological Flexibility}{1406}{section*.3003}%
\contentsline {paragraph}{Perspective-Aware Voxel Grid via Camera Frustum Alignment}{1406}{section*.3004}%
\contentsline {paragraph}{Summary and Advantages}{1409}{section*.3006}%
\contentsline {subsubsection}{Mesh Refinement Branch: Image-Guided Graph Deformation}{1410}{section*.3007}%
\contentsline {paragraph}{Fixed-Topology Refinement Pipeline}{1410}{section*.3008}%
\contentsline {paragraph}{Loss Functions}{1410}{section*.3009}%
\contentsline {paragraph}{Summary}{1411}{section*.3010}%
\contentsline {subsubsection}{Experiments and Ablations}{1412}{section*.3011}%
\contentsline {paragraph}{Datasets}{1412}{section*.3012}%
\contentsline {paragraph}{Evaluation Metrics}{1412}{section*.3013}%
\contentsline {paragraph}{Key Results on ShapeNet}{1412}{section*.3014}%
\contentsline {paragraph}{Key Results on Pix3D}{1413}{section*.3016}%
\contentsline {paragraph}{Amodal Completion}{1413}{section*.3018}%
\contentsline {paragraph}{Failure Modes}{1414}{section*.3020}%
\contentsline {paragraph}{Ablation Studies}{1414}{section*.3022}%
\contentsline {paragraph}{Conclusion}{1415}{section*.3023}%
\contentsline {section}{\numberline {23.7}Implicit Surface Representations}{1416}{section.23.7}%
\contentsline {paragraph}{From Discrete to Continuous Geometry}{1416}{section*.3024}%
\contentsline {paragraph}{Occupancy Fields vs.\ Signed Distance Functions}{1416}{section*.3025}%
\contentsline {paragraph}{Neural Implicit Models}{1416}{section*.3027}%
\contentsline {paragraph}{Why Surface Extraction Is Required}{1417}{section*.3028}%
\contentsline {subsection}{\numberline {23.7.1}Multi-Scale IsoSurface Extraction (MISE)}{1417}{subsection.23.7.1}%
\contentsline {subsection}{\numberline {23.7.2}Implicit Surface Advantages \& Limitations}{1418}{subsection.23.7.2}%
\contentsline {paragraph}{Advantages}{1418}{section*.3030}%
\contentsline {paragraph}{Limitations}{1418}{section*.3031}%
\contentsline {paragraph}{Relation to Octrees and Voxel Refinement}{1418}{section*.3032}%
\contentsline {section}{\numberline {23.8}General 3D Topics}{1419}{section.23.8}%
\contentsline {subsection}{\numberline {23.8.1}Shape Comparison Metrics}{1419}{subsection.23.8.1}%
\contentsline {paragraph}{Voxel IoU: Intuitive but Limited}{1419}{section*.3033}%
\contentsline {paragraph}{Chamfer Distance: Simple and Effective}{1419}{section*.3035}%
\contentsline {paragraph}{F1 Score: Thresholded Surface Accuracy}{1420}{section*.3037}%
\contentsline {paragraph}{Threshold Sensitivity}{1421}{section*.3039}%
\contentsline {subsection}{\numberline {23.8.2}Camera Coordinates: Canonical vs.\ View-Aligned}{1421}{subsection.23.8.2}%
\contentsline {paragraph}{Canonical Coordinates}{1421}{section*.3042}%
\contentsline {paragraph}{View Coordinates}{1422}{section*.3043}%
\contentsline {paragraph}{Conclusion}{1422}{section*.3046}%
\contentsline {subsection}{\numberline {23.8.3}3D Datasets}{1423}{subsection.23.8.3}%
\contentsline {paragraph}{Core Benchmarks for Single-View Reconstruction}{1423}{section*.3047}%
\contentsline {paragraph}{ShapeNet}{1423}{section*.3048}%
\contentsline {paragraph}{Pix3D}{1423}{section*.3049}%
\contentsline {paragraph}{Training Strategy}{1424}{section*.3051}%
\contentsline {paragraph}{CO3D: Common Objects in 3D}{1424}{section*.3052}%
\contentsline {paragraph}{Objaverse and Objaverse-XL}{1424}{section*.3053}%
\contentsline {paragraph}{ScanNet}{1424}{section*.3054}%
\contentsline {paragraph}{Supplementary Datasets}{1424}{section*.3055}%
\contentsline {paragraph}{Summary}{1424}{section*.3056}%
\contentsline {section}{\numberline {23.9}Neural Radiance Fields (NeRF)}{1425}{section.23.9}%
\contentsline {subsection}{\numberline {23.9.1}Problem Setup: Novel View Synthesis with Known Cameras}{1425}{subsection.23.9.1}%
\contentsline {paragraph}{What Is Novel View Synthesis?}{1425}{section*.3057}%
\contentsline {paragraph}{Limitations of Traditional Novel View Synthesis Pipelines}{1425}{section*.3059}%
\contentsline {subsection}{\numberline {23.9.2}A New Paradigm: Neural Fields}{1426}{subsection.23.9.2}%
\contentsline {paragraph}{Scene Representation}{1426}{section*.3060}%
\contentsline {paragraph}{Why Only Direction Matters}{1427}{section*.3061}%
\contentsline {paragraph}{Training Supervision: From Images to Rays}{1427}{section*.3062}%
\contentsline {subsection}{\numberline {23.9.3}Camera Parameters as a Prerequisite}{1427}{subsection.23.9.3}%
\contentsline {paragraph}{Recovering Camera Parameters via SfM}{1428}{section*.3063}%
\contentsline {subsection}{\numberline {23.9.4}Volume Rendering: From Rays to Pixels}{1429}{subsection.23.9.4}%
\contentsline {paragraph}{Discretizing the Rendering Equation: Stratified Sampling and Alpha Compositing}{1431}{section*.3065}%
\contentsline {paragraph}{From Pixel Color to Supervised 3D Sampling}{1433}{section*.3068}%
\contentsline {paragraph}{Hierarchical Sampling: Coarse-to-Fine Supervision and Loss}{1433}{section*.3069}%
\contentsline {paragraph}{Why This Works: Learning Geometry from Pixel Colors}{1434}{section*.3070}%
\contentsline {paragraph}{A Differentiable Rendering Engine for View Synthesis}{1434}{section*.3071}%
\contentsline {subsection}{\numberline {23.9.5}Practical Implementation Details}{1435}{subsection.23.9.5}%
\contentsline {subsubsection}{Positional Encoding for High-Frequency Detail}{1435}{section*.3072}%
\contentsline {paragraph}{Intuition Behind Positional Encoding}{1436}{section*.3073}%
\contentsline {subsubsection}{Network Architecture and Functional Mapping}{1437}{section*.3074}%
\contentsline {subsubsection}{Training vs. Inference: Pixel-Level Supervision and Scene Reconstruction}{1438}{section*.3076}%
\contentsline {paragraph}{Training Procedure}{1438}{section*.3077}%
\contentsline {paragraph}{Inference Procedure}{1439}{section*.3078}%
\contentsline {paragraph}{Why Pixel-Level Supervision Works}{1439}{section*.3079}%
\contentsline {paragraph}{Outlook}{1439}{section*.3080}%
\contentsline {subsection}{\numberline {23.9.6}Experiments and Ablation Studies}{1440}{subsection.23.9.6}%
\contentsline {subsubsection}{Quantitative and Qualitative Evaluation}{1440}{section*.3081}%
\contentsline {subsubsection}{Ablation Studies}{1441}{section*.3084}%
\contentsline {subsection}{\numberline {23.9.7}Limitations of the Original NeRF Architecture}{1443}{subsection.23.9.7}%
\contentsline {paragraph}{Summary}{1444}{section*.3093}%
\contentsline {section}{Enrichment 23.10: NeRF: Acceleration and Representation Revisions}{1445}{section*.3094}%
\contentsline {paragraph}{Explicit Voxel and Point Grid Representations}{1445}{section*.3095}%
\contentsline {subsection}{Enrichment 23.10.1: Plenoxels: Sparse Voxel Grids with Spherical Harmonics}{1445}{section*.3096}%
\contentsline {paragraph}{Inference and Volume Rendering.}{1446}{section*.3098}%
\contentsline {paragraph}{Training via Reconstruction Loss.}{1446}{section*.3099}%
\contentsline {paragraph}{Why Spherical Harmonics?}{1446}{section*.3100}%
\contentsline {paragraph}{Fast Convergence via Coarse-to-Fine Refinement.}{1447}{section*.3102}%
\contentsline {paragraph}{Core Insight and Tradeoffs.}{1447}{section*.3103}%
\contentsline {subsection}{Enrichment 23.10.2: DVGO: Direct Optimization on Dense Voxel Grids}{1448}{section*.3104}%
\contentsline {paragraph}{Rendering Pipeline}{1448}{section*.3106}%
\contentsline {paragraph}{Coarse-to-Fine Upsampling and Fine Detail Reconstruction}{1449}{section*.3107}%
\contentsline {paragraph}{ForegroundBackground Decomposition}{1449}{section*.3108}%
\contentsline {paragraph}{DVGOv2 Improvements}{1449}{section*.3109}%
\contentsline {paragraph}{Comparison to Plenoxels and NeRF}{1449}{section*.3110}%
\contentsline {paragraph}{Efficiency and Tradeoffs}{1450}{section*.3111}%
\contentsline {paragraph}{Performance Across Scene Types}{1450}{section*.3112}%
\contentsline {paragraph}{Final Remarks}{1451}{section*.3117}%
\contentsline {paragraph}{Hash-Based Feature Grid Representations}{1451}{section*.3118}%
\contentsline {subparagraph}{Core Tradeoff.}{1451}{subparagraph*.3119}%
\contentsline {subsection}{Enrichment 23.10.3: Instant-NGP: Multiscale Hash Encoding for Real-Time NeRF}{1452}{section*.3120}%
\contentsline {paragraph}{Multiscale Hash Encoding}{1452}{section*.3121}%
\contentsline {paragraph}{Motivation and Benefits}{1452}{section*.3122}%
\contentsline {paragraph}{Hash Function and Learning Dynamics}{1453}{section*.3123}%
\contentsline {paragraph}{Fast MLP Decoder and View Conditioning}{1454}{section*.3124}%
\contentsline {paragraph}{Occupancy Grid Acceleration}{1455}{section*.3126}%
\contentsline {paragraph}{Training and Inference}{1455}{section*.3127}%
\contentsline {paragraph}{Advantages and Limitations}{1456}{section*.3128}%
\contentsline {subsection}{Enrichment 23.10.4: Nerfacto: Merging Instant-NGP \& NR Pipelines}{1457}{section*.3130}%
\contentsline {paragraph}{Applications and Design Goals}{1458}{section*.3133}%
\contentsline {paragraph}{Core Insight and Tradeoffs}{1458}{section*.3134}%
\contentsline {subsection}{Enrichment 23.10.5: TensoRF: Tensor-Factorized Fields}{1459}{section*.3135}%
\contentsline {paragraph}{Overview}{1459}{section*.3136}%
\contentsline {paragraph}{Radiance Field Decomposition via Tensor Approximation}{1459}{section*.3137}%
\contentsline {paragraph}{Vector--Matrix (VM) Decomposition}{1459}{section*.3138}%
\contentsline {paragraph}{Interpolation: From Discrete Grids to Continuous Coordinates}{1460}{section*.3139}%
\contentsline {paragraph}{Differentiability and Training Efficiency}{1460}{section*.3140}%
\contentsline {paragraph}{Geometry: View-Independent Density Estimation}{1461}{section*.3141}%
\contentsline {paragraph}{Appearance: View-Dependent Color Prediction}{1461}{section*.3142}%
\contentsline {paragraph}{Comparison to CP Decomposition}{1461}{section*.3143}%
\contentsline {paragraph}{Summary}{1462}{section*.3144}%
\contentsline {paragraph}{Quantitative Comparison}{1462}{section*.3146}%
\contentsline {paragraph}{Qualitative Results}{1463}{section*.3148}%
\contentsline {subsection}{Enrichment 23.10.6: Mip-NeRF: Anti-Aliased Radiance Fields}{1463}{section*.3150}%
\contentsline {paragraph}{Motivation: scale ambiguity and aliasing}{1463}{section*.3151}%
\contentsline {paragraph}{From pixels to cones}{1464}{section*.3153}%
\contentsline {paragraph}{Why cones are divided into frustums}{1465}{section*.3154}%
\contentsline {paragraph}{From frustums to a pixels color}{1466}{section*.3156}%
\contentsline {paragraph}{From pixels to cones}{1466}{section*.3157}%
\contentsline {paragraph}{Approximating the footprint as a disk}{1466}{section*.3158}%
\contentsline {paragraph}{Frustum geometry and indicator function}{1467}{section*.3159}%
\contentsline {paragraph}{Expected positional encoding over a frustum}{1469}{section*.3160}%
\contentsline {paragraph}{Intuition}{1469}{section*.3161}%
\contentsline {paragraph}{Moment-matched Gaussian approximation}{1470}{section*.3162}%
\contentsline {subparagraph}{Frustum-centric coordinates}{1470}{subparagraph*.3163}%
\contentsline {paragraph}{Marginal depth distribution $p(t)$}{1471}{section*.3164}%
\contentsline {paragraph}{Mean depth $\mu _t$}{1472}{section*.3165}%
\contentsline {paragraph}{Stable reparameterization of $\mu _t$}{1472}{section*.3166}%
\contentsline {paragraph}{Axial variance $\sigma _t^2$}{1472}{section*.3167}%
\contentsline {paragraph}{Stable reparameterization of $\sigma _t^2$}{1473}{section*.3168}%
\contentsline {paragraph}{Radial (perpendicular) variance $\sigma _r^2$}{1473}{section*.3169}%
\contentsline {subparagraph}{Step 1: Conditional second moment at fixed depth}{1473}{subparagraph*.3170}%
\contentsline {subparagraph}{Step 2: Averaging over depth}{1474}{subparagraph*.3171}%
\contentsline {paragraph}{Moment-Matched Gaussian in World Space}{1475}{section*.3172}%
\contentsline {paragraph}{Rewriting positional encoding as Fourier features}{1477}{section*.3173}%
\contentsline {paragraph}{Fourier matrix formulation}{1477}{section*.3174}%
\contentsline {subsubsection}{Architecture \& Implementation Details}{1480}{section*.3175}%
\contentsline {paragraph}{Cone tracing and interval IPE features}{1480}{section*.3176}%
\contentsline {paragraph}{Single multiscale MLP with hierarchical sampling}{1480}{section*.3177}%
\contentsline {paragraph}{Training objective}{1481}{section*.3178}%
\contentsline {paragraph}{Smoothed importance sampling for the fine pass}{1481}{section*.3180}%
\contentsline {subparagraph}{Implementation Details.}{1482}{subparagraph*.3182}%
\contentsline {paragraph}{Benefits over NeRF}{1482}{section*.3183}%
\contentsline {subsubsection}{Results and Ablations}{1484}{section*.3185}%
\contentsline {paragraph}{Quantitative performance}{1484}{section*.3186}%
\contentsline {paragraph}{Qualitative performance}{1484}{section*.3188}%
\contentsline {paragraph}{Ablation insights (following table~\ref {tab:chapter23_mipnerf_results})}{1485}{section*.3190}%
\contentsline {paragraph}{Generalization to unseen scales}{1485}{section*.3191}%
\contentsline {subsubsection}{Limitations and Downsides}{1485}{section*.3192}%
\contentsline {subsubsection}{Notable Works Building on Mip-NeRF}{1485}{section*.3193}%
\contentsline {subsection}{Enrichment 23.10.7: NeuS: Neural Implicit Surfaces by Volume Rendering}{1486}{section*.3194}%
\contentsline {subsubsection}{Motivation}{1486}{section*.3195}%
\contentsline {subsubsection}{Method}{1487}{section*.3197}%
\contentsline {paragraph}{Scene representation and rendering objective}{1487}{section*.3198}%
\contentsline {paragraph}{From SDF to volume rendering}{1487}{section*.3199}%
\contentsline {paragraph}{Na\"{i}ve SDF$\to $density conversion and its bias}{1488}{section*.3200}%
\contentsline {paragraph}{A direct unbiased weighting that fails occlusion}{1489}{section*.3202}%
\contentsline {subsubsection}{Derivation of the NeuS Weight Function for the Single-Plane Case}{1490}{section*.3203}%
\contentsline {paragraph}{Step 1: Geometric Setup}{1490}{section*.3204}%
\contentsline {paragraph}{Step 2: Normal and Incidence Angle}{1490}{section*.3205}%
\contentsline {paragraph}{Step 3: SDF properties (geometry and intuition)}{1490}{section*.3206}%
\contentsline {paragraph}{Step 4: SDF Evolution Along the Ray}{1491}{section*.3207}%
\contentsline {paragraph}{Step 5: Local linearization near the surface}{1491}{section*.3208}%
\contentsline {paragraph}{Step 6: Direct unbiased weight construction}{1492}{section*.3209}%
\contentsline {paragraph}{Step 7: Derivative-of-CDF Identity}{1493}{section*.3210}%
\contentsline {paragraph}{Step 8: Interpretation as Soft Visibility}{1493}{section*.3211}%
\contentsline {paragraph}{Step 9: Embedding into Volume Rendering}{1493}{section*.3212}%
\contentsline {subsubsection}{Multi-Surface Generalization}{1494}{section*.3213}%
\contentsline {paragraph}{Enforcing Physical Validity}{1494}{section*.3214}%
\contentsline {paragraph}{Intuition}{1494}{section*.3215}%
\contentsline {paragraph}{Weights Construction Summary}{1494}{section*.3216}%
\contentsline {paragraph}{Discretization}{1495}{section*.3218}%
\contentsline {paragraph}{Training}{1496}{section*.3222}%
\contentsline {paragraph}{Training stabilization via geometry initialization}{1497}{section*.3227}%
\contentsline {subsubsection}{Experiments and Ablations}{1498}{section*.3228}%
\contentsline {paragraph}{Experimental setup}{1498}{section*.3229}%
\contentsline {paragraph}{Quantitative results}{1498}{section*.3230}%
\contentsline {paragraph}{Qualitative comparisons}{1499}{section*.3232}%
\contentsline {paragraph}{Ablation studies}{1500}{section*.3235}%
\contentsline {paragraph}{Limitations and Related Work}{1500}{section*.3238}%
\contentsline {subsection}{Enrichment 23.10.8: Point-NeRF: Point-based Neural Radiance Fields}{1502}{section*.3239}%
\contentsline {subsubsection}{Motivation}{1502}{section*.3240}%
\contentsline {subsubsection}{Method}{1502}{section*.3242}%
\contentsline {paragraph}{Overview}{1502}{section*.3243}%
\contentsline {paragraph}{Ray setup and sampling (as in NeRF)}{1504}{section*.3244}%
\contentsline {paragraph}{Local neighbor query (surface-aware shading)}{1504}{section*.3245}%
\contentsline {paragraph}{Local feature regression (Eq.~3)}{1505}{section*.3246}%
\contentsline {paragraph}{Radiance aggregation (Eqs.~45)}{1505}{section*.3247}%
\contentsline {paragraph}{Density aggregation (Eqs.~67)}{1505}{section*.3248}%
\contentsline {paragraph}{Putting the pieces together}{1505}{section*.3249}%
\contentsline {paragraph}{End-to-end optimization objective}{1505}{section*.3250}%
\contentsline {paragraph}{Topology edits during refinement (per paper)}{1506}{section*.3251}%
\contentsline {subsubsection}{Architecture and Implementation Details}{1506}{section*.3252}%
\contentsline {subsubsection}{Experiments and Ablations}{1507}{section*.3254}%
\contentsline {paragraph}{DTU}{1507}{section*.3256}%
\contentsline {paragraph}{NeRF-Synthetic}{1507}{section*.3258}%
\contentsline {paragraph}{Tanks\&Temples and ScanNet}{1508}{section*.3260}%
\contentsline {paragraph}{Initialization from external COLMAP clouds}{1508}{section*.3261}%
\contentsline {paragraph}{Feature-initialization ablation (papers Table 5)}{1509}{section*.3264}%
\contentsline {subsubsection}{Limitations}{1509}{section*.3265}%
\contentsline {paragraph}{Outlook toward 3D Gaussian Splatting.}{1510}{section*.3266}%
\contentsline {subsection}{Enrichment 23.10.9: 3D Gaussian Splatting: RT Radiance Field Rendering}{1511}{section*.3267}%
\contentsline {subsubsection}{Motivation and big picture}{1511}{section*.3268}%
\contentsline {paragraph}{Context and objective}{1511}{section*.3269}%
\contentsline {paragraph}{Key idea}{1511}{section*.3271}%
\contentsline {paragraph}{Core terms}{1511}{section*.3272}%
\contentsline {paragraph}{How 3DGS uses Gaussians}{1511}{section*.3273}%
\contentsline {paragraph}{From 3D to 2D footprints}{1512}{section*.3274}%
\contentsline {paragraph}{View-dependent color with spherical harmonics}{1513}{section*.3275}%
\contentsline {paragraph}{Rasterize and composite}{1513}{section*.3276}%
\contentsline {paragraph}{Why this design is effective}{1513}{section*.3277}%
\contentsline {subsubsection}{3D Gaussian Splatting stages}{1514}{section*.3278}%
\contentsline {subsubsection}{Representation and parameterization}{1515}{section*.3280}%
\contentsline {paragraph}{From meancovariance to a renderable primitive}{1515}{section*.3281}%
\contentsline {paragraph}{Initialization and the need for valid, optimizable covariances}{1515}{section*.3282}%
\contentsline {paragraph}{Choosing a geometry parameterization for valid optimization}{1516}{section*.3283}%
\contentsline {paragraph}{Intuition and practical knobs for $R$ and $S$}{1516}{section*.3284}%
\contentsline {paragraph}{Opacity as a direct parameter}{1516}{section*.3285}%
\contentsline {paragraph}{Appearance as a directional color field}{1518}{section*.3287}%
\contentsline {subsubsection}{Image formation and compositing}{1518}{section*.3288}%
\contentsline {paragraph}{What each splat provides}{1518}{section*.3289}%
\contentsline {paragraph}{Depth ordering for visibility}{1519}{section*.3290}%
\contentsline {paragraph}{Fronttoback transmittance (premultiplied form)}{1519}{section*.3291}%
\contentsline {paragraph}{Why \emph {fronttoback} matters now}{1519}{section*.3292}%
\contentsline {paragraph}{Practical footprint (compact support)}{1519}{section*.3293}%
\contentsline {subsubsection}{Adaptive densification}{1519}{section*.3294}%
\contentsline {paragraph}{Goal and signal}{1519}{section*.3295}%
\contentsline {paragraph}{Clone (add coverage)}{1520}{section*.3297}%
\contentsline {paragraph}{Split (resolve detail)}{1520}{section*.3298}%
\contentsline {paragraph}{Prune (stay compact)}{1520}{section*.3299}%
\contentsline {paragraph}{When and how often}{1521}{section*.3300}%
\contentsline {paragraph}{Effect on optimization}{1521}{section*.3301}%
\contentsline {subsubsection}{Training objective and schedules}{1521}{section*.3302}%
\contentsline {paragraph}{Photometric objective}{1521}{section*.3303}%
\contentsline {paragraph}{SSIM, D-SSIM, and the notion of structure}{1521}{section*.3304}%
\contentsline {paragraph}{Why not pure MSE/PSNR}{1521}{section*.3305}%
\contentsline {paragraph}{LPIPS vs.\ SSIM in training}{1522}{section*.3306}%
\contentsline {paragraph}{Update schedule and stability}{1522}{section*.3307}%
\contentsline {subsubsection}{Differentiable tilebased rasterizer}{1522}{section*.3308}%
\contentsline {paragraph}{Goal and inputs}{1522}{section*.3309}%
\contentsline {paragraph}{Stage A: cull and bound}{1522}{section*.3310}%
\contentsline {paragraph}{Stage B: tile binning}{1523}{section*.3311}%
\contentsline {paragraph}{Stage C: global sort by (tile, depth)}{1523}{section*.3312}%
\contentsline {paragraph}{Stage D: pertile blending (forward)}{1523}{section*.3313}%
\contentsline {paragraph}{Stage E: pertile gradients (backward)}{1523}{section*.3314}%
\contentsline {paragraph}{Numerical and implementation notes}{1524}{section*.3315}%
\contentsline {subsubsection}{Experiments and ablations}{1524}{section*.3316}%
\contentsline {paragraph}{Datasets and evaluation protocol}{1524}{section*.3317}%
\contentsline {paragraph}{Quantitative comparison (held-out views)}{1525}{section*.3319}%
\contentsline {paragraph}{Qualitative comparisons}{1526}{section*.3323}%
\contentsline {paragraph}{Training-time vs.\ quality}{1527}{section*.3325}%
\contentsline {paragraph}{Synthetic NeRF (Blender) PSNR}{1527}{section*.3327}%
\contentsline {paragraph}{Ablations}{1527}{section*.3329}%
\contentsline {paragraph}{Takeaways}{1529}{section*.3335}%
\contentsline {subsubsection}{Limitations and future work}{1530}{section*.3336}%
\contentsline {paragraph}{Observed failure modes}{1530}{section*.3337}%
\contentsline {paragraph}{Future work}{1531}{section*.3340}%
\contentsline {section}{Enrichment 23.11: NeRF: Real-World Robustness \& Sparse Supervision}{1532}{section*.3341}%
\contentsline {subsection}{Enrichment 23.11.1: BARF: Bundle-Adjusting Neural Radiance Fields}{1532}{section*.3342}%
\contentsline {subsubsection}{Motivation and problem setting}{1532}{section*.3343}%
\contentsline {paragraph}{Why this problem matters}{1532}{section*.3344}%
\contentsline {paragraph}{What makes joint optimization hard}{1532}{section*.3345}%
\contentsline {paragraph}{A lesson from classical image alignment}{1532}{section*.3346}%
\contentsline {paragraph}{The paper's idea and contribution}{1533}{section*.3347}%
\contentsline {subsubsection}{High level overview of BARF}{1534}{section*.3349}%
\contentsline {paragraph}{Joint objective}{1534}{section*.3350}%
\contentsline {paragraph}{Bandwidth scheduling via windowed positional encoding}{1534}{section*.3351}%
\contentsline {paragraph}{Roadmap}{1535}{section*.3352}%
\contentsline {subsubsection}{Method and derivations}{1535}{section*.3353}%
\contentsline {paragraph}{NeRF with differentiable volume rendering}{1535}{section*.3354}%
\contentsline {paragraph}{Joint objective (reference)}{1535}{section*.3355}%
\contentsline {paragraph}{From rigid motions to minimal pose updates}{1535}{section*.3356}%
\contentsline {paragraph}{Twist updates via the exponential map}{1536}{section*.3357}%
\contentsline {paragraph}{How a pose increment moves 3D samples (and affects colors)}{1537}{section*.3358}%
\contentsline {paragraph}{Ray-compositing gradients (decomposition)}{1538}{section*.3359}%
\contentsline {paragraph}{Why smooth inputs help pose gradients}{1538}{section*.3360}%
\contentsline {subsubsection}{Coarse-to-fine positional encoding}{1539}{section*.3361}%
\contentsline {paragraph}{Windowed positional encoding}{1539}{section*.3362}%
\contentsline {paragraph}{Architecture and implementation details}{1539}{section*.3363}%
\contentsline {paragraph}{Network and sampling}{1540}{section*.3364}%
\contentsline {paragraph}{Optimization}{1540}{section*.3365}%
\contentsline {subsubsection}{Experiments and ablations}{1540}{section*.3366}%
\contentsline {paragraph}{Datasets and evaluation protocol}{1540}{section*.3367}%
\contentsline {paragraph}{Planar image alignment}{1540}{section*.3368}%
\contentsline {paragraph}{Synthetic NeRF scenes}{1542}{section*.3372}%
\contentsline {paragraph}{Real LLFF scenes with unknown poses}{1543}{section*.3376}%
\contentsline {subsubsection}{Limitations and future work}{1544}{section*.3380}%
\contentsline {paragraph}{Limitations}{1544}{section*.3381}%
\contentsline {paragraph}{Follow-ups addressing BARFs limitations}{1544}{section*.3382}%
\contentsline {subsection}{Enrichment 23.11.2: NeRF-W: NeRF for Unconstrained Photo Collections}{1545}{section*.3383}%
\contentsline {subsubsection}{Motivation}{1545}{section*.3384}%
\contentsline {paragraph}{NeRF-W at a glance}{1545}{section*.3386}%
\contentsline {subsubsection}{Method: Formulation and Derivation}{1547}{section*.3388}%
\contentsline {paragraph}{Rendering operator}{1547}{section*.3389}%
\contentsline {paragraph}{What \(f\) is in practice and how NeRF-W instantiates \(R\)}{1547}{section*.3390}%
\contentsline {subsubsection}{Architecture \& Implementation Details}{1549}{section*.3392}%
\contentsline {subsubsection}{Experiments and Ablations}{1550}{section*.3394}%
\contentsline {subsubsection}{Limitations and Future Work}{1551}{section*.3400}%
\contentsline {subsection}{Enrichment 23.11.3: IBRNet: Learning Multi-View Image-Based Rendering}{1553}{section*.3402}%
\contentsline {subsubsection}{Motivation}{1553}{section*.3403}%
\contentsline {subsubsection}{Method: image-conditioned RGB--$\sigma $ prediction and NeRF-style rendering}{1553}{section*.3404}%
\contentsline {paragraph}{Setup and notation.}{1553}{section*.3405}%
\contentsline {paragraph}{Pipeline overview (stages).}{1554}{section*.3406}%
\contentsline {paragraph}{Why fast and zero-shot.}{1556}{section*.3409}%
\contentsline {subsubsection}{Architecture \& Implementation Details}{1557}{section*.3410}%
\contentsline {paragraph}{High-level architecture.}{1557}{section*.3411}%
\contentsline {paragraph}{Network size and compute.}{1558}{section*.3413}%
\contentsline {subsubsection}{Experiments \& Ablations}{1558}{section*.3415}%
\contentsline {paragraph}{Datasets and evaluation protocol.}{1558}{section*.3416}%
\contentsline {paragraph}{Baselines.}{1558}{section*.3417}%
\contentsline {paragraph}{Quantitative comparison (synthetic datasets).}{1558}{section*.3418}%
\contentsline {paragraph}{Quantitative comparison (Real Forward-Facing).}{1559}{section*.3420}%
\contentsline {paragraph}{Ablation studies.}{1559}{section*.3422}%
\contentsline {paragraph}{Sensitivity to source-view density.}{1559}{section*.3424}%
\contentsline {paragraph}{Qualitative comparisons.}{1561}{section*.3426}%
\contentsline {paragraph}{With/without ray transformer.}{1561}{section*.3428}%
\contentsline {paragraph}{Geometry and additional results.}{1562}{section*.3430}%
\contentsline {subsubsection}{Limitations and Future Directions}{1563}{section*.3433}%
\contentsline {paragraph}{Limitations.}{1563}{section*.3434}%
\contentsline {paragraph}{Concurrent and prior generalizable radiance-field methods.}{1564}{section*.3435}%
\contentsline {paragraph}{Subsequent follow-ups building on IBRNets goals.}{1564}{section*.3436}%
\contentsline {subsection}{Enrichment 23.11.4: pixelNeRF: Neural Radiance Fields from One or Few Images}{1565}{section*.3437}%
\contentsline {subsubsection}{Motivation}{1565}{section*.3438}%
\contentsline {subsubsection}{Method}{1565}{section*.3440}%
\contentsline {paragraph}{Radiance field prediction}{1565}{section*.3441}%
\contentsline {paragraph}{Feature encoding and alignment}{1565}{section*.3442}%
\contentsline {paragraph}{Feature-conditioned NeRF}{1566}{section*.3443}%
\contentsline {paragraph}{Volume rendering loss}{1566}{section*.3444}%
\contentsline {paragraph}{Why view-space conditioning}{1566}{section*.3445}%
\contentsline {paragraph}{Multi-view extension}{1566}{section*.3446}%
\contentsline {paragraph}{Intuition and significance}{1567}{section*.3447}%
\contentsline {subsubsection}{Architecture and Implementation}{1567}{section*.3449}%
\contentsline {subsubsection}{Experiments and Ablations}{1568}{section*.3451}%
\contentsline {paragraph}{Category-specific single-view reconstruction}{1568}{section*.3452}%
\contentsline {paragraph}{Category-specific two-view reconstruction}{1568}{section*.3454}%
\contentsline {paragraph}{Ablation on local features and view directions}{1569}{section*.3457}%
\contentsline {paragraph}{Category-agnostic single-view reconstruction}{1570}{section*.3459}%
\contentsline {paragraph}{Unseen categories and multi-object scenes}{1570}{section*.3462}%
\contentsline {paragraph}{Real images: Stanford Cars and DTU MVS}{1571}{section*.3466}%
\contentsline {subsubsection}{Limitations and Future Work}{1572}{section*.3470}%
\contentsline {paragraph}{Limitations}{1572}{section*.3471}%
\contentsline {paragraph}{Future work and influence}{1573}{section*.3472}%
\contentsline {section}{Enrichment 23.12: NeRF: Unbounded, Dynamic, Large-Scale Scenes}{1574}{section*.3473}%
\contentsline {subsection}{Enrichment 23.12.1: Block-NeRF: Scalable Large Scene Neural View Synthesis}{1574}{section*.3474}%
\contentsline {subsubsection}{Motivation}{1574}{section*.3475}%
\contentsline {subsubsection}{Method}{1575}{section*.3477}%
\contentsline {paragraph}{High-level overview}{1575}{section*.3478}%
\contentsline {paragraph}{Block partitioning and structure}{1575}{section*.3479}%
\contentsline {paragraph}{Architectural design choices}{1576}{section*.3480}%
\contentsline {paragraph}{How $f_v$ integrates into the pipeline}{1577}{section*.3482}%
\contentsline {paragraph}{Why $f_v$ is essential}{1578}{section*.3484}%
\contentsline {paragraph}{Compositing across blocks}{1578}{section*.3485}%
\contentsline {paragraph}{Appearance control and cross-block alignment}{1578}{section*.3486}%
\contentsline {paragraph}{Why this design works}{1579}{section*.3490}%
\contentsline {subsubsection}{Experiments and Ablations}{1579}{section*.3491}%
\contentsline {paragraph}{Ablations on Alamo Square}{1579}{section*.3492}%
\contentsline {paragraph}{Block granularity on Mission Bay}{1580}{section*.3495}%
\contentsline {subsubsection}{Limitations and Future Work}{1581}{section*.3497}%
\contentsline {subsection}{Enrichment 23.12.2: Mip-NeRF 360: Unbounded Anti-Aliased NeRF}{1581}{section*.3498}%
\contentsline {subsubsection}{Motivation}{1581}{section*.3499}%
\contentsline {paragraph}{Challenges in unbounded 360 scenes}{1581}{section*.3500}%
\contentsline {paragraph}{MipNeRF360 solutions to unbounded scenes challenges}{1582}{section*.3501}%
\contentsline {subsubsection}{Method}{1583}{section*.3503}%
\contentsline {paragraph}{Preliminaries: mip-NeRF}{1583}{section*.3504}%
\contentsline {paragraph}{Scene and ray parameterization}{1584}{section*.3505}%
\contentsline {paragraph}{Coarse-to-fine online distillation}{1587}{section*.3507}%
\contentsline {paragraph}{Regularization for interval-based models}{1591}{section*.3512}%
\contentsline {paragraph}{Optimization and training recipe}{1592}{section*.3515}%
\contentsline {subsubsection}{Results and Ablations}{1593}{section*.3516}%
\contentsline {paragraph}{Quantitative evaluation}{1593}{section*.3517}%
\contentsline {paragraph}{Qualitative comparison}{1593}{section*.3518}%
\contentsline {paragraph}{Ablations}{1593}{section*.3519}%
\contentsline {paragraph}{Generalization across datasets}{1593}{section*.3520}%
\contentsline {subsubsection}{Limitations}{1593}{section*.3521}%
\contentsline {paragraph}{Outlook}{1593}{section*.3522}%
\contentsline {subsection}{Enrichment 23.12.3: D-NeRF: Neural Radiance Fields for Dynamic Scenes}{1594}{section*.3523}%
\contentsline {subsubsection}{Motivation}{1594}{section*.3524}%
\contentsline {subsubsection}{Problem Setup}{1594}{section*.3526}%
\contentsline {paragraph}{Challenges of direct spatio-temporal regression.}{1595}{section*.3528}%
\contentsline {subsubsection}{Method}{1595}{section*.3529}%
\contentsline {paragraph}{Canonical network}{1596}{section*.3531}%
\contentsline {paragraph}{Deformation network}{1596}{section*.3532}%
\contentsline {paragraph}{Volume rendering with deformations}{1596}{section*.3533}%
\contentsline {paragraph}{Learning objective}{1597}{section*.3534}%
\contentsline {subsubsection}{Architecture and Implementation Details}{1597}{section*.3535}%
\contentsline {paragraph}{Network design}{1597}{section*.3536}%
\contentsline {paragraph}{Positional encoding}{1598}{section*.3537}%
\contentsline {paragraph}{Canonical reference frame}{1598}{section*.3538}%
\contentsline {paragraph}{Curriculum strategy}{1598}{section*.3539}%
\contentsline {paragraph}{Optimization details}{1598}{section*.3540}%
\contentsline {subsubsection}{Experiments and Ablations}{1599}{section*.3541}%
\contentsline {paragraph}{Learned canonical scene and displacement fields}{1599}{section*.3542}%
\contentsline {paragraph}{Shading and appearance consistency}{1600}{section*.3544}%
\contentsline {paragraph}{Quantitative and qualitative comparisons}{1600}{section*.3546}%
\contentsline {paragraph}{Time and view conditioning}{1602}{section*.3549}%
\contentsline {paragraph}{Limitations}{1603}{section*.3551}%
\contentsline {paragraph}{Future directions}{1603}{section*.3552}%
\contentsline {subsection}{Enrichment 23.12.4: Nerfies: Deformable Neural Radiance Fields}{1604}{section*.3553}%
\contentsline {subsubsection}{Motivation}{1604}{section*.3554}%
\contentsline {subsubsection}{Method}{1604}{section*.3556}%
\contentsline {paragraph}{Motivation relative to D-NeRF}{1604}{section*.3557}%
\contentsline {paragraph}{Canonical radiance field}{1605}{section*.3558}%
\contentsline {paragraph}{Observation-to-canonical deformation}{1605}{section*.3559}%
\contentsline {paragraph}{Why dense \(\mathrm {SE}(3)\) fields}{1606}{section*.3561}%
\contentsline {paragraph}{Observation vs.\ canonical frames}{1607}{section*.3563}%
\contentsline {paragraph}{Elastic regularization (why, what, how)}{1608}{section*.3565}%
\contentsline {paragraph}{Background regularization}{1608}{section*.3567}%
\contentsline {paragraph}{Coarse-to-fine optimization (why, what, how)}{1609}{section*.3568}%
\contentsline {paragraph}{Latent-code interpolation}{1610}{section*.3570}%
\contentsline {subsubsection}{Architecture and implementation details}{1610}{section*.3572}%
\contentsline {subsubsection}{Experiments and Ablations}{1611}{section*.3573}%
\contentsline {subsubsection}{Limitations and Future Work}{1614}{section*.3580}%
\contentsline {section}{Enrichment 23.13: NeRF: Editing, Controllability \& Semantic Manipulation}{1615}{section*.3582}%
\contentsline {subsection}{Enrichment 23.13.1: Language Embedded Radiance Fields (LERF)}{1615}{section*.3583}%
\contentsline {paragraph}{Motivation}{1615}{section*.3584}%
\contentsline {paragraph}{High-level overview}{1616}{section*.3586}%
\contentsline {paragraph}{How it works at a glance}{1616}{section*.3587}%
\contentsline {paragraph}{Why this suits open-vocabulary 3D queries}{1616}{section*.3588}%
\contentsline {subsubsection}{Method}{1617}{section*.3589}%
\contentsline {paragraph}{Language field definition}{1617}{section*.3590}%
\contentsline {paragraph}{Supervision via CLIP pyramid}{1617}{section*.3591}%
\contentsline {paragraph}{Volumetric language rendering}{1619}{section*.3592}%
\contentsline {paragraph}{Regularization with DINO}{1619}{section*.3594}%
\contentsline {paragraph}{Inference: scale selection and heatmap rendering}{1620}{section*.3595}%
\contentsline {subsubsection}{Results and Ablations}{1623}{section*.3596}%
\contentsline {paragraph}{Qualitative results}{1623}{section*.3597}%
\contentsline {paragraph}{2D CLIP vs.\ volumetric LERF}{1624}{section*.3599}%
\contentsline {paragraph}{Localization against LSeg (3D) and OWL-ViT}{1625}{section*.3601}%
\contentsline {paragraph}{3D existence: precision--recall}{1626}{section*.3605}%
\contentsline {paragraph}{Ablation studies}{1627}{section*.3607}%
\contentsline {paragraph}{Failure cases and ambiguities}{1628}{section*.3609}%
\contentsline {paragraph}{Prompt sensitivity (prompt tuning)}{1629}{section*.3612}%
\contentsline {paragraph}{CLIP bag-of-words behavior}{1629}{section*.3614}%
\contentsline {paragraph}{Efficiency analysis}{1630}{section*.3616}%
\contentsline {paragraph}{Summary of findings}{1630}{section*.3617}%
\contentsline {subsection}{Enrichment 23.13.2: InstructNeRF2NeRF: Editing 3D Scenes with Instructions}{1631}{section*.3618}%
\contentsline {subsubsection}{Motivation}{1631}{section*.3619}%
\contentsline {subsubsection}{Background on InstructPix2Pix}{1631}{section*.3621}%
\contentsline {paragraph}{Core idea of InstructPix2Pix}{1631}{section*.3622}%
\contentsline {paragraph}{Crucial controls: dual guidance scales}{1632}{section*.3623}%
\contentsline {paragraph}{How InstructPix2Pix is trained and why Prompt-to-Prompt alone is insufficient}{1632}{section*.3625}%
\contentsline {paragraph}{What IP2P brings beyond text-to-image diffusion and Prompt-to-Prompt}{1633}{section*.3627}%
\contentsline {paragraph}{Connection to InstructNeRF2NeRF}{1633}{section*.3628}%
\contentsline {subsubsection}{Method}{1634}{section*.3629}%
\contentsline {paragraph}{Editing a single dataset image}{1634}{section*.3630}%
\contentsline {paragraph}{Iterative Dataset Update}{1634}{section*.3631}%
\contentsline {paragraph}{Training objective and relation to SDS}{1634}{section*.3632}%
\contentsline {subsubsection}{Architecture and Implementation Details}{1636}{section*.3635}%
\contentsline {subsubsection}{Experiments and Ablation}{1636}{section*.3637}%
\contentsline {paragraph}{Baselines and iterative update importance}{1637}{section*.3640}%
\contentsline {paragraph}{Quantitative evaluation}{1638}{section*.3642}%
\contentsline {subsubsection}{Limitations and Future Work}{1639}{section*.3644}%
\contentsline {paragraph}{Observed failure modes}{1639}{section*.3646}%
\contentsline {paragraph}{Future directions}{1639}{section*.3647}%
\contentsline {section}{Enrichment 23.14: NeRF: Generative \& Cross-Modal Foundations}{1640}{section*.3648}%
\contentsline {subsection}{Enrichment 23.14.1: DreamFusion: Text-to-3D with Score Distillation Sampling}{1640}{section*.3649}%
\contentsline {subsubsection}{Motivation}{1640}{section*.3650}%
\contentsline {paragraph}{Why many valid 2D views need not imply valid 3D}{1640}{section*.3651}%
\contentsline {paragraph}{How DreamFusion closes the loophole}{1641}{section*.3652}%
\contentsline {subsubsection}{Method}{1642}{section*.3654}%
\contentsline {paragraph}{High-level optimization loop}{1642}{section*.3655}%
\contentsline {paragraph}{Foregroundbackground separation}{1642}{section*.3656}%
\contentsline {paragraph}{From density to orientation: making shape visible}{1642}{section*.3657}%
\contentsline {paragraph}{Render modes: complementary recipes for supervision}{1643}{section*.3658}%
\contentsline {paragraph}{Score Distillation Sampling: turning a 2D prior into 3D updates}{1644}{section*.3659}%
\contentsline {paragraph}{Albedo (unlit): appearance-centric updates}{1645}{section*.3660}%
\contentsline {paragraph}{Shaded color (lit): coupled appearance\(\to \)geometry updates}{1645}{section*.3661}%
\contentsline {paragraph}{Textureless shaded (lit, no texture): pure geometry updates}{1645}{section*.3662}%
\contentsline {paragraph}{View sampling and view-aware prompting}{1646}{section*.3663}%
\contentsline {paragraph}{Putting the loop together}{1646}{section*.3665}%
\contentsline {subsubsection}{Implementation Details}{1646}{section*.3666}%
\contentsline {paragraph}{Frozen diffusion prior}{1646}{section*.3667}%
\contentsline {paragraph}{Foregroundbackground composition}{1646}{section*.3668}%
\contentsline {subsubsection}{Experiments and Ablation}{1647}{section*.3669}%
\contentsline {paragraph}{Qualitative gallery and comparisons}{1647}{section*.3670}%
\contentsline {paragraph}{Captionimage coherence via CLIP retrieval}{1648}{section*.3673}%
\contentsline {paragraph}{Ablations: what unlocks geometry?}{1650}{section*.3675}%
\contentsline {paragraph}{Iterative refinement and compositional editing}{1651}{section*.3677}%
\contentsline {subsubsection}{Limitations and Future Work}{1652}{section*.3679}%
\contentsline {subsection}{Enrichment 23.14.2: Latent-NeRF for Shape-Guided 3D Generation}{1653}{section*.3680}%
\contentsline {subsubsection}{Motivation}{1653}{section*.3681}%
\contentsline {paragraph}{From DreamFusion to Latent-NeRF}{1653}{section*.3682}%
\contentsline {paragraph}{Why latent supervision}{1653}{section*.3684}%
\contentsline {subsubsection}{Method}{1654}{section*.3685}%
\contentsline {paragraph}{Overview and connection to DreamFusion}{1654}{section*.3686}%
\contentsline {paragraph}{NeRF in Stable Diffusion latent space}{1655}{section*.3688}%
\contentsline {paragraph}{SDS in latent space}{1655}{section*.3689}%
\contentsline {paragraph}{Step-by-step training loop}{1655}{section*.3690}%
\contentsline {paragraph}{Rendering and latent image formation}{1656}{section*.3691}%
\contentsline {paragraph}{Diffusion guidance in latent space (SDS)}{1656}{section*.3692}%
\contentsline {paragraph}{Classifier-free guidance (CFG) in latent SDS}{1656}{section*.3693}%
\contentsline {paragraph}{Sparsity / anti-fog regularization}{1656}{section*.3694}%
\contentsline {paragraph}{Total objective (normal/text-only mode)}{1657}{section*.3695}%
\contentsline {paragraph}{Sketch-Shape guidance: Rationale and Mechanism}{1658}{section*.3696}%
\contentsline {paragraph}{Effect of the leniency parameter $\sigma _S$}{1659}{section*.3698}%
\contentsline {paragraph}{Latent-Paint for explicit meshes}{1660}{section*.3700}%
\contentsline {paragraph}{RGB refinement with a learnable linear adapter}{1661}{section*.3702}%
\contentsline {subsubsection}{Architecture and Implementation Details}{1662}{section*.3704}%
\contentsline {paragraph}{Backbones}{1662}{section*.3705}%
\contentsline {paragraph}{Schedules and regularizers}{1662}{section*.3706}%
\contentsline {subsubsection}{Experiments and Ablations}{1662}{section*.3707}%
\contentsline {paragraph}{Text-only generation and multi-view consistency}{1662}{section*.3708}%
\contentsline {paragraph}{Qualitative comparison}{1663}{section*.3710}%
\contentsline {paragraph}{RGB refinement improvements}{1664}{section*.3712}%
\contentsline {paragraph}{Controllability via Sketch-Shape}{1665}{section*.3714}%
\contentsline {paragraph}{More Sketch-Shape results}{1665}{section*.3716}%
\contentsline {paragraph}{Latent-Paint on generic meshes}{1667}{section*.3719}%
\contentsline {paragraph}{Texturing comparison on a common mesh}{1668}{section*.3722}%
\contentsline {paragraph}{Personalization via Textual Inversion}{1669}{section*.3724}%
\contentsline {subsubsection}{Limitations and Future Work}{1669}{section*.3726}%
\contentsline {paragraph}{View ambiguity and Janus artifacts}{1669}{section*.3727}%
\contentsline {paragraph}{Controllability and future directions}{1669}{section*.3729}%
\contentsline {chapter}{\numberline {24}Lecture 24: Videos (Video Understanding)}{1670}{chapter.24}%
\contentsline {section}{\numberline {24.1}Introduction to Video Understanding}{1670}{section.24.1}%
\contentsline {subsection}{\numberline {24.1.1}From Images to Videos}{1670}{subsection.24.1.1}%
\contentsline {subsection}{\numberline {24.1.2}Challenges of Video Data and Clip-Based Training}{1671}{subsection.24.1.2}%
\contentsline {section}{\numberline {24.2}Video Classification as a Canonical Task}{1672}{section.24.2}%
\contentsline {subsection}{\numberline {24.2.1}Single-Frame Baseline}{1673}{subsection.24.2.1}%
\contentsline {subsection}{\numberline {24.2.2}Late Fusion}{1673}{subsection.24.2.2}%
\contentsline {subsection}{\numberline {24.2.3}Early Fusion}{1675}{subsection.24.2.3}%
\contentsline {subsection}{\numberline {24.2.4}3D CNNs: Slow Fusion}{1676}{subsection.24.2.4}%
\contentsline {subsection}{\numberline {24.2.5}2D vs 3D Convolutions}{1677}{subsection.24.2.5}%
\contentsline {paragraph}{Clarifying Input Channels vs Temporal Dimension}{1679}{section*.3742}%
\contentsline {subsection}{\numberline {24.2.6}Sports-1M Dataset and Baseline Comparisons}{1679}{subsection.24.2.6}%
\contentsline {subsection}{\numberline {24.2.7}Baseline Model Performance}{1680}{subsection.24.2.7}%
\contentsline {subsection}{\numberline {24.2.8}C3D: The VGG of 3D CNNs}{1680}{subsection.24.2.8}%
\contentsline {paragraph}{Computation cost}{1681}{section*.3746}%
\contentsline {paragraph}{Summary}{1682}{section*.3748}%
\contentsline {section}{\numberline {24.3}Separating Time and Space in 3D Processing}{1682}{section.24.3}%
\contentsline {subsection}{\numberline {24.3.1}Measuring Motion: Optical Flow}{1682}{subsection.24.3.1}%
\contentsline {paragraph}{Dense vs.\ sparse flow}{1682}{section*.3749}%
\contentsline {paragraph}{Why this helps}{1682}{section*.3750}%
\contentsline {subsection}{\numberline {24.3.2}Two-Stream Networks}{1683}{subsection.24.3.2}%
\contentsline {subsubsection}{Evaluation on UCF-101}{1683}{section*.3753}%
\contentsline {section}{\numberline {24.4}Modeling Long-Term Temporal Structure}{1684}{section.24.4}%
\contentsline {subsection}{\numberline {24.4.1}CNN Features + Recurrent Networks}{1684}{subsection.24.4.1}%
\contentsline {paragraph}{From vector RNNs to recurrent convs}{1685}{section*.3756}%
\contentsline {paragraph}{Gated variants and practicality}{1685}{section*.3758}%
\contentsline {subsection}{\numberline {24.4.2}Spatio-Temporal Self-Attention and the Nonlocal Block}{1686}{subsection.24.4.2}%
\contentsline {paragraph}{Definition}{1686}{section*.3760}%
\contentsline {paragraph}{Initialization and integration}{1687}{section*.3762}%
\contentsline {paragraph}{Takeaway}{1687}{section*.3764}%
\contentsline {subsection}{\numberline {24.4.3}Inflating 2D Networks to 3D (I3D)}{1687}{subsection.24.4.3}%
\contentsline {paragraph}{Inflating the architecture}{1687}{section*.3765}%
\contentsline {paragraph}{Inflating the weights: replication and normalization}{1688}{section*.3767}%
\contentsline {paragraph}{Why divide by $K_t$}{1688}{section*.3768}%
\contentsline {paragraph}{Why inflation is a natural fit}{1689}{section*.3770}%
\contentsline {paragraph}{Evidence on Kinetics-400}{1689}{section*.3771}%
\contentsline {paragraph}{Takeaway}{1690}{section*.3773}%
\contentsline {subsection}{\numberline {24.4.4}Transformers for Video Understanding}{1690}{subsection.24.4.4}%
\contentsline {paragraph}{What is a token in video}{1690}{section*.3774}%
\contentsline {paragraph}{Attention over space and time}{1690}{section*.3775}%
\contentsline {paragraph}{ViViT in depth: tokenization, factorization, computation, and findings}{1691}{section*.3776}%
\contentsline {subparagraph}{Tokenization}{1691}{subparagraph*.3777}%
\contentsline {subparagraph}{What ``spatial'' and ``temporal'' transformers mean}{1691}{subparagraph*.3778}%
\contentsline {subparagraph}{Architectural variants and compute}{1691}{subparagraph*.3779}%
\contentsline {subparagraph}{Positioning relative to contemporaries}{1691}{subparagraph*.3780}%
\contentsline {subparagraph}{Practical guidance and empirical takeaways from ViViT}{1692}{subparagraph*.3781}%
\contentsline {paragraph}{Why transformers for video}{1692}{section*.3783}%
\contentsline {subsection}{\numberline {24.4.5}Visualizing and Localizing Actions}{1693}{subsection.24.4.5}%
\contentsline {subsubsection}{Visualizing Video Models}{1693}{section*.3784}%
\contentsline {paragraph}{Qualitative examples}{1693}{section*.3786}%
\contentsline {subsubsection}{Temporal Action Localization}{1694}{section*.3789}%
\contentsline {subsubsection}{Spatio-Temporal Action Detection}{1695}{section*.3791}%
\contentsline {subsubsection}{Ego4D: Large-Scale Egocentric Video}{1696}{section*.3793}%
\contentsline {section}{Enrichment 24.5: Vision--Language Alignment Precursors}{1697}{section*.3795}%
\contentsline {subsection}{Enrichment 24.5.1: SigLIP: Contrastive Alignment with Sigmoid Loss}{1697}{section*.3796}%
\contentsline {paragraph}{From CLIP to SigLIP (Intuition First)}{1697}{section*.3797}%
\contentsline {paragraph}{Algorithmic Formulation and Intuition}{1697}{section*.3798}%
\contentsline {paragraph}{CLIP vs.\ SigLIPwhy it matters}{1698}{section*.3799}%
\contentsline {paragraph}{Efficient Implementation}{1698}{section*.3800}%
\contentsline {paragraph}{Empirical Comparison to CLIP: What Improves in Practice}{1699}{section*.3802}%
\contentsline {paragraph}{Impact, Limitations, and Legacy}{1699}{section*.3803}%
\contentsline {subsection}{Enrichment 24.5.2: BLIP: Bootstrapping Language--Image Pretraining}{1700}{section*.3804}%
\contentsline {paragraph}{High-Level Idea}{1700}{section*.3805}%
\contentsline {paragraph}{BLIPs Two-Part Strategy}{1700}{section*.3806}%
\contentsline {subsubsection}{Method}{1701}{section*.3807}%
\contentsline {paragraph}{Unified Architecture with Three Functional Modes}{1701}{section*.3808}%
\contentsline {paragraph}{Why Causal vs.\ Bidirectional Attention?}{1702}{section*.3810}%
\contentsline {paragraph}{Objectives in Mathematical Form.}{1702}{section*.3811}%
\contentsline {paragraph}{Training Framework: End-to-End Chronology (CapFilt $\rightarrow $ Final BLIP)}{1703}{section*.3812}%
\contentsline {paragraph}{Downstream Usage}{1704}{section*.3814}%
\contentsline {subsubsection}{Experiments and Ablations}{1705}{section*.3816}%
\contentsline {paragraph}{CapFilt Effectiveness}{1705}{section*.3817}%
\contentsline {paragraph}{Ablations}{1705}{section*.3818}%
\contentsline {subsubsection}{Limitations and Future Work}{1705}{section*.3819}%
\contentsline {paragraph}{Observed Constraints}{1705}{section*.3820}%
\contentsline {paragraph}{Toward BLIP-2}{1705}{section*.3821}%
\contentsline {subsection}{Enrichment 24.5.3: BLIP-2: Bridging Vision Encoders and LLMs via Q-Former}{1706}{section*.3822}%
\contentsline {paragraph}{High-Level Idea}{1706}{section*.3823}%
\contentsline {subsubsection}{Method: A Small Q-Former Bridging Two Frozen Experts}{1707}{section*.3825}%
\contentsline {paragraph}{Stage~1: Vision--Language representation with a frozen image encoder}{1707}{section*.3826}%
\contentsline {paragraph}{Stage~2: Vision-to-language generation with a frozen LLM}{1707}{section*.3827}%
\contentsline {paragraph}{Two-Stage Curriculum: What Trains When and Why}{1708}{section*.3830}%
\contentsline {paragraph}{Objectives (concise math + intuition)}{1708}{section*.3831}%
\contentsline {paragraph}{How the pieces fit during training}{1709}{section*.3832}%
\contentsline {subsubsection}{Experiments \& Ablations (Concise)}{1711}{section*.3837}%
\contentsline {subsubsection}{Limitations \& Future Work}{1712}{section*.3838}%
\contentsline {subsection}{Enrichment 24.5.4: SigLIP~2: Multilingual \& Dense Vision--Language Encoding}{1713}{section*.3839}%
\contentsline {paragraph}{High-Level Overview}{1713}{section*.3840}%
\contentsline {subsubsection}{Foundational Reminder: How Sigmoid Loss (SigLIP) Works}{1714}{section*.3842}%
\contentsline {paragraph}{Compact formulation (per step)}{1714}{section*.3843}%
\contentsline {subsubsection}{Method: A Staged Curriculum that Teaches \emph {Where}, \emph {Detail}, and \emph {Robustness}}{1715}{section*.3844}%
\contentsline {paragraph}{Stage layout (flow first).}{1715}{section*.3845}%
\contentsline {paragraph}{Decoder for captioning and grounding (LocCa-style)}{1715}{section*.3846}%
\contentsline {paragraph}{Late self-distillation and masked prediction (SILC/TIPS-style)}{1715}{section*.3847}%
\contentsline {paragraph}{Resolution and aspect-ratio adaptation}{1716}{section*.3848}%
\contentsline {paragraph}{Curation-focused fine-tuning for small models}{1716}{section*.3849}%
\contentsline {paragraph}{Multilingual training mix}{1716}{section*.3850}%
\contentsline {paragraph}{Why these additions work (unifying intuition)}{1716}{section*.3851}%
\contentsline {subsubsection}{Experiments and Ablations (Concise)}{1717}{section*.3852}%
\contentsline {paragraph}{What we learn (vs.\ SigLIP/BLIP/BLIP-2) \& which to choose}{1717}{section*.3853}%
\contentsline {section}{Enrichment 24.6: Self-Supervised Video Pretraining for VLLMs}{1718}{section*.3854}%
\contentsline {subsection}{Enrichment 24.6.1: VideoMAE: Masked Autoencoders for Video SSL}{1718}{section*.3855}%
\contentsline {paragraph}{Scope and positioning}{1718}{section*.3856}%
\contentsline {subsubsection}{Motivation}{1718}{section*.3857}%
\contentsline {paragraph}{Why masked autoencoding for video}{1718}{section*.3858}%
\contentsline {subsubsection}{Method}{1720}{section*.3861}%
\contentsline {paragraph}{Preliminaries and notation}{1720}{section*.3862}%
\contentsline {paragraph}{Tube masking with extremely high ratios}{1720}{section*.3863}%
\contentsline {paragraph}{Asymmetric encoderdecoder}{1720}{section*.3864}%
\contentsline {paragraph}{Reconstruction objective on masked cubes}{1720}{section*.3865}%
\contentsline {paragraph}{Design choices justified}{1720}{section*.3866}%
\contentsline {paragraph}{Algorithmic flow (pseudo code)}{1721}{section*.3867}%
\contentsline {subsubsection}{Architecture, Training, and Datasets}{1721}{section*.3868}%
\contentsline {paragraph}{Backbone and attention}{1721}{section*.3869}%
\contentsline {paragraph}{Training setup}{1721}{section*.3870}%
\contentsline {paragraph}{Datasets used in experiments and ablations}{1722}{section*.3871}%
\contentsline {subsubsection}{Experiments}{1722}{section*.3872}%
\contentsline {subsubsection}{Ablations}{1723}{section*.3875}%
\contentsline {subsubsection}{Limitations and Future Work}{1728}{section*.3888}%
\contentsline {paragraph}{Observed constraints}{1728}{section*.3889}%
\contentsline {paragraph}{Promising directions}{1728}{section*.3890}%
\contentsline {paragraph}{Summary}{1728}{section*.3891}%
\contentsline {subsection}{Enrichment 24.6.2: VideoMAEv2: Dual Masking at Scale}{1729}{section*.3892}%
\contentsline {paragraph}{Scope and positioning}{1729}{section*.3893}%
\contentsline {subsubsection}{Motivation}{1729}{section*.3894}%
\contentsline {paragraph}{Why mask the decoder too}{1729}{section*.3895}%
\contentsline {subsubsection}{Method}{1730}{section*.3897}%
\contentsline {paragraph}{Preliminaries and notation}{1730}{section*.3898}%
\contentsline {paragraph}{Dual masking: decoder-side selection}{1730}{section*.3899}%
\contentsline {paragraph}{Loss on encoder-invisible \& decoder-visible cubes}{1730}{section*.3900}%
\contentsline {paragraph}{Running-cell masking for decoder supervision}{1730}{section*.3901}%
\contentsline {paragraph}{Algorithmic flow (pseudo code)}{1732}{section*.3902}%
\contentsline {subsubsection}{Architecture and Implementation Details}{1732}{section*.3903}%
\contentsline {paragraph}{Backbones and decoder}{1732}{section*.3904}%
\contentsline {paragraph}{Masking specifics}{1732}{section*.3905}%
\contentsline {paragraph}{Data and schedules}{1732}{section*.3906}%
\contentsline {subsubsection}{Experiments and Ablation}{1733}{section*.3907}%
\contentsline {paragraph}{Decoder masking strategies}{1733}{section*.3908}%
\contentsline {paragraph}{Efficiency of dual masking}{1734}{section*.3910}%
\contentsline {paragraph}{Kinetics-400}{1734}{section*.3912}%
\contentsline {paragraph}{Something-Something V2}{1734}{section*.3914}%
\contentsline {paragraph}{Progressive pre-training (K710)}{1734}{section*.3916}%
\contentsline {paragraph}{State of the art (selected benchmarks)}{1735}{section*.3918}%
\contentsline {subsubsection}{Limitations and Future Work}{1737}{section*.3927}%
\contentsline {paragraph}{Observed constraints}{1737}{section*.3928}%
\contentsline {paragraph}{Future directions (path toward distillation and beyond)}{1737}{section*.3929}%
\contentsline {subsection}{Enrichment 24.6.3: MVD: Masked Video Distillation}{1738}{section*.3930}%
\contentsline {paragraph}{Scope and positioning}{1738}{section*.3931}%
\contentsline {subsubsection}{Motivation}{1738}{section*.3933}%
\contentsline {paragraph}{Limits of pixel-level MVM (VideoMAE).}{1738}{section*.3934}%
\contentsline {paragraph}{From pixels to features: cleaner targets and inductive bias.}{1739}{section*.3935}%
\contentsline {paragraph}{Why two teachers: complementary spatial and temporal cues.}{1739}{section*.3936}%
\contentsline {subsubsection}{Method}{1740}{section*.3938}%
\contentsline {paragraph}{Preliminaries: masked feature modeling}{1740}{section*.3939}%
\contentsline {paragraph}{Teacher targets}{1740}{section*.3940}%
\contentsline {paragraph}{Spatialtemporal co-teaching}{1740}{section*.3941}%
\contentsline {paragraph}{Algorithmic view}{1741}{section*.3942}%
\contentsline {paragraph}{Intuition and failure-mode mitigation}{1741}{section*.3943}%
\contentsline {subsubsection}{Architecture and implementation details}{1741}{section*.3944}%
\contentsline {paragraph}{Backbone and tokenization}{1741}{section*.3945}%
\contentsline {paragraph}{Attention}{1741}{section*.3946}%
\contentsline {paragraph}{Decoders and objectives}{1742}{section*.3947}%
\contentsline {paragraph}{Pretraining schedules}{1742}{section*.3948}%
\contentsline {subsubsection}{Experiments and ablation}{1742}{section*.3949}%
\contentsline {paragraph}{Main results and efficiency}{1742}{section*.3950}%
\contentsline {paragraph}{Gains over VideoMAE across scales}{1742}{section*.3952}%
\contentsline {paragraph}{Co-teaching vs single teacher}{1743}{section*.3954}%
\contentsline {paragraph}{Gains over VideoMAE across scales}{1743}{section*.3956}%
\contentsline {paragraph}{End-to-end comparisons}{1743}{section*.3958}%
\contentsline {paragraph}{Co-teaching vs single teacher}{1743}{section*.3959}%
\contentsline {paragraph}{Transfer: UCF101 and HMDB51}{1747}{section*.3964}%
\contentsline {paragraph}{Training time}{1747}{section*.3966}%
\contentsline {paragraph}{Ablations: pixels during distillation}{1747}{section*.3968}%
\contentsline {paragraph}{Bootstrapped teachers and IN1K-initialized students}{1748}{section*.3970}%
\contentsline {paragraph}{Ablations: masked reconstruction vs. per-token feature distillation}{1748}{section*.3972}%
\contentsline {subsubsection}{Limitations and future directions}{1748}{section*.3974}%
\contentsline {paragraph}{Observed constraints}{1748}{section*.3975}%
\contentsline {paragraph}{Future work}{1748}{section*.3976}%
\contentsline {paragraph}{Summary}{1748}{section*.3977}%
\contentsline {section}{Enrichment 24.7: Instruction-Tuned VLLM Precursors}{1749}{section*.3978}%
\contentsline {subsection}{Enrichment 24.7.1: InstructBLIP: Instruction-Tuned Multimodal Alignment}{1749}{section*.3979}%
\contentsline {paragraph}{Motivation and Positioning}{1749}{section*.3980}%
\contentsline {paragraph}{High-Level Idea}{1749}{section*.3981}%
\contentsline {paragraph}{How It Works (Mechanism)}{1750}{section*.3983}%
\contentsline {paragraph}{Why Instruction Tuning Helps (Intuition)}{1750}{section*.3984}%
\contentsline {paragraph}{Data \& Formatting: From Multi-Task to Instruction-Tuning}{1752}{section*.3986}%
\contentsline {paragraph}{Ablations: What Matters}{1753}{section*.3989}%
\contentsline {paragraph}{Instruction Tuning vs.\ Multi-Task Training}{1753}{section*.3991}%
\contentsline {paragraph}{Downstream Fine-Tuning}{1754}{section*.3993}%
\contentsline {paragraph}{Takeaways (Sharper Reading of the Evidence)}{1754}{section*.3995}%
\contentsline {paragraph}{Limitations and Future Work}{1754}{section*.3996}%
\contentsline {subsection}{Enrichment 24.7.2: LLaVA: Large Language and Vision Assistant}{1756}{section*.3997}%
\contentsline {paragraph}{High-Level Idea}{1756}{section*.3998}%
\contentsline {paragraph}{Architecture}{1756}{section*.3999}%
\contentsline {paragraph}{Why freeze vision but (partly) train the LLM?}{1756}{section*.4001}%
\contentsline {paragraph}{Data Pipeline: Visual Instruction Tuning}{1757}{section*.4002}%
\contentsline {paragraph}{Why It Works (vs.\ BLIP/BLIP-2)}{1758}{section*.4005}%
\contentsline {paragraph}{Instruction Following and Reasoning (Qualitative)}{1759}{section*.4006}%
\contentsline {paragraph}{Benchmarks: LLaVA-Bench, COCO ablations, In-the-Wild, ScienceQA}{1760}{section*.4009}%
\contentsline {paragraph}{What the Ablations Say (and How This Differs from BLIP-2)}{1761}{section*.4014}%
\contentsline {paragraph}{Positioning vs.\ BLIP/BLIP-2}{1762}{section*.4015}%
\contentsline {paragraph}{Limitations and Next Steps (segue to LLaVA-NeXT / OneVision)}{1762}{section*.4016}%
\contentsline {subsection}{Enrichment 24.7.3: LLaVA-OneVision: Unified Multimodal Transfer}{1763}{section*.4017}%
\contentsline {paragraph}{From LLaVA to OneVision: Motivation \& Goal}{1763}{section*.4018}%
\contentsline {paragraph}{High-Level Idea}{1763}{section*.4019}%
\contentsline {paragraph}{Architecture Overview (What changes vs.\ LLaVA)}{1763}{section*.4022}%
\contentsline {paragraph}{Training Curriculum (How capabilities are built)}{1767}{section*.4025}%
\contentsline {paragraph}{Data Collections (for SFT)}{1768}{section*.4027}%
\contentsline {paragraph}{What the Experiments Show}{1770}{section*.4031}%
\contentsline {paragraph}{Ablation Themes (High-Level)}{1771}{section*.4032}%
\contentsline {paragraph}{Qualitative Capabilities (Selected Examples)}{1771}{section*.4033}%
\contentsline {paragraph}{Current Constraints}{1779}{section*.4044}%
\contentsline {paragraph}{Directions and the Move to OV-1.5}{1779}{section*.4045}%
\contentsline {paragraph}{Future Directions}{1779}{section*.4046}%
\contentsline {section}{Enrichment 24.8: Large-Scale Video Foundation Models}{1780}{section*.4047}%
\contentsline {subsection}{Enrichment 24.8.1: InternVideo: General Video Backbones}{1780}{section*.4048}%
\contentsline {paragraph}{Scope and positioning}{1780}{section*.4049}%
\contentsline {subsubsection}{Motivation}{1781}{section*.4051}%
\contentsline {subsubsection}{Preliminaries: UniFormer and UniFormerV2}{1782}{section*.4053}%
\contentsline {paragraph}{Why these preliminaries matter here.}{1782}{section*.4054}%
\contentsline {paragraph}{UniFormer (CVPR22)~\blx@tocontentsinit {0}\cite {li2022_uniformer}}{1782}{section*.4055}%
\contentsline {paragraph}{Dynamic Position Embedding (DPE): learnable relative spatiotemporal bias.}{1783}{section*.4056}%
\contentsline {paragraph}{MHRA (general form): one template that adapts with depth.}{1783}{section*.4057}%
\contentsline {paragraph}{MHRALocal (shallow stages): cheap neighborhood mixing.}{1783}{section*.4058}%
\contentsline {paragraph}{MHRAGlobal (deep stages): full spacetime self-attention when it counts.}{1784}{section*.4059}%
\contentsline {paragraph}{FFN: per-token refinement.}{1784}{section*.4060}%
\contentsline {paragraph}{Putting it together: why this staging works for video.}{1784}{section*.4061}%
\contentsline {paragraph}{Concrete cue.}{1785}{section*.4062}%
\contentsline {paragraph}{From UniFormer (V1): what we gained, and what still needs fixing.}{1785}{section*.4064}%
\contentsline {paragraph}{UniFormerV2 (ICCV22)~\blx@tocontentsinit {0}\cite {li2022_uniformerv2}: arming image ViTs for video, with full formulation and clear integration.}{1786}{section*.4065}%
\contentsline {paragraph}{Bridging to the method: why UniFormer/UniFormerV2 set the stage.}{1787}{section*.4066}%
\contentsline {subsubsection}{Method}{1789}{section*.4069}%
\contentsline {paragraph}{High-level overview}{1789}{section*.4070}%
\contentsline {paragraph}{Notation}{1789}{section*.4072}%
\contentsline {paragraph}{1) Generative path --- Masked Video Encoder (MVE)}{1789}{section*.4073}%
\contentsline {paragraph}{2) Discriminative path --- Multimodal Video Encoder (MMVE)}{1790}{section*.4074}%
\contentsline {paragraph}{Captioning loss: concise mechanics and intuition}{1790}{section*.4075}%
\contentsline {paragraph}{3) Coordination  Cross-Model Attention (CMA).}{1791}{section*.4076}%
\contentsline {paragraph}{4) Prediction heads and supervised adaptation}{1792}{section*.4079}%
\contentsline {paragraph}{5) End-to-end flow (one pass)}{1792}{section*.4080}%
\contentsline {subsubsection}{Architecture and Implementation Details}{1793}{section*.4081}%
\contentsline {paragraph}{Backbone choices}{1793}{section*.4082}%
\contentsline {paragraph}{Tokenization and shapes}{1793}{section*.4083}%
\contentsline {paragraph}{UniFormerV2 block order in MMVE}{1793}{section*.4084}%
\contentsline {paragraph}{Cross-Model Attention (CMA) placement}{1793}{section*.4085}%
\contentsline {paragraph}{Training schedule}{1793}{section*.4086}%
\contentsline {subsubsection}{Experiments and Ablations}{1794}{section*.4087}%
\contentsline {paragraph}{Bottom-line summary across tasks}{1794}{section*.4088}%
\contentsline {paragraph}{Key ablations and what they imply}{1794}{section*.4089}%
\contentsline {paragraph}{Representative takeaways}{1794}{section*.4090}%
\contentsline {subsubsection}{Limitations and Follow-up Works}{1795}{section*.4091}%
\contentsline {paragraph}{Current limitations}{1795}{section*.4092}%
\contentsline {paragraph}{Buildup toward InternVideoV2}{1795}{section*.4093}%
\contentsline {paragraph}{Takeaway}{1795}{section*.4094}%
\contentsline {subsection}{Enrichment 24.8.2: OmniVL: One Model for ImageVideoLanguage}{1796}{section*.4095}%
\contentsline {paragraph}{Scope and positioning}{1796}{section*.4096}%
\contentsline {subsubsection}{Motivation}{1796}{section*.4098}%
\contentsline {paragraph}{Fragmentation problem}{1796}{section*.4099}%
\contentsline {paragraph}{Design hypothesis}{1797}{section*.4100}%
\contentsline {subsubsection}{Method: high-level flow and detailed breakdown}{1797}{section*.4101}%
\contentsline {paragraph}{High-level overview}{1797}{section*.4102}%
\contentsline {paragraph}{Data format and prompting}{1797}{section*.4103}%
\contentsline {paragraph}{Step-by-step data flow}{1797}{section*.4104}%
\contentsline {paragraph}{Pretraining objectives}{1800}{section*.4105}%
\contentsline {paragraph}{Decoupled joint pretraining}{1801}{section*.4111}%
\contentsline {paragraph}{Task routing and inference}{1801}{section*.4112}%
\contentsline {subsubsection}{Architecture \& implementation details}{1802}{section*.4113}%
\contentsline {paragraph}{Backbone design at a glance}{1802}{section*.4114}%
\contentsline {paragraph}{Unified visual encoder: shapes, blocks, and schedules}{1802}{section*.4115}%
\contentsline {paragraph}{Text encoder: tokenization and heads}{1802}{section*.4116}%
\contentsline {paragraph}{Decoders: attention masks, fusion, and outputs}{1802}{section*.4117}%
\contentsline {paragraph}{Projection heads, similarities, and temperatures}{1802}{section*.4118}%
\contentsline {paragraph}{Queues, EMA encoders, and retrieval runtime}{1803}{section*.4119}%
\contentsline {paragraph}{Data, batching, and curriculum specifics}{1803}{section*.4120}%
\contentsline {paragraph}{Optimization and training stability}{1803}{section*.4121}%
\contentsline {subsubsection}{Experiments and ablations}{1803}{section*.4122}%
\contentsline {paragraph}{Result highlights}{1803}{section*.4123}%
\contentsline {paragraph}{What the curriculum buys}{1803}{section*.4125}%
\contentsline {paragraph}{What UniVLC adds}{1804}{section*.4126}%
\contentsline {paragraph}{Retrieval pipeline ablation}{1804}{section*.4127}%
\contentsline {paragraph}{Takeaways}{1804}{section*.4128}%
\contentsline {subsubsection}{Limitations and future directions}{1805}{section*.4130}%
\contentsline {paragraph}{Token budget and long-form video}{1805}{section*.4131}%
\contentsline {paragraph}{Prompting sensitivity and text targets}{1805}{section*.4132}%
\contentsline {paragraph}{Fine-grained localization and grounding}{1805}{section*.4133}%
\contentsline {paragraph}{Data curation and balance}{1805}{section*.4134}%
\contentsline {paragraph}{From unified encoders to instruction following}{1805}{section*.4135}%
\contentsline {paragraph}{Scaling outlook}{1805}{section*.4136}%
\contentsline {subsection}{Enrichment 24.8.3: InternVideo2: Generative + Discriminative Pretraining}{1806}{section*.4137}%
\contentsline {paragraph}{Scope and positioning}{1806}{section*.4138}%
\contentsline {subsubsection}{Motivation}{1806}{section*.4139}%
\contentsline {paragraph}{Problem framing}{1806}{section*.4140}%
\contentsline {paragraph}{Why InternVideo (V1) is not enough}{1806}{section*.4141}%
\contentsline {paragraph}{Design principles for a scalable VFM}{1806}{section*.4142}%
\contentsline {paragraph}{What success looks like}{1806}{section*.4143}%
\contentsline {paragraph}{Key idea}{1807}{section*.4144}%
\contentsline {subsubsection}{Method: objectives, training stages, and intuition}{1807}{section*.4146}%
\contentsline {paragraph}{Notation}{1807}{section*.4147}%
\contentsline {paragraph}{Stage 1: Video-only masked autoencoding}{1807}{section*.4148}%
\contentsline {paragraph}{Stage 2: Multimodal contrastive alignment (imagetext and videotext)}{1808}{section*.4149}%
\contentsline {paragraph}{Stage 3: Video-centric instruction tuning with a Q-Former bridge}{1808}{section*.4150}%
\contentsline {paragraph}{Notation (simplified)}{1808}{section*.4151}%
\contentsline {paragraph}{One Q-Former layer}{1809}{section*.4152}%
\contentsline {paragraph}{Total training recipe}{1810}{section*.4154}%
\contentsline {paragraph}{Practical schedule and hyperparameters}{1810}{section*.4155}%
\contentsline {subsubsection}{Experiments}{1811}{section*.4156}%
\contentsline {paragraph}{Experimental setup and scaling}{1811}{section*.4157}%
\contentsline {paragraph}{Efficiency and compute}{1811}{section*.4158}%
\contentsline {paragraph}{Headline results}{1811}{section*.4159}%
\contentsline {paragraph}{Action understanding (``what'' and ``when'')}{1811}{section*.4160}%
\contentsline {paragraph}{Videolanguage retrieval (the ``search engine'')}{1812}{section*.4163}%
\contentsline {paragraph}{Temporal grounding (finding the exact moment)}{1813}{section*.4166}%
\contentsline {paragraph}{Video dialogue and reasoning (the ``conversational AI'')}{1813}{section*.4168}%
\contentsline {paragraph}{Scaling validation}{1813}{section*.4170}%
\contentsline {subsubsection}{Ablations}{1814}{section*.4171}%
\contentsline {paragraph}{What is varied}{1814}{section*.4172}%
\contentsline {paragraph}{Takeaway}{1814}{section*.4174}%
\contentsline {paragraph}{Takeaway}{1814}{section*.4176}%
\contentsline {paragraph}{Takeaway}{1814}{section*.4178}%
\contentsline {paragraph}{Takeaway}{1815}{section*.4180}%
\contentsline {paragraph}{Takeaway}{1815}{section*.4182}%
\contentsline {paragraph}{Qualitative comparisons}{1816}{section*.4183}%
\contentsline {subsubsection}{Limitations}{1819}{section*.4190}%
\contentsline {subsubsection}{Future work and toward InternVideo2.5}{1819}{section*.4191}%
\contentsline {paragraph}{Empirical findings and position vs.\ prior work}{1820}{section*.4193}%
\contentsline {paragraph}{What changes, how it is implemented, and why it helps}{1820}{section*.4195}%
\contentsline {paragraph}{Intuition and expected impact}{1821}{section*.4196}%
\contentsline {section}{Enrichment 24.9: Video--Language Large Models}{1822}{section*.4197}%
\contentsline {subsection}{Enrichment 24.9.1: LaViLa: Learning Video Representations from LLMs}{1822}{section*.4198}%
\contentsline {paragraph}{Scope and positioning}{1823}{section*.4200}%
\contentsline {paragraph}{Motivation / Problem framing}{1823}{section*.4201}%
\contentsline {subsubsection}{Method: narration-supervised contrastive learning}{1823}{section*.4202}%
\contentsline {paragraph}{Highlevel flow}{1823}{section*.4203}%
\contentsline {paragraph}{Why NARRATOR and REPHRASER}{1823}{section*.4204}%
\contentsline {paragraph}{Setup and notation}{1823}{section*.4205}%
\contentsline {paragraph}{Contrastive objective on mixed sources}{1823}{section*.4206}%
\contentsline {paragraph}{Offline generators and their training}{1824}{section*.4207}%
\contentsline {paragraph}{Visual conditioning mechanism}{1824}{section*.4208}%
\contentsline {paragraph}{Batching and curriculum in practice}{1824}{section*.4209}%
\contentsline {paragraph}{Why this design works}{1824}{section*.4210}%
\contentsline {paragraph}{High-level training loop}{1825}{section*.4211}%
\contentsline {subsubsection}{Architecture and implementation details}{1825}{section*.4212}%
\contentsline {paragraph}{Dual-encoder backbone}{1825}{section*.4213}%
\contentsline {paragraph}{NARRATOR design and training}{1826}{section*.4214}%
\contentsline {paragraph}{Pretraining schedule and input processing}{1826}{section*.4217}%
\contentsline {subsubsection}{Experiments}{1827}{section*.4218}%
\contentsline {paragraph}{Benchmarks and protocols}{1827}{section*.4219}%
\contentsline {paragraph}{Headline results}{1827}{section*.4221}%
\contentsline {paragraph}{Summary of main experiments and ablations}{1828}{section*.4223}%
\contentsline {paragraph}{Ablations}{1829}{section*.4224}%
\contentsline {subsubsection}{Limitations and future directions}{1830}{section*.4225}%
\contentsline {paragraph}{Observed constraints}{1830}{section*.4226}%
\contentsline {paragraph}{Future work}{1830}{section*.4227}%
\contentsline {paragraph}{Bridge to instruction-tuned videoLLMs}{1830}{section*.4228}%
\contentsline {subsection}{Enrichment 24.9.2: Video\mbox {-}LLaMA 1: Instruction\mbox {-}Tuned Video LLM}{1831}{section*.4229}%
\contentsline {subsubsection}{Motivation}{1831}{section*.4230}%
\contentsline {paragraph}{Why audio--visual LLMs?}{1831}{section*.4231}%
\contentsline {paragraph}{Design goal}{1831}{section*.4232}%
\contentsline {subsubsection}{Method: Multi-Branch Cross-Modal Training with Q-Formers}{1832}{section*.4234}%
\contentsline {paragraph}{Problem setup and notation}{1832}{section*.4235}%
\contentsline {paragraph}{VisionLanguage branch}{1832}{section*.4236}%
\contentsline {paragraph}{AudioLanguage branch}{1832}{section*.4237}%
\contentsline {paragraph}{Training curriculum}{1832}{section*.4238}%
\contentsline {paragraph}{How images and videos share one encoder}{1833}{section*.4239}%
\contentsline {paragraph}{Positional encoding (vision \& audio)}{1833}{section*.4240}%
\contentsline {paragraph}{Learning objective (unified view)}{1833}{section*.4241}%
\contentsline {paragraph}{Intuition and roles}{1834}{section*.4242}%
\contentsline {subsubsection}{Architecture \& Implementation Details}{1834}{section*.4243}%
\contentsline {paragraph}{Backbones and frozen parts}{1834}{section*.4244}%
\contentsline {paragraph}{Video tokens}{1834}{section*.4245}%
\contentsline {paragraph}{Audio tokens}{1834}{section*.4246}%
\contentsline {paragraph}{GEMINI additions (intuitive recap)}{1834}{section*.4247}%
\contentsline {subsubsection}{Experiments and Ablations}{1835}{section*.4249}%
\contentsline {paragraph}{Qualitative capabilities}{1835}{section*.4250}%
\contentsline {paragraph}{Tasks and metrics (at a glance)}{1835}{section*.4252}%
\contentsline {paragraph}{Training stages and ablations}{1836}{section*.4253}%
\contentsline {paragraph}{Positioning w.r.t.\ LaViLa and related LMMs}{1836}{section*.4254}%
\contentsline {subsubsection}{Limitations and Future Directions}{1836}{section*.4255}%
\contentsline {paragraph}{Observed constraints}{1836}{section*.4256}%
\contentsline {paragraph}{Future work}{1836}{section*.4257}%
\contentsline {subsection}{Enrichment 24.9.3: Video\mbox {-}LLaMA 2: Enhanced Understanding, Efficiency}{1837}{section*.4258}%
\contentsline {paragraph}{Overview and motivation}{1837}{section*.4259}%
\contentsline {subsubsection}{Method}{1837}{section*.4261}%
\contentsline {paragraph}{Modality branches (concise)}{1837}{section*.4262}%
\contentsline {paragraph}{STC connector: step\mbox {-}by\mbox {-}step mechanics and intuition}{1838}{section*.4263}%
\contentsline {paragraph}{Why STC instead of a plain 3D CNN or a Q\mbox {-}Former?}{1839}{section*.4265}%
\contentsline {paragraph}{Implementation of STC in Python (from the paper)}{1840}{section*.4266}%
\contentsline {paragraph}{Training signal and integration}{1840}{section*.4267}%
\contentsline {paragraph}{Key changes vs.\ V1 (what changed and why)}{1840}{section*.4268}%
\contentsline {paragraph}{Architecture and implementation details}{1841}{section*.4269}%
\contentsline {paragraph}{Training curriculum}{1841}{section*.4270}%
\contentsline {subsubsection}{Experiments and Ablations}{1842}{section*.4271}%
\contentsline {paragraph}{STC Ablations}{1842}{section*.4272}%
\contentsline {paragraph}{Data Recipe Overview}{1842}{section*.4273}%
\contentsline {paragraph}{Multiple\mbox {-}Choice VQA and Perception}{1842}{section*.4274}%
\contentsline {paragraph}{Open\mbox {-}Ended Video QA}{1842}{section*.4275}%
\contentsline {paragraph}{Audio QA}{1842}{section*.4276}%
\contentsline {paragraph}{Open\mbox {-}Ended Audio--Video QA}{1842}{section*.4277}%
\contentsline {paragraph}{Limitations and future directions}{1844}{section*.4279}%
\contentsline {subsection}{Enrichment 24.9.4: Video\mbox {-}LLaMA 3: Frontier Multimodal Foundation Models}{1845}{section*.4281}%
\contentsline {subsubsection}{Motivation}{1845}{section*.4282}%
\contentsline {paragraph}{A vision\mbox {-}first redesign}{1845}{section*.4283}%
\contentsline {paragraph}{Design objectives}{1845}{section*.4285}%
\contentsline {paragraph}{Mechanisms chosen to meet these goals}{1846}{section*.4286}%
\contentsline {paragraph}{Scope: vision focus in V3}{1846}{section*.4287}%
\contentsline {paragraph}{Anticipated benefits over V2}{1846}{section*.4288}%
\contentsline {subsubsection}{Method}{1847}{section*.4289}%
\contentsline {paragraph}{Pipeline at a glance}{1847}{section*.4290}%
\contentsline {paragraph}{Why a resolution\mbox {-}agnostic encoder}{1847}{section*.4291}%
\contentsline {paragraph}{Any\mbox {-}resolution Vision Tokenization (AVT)}{1847}{section*.4292}%
\contentsline {paragraph}{How 2D\mbox {-}RoPE encodes spatial relations}{1848}{section*.4293}%
\contentsline {paragraph}{Differential Frame Pruner (DiffFP)}{1850}{section*.4294}%
\contentsline {paragraph}{Data representations for multi\mbox {-}image, video, and streaming}{1851}{section*.4296}%
\contentsline {subsubsection}{Architecture \& Implementation Details}{1852}{section*.4298}%
\contentsline {paragraph}{Backbone and projector}{1852}{section*.4299}%
\contentsline {paragraph}{Training paradigm}{1852}{section*.4300}%
\contentsline {paragraph}{Where AVT and DiffFP plug in}{1852}{section*.4301}%
\contentsline {paragraph}{Summary of design choices}{1853}{section*.4302}%
\contentsline {subsubsection}{Experiments and Ablations}{1853}{section*.4304}%
\contentsline {paragraph}{Benchmarks and headline performance}{1853}{section*.4305}%
\contentsline {paragraph}{Effect of AVT and DiffFP}{1854}{section*.4308}%
\contentsline {paragraph}{Comparisons to related systems}{1854}{section*.4309}%
\contentsline {paragraph}{Vision backbone ablation.}{1854}{section*.4310}%
\contentsline {paragraph}{Data curation and mixtures}{1855}{section*.4311}%
\contentsline {subsubsection}{Limitations and Future Work}{1855}{section*.4312}%
\contentsline {paragraph}{Long-context and token budgets.}{1855}{section*.4313}%
\contentsline {paragraph}{Temporal precision and rare events}{1855}{section*.4314}%
\contentsline {paragraph}{Data biases and domain transfer}{1855}{section*.4315}%
\contentsline {paragraph}{Toward \emph {Video-LLaMA4}}{1855}{section*.4316}%
\contentsline {subsection}{Enrichment 24.9.5: Qwen-VL: Versatile Vision--Language Foundation}{1856}{section*.4317}%
\contentsline {subsubsection}{Motivation}{1856}{section*.4318}%
\contentsline {paragraph}{Reading the radar chart (intuition)}{1856}{section*.4320}%
\contentsline {subsubsection}{Method}{1857}{section*.4321}%
\contentsline {paragraph}{Architecture (visual receptor + LLM)}{1857}{section*.4322}%
\contentsline {paragraph}{Input\mbox {--}output interface (tokenization and special tokens)}{1857}{section*.4324}%
\contentsline {paragraph}{Cross\mbox {-}attention compression (derivation and intuition)}{1857}{section*.4325}%
\contentsline {paragraph}{Training pipeline (three stages)}{1858}{section*.4326}%
\contentsline {paragraph}{Why this design}{1858}{section*.4327}%
\contentsline {paragraph}{Data}{1858}{section*.4329}%
\contentsline {paragraph}{Pseudo\mbox {-}code}{1859}{section*.4330}%
\contentsline {subsubsection}{Architecture \& Implementation Details}{1860}{section*.4331}%
\contentsline {paragraph}{Backbone and adapter}{1860}{section*.4332}%
\contentsline {paragraph}{Resolution and sequence length}{1860}{section*.4333}%
\contentsline {paragraph}{Special tokens and grounding format}{1860}{section*.4334}%
\contentsline {subsubsection}{Experiments and Ablations}{1860}{section*.4335}%
\contentsline {paragraph}{Benchmarks and headline performance}{1860}{section*.4336}%
\contentsline {paragraph}{What the ablations test}{1860}{section*.4337}%
\contentsline {paragraph}{How these results compare}{1861}{section*.4338}%
\contentsline {paragraph}{Design choices the ablations support}{1861}{section*.4339}%
\contentsline {paragraph}{Takeaways}{1861}{section*.4340}%
\contentsline {paragraph}{Qualitative capabilities}{1862}{section*.4342}%
\contentsline {subsubsection}{Limitations and Future Work}{1862}{section*.4343}%
\contentsline {paragraph}{Bridge to Qwen2\mbox {-}VL}{1862}{section*.4344}%
\contentsline {subsection}{Enrichment 24.9.6: Qwen2-VL: Dynamic Resolution Vision--Language Modeling}{1863}{section*.4345}%
\contentsline {paragraph}{Motivation}{1863}{section*.4346}%
\contentsline {subsubsection}{Method}{1863}{section*.4348}%
\contentsline {paragraph}{Design overview}{1863}{section*.4349}%
\contentsline {paragraph}{Naive dynamic resolution}{1864}{section*.4350}%
\contentsline {paragraph}{M\mbox {-}RoPE for space--time}{1864}{section*.4351}%
\contentsline {paragraph}{Pseudo\mbox {-}code for dynamic resolution and M\mbox {-}RoPE}{1865}{section*.4352}%
\contentsline {paragraph}{Why M\mbox {-}RoPE instead of 2D absolute encodings}{1865}{section*.4353}%
\contentsline {paragraph}{Unified multimodal serialization}{1866}{section*.4354}%
\contentsline {subsubsection}{Architecture \& Implementation Details}{1866}{section*.4355}%
\contentsline {paragraph}{Model variants}{1866}{section*.4356}%
\contentsline {paragraph}{Implementation notes}{1866}{section*.4358}%
\contentsline {subsubsection}{Experiments and Ablations}{1867}{section*.4361}%
\contentsline {paragraph}{Benchmarks and headline performance}{1867}{section*.4362}%
\contentsline {paragraph}{Video understanding}{1867}{section*.4363}%
\contentsline {paragraph}{Grounding}{1867}{section*.4364}%
\contentsline {paragraph}{Multilingual OCR (internal)}{1867}{section*.4365}%
\contentsline {paragraph}{Why dynamic resolution helps}{1867}{section*.4366}%
\contentsline {paragraph}{Why M\mbox {-}RoPE matters}{1868}{section*.4367}%
\contentsline {paragraph}{Length extrapolation}{1868}{section*.4368}%
\contentsline {paragraph}{Resolution sensitivity}{1868}{section*.4370}%
\contentsline {paragraph}{Scaling behavior and training curriculum}{1868}{section*.4372}%
\contentsline {subsubsection}{Limitations and Future Work}{1869}{section*.4373}%
\contentsline {section}{Enrichment 24.10: Long-Context Modeling}{1870}{section*.4374}%
\contentsline {subsection}{Enrichment 24.10.1: MeMViT: Memory-Augmented Multiscale ViTs}{1871}{section*.4375}%
\contentsline {paragraph}{Motivation}{1871}{section*.4376}%
\contentsline {paragraph}{Preliminaries: ViT and MViT}{1872}{section*.4378}%
\contentsline {paragraph}{Method: Memory Attention and Hierarchical Caching}{1873}{section*.4379}%
\contentsline {paragraph}{Algorithmic sketch (from the paper)}{1875}{section*.4380}%
\contentsline {subsubsection}{Architecture \& Implementation Details}{1876}{section*.4382}%
\contentsline {paragraph}{Backbone and stages}{1876}{section*.4383}%
\contentsline {paragraph}{Data loading and training}{1876}{section*.4384}%
\contentsline {subsubsection}{Experiments and Ablations}{1876}{section*.4385}%
\contentsline {paragraph}{Scaling strategies}{1876}{section*.4386}%
\contentsline {paragraph}{Ablations: how memory is used}{1877}{section*.4388}%
\contentsline {paragraph}{Pipeline vs.\ naive compression}{1877}{section*.4389}%
\contentsline {paragraph}{Generalization across backbones and datasets}{1877}{section*.4391}%
\contentsline {paragraph}{Takeaways from the ablations}{1878}{section*.4392}%
\contentsline {subsubsection}{Limitations and Future Work}{1878}{section*.4393}%
\contentsline {subsection}{Enrichment 24.10.2: LongVLM: Efficient Long-Video Reasoning}{1879}{section*.4394}%
\contentsline {paragraph}{Motivation}{1879}{section*.4395}%
\contentsline {paragraph}{Method}{1879}{section*.4397}%
\contentsline {paragraph}{Algorithmic sketch (token merging within a segment)}{1881}{section*.4399}%
\contentsline {subsubsection}{Architecture \& Implementation Details}{1882}{section*.4400}%
\contentsline {subsubsection}{Experiments and Ablations}{1882}{section*.4401}%
\contentsline {paragraph}{Benchmarks and metrics}{1882}{section*.4402}%
\contentsline {paragraph}{Ablations}{1884}{section*.4406}%
\contentsline {paragraph}{Qualitative analyses}{1884}{section*.4408}%
\contentsline {paragraph}{Limitations and Future Work}{1885}{section*.4411}%
\contentsline {subsection}{Enrichment 24.10.3: LWM: Blockwise RingAttention for Million-Token Contexts}{1887}{section*.4412}%
\contentsline {paragraph}{Motivation}{1887}{section*.4413}%
\contentsline {paragraph}{Method}{1887}{section*.4415}%
\contentsline {paragraph}{Training Curriculum}{1888}{section*.4416}%
\contentsline {subsubsection}{Architecture \& Implementation Details}{1892}{section*.4418}%
\contentsline {subsubsection}{Experiments and Ablations}{1894}{section*.4420}%
\contentsline {paragraph}{Long-context retrieval (needle and multi-needle)}{1894}{section*.4421}%
\contentsline {paragraph}{Language tasks at short context}{1895}{section*.4423}%
\contentsline {paragraph}{LOFT benchmarks (512K)}{1895}{section*.4424}%
\contentsline {paragraph}{Long-video understanding}{1896}{section*.4426}%
\contentsline {paragraph}{Generation}{1897}{section*.4428}%
\contentsline {subsubsection}{Limitations and Future Work}{1897}{section*.4430}%
\contentsline {section}{Enrichment 24.11: Specialized Directions}{1898}{section*.4431}%
\contentsfinish 
